text,label
"Arboviruses are responsible for epidemics and are emerging and re-emerging in sub- Saharan Africa. However, the risk factors for arboviral diseases are poorly described in Kenyan children. Knowledge of risk factors can facilitate earlier diagnosis and better treatment and implementation of effective prevention in children. This study determined risk factors for seropositivity to Yellow fever (YFV), Dengue (DENV), Chikungunya (CHIKV) and West Nile (WNV) viruses among children at two facilities in Teso Sub-County in Western Kenya. In a hospital-based cross-sectional survey, the risk factors for seropositivity to the arboviruses were assessed. Eligible children aged 1 to 12 (n = 656) who visited Alupe Sub County Hospital and KEMRI Alupe Clinic in Teso Sub County were recruited. Socio-demographic, environmental, behavioural and medical information was collected using a questionnaire. Blood drawn from these children was screened for antibodies to YFV, DENV, CHIKV and WNV using Indirect Enzyme-Linked Immunosorbent 
Assays. 
Descriptive 
statistics 
were 
used 
to 
summarise seroprevalence, socio-demographic, clinical and environmental variables. Binomial logistic regression described the relationship between the risk factors and arbovirus seropositivity. Seropositivity to at least one arbovirus was found in 27.7%, with 15.7% being positive for DENV, 9.6% for WNV, 5.6% for CHIKV and 4.4% for YFV. The factors that significantly increased the risk to at least one of the arboviruses were: age 6-9 years (by 18%, p=0.006) compared to those 1-3 years, school attendance (by 66%, p=0.000) compared to none, the primary caregiver being “Other” (by 17%, . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint p=0.026) and not the parent, the use of Olyset (by 7%, p=0.039), or an unknown mosquito net (by 26%, p=0.020) compared to Permanet. The risk of yellow fever seropositivity was increased where vegetation was close to the house (by 5%, p=0.042) compared to where vegetation was far. The risk was decreased by the use of an unknown bed net (by 4%, p=0.046) compared to Permanet and having a past history of rash (by 6%, p=0.018). For Dengue Fever, females were at an increased risk (by 8%, p=0.002) compared to males and having water bodies near the house (7%, p=0.030). The risk of chikungunya was increased by school attendance (by 25%, p=0.021) compared to not, the use of mosquito repellents (by 10%, p=0.006) compared to no interventions and having had a rash in the past (by 6%, p=0.043). The risk was decreased by roofing with iron sheets (by 3%, p=0.048) compared to grass-thatching. WNV seropositivity risk was higher in those aged 3-6 years (by 8%, p=0.004) and 6-9 years (by 15%, p=0.004) than in those aged 1-3 years. It was increased in those attending school (by 37%, p=0.006) compared to those not, and those using Olyset (by 11%, p=0.000) or an unknown bed net (by 30%, p=0.001) compared to Permanet. The risk was lower by between 25% and 33% (p<0.003) in those in pre-school, in lower and upper primary compared to those not in school. These factors are amenable to interventions that can be implemented to prevent and reduce arbovirus infections in children in endemic areas in Kenya. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint Author Summary Yellow fever, Dengue, Chikungunya and West Nile are viruses (Arboviruses) transmitted to humans by mosquitoes. These infections are common in Sub-Saharan Africa and often affect children. However, the risk factors associated with arboviral infections are not well described, and yet, knowledge of these predisposing factors in children is essential for early diagnosis, correct treatment, and prevention. We carried out this study to determine the factors associated with these infections. We recruited 656 children aged between 1-12 years who sought health services at Alupe Sub- County Hospital and KEMRI Alupe Clinic in Teso Sub-County, Western Kenya. We used a structured questionnaire to collect data on sociodemographic, behavioural, environmental, and clinical factors. We then drew blood from these children and screened it for the four arboviruses. Out of 656 participants, 182 (27.7%) were seropositive for at least one of the four arboviruses, 29 (4.4%) for Yellow Fever, 102 (15.7%) for Dengue, 36 (5.6%) for Chikungunya and 62 (9.6%) for West Nile virus. We established that gender, age, school attendance, the primary caregiver, design of the house, type of mosquito nets used, skin rashes and other mosquito control methods all influence the risk of seropositivity. These behavioural, environmental, sociodemographic and clinical factors that influence arbovirus seropositivity are amenable to interventions that can be implemented within the community to reduce the risk and prevalence of arboviruses in children in endemic areas in Kenya. Introduction Arthropod-borne viruses (arboviruses) such as Chikungunya (CHIKV), Yellow Fever (YFV), Dengue Virus (DENV), and West Nile Virus (WNV) are transmitted by Aedes, . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint Culex and Anopheles mosquitoes (1-9). They affect both adults and children, causing widespread morbidity. They are responsible for epidemics and are emerging and re- emerging in many parts of sub-Saharan Africa. Several outbreaks have been reported in Kenya (4, 6, 7, 10-12). Due to their immature immune system, children in endemic areas are highly susceptible to arbovirus infections (13). Infections present with non- specific signs and symptoms (13), including fever, jaundice, swollen lymph glands, neuro-invasive disease (8, 14), joint inflammation (15) and rashes (16). In sub–Saharan Africa, arbovirus infections are often undiagnosed and unreported, with febrile illnesses often assumed to be malaria, typhoid or other bacterial infections (8, 17). Due to a lack of diagnostics in some places, febrile illnesses and malaria- negative fevers are often treated presumptively with antimalarials and antibiotics.  The risk factors for arboviral disease are not well described. Therefore doctors in sub– Saharan Africa lack the index of suspicion necessary to make a presumptive diagnosis of arboviral infection (18). Despite evidence of these arbovirus outbreaks reported in Kenya and the morbidity associated with them, few surveys have been done to document the burden or magnitude of infections from these viruses (19), and little is known about the epidemiologic characteristics of arbovirus prevalence and the associated risk factors in Kenyan children (20, 21). This paper describes sociodemographic, clinical, behavioural and environmental factors associated with seropositivity to Yellow Fever, Dengue, Chikungunya and West Nile viruses among children in Teso South Sub County, Western Kenya. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint Materials and methods In this hospital-based cross-sectional study conducted between August 2010 and February 2011, we assessed the risk factors for seropositivity to Yellow Fever, Dengue, Chikungunya and West Nile viruses amongst children in Teso Sub County, Western Kenya. We recruited children aged 1 to 12 years (n = 656) from those who visited Alupe Sub County Hospital and KEMRI Alupe Clinic for health services. Socio-demographic, environmental and medical information was collected using a structured questionnaire. The participant’s clinical records and immunisation card were reviewed where available. Approximately 2.5 ml of venous blood was drawn from these children and tested for antibodies to YFV, WNV, DENV and CHIKV. An indirect Enzyme-Linked Immunosorbent Assay (Indirect ELISA) was performed to detect virus- specific IgA/IgM/IgG serocomplex antibodies using an in-house kit method described by Igarashi (22, 23) with a few modifications to suit the local laboratory settings (24). The optical density (OD) was measured with an ELISA plate reader at 492 nm within 20 minutes after adding the stop solution. OD specific for the virus was calculated as follows: (Mean OD of virus-coated wells) – (Mean OD of PBS-F coated wells). If the particular OD reading was more than 1.0, that serum was regarded as positive. Where serum samples were of insufficient volume, YFV ELISA was performed first, followed by DENV2, CHIKV, WNV, DENV1 and DENV3 in order of preference. The primary outcome was YFV, DENV, CHIKV and WNV IgA/IgM/IgG seropositivity. Statistical Methods All the data collected was saved in an excel database on a secure computer, cleaned and coded. Overall seropositivity rates, as well as virus-specific seropositivity rates, . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint were calculated. Simple descriptive statistics were used to summarise socio- demographic, clinical and environmental exposure variables.  Sociodemographic, environmental and clinical factors were evaluated for correlation with arbovirus seropositivity. The predictor variables included age, sex, schooling, vaccination status, caregiver characteristics and behavioural, clinical and environmental characteristics. Before fitting our model, we hypothesised that the various sociodemographic, clinical, behavioural and environmental factors influence exposure to arboviruses, hence seropositivity. Before fitting our model to predict arbovirus seropositivity, Spearman’s correlation was used to examine the relationship between predictor and outcome variables. The Poisson regression model was used to calculate incidence rate ratios (IRR) and associated 95% confidence intervals (CIs). Only those variables with a p < 0.05 at univariate analysis were included in the full model. All statistical analyses were conducted with Stata/SE 17.0 for Mac (Stata Corp, College Station, Texas, USA). The level of significance was set at p ≤ 0.05. Ethical Statement This study was approved by the ethical review committee of Kenyatta National Hospital (Approval number P108/03/2010). Informed written consent was obtained from caregivers, and verbal assent was obtained from children above seven years of age before participating in the study. The study was conducted in line with the requirements of the Helsinki Declaration of 1975, as revised in 2000. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint Results The study recruited 656 participants, 316 (48.2%) males and 340 (51.8%) females. All were successfully screened for YFV and DENV2. Due to insufficient sample volume, varying numbers of samples were screened for WNV (n=649), CHIKV (n=649), DENV1 (n=368) and DENV3 (n=203). We have previously reported these participant characteristics (13). Risk of seropositivity to any arbovirus Univariate and multivariate analyses of the risk of seropositivity to any arbovirus are presented in Table 1 and Table 2, respectively. Of the study subjects, 182 (27.7%) were seropositive for at least one of the four arboviruses (13). In univariate analysis, seropositivity to any arbovirus was influenced by gender, age, school, caregiver, bed net type, eaves and rash. Being female was associated with a 7% increase in seropositivity to any arbovirus (IRR 1.07, 95% CI 1.01-1.13, p=0.016) compared to being male; this difference was not observed in the multivariate model.  The risk was also significantly increased in those aged 6-9 years and 9-12 years (p<0.05) compared to those aged 1-3 years, and in the multivariate model, only those aged 6-9 years had an increase in seropositivity when compared to those aged 1-3 years (IRR 1.18, 95% CI 1.05-1.33, p = 0.006).  Being in school was associated with an 8% increase in overall arbovirus seropositivity (IRR 1.08, 95%CI 1.02-1.13, p=0.007) when compared with not being in school. In the multivariate model, the risk was higher by 66% (IRR 1.66, 95% CI 1.29-2.13, p=0.000). The risk increased by 16% in children who were in upper primary school (IRR 1.16, 95% CI 1.07-1.27, p=0.001) when compared to those, not in school. However, in the multivariate model, all children in any class had at least . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint a 37% decrease in the risk of being seropositive for any arbovirus (p=0.000) compared to those not in school. The risk of being seropositive was also elevated when the caregiver was “Other” and not the parent (IRR 1.29, 95% CI 1.14-1.45, p=0.000); a significant increase was also seen in the multivariate model. The risk of being seropositive was increased if the type of bed net used was either Olyset (IRR 1.10, 95%CI 1.04-1.18, p=0.002) or unknown (IRR 1.29, 95% CI 1.13-1.48, p=0.000) and not Permanet. The multivariate model also shows these increased risks (p<0.05). Risk of seropositivity to Yellow Fever The risk of yellow fever seropositivity decreased by 4% (IRR 0.96, 95% CI 0.92-0.99, p=0.021) in those 9-12 years old, compared to those aged 1-3 years, in the univariate model. In the multivariate model, there was no significant difference. Those using mosquito nets of an unknown brand had a 5% decrease in risk of yellow fever seropositivity (IRR 0.95, 95% CI 0.92-0.97, p=0.000); in the multivariate model, it was 4% (p=0.046). In the univariate model, the risk for YFV seropositivity decreased by 3% (IRR 0.97, 95% CI 0.94-0.1.00, p=0.040) in those who lived close to water bodies when compared to those who did not. The multivariate model showed a 7% decrease (p=0.030). In the multivariate model, vegetation near the house was associated with a 5% increase in risk (IRR 1.05, 95% CI 1.00-1.09, p=0.042), though the univariate model did not show this. These risks are detailed in Table 1 and Table 2, respectively. Risk of seropositivity to any Dengue Virus In univariate analysis, the risk of being seropositive to any of the three DENV viruses increased by 8% in females (IRR 1.08, 95% CI 1.03-1.13, p=0.001) when compared to males; the increase was also 8% (IRR 1.08, 95% CI 1.03-1.13, p=0.002) in the . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint multivariate model. It was 10% higher in children aged 6-9 years (IRR 1.10, 95% CI 1.02-1.19, p=0.015) when compared to children aged 1-3 years; it was insignificant in the multivariate model. The risk of being seropositive to any dengue virus decreased by 8% (IRR 0.92, 95% CI 0.86-0.98, p=0.011) in those whose KEPI Vaccination cards were fully filled when compared with those whose cards were not; this difference was not present in the multivariate model. In univariate analysis, the risk was significantly higher in those who lived in houses roofed with iron sheets when compared to those who lived in grass-thatched houses (IRR 1.05, 95% CI 1.00-1.11, p=0.039), but this was not reflected in the multivariate model. The risk was reduced in the univariate model by 10% if the house had open eaves (IRR 0.90, 95% CI 0.84-0.95, p=0.001); this decrease was absent in the multivariate model. The risk was 5% lower for those who lived next to water bodies (rivers, swamps, ponds, canals, or lakes) when compared to those who lived far from water bodies (IRR 0.95, 95% CI 0.91-1.00, p=0.032). In the multivariate model, the risk for those who lived near water bodies was reduced by 7% (IRR 0.93, 95% CI 0.87-0.99, p=0.030). These risks are also detailed in Table 1 and Table 2, respectively. Risk of seropositivity to Chikungunya Virus For CHIKV, the risk of seropositivity was elevated in those who attended school (IRR 1.25, 95% CI 1.03-1.50, p=0.021) in the multivariate model. However, at the granular level, the risk decreased by at least 16% in those in any class in primary school in the multivariate model (p<0.05) when compared to those, not in school; this was not seen in the univariate model. In the univariate model, the risk decreased by 6% in those who had received all the KEPI vaccines (IRR 0.94, 95% CI 0.89-1.00, p=0.041). It . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint increased by 11% for those who used mosquito repellents (IRR 1.11, 95% CI 1.03- 1.19, p=0.007) when compared to those who did not use any mosquito control method; the increase was 10% in the multivariate model (IRR 1.10, 95% CI 1.03-1.17, p=0.006). The risk was 4% lower in those who complained of feeling sick in the univariate model (IRR 0.96, 95% CI 0.93-0.99, p=0.008); this was not seen in the multivariate model. In the multivariate model, the risk was increased by 7% in those with a past history of rash compared to those who did not (IRR 1.06, 95% CI 1.00- 1.13, p=0.043). Further, the risk of being seropositive for CHIKV in the multivariate analysis was 3% lower in those who lived in houses roofed with iron sheets when compared to those who lived in grass thatched houses (IRR 0.97, 95% CI 0.94-1.00, p=0.048); Univariate and multivariate analysis of the risk of seropositivity to CHIKV are presented in Table 1 and Table 2 respectively. Risk of seropositivity to West Nile Virus For WNV, the risk of seropositivity increased by at least 8% in any age category older than the 1-3 years age category (p<0.05).  In the multivariate model, only the age groups 3-6 years and 6-9 years had significantly increased risks. It decreased by 7% for those who lived in the village compared to those who lived in town (IRR 0.93, 95% CI 0.87-0.99, p=0.018); this was not seen in the multivariate model. The risk increased by 5% (IRR 1.05, 95% CI 1.01-1.10, p=0.015) for those attending school compared to those not, but this risk was 37% higher in the multivariate model (IRR 1.37, 95%CI 1.09-1.72, p=0.006). The risk was 10% higher in children in upper primary school (IRR 1.10, 95%CI 1.02-1.19, p=0.017) when compared to those not in school, but in the multivariate model, the risk was at least 25% lower for children in any class (p<0.05). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint For those whose caregiver was “Other”, the risk increased by 17% (IRR 1.17, 95% CI 1.02-1.34, p=0.024) compared to those whose caregiver was the parent. There was no significant difference in the multivariate model. The risk increased by at least 11% for those who slept under Olyset mosquito nets, 13% for Supanet and 32% if the bed net was an unknown brand compared to Permanet (p<0.05). In the multivariate model, the risk was at least 11% higher for those sleeping under Olyset and 30% if the brand of the net was unknown (p<0.05). In the univariate model, the risk of WNV seropositivity increased by 9% in those who had a rash when compared to those who did not have a rash (IRR 1.09, 95% CI 1.04-1.14, p=0.000) and by 9% in those who had had a past rash when compared to those who had not had one (IRR 1.09, 95% CI 1.02-1.17 p=0.011). This was not reflected in the multivariate model. The risk was significantly decreased in those who felt sick compared to those who did not (IRR 0.94, 95% CI 0.91-0.98, p=0.004). These significant differences were not seen in the multivariate model. In the univariate model, living under a mixed iron sheets and tiles roof increased the risk by 18% (IRR 1.18, 95% CI 1.06-1.32, p=0.004) compared to living under a thatched grass roof. This was not the case in multivariate analysis. In the univariate model, living near water bodies and having vegetation close to the house significantly increased the risk by 9% and 5%, respectively (p<0.05). No significant differences were detected in the multivariate model. These risks are detailed in Table 1 (Univariate analysis) and Table 2 (Multivariate analysis). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint Table 1: Univariate Poisson regression analysis of risk factors for Arbovirus, Yellow, Dengue, Chikungunya and West Nile Virus seropositivity Participant characteristics 
Total pop 
Any arbovirus +ve (n=182) 
YFV +ve (n=29) 
Any DENV +ve (n=102) 
CHIKV +ve (n=36) 
WNV +ve (n=62) n (%) 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value Sociodemographic Factors Gender Male 
316 (48.2) 
Ref Female 
340 (51.8) 
1.07 
1.01-1.13 
0.016 
1.02 
0.99-1.05 
0.255 
1.08 
1.03-1.13 
0.001 
1.02 
0.99-1.05 
0.253 
1.00 
0.96-1.05 
0.829 Age group (Years) 1 - 3 
228 (36.8) 
Ref >3 - 6 
211 (34.0) 
1.06 
0.99-1.13 
0.090 
0.99 
0.95-1.03 
0.488 
0.97 
0.92-1.03 
0.306 
1.02 
0.98-1.06 
0.396 
1.08 
1.03-1.13 
0.001 >6 - 9 
99 (16.0) 
1.13 
1.05-1.23 
0.002 
1.00 
0.95-1.06 
0.900 
1.10 
1.02-1.19 
0.015 
0.99 
0.95-1.03 
0.721 
1.11 
1.04-1.19 
0.001 >9 - 12 
82 (13.2) 
1.17 
1.07-1.27 
0.001 
0.96 
0.92-0.99 
0.021 
1.07 
0.99-1.17 
0.095 
1.03 
0.97-1.10 
0.264 
1.10 
1.03-1.18 
0.006 Residence Town 
115 (17.8) 
Ref Village 
532 (82.2) 
0.99 
0.93-1.07 
0.850 
1.01 
0.98-1.05 
0.531 
1.02 
0.96-1.09 
0.501 
1.01 
0.97-1.05 
0.545 
0.93 
0.87-0.99 
0.018 School attendance Not in school 
347 (52.9) 
Ref In school 
309 (47.1) 
1.08 
1.02-1.13 
0.007 
0.98 
0.95-1.01 
0.157 
1.05 
1.00-1.10 
0.069 
1.02 
0.99-1.05 
0.269 
1.05 
1.01-1.10 
0.015 Class None 
346 (52.8) 
Ref Pre- school 
136 (20.8) 
1.04 
0.97-1.12 
0.237 
0.98 
0.94-1.01 
0.180 
1.03 
0.96-1.09 
0.427 
1.01 
0.96-1.05 
0.800 
1.05 
0.99-1.10 
0.108 Participant characteristics 
Total pop 
Any arbovirus +ve (n=182) 
YFV +ve (n=29) 
Any DENV +ve (n=102) 
CHIKV +ve (n=36) 
WNV +ve (n=62) n (%) 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value Lower primary 
97 (14.8) 
1.04 
0.97-1.13 
0.282 
0.99 
0.94-1.03 
0.564 
1.05 
0.97-1.13 
0.220 
1.01 
0.96-1.06 
0.772 
1.03 
0.97-1.09 
0.411 Upper primary 
76 (11.6) 
1.16 
1.07-1.27 
0.001 
0.97 
0.93-1.01 
0.198 
1.07 
0.98-1.16 
0.114 
1.06 
0.99-1.13 
0.094 
1.10 
1.02-1.19 
0.017 KEPI Card Completed No 
132 (20.3) 
Ref Yes 
519 (79.7) 
0.93 
0.87-1.00 
0.091 
1.03 
1.00-1.06 
0.091 
0.92 
0.86-0.98 
0.011 
0.97 
0.92-1.02 
0.196 
1.00 
0.95-1.05 
0.982 KEPI Vaccines completed No 
92 (14.3) 
Ref Yes 
551 (85.7) 
0.98 
0.91-1.06 
0.624 
1.03 
0.99-1.06 
0.128 
1.01 
0.95-1.08 
0.744 
0.94 
0.89-1.00 
0.041 
1.00 
0.94-1.06 
0.918 YFV vaccinated No 
644 (98.3) 
Ref Yes 
11 (1.7) 
0.85 
0.73-1.00 
0.047 
0.96 
0.94-0.97 
0.000 
0.86 
0.84-0.88 
0.000 
0.95 
0.93-0.96 
0.000 
1.00 
0.85-1.17 
0.956 Primary Caregiver Parent 
590 (90.1) 
Ref Grandparent 
39 (6.0) 
1.08 
0.97-1.21 
0.177 
1.03 
0.95-1.12 
0.420 
1.03 
0.92-1.14 
0.627 
1.03 
0.95-1.12 
0.465 
1.02 
0.93-1.13 
0.627 Other  
26 (4.0) 
1.29 
1.14-1.45 
0.000 
1.00 
0.93-1.07 
0.920 
1.10 
0.96-1.27 
0.154 
1.10 
0.97-1.24 
0.126 
1.17 
1.02-1.34 
0.024 Mosquito repellents use None 
512 (78.2) 
Ref Repellents 
73 (11.2) 
1.06 
0.97-1.16 
0.171 
0.98 
0.94-1.02 
0.363 
0.98 
0.91-1.06 
0.651 
1.11 
1.03-1.19 
0.007 
1.01 
0.94-1.07 
0.865 Sprays 
70 (10.7) 
1.04 
0.95-1.14 
0.383 
1.00 
0.95-1.05 
0.877 
1.03 
0.95-1.11 
0.545 
1.02 
0.96-1.07 
0.589 
1.03 
0.96-1.11 
0.364 Sleep under a bed net Participant characteristics 
Total pop 
Any arbovirus +ve (n=182) 
YFV +ve (n=29) 
Any DENV +ve (n=102) 
CHIKV +ve (n=36) 
WNV +ve (n=62) n (%) 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value No  
38 (5.8) 
Ref Yes 
618 (94.2) 
0.97 
0.86-1.09 
0.595 
1.02 
0.97-1.07 
0.490 
1.05 
0.96-1.15 
0.300 
0.98 
0.90-1.06 
0.573 
0.90 
0.81-1.00 
0.056 Type of bed-net Permanet 
290 (44.2) 
Ref Unknown 
23 (3.5) 
1.29 
1.13-1.48 
0.000 
0.95 
0.92-0.97 
0.000 
1.01 
0.88-1.16 
0.850 
1.13 
0.99-1.29 
0.007 
1.32 
1.14-1.52 
0.000 Olyset 
211 (32.2) 
1.10 
1.04-1.18 
0.002 
0.98 
0.95-1.02 
0.279 
1.03 
0.97-1.09 
0.318 
1.03 
0.99-1.08 
0.109 
1.11 
1.06-1.16 
0.000 Supanet 
67 (10.2) 
1.05 
0.96-1.16 
0.270 
0.99 
0.94-1.05 
0.730 
0.94 
0.88-1.02 
0.120 
0.99 
0.95-1.04 
0.744 
1.13 
1.04-1.22 
0.003 Mixed 
51 (7.8) 
1.04 
0.94-1.15 
0.458 
0.98 
0.93-1.04 
0.599 
0.97 
0.88-1.06 
0.455 
1.00 
0.95-1.06 
0.966 
1.05 
0.98-1.13 
0.151 Environmental Factors Type of Roof Grass thatch 
228 (34.8) 
Ref Iron sheets 
387 (59.0) 
1.04 
0.98-1.10 
0.171 
0.99 
0.96-1.03 
0.692 
1.05 
1.00-1.11 
0.039 
0.97 
0.93-1.00 
0.066 
0.98 
0.94-1.03 
0.470 Iron sheets/tiles 
41 (6.25) 
1.14 
1.02-1.28 
0.023 
1.00 
0.93-1.07 
0.988 
1.05 
0.94-1.16 
0.395 
0.97 
0.90-1.04 
0.423 
1.18 
1.06-1.32 
0.004 Presence of eaves No 
611 (93.1) 
Ref Yes 
45 (6.9) 
0.95 
0.86-1.06 
0.369 
0.98 
0.93-1.02 
0.321 
0.90 
0.84-0.95 
0.001 
1.04 
0.96-1.13 
0.366 
1.00 
0.92-1.08 
0.953 Water bodies near house No 
385 (58.7) 
Ref Yes 
271 (41.3) 
1.03 
0.97-1.09 
0.305 
0.97 
0.94-1.00 
0.040 
0.95 
0.91-1.00 
0.032 
1.03 
1.00-1.07 
0.092 
1.09 
1.04-1.13 
0.000 Participant characteristics 
Total pop 
Any arbovirus +ve (n=182) 
YFV +ve (n=29) 
Any DENV +ve (n=102) 
CHIKV +ve (n=36) 
WNV +ve (n=62) n (%) 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value Vegetation near house No 
411 (62.7) 
Ref Yes 
245 (37.4) 
1.01 
0.96-1.07 
0.716 
1.02 
0.99-1.05 
0.236 
0.97 
0.92-1.02 
0.215 
1.01 
0.97-1.05 
0.584 
1.05 
1.00-1.10 
0.037 Clinical Factors Feeling sick No 
379 (57.8) 
Ref Yes 
277 (42.2) 
0.94 
0.89-1.00 
0.034 
1.02 
0.99-1.05 
0.302 
1.00 
0.95-1.05 
0.912 
0.96 
0.93-0.99 
0.008 
0.94 
0.91-0.98 
0.004 Rash No 
451 (68.8) 
Ref Yes 
205 (31.3) 
1.06 
1.00-1.13 
0.039 
0.98 
0.95-1.01 
0.167 
1.00 
0.95-1.05 
0.965 
1.01 
0.97-1.05 
0.548 
1.09 
1.04-1.14 
0.000 Past history of rash No 
563 (85.8) 
Ref Yes 
93 (14.2) 
1.05 
0.97-1.14 
0.205 
0.95 
0.93-0.97 
0.000 
0.95 
0.89-1.01 
0.113 
1.06 
1.00-1.12 
0.062 
1.09 
1.02-1.17 
0.011 Sore throat No 
611 (93.1) 
Ref Yes 
45 (6.86) 
0.95 
0.86-1.06 
0.369 
0.98 
0.93-1.02 
0.321 
0.92 
0.85-0.99 
0.019 
0.99 
0.93-1.05 
0.714 
1.06 
0.96-1.16 
0.232 Table 2: Multivariate Poisson regression analysis of Arbovirus, Yellow, Dengue, Chikungunya and West Nile Virus seropositivity Participant characteristics 
Total pop 
Any arbovirus +ve (n=182) 
YFV +ve (n=29) 
Any DENV +ve (n=102) 
CHIKV +ve (n=36) 
WNV +ve (n=62) n (%) 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value 
IRR 
95% CI 
p value Sociodemographic Factors Gender Male 
316 (48.2) 
Ref Female 
340 (51.8) 
1.05 
0.99-1.11 
0.082 
1.03 
0.99-1.07 
0.124 
1.08 
1.03-1.13 
0.002 
1.01 
0.98-1.04 
0.642 
0.99 
0.95-1.03 
0.573 Age group (Years) 1 - 3 
228 (36.8) 
Ref >3 - 6 
211 (34.0) 
1.06 
0.98-1.15 
0.141 
1.01 
0.96-1.06 
0.664 
0.97 
0.91-1.04 
0.386 
1.01 
0.97-1.05 
0.659 
1.08 
1.03-1.14 
0.004 >6 - 9 
99 (16.0) 
1.18 
1.05-1.33 
0.006 
1.03 
0.96-1.11 
0.391 
1.10 
0.99-1.22 
0.078 
1.01 
0.94-1.05 
0.650 
1.15 
1.03-1.27 
0.004 >9 - 12 
82 (13.2) 
1.13 
0.97-1.32 
0.117 
0.93 
0.85-1.02 
0.113 
1.06 
0.91-1.23 
0.440 
1.00 
0.91-1.09 
0.513 
1.08 
0.97-1.20 
0.182 Residence Town 
115 (17.8) 
Ref Village 
532 (82.2) 
1.04 
0.96-1.12 
0.350 
0.99 
0.95-1.03 
0.676 
1.04 
0.97-1.12 
0.285 
1.03 
1.00-1.06 
0.092 
0.96 
0.90-1.02 
0.202 School attendance Not in school 
347 (52.9) 
Ref In school 
309 (47.1) 
1.66 
1.29-2.13 
0.000 
1.07 
0.96-1.21 
0.232 
1.13 
0.88-1.45 
0.323 
1.25 
1.03-1.50 
0.021 
1.37 
1.09-1.72 
0.006 Class (n=655) None 
346 (52.8) 
Ref Pre- school 
136 (20.8) 
0.58 
0.44-0.75 
0.000 
0.91 
0.80-1.03 
0.142 
0.90 
0.69-1.17 
0.432 
0.79 
0.66-0.96 
0.015 
0.67 
0.53-0.84 
0.000 Lower primary 
97 (14.8) 
0.55 
0.43-0.71 
0.000 
0.95 
0.84-1.06 
0.349 
0.86 
0.68-1.10 
0.226 
0.81 
0.67-0.98 
0.028 
0.70 
0.53-0.89 
0.003 Upper primary 
76 (11.6) 
0.63 
0.52-0.76 
0.000 
0.10 
0.93-1.10 
0.864 
0.91 
0.75-1.10 
0.331 
0.84 
0.74-0.97 
0.016 
0.75 
0.63-0.89 
0.001 KEPI card completed No 
132 (20.3) 
Ref Yes 
519 (79.7) 
0.98 
0.89-1.09 
0.748 
1.04 
0.99-1.10 
0.116 
0.93 
0.84-1.02 
0.119 
1.00 
0.95-1.07 
0.875 
1.06 
0.98-1.15 
0.123 KEPI Vaccines completed No 
92 (14.3) 
Ref Yes 
551 (85.7) 
1.05 
0.94-1.17 
0.418 
0.99 
0.94-1.05 
0.731 
1.09 
0.98-1.21 
0.112 
0.99 
0.92-1.06 
0.731 
0.97 
0.90-1.04 
0.430 YFV vaccinated No 
644 (98.3) 
Ref Yes 
11 (1.7) 
0.81 
0.69-0.96 
0.012 
0.96 
0.92-1.01 
0.104 
0.84 
0.77-0.90 
0.000 
0.93 
0.88-0.99 
0.017 
0.95 
0.82-1.10 
0.483 Primary Caregiver Parent 
590 (90.1) 
Ref Grandparent 
39 (6.0) 
1.02 
0.89-1.16 
0.804 
1.06 
0.96-1.17 
0.246 
1.01 
0.90-1.13 
0.875 
0.98 
0.93-1.04 
0.585 
0.97 
0.88-1.07 
0.501 Other  
26 (4.0) 
1.17 
1.02-1.35 
0.026 
1.02 
0.94-1.09 
0.660 
1.08 
0.94-1.24 
0.296 
1.00 
0.92-1.08 
0.981 
1.10 
0.96-1.26 
0.184 Mosquito repellents use None 
512 (78.2) 
Ref Repellants 
73 (11.2) 
1.06 
0.96-1.17 
0.258 
1.01 
0.96-1.07 
0.707 
1.04 
0.95-1.13 
0.400 
1.10 
1.03-1.17 
0.006 
0.96 
0.89-1.05 
0.403 Sprays 
70 (10.7) 
1.01 
0.92-1.12 
0.787 
1.00 
0.94-1.07 
0.898 
1.02 
0.93-1.11 
0.679 
1.00 
0.95-1.06 
0.903 
0.99 
0.92-1.06 
0.705 Sleep under a bed net No  
38 (5.8) 
Ref Yes 
618 (94.2) 
0.99 
0.86-1.14 
0.851 
0.97 
0.90-1.05 
0.491 
0.93 
0.81-1.08 
0.355 
1.03 
0.99-1.07 
0.107 
0.97 
0.86-1.10 
0.631 Type of bed-net Permanet   
290 (44.2) 
Ref Unknown 
23 (3.5) 
1.26 
1.09-1.46 
0.020 
0.96 
0.93-1.00 
0.046 
0.98 
0.84-1.15 
0.833 
1.12 
0.98-1.29 
0.106 
1.30 
1.12-1.52 
0.001 Olyset 
211 (32.2) 
1.07 
1.00-1.15 
0.039 
0.99 
0.95-1.03 
0.505 
1.01 
0.95-1.07 
0.780 
1.02 
0.98-1.05 
0.398 
1.11 
1.05-1.16 
0.000 Supanet 
67 (10.2) 
1.02 
0.91-1.14 
0.728 
1.00 
0.93-1.07 
0.954 
0.96 
0.87-1.06 
0.419 
0.97 
0.93-1.02 
0.230 
1.08 
1.00-1.16 
0.062 Mixed 
51 (7.8) 
1.00 
0.90-1.11 
0.977 
1.00 
0.94-1.07 
0.956 
0.93 
0.86-1.01 
0.100 
0.99 
0.94-1.04 
0.685 
1.06 
0.99-1.14 
0.098 None 
14 (2.1) 
1.11 
0.86-1.42 
0.437 
0.96 
0.88-1.05 
0.387 
0.87 
0.74-1.02  
0.078 
1.19 
0.97-1.46 
0.097 
1.10 
0.89-1.35 
0.387 Environmental Factors Type of Roof Grass thatch 
228 (34.8) 
Ref Iron sheets 
387 (59.0) 
1.02 
0.96-1.08 
0.503 
0.99 
0.96-1.03 
0.747 
1.03 
0.98-1.08 
0.300 
0.97 
0.94-1.00 
0.048 
0.97 
0.93-1.02 
0.246 Iron sheets/tiles 
41 (6.25) 
1.06 
0.93-1.20 
0.385 
1.02 
0.94-1.10 
0.655 
1.02 
0.91-1.15 
0.687 
0.95 
0.89-1.01 
0.084 
1.08 
0.96-1.22 
0.211 Presence of eaves No 
611 (93.1) 
Ref Yes 
45 (6.9) 
0.95 
0.84-1.07 
0.391 
0.99 
0.92-1.06 
0.742 
0.94 
0.87-1.03 
0.197 
0.98 
0.91-1.05 
0.584 
0.96 
0.87-1.06 
0.457 Water bodies near house No 
385 (58.7) 
Ref Yes 
271 (41.3) 
0.94 
0.87-1.02 
0.120 
0.97 
0.92-1.02 
0.202 
0.93 
0.87-0.99 
0.030 
0.99 
0.95-1.03 
0.712 
0.97 
0.92-1.03 
0.407 Vegetation near house No 
411 (62.7) 
Ref Yes 
245 (37.4) 
0.99 
0.93-1.06 
0.728 
1.05 
1.00-1.09 
0.042 
1.00 
0.94-1.06 
0.905 
1.00 
0.96-1.04 
0.828 
1.00 
0.95-1.05 
0.979 Water containers Covered No 
550 (84.0) 
Ref Yes 
105 (16.0) 
1.11 
0.89-1.39 
0.364 
0.92 
0.76-1.12 
0.428 
1.03 
0.83-1.27 
0.807 
1.04 
1.00-1.08 
0.067 
1.07 
0.99-1.16 
0.089 Clinical Factors Feeling sick No 
379 (57.8) 
Ref Yes 
277 (42.2) 
0.98 
0.93-1.05 
0.603 
1.00 
0.96-1.04 
0.884 
0.96 
0.93-1.00 
0.043 
0.97 
0.93-1.00 
0.068 
1.02 
0.98-1.06 
0.332 Presence of Rash No 
451 (68.8) 
Ref Yes 
205 (31.3) 
1.01 
0.94-1.09 
0.790 
1.00 
0.95-1.06 
0.876 
0.97 
0.93-1.00 
0.047 
0.97 
0.93-1.00 
0.077 
1.04 
0.98-1.11 
0.183 Past history of rash No 
563 (85.8) 
Ref Yes 
93 (14.2) 
1.04 
0.94-1.16 
0.414 
0.94 
0.90-0.99 
0.018 
1.07 
1.01-1.14 
0.018 
1.06 
1.00-1.13 
0.043 
1.06 
0.97-1.81 
0.436 Throat Infection No 
611 (93.1) 
Ref Yes 
45 (6.9) 
1.06 
0.96-1.15 
0.172 
0.97 
0.93-1.00 
0.065 
1.02 
0.97-1.07 
0.465 
1.02 
0.97-1.07 
0.384 
1.05 
0.98-1.35 
0.336 Discussion In this study, we found that the risk of being seropositive to selected arboviruses varied with sociodemographic, clinical and environmental factors. Being female was seen to be a risk factor for DENV in both univariate and multivariate analyses. In Sub-Saharan Africa, females are more commonly involved in activities that increase their exposure to daytime-biting mosquito bites, leading to a higher risk of seropositivity to DENV. Other studies have reported similar findings with a higher risk in female participants (21, 25, 26). We noted that older children were at an increased risk for any arbovirus seropositivity in the univariate model. Still, in the multivariate model, the increased risk was seen in those aged 3-6 and 6-9 years and was attributable to WNV. This finding was supported by the increased risk in those attending school compared to those not attending. Increasing age has been associated with arbovirus exposure in various studies (27). Our thesis is that at these age groups, children begin going to school, they begin to play outside much earlier and stay late, leading to increased exposure to aedes mosquitos which bite mainly during the daytime (28) but also at dusk in East Africa (29), and culex mosquitoes which are more active at dawn and dusk (30). When we looked at preschool, lower primary and upper primary groups, we found that the upper primary group had a significantly increased risk of arbovirus seropositivity attributable to WNV. However, in the multivariate model, each group had a significantly decreased risk of seropositivity to an arbovirus, mainly attributable to CHIKV and WNV. This finding has baffled us, and we posit that there is an additional variable that may explain this finding. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint Where the primary caregiver was not the parent or the grandparent, the risk of being seropositivity for any arbovirus was higher. In the univariate model, this was attributable to WNV; in the multivariate, the risk was also significant.  Parents and grandparents tend to give more protection and care to the children than other relatives that are further removed, reducing exposure to mosquito vectors. For CHIKV, the risk of being seropositive was elevated if mosquito repellents were used. The CHIKV vector, Aedes spp. mainly bites during the day (28). Mosquito control measures such as nets, repellents and coils are used at night and may not affect the risk. Therefore, the increase in risk is difficult to explain. While sprays may contain agents that effectively reduce mosquito density, repellents may expel mosquitoes from indoors and lead to a rise in mosquitoes outdoors, where biting will occur during the day. Mosquito nets are primarily used to prevent malaria transmission, and for this, they have been very effective. In our multivariate model, we found that using Olyset, Supanet and unknown brands of bednets was associated with an increased risk of seropositivity to WNV. WNV is transmitted mainly by Culex spp, a medium-sized mosquito that bites in the evening and at night. It could be that the treatment of these nets is ineffective, or the mesh size is large enough to allow culicine species through. Unknown brands of bed nets had a >26% higher risk of seropositivity when compared to Permanet. The unknown brands may be those tailored locally and may not be insecticide-treated and, therefore, less effective than the insecticide-treated brands. On the other hand, nets often get holes when damaged, with some bed nets more likely to get holes than others (31). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint We studied the risk of seropositivity to arboviruses in children who reported or manifested clinical signs and symptoms. The univariate model shows that feeling sick, common during arbovirus and other infections, was associated with a reduced risk of CHIKV and WNV infection. This was not the case in the multivariate model, and the risk was elevated for any dengue virus seropositivity. But having a rash was associated with an increased risk of WNV seropositivity, as expected in the univariate model; in the multivariate, a rash was associated with a decreased risk of YFV seropositivity. Since this cross-sectional study did not diagnose current arbovirus infections, the signs and symptoms we recorded could not necessarily be linked to arbovirus infection.  A study by Chauhan et al., 2019, showed an overlap of signs and symptoms between malaria, typhoid and arboviruses; thus, there could be similarity in clinical features when a current arbovirus infection is present (32). The signs and symptoms of CHIKV and DENV are similar to other infectious diseases in the acute phase; hence, most cases initially classified as arbovirus in the clinical phase may not be arboviral (5). Be that as it may, our multivariate model suggested that a history of a past rash increased the risk of testing DENV or CHIKV seropositive. Our findings indicate that YFV, DENV and CHIKV should be important differential diagnoses for clinicians who record a past rash during history-taking. Water bodies and vegetation cover near houses have been associated with an increased risk of arbovirus infections (5, 33). This was true for vegetation in our study, for YFV, as expected. Interestingly, the presence of water bodies near the home did not increase our study's overall risk of arbovirus seropositivity. Rather, this was associated with a decreased risk of seropositivity to any DENV. This study was conducted in Western Kenya, where major programs have regularly targeted mosquito . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint reduction in and around large water bodies using drones and other modern technologies. In houses roofed with iron sheets, the risk of CHIKV seropositivity was lower than in those thatched with grass. Temperatures in houses roofed with iron sheets drop quickly as soon as the sun sets; culicine species are temperature sensitive (34). Conclusion We find compelling evidence that there are socio-demographic, clinical and environmental risk factors within the community in Teso Sub County that are associated with arbovirus seropositivity. Some of these risk factors are amenable to interventions that can be initiated and implemented within the community.  We also conclude that mosquito nets are not nearly as effective at preventing arbovirus infection as they have been for malaria. Recommendations From the foregoing, we recommend that programs aimed at reducing exposure to arbovirus be designed based on risk factors identified within the community.  We recommend that mosquito nets that have well-established efficacy be used. We also recommend additional mosquito prevention measures to reduce daytime biting. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288220
doi: 
medRxiv preprint",1
"Investigating the role of host genetic factors in COVID-19 severity and susceptibility can inform 
our understanding of the underlying biological mechanisms that influence adverse outcomes and 
drug development1,2. Here we present a second updated genome-wide association study (GWAS) 
on COVID-19 severity and infection susceptibility to SARS-CoV-2 from the COVID-19 Host 
Genetic Initiative (data release 7). We performed a meta-analysis of up to 219,692 cases and 
over 3 million controls, identifying 51 distinct genome-wide significant loci—adding 28 loci from 
the previous data release2. The increased number of candidate genes at the identified loci helped 
to map three major biological pathways involved in susceptibility and severity: viral entry, airway 
defense in mucus, and type I interferon. Main Text Investigating the role of host genetic factors in COVID-19 severity and susceptibility can inform 
our understanding of the underlying biological mechanisms that influence adverse outcomes and 
drug development1,2. Here we present a second updated genome-wide association study (GWAS) 
on COVID-19 severity and infection susceptibility to SARS-CoV-2 from the COVID-19 Host 
Genetic Initiative (data release 7). We performed a meta-analysis of up to 219,692 cases and 
over 3 million controls, identifying 51 distinct genome-wide significant loci—adding 28 loci from 
the previous data release2. The increased number of candidate genes at the identified loci helped 
to map three major biological pathways involved in susceptibility and severity: viral entry, airway 
defense in mucus, and type I interferon. 
 
We conducted a meta-analysis for three phenotypes across 82 studies from 35 countries, 
including 36 studies with non-European ancestry (Fig. 1, Supplementary Fig. 1, 2, 
Supplementary Table 1): critical illness (respiratory support or death; 21,194 cases), 
hospitalization (49,033 cases), and SARS-CoV-2 infection (219,692 cases). Most of the studies 
were collected before the widespread introduction of COVID-19 vaccination. We found 30, 40, 
and 21 loci associated with critical illness, hospitalization, and infection due to SARS-CoV-2 
respectively, for a total of 51 distinct genome-wide significant loci across all three phenotypes (P 
< 5 × 10–8; Fig. 2, Supplementary Fig. 3, Supplementary Table 2), adding 28 genome-wide 
significant loci to the 23 previously identified by the COVID-19 HGI (data release 6)1,2. We 
observed a median increase of 2.9-fold in statistical power across lead variants owing to a median 
increase of 1.6-fold in effective sample sizes from the previous release (Supplementary Table 
3). After correcting for the number of phenotypes examined, 46 loci remain significant (P < 1.67 
× 10–8). Of the 28 additional loci, six loci were originally reported by the GenOMICC3 which also 
contributed to the current meta-analysis; and nine other loci were identified by the new 
GenOMICC meta-analysis4 during the preparation of this manuscript. We found nine more loci 
reached genome-wide significance, but excluded them due to being likely false positives identified 
via newly developed leave-most-significant-biobank-out analysis (Supplementary Table 4, 
Supplementary Note). Comparing the effect sizes and statistical significance between the . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.24.22283874
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. previous2 and current analysis indicated that all the previously identified loci were replicated and 
showed an increase in statistical significance (Supplementary Fig. 4). Using our previously 
developed two-class Bayesian model for classifying loci as being more likely involved in infection 
susceptibility or severity2, we determined that 36 loci are substantially more likely (> 99% posterior 
probability) to impact disease severity (hospitalization) and 9 loci to influence susceptibility to 
SARS-CoV-2 infection, while the remaining 6 loci could not be classified (Supplementary Fig. 5, 
Supplementary Table 5, Supplementary Note). We observed that 1q22 locus (lead variant: 
rs12752585:G>A) showed significant effect size heterogeneity across ancestries (Phet < 9.80 × 
10–4 = 0.05 / 51), while the previously reported heterogenous locus (FOXP4) remained at the 
same level of significance as before2 despite increase in sample size (Phet = 2.01 × 10–3; 
Supplementary Fig. 6, Supplementary Table 6). We found significant observed-scale SNP 
heritabilities of all the three phenotypes (1.2–8.2%, P < 0.0001). We also estimated liability-scale 
heritabilities for a range of population prevalences (Supplementary Fig. 7, Supplementary 
Table 7, Supplementary Note). 
 
To better understand the biological mechanisms underlying COVID-19 susceptibility and severity, 
we further characterized candidate causal genes by mapping them onto biological pathways and 
performing a phenome-wide association analysis (Fig. 3, Supplementary Fig. 8, 
Supplementary Tables 2, 8, 9). 15 out of 51 loci could be linked to three major pathways involved 
in susceptibility and severity defined by expert-driven classification (Supplementary Note): a) 
viral entry, b) entry defense in airway mucus, and c) type I interferon response. In addition, the 
phenome-wide association analysis identified nine loci involved in the upkeep of healthy lung 
tissue. 
 
First, five loci include candidate causal genes involved in viral entry pathway (Fig. 3a), such as 
previously reported SLC6A20 (3p21.31), ABO (9q34.2), SFTPD (10q22.3), and ACE2 (Xp22.2), 
as well as TMPRSS2 (21q22.3) which was additionally identified in the data release 7. We found 
a lead variant rs9305744:G>A, an intronic variant of TMPRSS2, is protective against critical illness 
(OR = 0.92, 95% CI = 0.89–0.95, P = 1.4 × 10–8) and in LD with a missense variant 
rs12329760:C>T (p.Val197Met; r2 = 0.68). SARS-CoV-2 employs the serine protease TMPRSS2 
for viral spike (S) protein priming, as well as the previously reported ACE2 for host cell entry which 
functionally interacts with SLC6A205,6. Of note, the previously reported association between ABO 
blood groups and susceptibility could be attributed to the interference of anti-A and anti-B 
antibodies with the S protein, potentially interfering with viral entry7. In addition, the previously 
reported SFTPD encodes pulmonary surfactant protein D (SP-D) which contributes dually to the 
lung’s innate immune molecules, and viral entry response in pulmonary epithelia8,9 along with 
other genes for airway defense. 
 
Second, four loci contain candidate causal genes for entry defense in airway mucus (Fig. 3b), 
such as previously reported MUC1/THBS3 (1q22) and MUC5B (11p15.5) as well as novel MUC4 
(3q29) and MUC16 (19p13.2). We found that novel lead variants rs2260685:T>C in MUC4 
(intronic variant; in LD [r2 = 0.65] with a missense variant rs2259292:C>T [p.Gly4324Asp]) and 
rs73005873:G>A in MUC16 (intronic variant) increase risk for SARS-CoV-2 infection (OR = 1.03 
and 1.03, 95% CI = 1.02–1.04 and 1.02–1.04, P = 4.1 × 10–8 and 6.4 × 10–10, respectively). In 
addition, the previously reported locus 1q22 contains an intergenic lead variant rs12752585:G>A 
that decreases risk for the infection (OR = 0.98, 95% CI = 0.97–0.98, P = 1.5 × 10–11) and 
increases MUC1 expression in esophagus mucosa in GTEx v8 (P = 5.2 × 10–9). Notably, the 1q22 
locus also harbors an independent lead variant rs35154152:T>C, a missense variant 
(p.Ser279Gly) of THBS3, that decreases risk for hospitalization (OR = 0.88, 95% CI = 0.86–0.90, 
P = 5.6 × 10–22) but not infection (P = 5.7 × 10–4), suggesting potential distinct mechanisms in the 
locus. Consistent with these association patterns, MUC1, MUC4 and MUC16 are three known . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.24.22283874
doi: 
medRxiv preprint major transmembrane mucins of the respiratory tracts that prevent microbial invasion, while 
previously reported MUC5B, together with nearby MUC5AC, are primary structural components 
of airways mucus that enable mucociliary clearance of pathogens10. 
 
Third, six loci harbor candidate causal genes that are linked to the type I interferon pathway (Fig. 
3c), such as previously reported IFNAR2 (21q22.11), OAS1 (12q24.13), and TYK2 (19p13.2), as 
well as additionally identified JAK1 (1p31.3), IRF1 (5q31.1) and IFN-α genes (9p21.3). Previous 
studies have reported additional genes in this pathway: TLR7 (ref. 11,12) and DOCK2 (ref. 13). In 
the present study, we found that a lead variant rs28368148:C>G, a missense variant 
(p.Trp164Cys) of IFNA10 located within the IFN-α gene cluster, increases risk for critical illness 
(OR = 1.56, 95% CI = 1.38–1.77, P = 3.7 × 10–12). The IFN-α is one of the type I interferons that 
binds specifically to the IFN-α receptor consisting of IFNAR1/IFNAR2 chains, whose mutations 
are also known for increasing risk of hospitalization and critical illness. On the genes that enable 
signaling downstream of IFNAR, we identified a lead variant rs11208552:G>T, an intronic variant 
of JAK1, is protective against critical illness and hospitalization (OR = 0.92 and 0.95, 95% CI = 
0.89–0.94 and 0.93–0.96, P = 5.5 × 10–10 and 2.2 × 10–9, respectively). This variant is previously 
reported to decrease lymphocyte counts14 (β = –0.016, P = 5.5 × 10–15) and increase the JAK1 
expression in thyroid in GTEx15 (P = 6.1 × 10–23). JAK1 and previously reported TYK2 are Janus 
kinases (JAKs) required for the type I interferon induced JAK-STAT signaling. JAK inhibitors are 
used to treat severe COVID-19 patients16. In addition, on the downstream of the JAK-STAT 
signaling, we found that a lead variant rs10066378:T>C, located 67 kb upstream of IRF1, 
increases risk for critical illness and hospitalization (OR = 1.09 and 1.07, 95% CI = 1.06–1.13 and 
1.05–1.09, P = 2.7 × 10–9 and 3.74 × 10–10, respectively). 
 
Furthermore, the phenome-wide association analysis identified nine loci previously associated 
with lung function and respiratory diseases. There loci contain genes involved in the upkeep of 
healthy lung tissue such as previously reported FOXP4 (6p21.1), SFTPD (10q22.3), MUC5B 
(11p15.5), and DPP9 (19p13.3), as well as additionally identified CIB4 (2p23.3), NPNT (4q24), 
ZKSCAN1 (7q22.1), ATP11A (13q34), and PSMD3 (17q21.1). For example, we found that three 
lead variants rs1662979:G>T (intronic variant of CIB4), rs34712979:G>A (splice region variant of 
NPNT), and rs2897075:C>T (intronic variant of ZKSCAN1) significantly associated with 
hospitalization (OR = 1.05, 0.94, and 1.05, 95% CI = 1.03–1.07, 0.92–0.96 and 1.03–1.07, P = 
5.6 × 10–9, 3.8 × 10–8 and 8.9 × 10–9, respectively) and lung function (FEV1/FVC)17, similar to the 
previously reported lead variant rs3934643:G>A (intronic variant of SFTPD). Notably, while the 
COVID-19 severity risk-increasing alleles of rs1662979 and rs3934643 decrease lung function (β 
= –0.013 and –0.025, P = 5.3 × 10–8 and 6.3 × 10–10), those of rs34712979 and rs2897075 
increase it (β = 0.068 and 0.023, P = 4.2 × 10–134 and 1.6 × 10–20, respectively). Likewise, we 
found lead variants that significantly associated with hospitalization and idiopathic pulmonary 
fibrosis (IPF)18,19, such as the aforementioned rs2897075 and rs12585036:C>T (intronic variant 
of ATP11A; OR = 1.10, 95% CI = 1.08–1.12, P = 3.2 × 10–21), in addition to the previously reported 
rs35705950:G>T (promoter variant of MUC5B). While the COVID-19 severity risk-increasing 
alleles of rs2897075 and rs12585036 increase risk for IPF (OR = 1.12 and 1.27, P = 3.0 × 10–14 
and 7.0 × 10–9, respectively), those of rs35705950 decreases (OR = 0.50, P = 3.9 × 10–80). These 
results highlight the complex pleiotropic relationships between COVID-19 severity, lung function, 
and respiratory diseases. 
 
We applied genetic correlations and Mendelian randomization (MR) analyses to identify potential 
causal effects of risk factors on COVID-19 phenotypes (Supplementary Fig. 9, Supplementary 
Tables 10, 11). Fourteen novel genetic correlations and 10 novel robust exposure-COVID-19 trait 
pairs showed evidence of causal associations (Supplementary Note). In particular, smoking 
initiation and the number of cigarettes per day were positively correlated with severity and . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.24.22283874
doi: 
medRxiv preprint susceptibility phenotypes; with MR indicating that smoking was causally associated with 
increased risk for COVID-19 and further highlighting the role of the healthy lung tissue in COVID-
19 severity. Additionally, genetically instrumented higher glomerular filtration rate (eGFR; 
indicative of better kidney function) was associated with lower risk of COVID-19 critical illness, 
while genetically predicted chronic kidney disease was associated with increased risk of COVID-
19 critical illness, suggesting that better kidney function would be beneficial for lower risk of 
COVID-19 severity. 
 
In summary, we have substantially expanded the current knowledge of host genetics for COVID-
19 susceptibility and severity, by further doubling the case numbers from the previous data 
release2 and identifying 28 additional loci. The increased number of loci allows us to map genes 
to pathways involved in viral entry, airway defense and immune system response. Intriguingly, we 
observed severity loci mapped to type I interferon pathway, while susceptibility loci mapped to 
viral entry and airway defense pathways, with notable exceptions for severity-classified 
TMPRSS2 and MUC5B loci. Further investigation of how such susceptibility and severity loci map 
to different pathways would provide mechanistic insights into the human genetic architecture of 
COVID-19. Data availability Summary statistics generated by COVID-19 HGI are available online, including per-ancestry summary statistics for African, Admixed American, East Asian, European, and South Asian ancestries (https://www.covid19hg.org/results/r7/). The analyses described here utilize the data 
release 7. If available, individual-level data can be requested directly from contributing studies, 
listed in Supplementary Table 1. We used publicly available data from GTEx 
(https://gtexportal.org/home/), the Neale lab (http://www.nealelab.is/uk-biobank/), Finucane lab 
(https://www.finucanelab.org), 
FinnGen 
Freeze 
4 
cohort (https://www.finngen.fi/en/access_results), 
and 
eQTL 
catalogue 
release 
3 (http://www.ebi.ac.uk/eqtl/). Code Availability The code for summary statistics lift-over, the projection PCA pipeline including precomputed 
loadings and meta-analyses are available on GitHub (https://github.com/covid19-hg/); the code 
for 
custom 
analyses 
are 
available 
on 
GitHub—heritability 
estimation: https://github.com/AndrewsLabUCSF/COVID19_heritability; 
Mendelian 
randomization 
and genetic 
correlation: 
https://github.com/marcoralab/MRcovid; 
subtype 
analyses: https://github.com/mjpirinen/covid19-hgi_subtypes. Competing Interests A full list of competing interests is available in Supplementary Table 12. Acknowledgments We acknowledge M. O’Reilly and B. Cooley from the Pattern team at the Broad Institute of MIT 
and Harvard for designing the geographical map in Fig. 1. A full list of acknowledgement is 
available in Supplementary Table 12. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.24.22283874
doi: 
medRxiv preprint Author contributions Detailed author contributions are integrated in the authorship list.",1
"Objective: Registered reports relate to a new publication of a peer-review of the protocol before the start of the study, followed by an in-principle acceptance by the journal before the study starts. We aimed to describe randomized controlled trials (RCTs) in the clinical field published as registered reports. Study design and setting: This cross-sectional study (registration: https://osf.io/zf53p/) included registered report results for RCTs, identified on PubMed/Medline and on a list compiled by the Center for Open Science. It explored the proportion of reports that received in-principle acceptance (and/or published a protocol before inclusion of the first patient) and changes in the primary outcome. Results: A total of 93 RCT publications identified as registered reports were included. All but one were published in the same journal group. The date of the in-principle acceptance was never documented. For most of these reports (79/93, 84.9 %) a protocol was published after the date of inclusion of the first patient. A change in the primary outcome was noted in 40/93 (44%) of these publications. Three out of the 40 (33%) mentioned this change. Conclusions: Randomized controlled trials in the clinical field identified as registered reports were rare, they originated from a single journal group and did not comply with the basic features of this format. Protocol registration: https://osf.io/zf53p/ Keywords: Randomized controlled trials, registered report, reproducibility, outcomes switching, transparency, protocol . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 3 Introduction Clinical trial results shape clinical practice guidelines and support decisions that affect the health of millions of people. According to clinicaltrials.gov, “in a clinical trial, participants receive specific interventions according to the research plan or protocol created by the investigators. These interventions may be medical products, such as drugs or devices; procedures; or changes to participants' behavior, such as diet.” Randomization is a method used to randomly assign participants to the arms of a clinical study and is a key to ensuring the initial comparability of the groups. Following the Thalidomide crisis in 1962 (1), randomization became a gold standard for clinical trials, in order to maximize their internal validity. Because their results must be trustworthy and resist the considerable financial and ideological conflicts of interest inherent in the evaluation of therapeutics, clinical randomized controlled trials (RCTs) have long been at the forefront of efforts to ensure transparency and reproducibility. In 2005, to enhance transparency, the International Committee of Medical Journal Editors (ICMJE) made the registration of clinical RCTs compulsory before inclusion of the first patient (2). However, and despite these efforts, many issues remain. Many clinical RCT results remain unpublished, especially when conclusions are “negative” (3). When published, clinical RCT results are frequently reported with a certain degree of selective reporting and spin (4). Initiatives promoting clinical RCT data-sharing are too few (5). In addition, clinical RCTs all too often ask the wrong question, e.g. by relying on the wrong comparator, and/or they implement inadequate methods, e.g. the use of non-informative surrogate outcomes, or they lack adequate power (6). All these problems hamper reproducibility and reduce the value of therapeutic research. Similar concerns have been described in many scientific disciplines, such as psychology (7) and cancer biology (8), suggesting the existence of a reproducibility crisis in science (9). A new publication format – registered reports – was created to address these systemic flaws by increasing transparency and relying on a strict hypothetical-deductive model of scientific method (10,11). In a registered report, the study protocol – including theory, hypotheses, detailed methods, and analysis plan – is submitted directly to a journal, and peer review of this material – i.e. stage-1 peer review – occurs before the research is underway. Stage-1 peer review appraises both the importance of the research question and the quality of the methodology prior to data collection (12– 14). Following  stage-1 peer review, high quality protocols exploring relevant research questions receive an in-principle acceptance for publication in the journal, on condition that the study  is completed successfully in accordance with the protocol. Provisional acceptance is therefore given without knowledge of the research results, or whether or not the study hypothesis has been verified. Following an in-principle acceptance, the investigators can register their protocol and start the research. Once the research is completed, stage-2 peer review checks its compliance with the . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 4 protocol and the accuracy of the conclusions in view of the evidence. Figure 1 (adapted from the figure provided by the Center For Open Science (12)) gives details of the whole registered report process. This approach is expected to promote high methodological quality and to reduce issues such as publication bias. In addition stage-1 peer reviews, if qualitative, can deal with certain issues regarding design pre-emptively, and this should improve the robustness of the methods. Stage-2peer review ensures  compliance with the methodology validated in stage 1 and leads to the publication of the study whatever the results. This approach is expected to limit publication bias and selective outcome reporting. Registered reports also have the advantage of pre-emptively defining the terms of important issues such as data sharing, which should indeed be addressed before the first patient is included in order to make sure that trial participants are adequately informed. This process is therefore quite different from the publication of a protocol. Some journals (e.g. BMJ Open, Plos One, Trials, etc.) offer the opportunity to publish protocol papers for ongoing trials, which provide more details than clinical trial registries. However, while publishing the protocol is generally considered as good practice in terms of research transparency, it does not completely remove the possibility of publication bias or selective outcome reporting bias in the final report. In addition, unlike the registered report format, the publication of a protocol often occurs after the study has started and this does not allow  any issues to be resolved regarding the research question and design on the basis of the reviewers’ comments. Furthermore, in some cases, there is no specific peer review by the journal for these protocols, for instance at BMJ Open, acceptance is offered providing the study 1/ was funded by a funding body ensuring peer review and 2/ obtained  authorization from an appropriate Institutional review board (15). Finally, it is also worth noting that the use of registered reports in journals is generally associated with other open science practices (16). To date, more than 300 scientific journals, mainly, but not only, from the field of psychology (11), have adopted the registered report format. These journals include PLOS Biology, BMJ Open Science, Nature and Nature Human Behavior. Despite the adequate fit between the requirements of the registered format and the confirmatory nature of clinical RCTs (14), few medical journals have implemented registered reports. BMC Medicine launched the first registered report format for RCTs in 2017 (14). The mega-journal PLOS ONE, which occasionally publishes RCTs, has adopted the registered report format(17). The specialist journals affiliated to the Journal of Medical Internet Research (JMIR) also accept registered reports and assign them an International Registered Report Identifier (IRRID) number (18). Our objective was to identify and describe features of clinical randomized controlled trials published as Registered Reports. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 5 Method The methods of this cross-sectional study were specified in advance. They were documented in a protocol registered on the Open Science Framework on 17th December 2021 (https://osf.io/zf53p/). Eligibility criteria We surveyed clinical RCTs, i.e. randomized controlled trials that had a clinical orientation, published as final reports of a registered report. Registered report protocols (i.e. stage-1 registered reports) were not included. Search strategy and study selection The search for these registered reports used the Medline database with the following search string: ““registered report” or ""IRRID”. Following peer review, we also performed a sensitivity analysis  using the terms ""registered replication report*"" and ""preregistered report*. In addition, we searched a list of published registered reports in a Zotero library compiled by the Center for Open Science https://bit.ly/2pJRYz3. Database searches were performed on 17th December 2021. All references identified were first automatically screened using R, excluding references where the title included the word ""protocol"" and/or did not include the character string ""random"". This procedure was validated on 100 randomly sampled references, retrieving 100 % accuracy before being implemented (details of the validation procedure are shown in the Web Appendix 1). All remaining references were then manually selected by two independent reviewers (NA and AT). Any disagreement was resolved by consensus with FN. Outcomes Primary outcome The primary outcome was the proportion of RCTs identified as registered reports that had received in-principle acceptance and/or had published a protocol before inclusion of the first patient. Additional information was collected on i) the protocol (protocol availability, date of the time- stamped protocol, date of protocol submission, date of protocol acceptance, date of protocol publication in a journal and publication of the protocol in the same journal or journal group), ii) the primary outcome used in the registered report (change in the primary outcome, and in case of a change, mention of the change in the registered report), iii) the secondary outcomes used in the registered report (change in the secondary outcomes and in case of a change, mention of the change in the registered report), iv) result from the study for its primary outcome (positive or negative), v) sample size, vi) power to detect effect sizes of 0.3, 0.5 and 0.8, respectively (based on sample size in . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 6 each arm of the trial), vii) citation rate, viii) Altmetric attention score and ix) characteristics of the registered report (journal, journal impact factor and topic). Data extraction Two authors (NA and AT) independently extracted the data from the studies included clinical trial registry data and PubMed metadata on the articles (see Web Appendix 2). Disagreements were resolved by consensus or in consultation with a third reviewer (FN). Data extraction for the number of citations and the Altmetric attention score was performed on the 27th April 2022. Statistical methods A descriptive analysis was performed using the following: medians (and range) for quantitative variables, numbers and percentages for qualitative variables. Statistical analyses were performed using R version 4.0.3. Deviations from the protocol or addition of elements For the sake of simplicity, the power calculation was limited to parallel RCTs, which accounted for most of the registered reports. Since the date of the first inclusion of patients was only documented as month and year, we computed time lapses using months as the unit. Because we identified several publications during our searches which, despite being identified as registered reports, were secondary analyses and not the report on the primary analysis of the trial (e.g. based on the pre-specified analysis of its primary outcome), we added this feature (primary/secondary analysis) as one of our outcomes and provided detailed results according to this variable. Patient involvement We had no established contacts with specific patient groups who might be involved in this project. No patients were involved in defining the research question or the outcome measures, nor were they involved in the design and implementation of the study. There are no plans to involve patients in the dissemination of the results, nor will we disseminate the results directly to patients. Results A total of 2074 references were identified from Zotero (n=194) and Medline (n=1880). Regarding the sensitivity analysis, no additional paper matched the search criteria. One hundred and fifteen references remained after deduplication and automatic screening. Of these, 22 were not identified as registered reports, and 93 references were eligible for inclusion because they were identified as registered reports and/or had an IRRID (Figure 2). Fifty-three out of 93 (57%) of the reports were published in the JMIR and an additional 39 (42 %) were published in a JMIR-affiliated journal, leaving . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 7 only one other report, published in Nurse Educ Today. Of the 93, 78 (83.9 %) reported results on the primary trial analysis and 15 (16.1 %) reported on a secondary analysis. Primary outcome The date of the in-principle acceptance was never documented in the reports included. Consequently, we relied on the date of protocol publication. We identified protocols published in a journal before inclusion of the first patient for 8/93 reports and protocol acceptance before the first inclusion for 8/93 reports. The lack of dates prevented any checks regarding protocol publication and protocol acceptance for an additional 6/93 and 14/93 registered reports respectively. In other words, 79/93 (84.9 %) had a protocol published after the date of inclusion of the first patient and 71/93 (76.3%) had a protocol accepted after the first inclusion. Figure 3 reports the results for our primary outcome in the sample of 78/93 (83.9%) registered reports reporting results from primary analyses. In this sample, 7/78 (9%) had a protocol published in a journal before inclusion of the first patient, 5/78 (6.4 %) had missing data for this item and 66/78 (84.6 %) had a protocol published after the date of inclusion of the first patient. Among the registered reports on primary analyses, protocol publications ranged from 1.4 years before to 3 years after the inclusion of the first patient (the median time lapse was 1 year); of these 78, 42 (53.8%) had their protocols published up to one year after the first inclusion. Fifty-two out of 78 (66.7%) were registered in a trial registry before inclusion of the first patient, 21/78 (26.9 %) were registered after inclusion of the first patient, and dates were not available for 5/78 (6.4 %). Secondary outcomes A protocol was available for 90/93 (97%) reports and all of these 90 protocols were published in a journal. The published protocols and their  associated reports were not published in the same journal group for 55 (61%) of these 90. We found no mention of OSF registration in any of these papers. A change in primary outcome compared to the protocol was noted for 40/93 (44%) of the reports, including 27/78 (35%) of the reports reporting the results from primary analyses. Among these 40 registered reports, we identified 9 that added components to the primary outcomes, 7 that deleted components of the outcomes, 1 that switched the primary outcome with a secondary outcome and 23 that modified the definition of the primary outcome (7, 3 and 13 respectively for a change of period of assessment, complete change of criteria and secondary analysis of RCT data).  Thirteen out of 40 (33%) of the reports with a change in the primary outcome mentioned this change, and these 13 reports were exclusively secondary analyses. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 8 A change in at least one of the secondary outcomes was present in 46/87 (53%) reports of studies with secondary outcomes (absence of secondary outcomes in 5 protocols/reports) and this change was mentioned by 8/46 (17%) of them. Table 2 provides details for these results with a specific focus on reports providing  results from primary analyses versus those only reporting secondary analyses. The study outcomes were considered positive, mixed and negative in 49 (52.7%), 10 (10.8%) and 34 (36.6%) of the reports included respectively. The median sample size was 239 (range 13-14 482) for the 93 reports. Over the 75 trials with a parallel design, the median power (interquartile range) was 51% (26%-86%), 91% (59%-99%), 99% (94%-100%) to detect effect sizes of 0.3, 0.5 and 0.8, respectively (Figure 4). The median Altmetric score was 2 (0-574) and the median citation rate was 2 (0-26), for a median of 374 (206-528) days after publication. Descriptive statistics for all other outcomes are shown in Table 1. Discussion Statement of principal findings In short, the registered report format is virtually non-existent for published RCTs, except in the JMIR group. To date, BMC Medicine has only published 3 registered reports (19–21), and none concerned a RCT. None of the trials included in our study was from the mega journal PLOS One. We found 93 publications identified as registered reports and reporting  results from RCTs. Most of these reports (92 out of 93) were published in the JMIR or its companion journals. Some of these papers had protocols published in the JMIR Research Protocols. According to JMIR policy, these protocol papers are peer reviewed and their acceptance “guarantees the subsequent acceptance of papers containing results from the protocol in any JMIR publications journal” (22).  It is important to note that we also found registered reports  for which the protocol was published in other journals, such as BMJ open, a journal that does not systematically implement peer reviews of protocols (23). Unfortunately, we were not able to identify the dates of in-principle acceptance in the published articles. It would be relevant for this data to become standard in the reporting of registered reports, and also for publishers/editors to consider the interest of adding this to the article’s metadata. Rather than being typical registered reports, the publications tended to be RCTs with a published protocol, generally published or even formally accepted by peer review after inclusion of the first patient. However, protocols were generally submitted after the start of the study. This goes against the very principle of registered reports. Indeed, registered reports are not only  tools that can deal with selective outcome reporting, they are also a means to enhance a given trial methodology, providing a good quality peer review. When a study is started, it is already too late to determine critical points regarding the methodology. Therefore, we could argue that a posteriori . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 9 submission/acceptance of protocols is a major deviation from the original concept of registered reports. Furthermore, these protocols were often published in journals that were different from the journal publishing the final report. Of course, having a published protocol is surely a good thing for a RCT, since it should be associated with greater quality (24). For instance, the trials included in our survey tend to be larger than the prototypal RCT published in the field of biomedicine - as illustrated by a recent example on RCTs on interventions for mood, anxiety, and psychotic disorders (25), – most of which had sufficient power to detect effect sizes larger than 0.5. However, having a published protocol is not sufficient for it to become a registered report, especially when the protocol is published in a journal that is independent from the journal accepting the final report. In this case, despite having an IRRID, these final reports can hardly be considered as  registered reports and at the very least they present major deviation. An even greater subject of concern is that some of the trials included were registered retrospectively on a RCT registry. Some of the publications with an IRRID reported on the results for secondary analyses. Concerning the reports of results from primary analyses, we identified frequent changes in the primary outcomes between the published protocol and the final publication. Few of these changes were explicitly discussed in the reports. It is often assumed, in classic papers, that outcome switching is a consequence of the pressure to publish positive results. Our results suggest that there are some other incentives at play, since researchers still switch outcomes, despite the guarantee they will  be published. Strengths and weaknesses of this study Our focus on RCTs makes sense because of the importance these studies have in evidence- based medicine. In addition, the proponents of registered reports have explicitly suggested that this format should be mandatory for RCTs because of the confirmatory nature of these trials (14). However, we should be careful to avoid any generalization of our results to other types of study, other journals and indeed to other fields. We found a highly selective sample of trials, derived from a specific journal group - JMIR focuses mainly on digital interventions - with its own policies. Impact and attention measures - such as citations and attention scores - suggest that the trials included were given a modest impact/media coverage. Identifying registered reports is a difficult task. It is possible that our literature searches overlooked some references, as it has been previously noted that a substantial number of published final reports did not clearly identify themselves as registered reports (26). In contrast, the use of IRRID may have captured  JMIR publications more easily. During the peer-review process, we also used terms such as ""registered replication report*"" and ""preregistered report*"" and found no additional reference. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 10 One interesting perspective or “mise en abîme” would have been to publish this study as a registered report, but on this occasion, we chose not to, as  it was more a descriptive survey,  not based on hypothesis testing. To ensure transparency and reproducibility, the study was registered before data collection and all data, and all references included, are shared on the Open Science Framework. Strengths and weaknesses in relation to other studies Previous studies on registered reports have highlighted a number of implementation issues and have suggested  developing standards to ensure optimal implementation of this new publication format (14,26). These standards are definitely needed for registered reports on RCTs. Of course, there is a continuum of practices between protocol registration, protocol publication  and indeed the registered report (14) and some of the practices at JMIR may be good for reproducibility purposes. Still,  very particular attention should be paid to the use of the term registered report or to the assignment of a registered report identification number to these studies. While registered reports are believed to strengthen the integrity of the publication landscape, it must be acknowledged that evidence of their real impact is still preliminary. One study has specifically attempted to evaluate the quality of registered reports in comparison to non- registered report papers in psychology and neuroscience, with positive results regarding the soundness of the registered report methodology and overall quality (27). Another study in the psychology literature shows a lower proportion of positive results in registered reports than in the literature overall, suggesting a lower level of publication bias with registered reports (28). Regarding the impact of registered reports , an ongoing study shows that they could be associated with more citations, but this data is preliminary and it could be related to the originality of the format rather than the content  the reports that attracts these citations (29). Future development will require meta-research to demonstrate the beneficial impact of this format. A collaboration involving journals and the Center for Open Science (COS) is underway to conduct a large, pragmatic, randomized controlled trial on registered reports compared to standard practice, to assess their impact on publication, research outcomes, and methodological quality (30). Meaning of the study: possible explanations and implications for clinicians and policy-makers Overall, these results are compelling: not only is therapeutic research lagging behind with regard to the implementation of registered reports (26), but their implementation to date is far removed from the very idea of a registered report. With just a few exceptions, trialists interested in adopting this format cannot implement it because the journals - especially the leading general . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2022.08.02.22278318
doi: 
medRxiv preprint 11 medical journals - do not provide this opportunity. Planning for registered reports is a first step towards a much-needed open science pathway for RCTs (6). The development of a model of this sort is not without risk for journals, and specific methodological developments relating to specific features of RCTs need to be considered. For instance, these trials are usually long and entail a risk of premature trial discontinuation (24). There is also a need to coordinate the various peer reviews that a trial receives in its life-cycle, e.g. before funding, by the IRB, or by health authorities, in order to make the process as  efficient as possible. Reporting guidelines such as CONSORT (31) could also evolve to fit the needs of this new publication format. Unanswered questions and future research Registered reports were developed as a means to improve research trustworthiness. We encourage key RCT funders, the main health authorities and major medical journals to join forces to develop piloting systems registered reports for RCTs. We cannot be satisfied with the current status quo for RCTs, where the concept of registered reports is almost inexistent, if not subverted. In addition, once registered reports  are progressively implemented on a large scale, meta-research will be needed to evaluate how different journals and disciplines implement the format. The greater the number of adoptions of the format, the greater the risk of deviation from the general registered report policies.",1
"Background: Randomized controlled trials (RCTs) are essential for determining the safety and 
efficacy of healthcare interventions. However, both laypeople and clinicians often demonstrate 
experiment aversion: preferring to implement either of two interventions for everyone rather than 
comparing them to determine which is best. We studied whether clinician and layperson views of 
pragmatic RCTs for Covid-19 or other interventions became more positive early in the 
pandemic, which increased both the urgency and public discussion of RCTs. Methods: We conducted several survey studies with laypeople (total n=2,909) and two with 
clinicians (n=895; n=1,254) in 2020 and 2021. Participants read vignettes in which a 
hypothetical decision-maker who sought to improve health could choose to implement 
intervention A for all, implement intervention B for all, or experimentally compare A and B and 
implement the superior intervention. Participants rated and ranked the appropriateness of each 
decision. Results: Compared to our pre-pandemic results, we found no decrease in laypeople’s aversion to 
non-Covid-19 experiments involving catheterization checklists and hypertension drugs. Nor were 
either laypeople or clinicians less averse to Covid-19 RCTs (concerning corticosteroid drugs, 
vaccines, intubation checklists, proning, school reopening, and mask protocols), on average. 
Across all vignettes and samples, levels of experiment aversion ranged from 28% to 57%, while 
levels of experiment appreciation (in which the RCT is rated higher than the participant's 
highest-rated intervention) ranged from only 6% to 35%. Conclusions: Advancing evidence-based medicine through pragmatic RCTs will require 
anticipating and addressing experiment aversion among both patients and healthcare 
professionals. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 Introduction Randomized controlled trials (RCTs) are crucial for understanding how to safely, effectively, and equitably prevent and treat disease and deliver healthcare. They have repeatedly upended conventional clinical wisdom and the results of observational studies,1 and are urgently needed to evaluate new technologies.2 However, RCTs often prove controversial, even when they compare interventions that are within the standard of care or otherwise unobjectionable, and about which the relevant expert community is in equipoise.3 Prestigious medical journals have recently published several trials—including SUPPORT4, FIRST5, and iCOMPARE6—that have received considerable criticism from physician-scientists, ethicists, and regulators in those journals7,8 and the public square.9–12 Although criticisms of RCTs can be complex and nuanced, many reflect a rejection of the very idea that an experiment was conducted, as opposed to simply giving everyone the allegedly superior intervention. In prior studies—inspired by several “notorious RCTs,” including technology industry “A/B tests”13–15—we confirmed that substantial shares of both laypeople and clinicians can be averse to randomized evaluation of efforts to improve health. People rated a pragmatic RCT designed to compare the effectiveness of two interventions significantly lower than the average rating of implementing either one, untested, for everyone, a phenomenon we call the “A/B effect.”16 In some cases, the lower average rating of an experiment could be driven not by dislike of experiments, per se, but by the fact that many people believe one of its arms is inferior to the other,16,17 a belief that is often not evidence-based. We therefore also documented “experiment aversion”: rating an RCT comparing two interventions as worse than even one’s own least- preferred intervention.17 Both patterns of negative sentiments about experiments—including . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 3 experiments judged to compare two unobjectionable interventions—can impede efforts to identify what does and does not work to improve health outcomes. The Covid-19 pandemic presented a potential inflection point in attitudes towards health experimentation. In April 2020, 72 Covid-19 drug trials were already underway18 and RCTs became daily, front-page news. That sustained exposure might have educated people about RCTs, or made RCTs more normative. Separately, our previous research suggests that one cause of experiment aversion is an illusion of knowledge—a (mis)perception that experts already must know what works best, and should simply implement that. But Covid-19 was a novel disease, and—at least in the case of pharmaceutical interventions—no sensible person thought the correct treatments were already obvious. People therefore may be less averse to Covid-19 RCTs than to RCTs that test interventions against longstanding conditions or problems. On the other hand, because of the urgency attached to Covid-19, people may be more averse to Covid-19 RCTs, being even less inclined to risk giving someone a treatment that might turn out to “lose” in a comparison study.19,20 Finally, even if the pandemic did not affect public attitudes towards RCTs, it could have affected the attitudes of clinicians, many of whom were involved in Covid-19 research. Because clinicians strongly influence whether particular RCTs are conducted, their attitudes matter. We investigated attitudes towards experimentation in the first year of the pandemic by conducting a series of preregistered studies between August 2020 and February 2021. First, we used decision-making vignettes from our previous work to ask whether the extraordinary publicity around Covid-19 RCTs reduced general healthcare experiment aversion by the public. Next, we adapted these vignettes to determine whether the public was averse to experimentation on pharmaceutical and/or non-pharmaceutical interventions (NPIs) for Covid-19. Finally, we . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 4 recruited two large clinician samples to investigate how their attitudes compared to those of laypeople. All three studies were randomized survey experiments in which participants first read about a decision-maker faced with a problem who either implemented one of two interventions (A or B) or ran an experiment to compare them (and then implemented the superior one). Participants then evaluated how appropriate each of those three decisions was. Methods Lay Sentiments About Healthcare Experimentation In August 2020, we used the CloudResearch service to recruit 700 crowd workers on Amazon Mechanical Turk to participate in a brief online survey. These services provide samples that are broadly representative of the U.S. population and are well-accepted in social science research as providing as good or better-quality data than convenience samples such as student volunteers, with results that are similar to probability sampling methods.21,22 Each participant first read a vignette that described a problem that the decision-maker could address in three ways (see Table 1 for examples; see the Supplemental Appendix [SA] for text and motivations for all vignettes): by implementing intervention A for all patients (A); by implementing intervention B for all patients (B); or by conducting an experiment in which patients are randomly assigned to A or B and the superior intervention is then implemented for all (A/B). (Our vignettes are silent about whether consent will be obtained, but IRBs customarily waive consent when it would make low-risk pragmatic RCTs impracticable23; in separate work, we found that substantial shares of people object to such experiments even when we specify that consent will be obtained.24) Next, following standard methods in social and moral psychology for evaluating decisions,25 participants rated each option on a scale of appropriateness from 1 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 5 (“very inappropriate”) to 5 (“very appropriate”), with 3 as a neutral midpoint. Participants then rank-ordered the options from best to worst. We also collected demographic information, but found no substantial associations with it in any of our studies (Tables S8-11). Participants were randomly assigned to one of two vignettes. In Best Anti-Hypertensive Drug, some doctors in a walk-in clinic prescribe “Drug A” while others prescribe “Drug B” (both of which are affordable, tolerable, and FDA approved) and Dr. Jones prescribes either A or B for all his hypertensive patients, or runs a randomized experiment to compare their effectiveness. In Catheterization Safety Checklist, a hospital director similarly considers two locations where he might display a safety checklist for clinicians (see Table 1A). We define the “A/B Effect” as the degree to which participants’ ratings of the A/B test were lower than the average of their ratings of implementing A and B.16 “Experiment aversion” is the degree to which participants rated the A/B test lower than their own lowest-rated intervention (either A or B for each person). “Experiment appreciation” is the opposite: the degree to which the experiment is rated higher than each participant’s highest-rated intervention. (See the SA for full details on power analyses and sample sizes, statistical analyses, materials, preregistrations, and data availability.) Lay Sentiments About Covid-19 Healthcare Experimentation Between August 2020 and January 2021, we recruited 2,209 additional laypeople in the same manner described above. They read, rated, and ranked six new vignettes involving Covid- 19 interventions (N = 339–450 per vignette). Four vignettes were based on Covid-19-related interventions that were discussed, tested, and/or implemented at the time: Masking Rules (which described two masking policies, of varying scope); School Reopening (two school schedules . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 6 designed to increase social distancing); Best Vaccine (two types of vaccine—mRNA versus inactivated virus); and Ventilator Proning (two protocols for positioning ventilated Covid-19 patients; see Table 1B). The other two vignettes—Intubation Safety Checklist and Best Corticosteroid Drug—were adapted from the first study to apply to Covid-19. Clinician Sentiments About Covid-19 Healthcare Experimentation Between November 2020 and February 2021, clinicians (including physicians, physician assistants, and nurse practitioners) in a large health system in the Northeastern U.S. were recruited by email to read, rate, and rank one of four Covid-19-related vignettes (Masking Rules: n = 349; Intubation Safety Checklist: n = 271; Best Corticosteroid Drug: n = 275; Best Vaccine: n = 1254) from the second study. Results Lay Sentiments About Healthcare Experimentation We found substantial negative reactions to A/B testing in both vignettes (Table 2A), replicating our pre-pandemic findings.16,17 In Catheterization Safety Checklist (Figure 1A), we found evidence of the A/B Effect: participants rated the A/B test significantly below the average ratings they gave to implementing interventions A and B (d = 0.69, 95% CI: (0.53, 0.85)). Here, 41% ± 5% (95% CI) of participants expressed experiment aversion (rating the A/B test lower than their own lowest-rated intervention; d = 0.25, 95% CI: (0.11, 0.39)). When ranking the three options from best to worst, only 32% placed the A/B test first, while 48% placed it last. We also observed an A/B Effect in Best Anti-Hypertensive Drug (Figure 1B); d = 0.52, 95% CI: (0.36, 0.68)), where 44% ± 5% also expressed experiment aversion (d = 0.46, 95% CI: . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 7 (0.30, 0.52)). Notably, participants were averse to this experiment even though there is no reason to prefer “Drug A” to “Drug B,” and patients are effectively already randomized to A or B based on which clinician happens to see them—which occurs wherever unwarranted variation in practice determines treatments, such as walk-in clinics and emergency departments. Here, however, similar proportions of people ranked the A/B test best and worst (50% vs. 45%; p = 0.16). These levels of experiment aversion near the height of the pandemic were slightly (but not significantly) higher than those we observed among similar laypeople in 2019 (41% ± 5% in 2020 vs. 37% ± 6% in 2019 for Catheterization Safety Checklist, p = 0.31 ; 44% ± 5% in 2020 vs. 40% ± 6% in 2019 for Best Anti-Hypertensive Drug, p = 0.32).17 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 8 (A) Catheterization Safety Checklist
(B) Ventilator Proning Background
Some medical treatments require a doctor to 
insert a plastic tube into a large vein. These 
treatments can save lives, but they can also lead 
to deadly infections. Some coronavirus (Covid-19) patients have to be 
sedated and placed on a ventilator to help them 
breathe. Even with a ventilator, these patients 
can have dangerously low blood oxygenation 
levels, which can result in death. Current 
standards suggest that laying ventilated patients 
on their stomach for 12-16 hours per day can 
reduce pressure on the lungs and might increase 
blood oxygen levels and improve survival rates. Intervention A
A hospital director wants to reduce these 
infections, so he decides to give each doctor who 
performs this procedure a new ID badge with a 
list of standard safety precautions for the 
procedure printed on the back. All patients 
having this procedure will then be treated by 
doctors with this list attached to their clothing. A hospital director wants to save as many 
ventilated Covid-19 patients as possible, so he 
decides that all of these patients will be placed 
on their stomach for 12-13 hours per day. Intervention B
A hospital director wants to reduce these 
infections, so he decides to hang a poster with a 
list of standard safety precautions for this 
procedure in all procedure rooms. All patients 
having this procedure will then be treated in 
rooms with this list posted on the wall. A hospital director wants to save as many 
ventilated Covid-19 patients as possible, so he 
decides that all of these patients will be placed 
on their stomach for 15-16 hours per day. A/B test
A hospital director thinks of two different ways 
to reduce these infections, so he decides to run 
an experiment by randomly assigning patients to 
one of two test conditions. Half of patients will 
be treated by doctors who have received a new 
ID badge with a list of standard safety 
precautions for the procedure printed on the 
back. The other half will be treated in rooms 
with a poster listing the same precautions 
hanging on the wall. After a year, the director 
will have all patients treated in whichever way 
turns out to have the highest survival rate. A hospital director thinks of two different ways 
to save as many ventilated Covid-19 patients as 
possible, so he decides to run an experiment by 
randomly assigning ventilated Covid-19 patients 
to one of two test conditions. Half of these 
patients will be placed on their stomach for 12-
13 hours per day. The other half of these patients 
will be placed on their stomach for 15-16 hours 
per day. After one month, the director will have 
all ventilated Covid-19 patients treated in 
whichever way turns out to have the highest 
survival rate. Table 1 Vignette text for Catheterization Safety Checklist and Ventilator Proning . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 9 Experiment 
Aversion
A/B Effect More people 
averse than 
appreciative? More people 
rank AB test 
worst than 
best? More people 
rank AB test 
best than 
worst? More people 
appreciative 
than averse? Reverse 
A/B Effect Experiment 
Appreciation Catheterization Safety Checklist
✓
✓
✓
✓ Best Anti-Hypertensive Drug
✓
✓
✓ Ventilator Proning
✓
✓
✓ School Reopening
✓
✓
✓ Masking Rules
✓
✓
✓
✓ Intubation Safety Checklist
✓
✓
✓
✓ Best Corticosteroid Drug
✓
✓ Best Vaccine
✓
✓ Masking Rules
✓
✓
✓
✓ Intubation Safety Checklist
✓
✓
✓
✓ Best Corticosteroid Drug
✓
✓
✓ Best Vaccine
✓*
✓ Table 2 Sentiments about experiments by vignette and population Note.   The A/B Effect refers to the difference between the average rating of the two interventions and the rating of the A/B test. Experiment Aversion refers 
to the difference between the lowest-rated intervention and the rating of the A/B test. The Reverse A/B Effect refers to the difference between the rating of the 
A/B test and the average rating of the two interventions. Experiment Appreciation refers to the difference between the rating of the A/B test and the rating of 
the highest-rated intervention. 
Checkmarks (✓) represent a stastically significant effect at p  < .05. In one case, the checkmark is followed by an asterisk (*). This indicates that while the 
effect reaches statistical significance, the effect size is very small and might have only reached significance due to the large sample size (three times as large 
as that for other vignettes).
Variables to the right of the thick vertical line are the reverse of those on the left. If no checkmark appears in either of the corresponding columns to the left 
and right of the thick vertical line (e.g., ""More people rank A/B test worst than best?"" and ""More people rank A/B test best than worst?""), that means that 
there is no significant difference (e.g., there is no statistically significant difference between the proportion of people ranked that A/B test worst and the 
proportion of people who ranked the A/B test best). Negative sentiment
Positive sentiment (B) Lay Sentiments About Covid-19 Healthcare Experimentation (C) Clinician Sentiments About Covid-19 Healthcare Experimentation (A) Lay Sentiments About Healthcare Experimentation 10 Figure 1 Lay Sentiments About Healthcare Experimentation Note. (A) Percentages of participants objecting to implementing intervention A, intervention B, and the A/B test (objecting was defined as assigning a rating of 1 or 2—”very inappropriate” or “somewhat inappropriate”— on a 1–5 scale). (B) Mean appropriateness ratings, on a 1–5 scale, with SEs, for intervention A, intervention B, the highest-rated intervention, the average intervention, the lowest-rated intervention, and the A/B test. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 11 Lay Sentiments About Covid-19 Specific Healthcare Experimentation In all six Covid-19 vignettes, we found evidence of the A/B Effect (Table 2B). In three, however, we did not find experiment aversion: Best Vaccine, Best Corticosteroid Drug, and School Reopening. In the first two of these, participants rated the two interventions very similarly and the experiment only slightly lower (Figure 2B). These vignettes also elicited the largest proportion of participants (65% in Best Vaccine and 56% in Best Corticosteroid Drug) in any vignette who ranked the A/B test best among the three options, compared to 31–34% of participants who ranked it worst. In School Reopening, experiment aversion was not observed because participants on average clearly preferred intervention B to A and rated the experiment similar to intervention A.26,27 53% of participants ranked intervention B as the best of the three options (compared to 17% choosing intervention A and 30% choosing the A/B test). In the other three vignettes, participants rated the A/B test condition as significantly less appropriate than their lowest-rated intervention (Masking Rules: d = 0.56, 95% CI: (0.41, 0.71); Ventilator Proning: d = 0.17, 95% CI: (0.04, 0.30); Intubation Safety Checklist: d = 0.36, 95% CI: (0.21, 0.49)). These levels of aversion to Covid-19 RCTs are similar to the levels of aversion to non-Covid-19 RCTs both before17 and during the pandemic (see above). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 12 Figure 2 Lay Sentiments About Covid-19 Specific Healthcare Experimentation Note. (A) Percentages of participants objecting to implementing intervention A, intervention B, and the A/B test (objecting was defined as assigning a rating of 1 or 2—”very inappropriate” or “somewhat inappropriate”— on a 1–5 scale). (B) Mean appropriateness ratings, on a 1–5 scale, with SEs, for intervention A, intervention B, the highest-rated intervention, the average intervention, the lowest-rated intervention, and the A/B test. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 13 Clinician Sentiments About Covid-19 Specific Healthcare Experimentation We observed an A/B effect in all four vignettes. In two, clinicians, like laypeople, were also significantly experiment averse (Masking Rules: d = 0.74, 95% CI: (0.57, 0.91); Intubation Safety Checklist: d = 0.30, 95% CI: (0.15, 0.45)). In Best Vaccine, clinicians, like laypeople, did not show any significant difference in their ratings of the A/B test and their lowest-rated intervention (d = –0.03, 95% CI: (–0.10, 0.04)). Again, like laypeople, 58% of clinicians ranked the vaccine A/B test as the best of the three options, the highest proportion of any clinician-rated vignette. Clinicians differed from laypeople in their response to Best Corticosteroid Drug. Laypeople did not show experiment aversion, but clinicians rated the A/B test as significantly less appropriate than their lowest-rated intervention (d = 0.49, 95% CI: (0.32, 0.66)). This difference may be due to clinicians’ greater familiarity with the treatment of Covid-19. Clinicians may also have seen an urgent need for any drugs to treat Covid-1920 and thus rated adopting a clear treatment intervention as more appropriate than an RCT. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 14 Figure 3 Clinician Sentiments About Covid-19 Specific Healthcare Experimentation Note. (A) Percentages of participants objecting to implementing intervention A, intervention B, and the A/B test (objecting was defined as assigning a rating of 1 or 2—”very inappropriate” or “somewhat inappropriate”— on a 1–5 scale). (B) Mean appropriateness ratings, on a 1–5 scale, with SEs, for intervention A, intervention B, the highest-rated intervention, the average intervention, the lowest-rated intervention, and the A/B test. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 15 Discussion We found no diminution in general experiment aversion among laypeople during the first year of the Covid-19 pandemic, despite increased exposure to the nature and purpose of RCTs. Neither laypeople nor clinicians were overall less averse to Covid-19 experiments, despite the fact that confidence in anyone’s knowledge of what works should have been even more circumscribed than in the everyday contexts of hypertension and catheter infections. To the contrary, we found an A/B effect (the average rating of the RCT was lower than the average rating of the two policies) in all vignettes and samples. Most Covid-19 vignettes were met with experiment aversion (on average, participants rated the RCT lower than each participant’s lowest-rated intervention). This is consistent with an emphasis during the pandemic that we must “do” instead of “learn,” a false dichotomy that fails to recognize that implementing an untested intervention is itself a nonconsensual experiment from which, unlike an RCT, little or nothing can be learned.28–30 Similarly, across all vignettes and samples, between 28% and 57% of participants demonstrated experiment aversion, while only 6%–35% demonstrated experiment appreciation (by rating the RCT higher than their highest-rated intervention). In none of our 12 studies were more people appreciative of than averse to the RCT, in none was the average RCT rating higher than the average intervention rating, and in none was the RCT rating higher than each participant’s highest-rated intervention, on average. Notably, unlike trials with placebo or no-contact controls, the A/B tests in our vignettes compared two active, plausible interventions, neither of which was obviously known ex ante to be superior. Yet substantial shares of participants still preferred that one intervention simply be implemented without bothering to determine which (if either) worked best. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 16 There is one bright spot in our results: the most positive sentiment towards experiments was observed in both laypeople and clinicians in the vignettes involving Covid-19 drugs and vaccines. Here we observed the highest proportions of participants who demonstrated experiment appreciation (31%–46%) and who ranked the RCT first (49%–65%). This result is consistent with our previous findings that the illusion of knowledge—here, the belief that either the participant herself or some expert already does or should know the right thing to do and should simply do it—biases people to prefer universal intervention implementation to RCTs.16,17 Rightly or wrongly, both laypeople and clinicians might (a) appropriately recognize that near the start of a pandemic, no one knows which existing drugs, if any, are safe and effective in treating a novel disease, and that new vaccines need to be tested, yet (b) fail to sufficiently appreciate the level of uncertainty around NPIs like masking, proning, and social distancing, which can also benefit from rigorous evaluation. This is consistent with the dearth of RCTs of Covid-19 NPIs:31 of the more than 4,000 Covid-19 trials registered worldwide as of August 2021, only 41 tested NPIs.32 Explaining critical concepts like clinical equipoise or unwarranted variation in medical and NPI practice alike might diminish experiment aversion. Critics note that RCTs have limited external validity when they employ overly selective inclusion/exclusion criteria or are executed in ways that deviate from how interventions would be operationalized in diverse, real-world settings. However, the solution is not to abandon randomized evaluation, but to incorporate it into routine clinical care and healthcare delivery via pragmatic RCTs.1,33 It has been many years since the Institute of Medicine urged research of many varieties to be embedded in care.34 More recently, the FDA established a Real-World Evidence Program that promotes pragmatic RCTs to support post-marketing monitoring and other regulatory decision-making.35,36 Pragmatic RCTs have been fielded successfully and . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 17 informed healthcare practice and policy,37–39 but they remain far from ubiquitous and they require buy-in to be successful, as shown by the case of a Denmark school reopening trial that was abandoned due to lack of such support.40 Wider use of pragmatic RCTs will require not only redoubling investment in interoperable electronic health records and recalibrating regulators’ views of the comparative risks of research versus idiosyncratic practice variation,1 but also anticipating and addressing experiment aversion among patients and healthcare professionals. Acknowledgements Supported by Office of the Director, National Institutes of Health (NIH) (3P30AG034532-13S1) and funded by the Food and Drug Administration (FDA). The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the FDA. We thank Daniel Rosica and Tamara Gjorgjieva for excellent research assistance. Vogt and Heck contributed equally to this work. Meyer and Chabris contributed equally to this work. Data availability Participant response data, preregistrations, materials, and analysis code have been deposited in Open Science Framework and will be released upon final publication of this paper. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288189
doi: 
medRxiv preprint 18",1
"Importance Investigating the role of pre-infection humoral immunity against Omicron BA.5 infection risk and long COVID development is critical to inform public health guidance. Objective To investigate the association between pre-infection immunogenicity after the third vaccine dose and the risks of Omicron BA.5 infection and long coronavirus disease. Design, Setting, and Participants This nested case-control analysis was conducted among tertiary hospital staff in Tokyo, Japan who donated blood samples in June 2022 (1 month before Omicron BA.5 dominant wave onset [July–September 2022]) approximately 6 months after receiving the third dose of the historical monovalent coronavirus disease 2019 mRNA vaccine. Exposures Live virus-neutralizing antibody titers against Wuhan and Omicron BA.5 (NT50) and anti-SARS-CoV-2 spike protein antibody titers with Abbott (AU/mL) and Roche (U/mL) assays at pre-infection. Main Outcomes and Measures Symptomatic SARS-CoV-2 breakthrough infections during the Omicron BA.5 dominant wave vs. undiagnosed controls matched using a propensity score. Incidence of long COVID (persistent symptoms ≥4 weeks after infection) among breakthrough infection cases. Results Anti-spike antibody titers were compared between 243 breakthrough infection cases and their matched controls among the 2360 staff members who met the criteria. Neutralizing antibodies in 50 randomly selected matched pairs were measured and compared. Pre-infection anti-spike and neutralizing antibody titers were lower in breakthrough cases than in undiagnosed controls. Neutralizing antibody titers against Wuhan and Omicron BA.5 were 64% (95% CI: 42–77) and 72% (95% CI: 53–83) lower, respectively, in breakthrough cases than in undiagnosed controls. Individuals with previous SARS-CoV-2 infections were more frequent among undiagnosed controls than breakthrough cases (19.3% vs. 4.1%), and their neutralizing antibody titers were higher than those of infection-naïve individuals. Among the breakthrough cases, pre-infection antibody titers were not associated with the incidence of long COVID. Conclusions and Relevance Pre-infection immunogenicity against SARS-CoV-2 may play a role in protecting against the Omicron BA.5 infection, but not in preventing long COVID. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 5 Introduction mRNA vaccines are effective in lowering the risk of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection;1 however, whether a higher level of vaccine-induced pre-infection humoral immunity is associated with a lower risk of SARS-CoV-2 infection remains unclear.2-4 In our previous nested case-control studies, neutralizing antibody titers 1–2 months after the second and third doses of vaccines were not associated with the subsequent risk of SARS-CoV-2 infection during the Delta3 and Omicron BA.1/BA.24 epidemics, respectively. This lack of association is partly attributed to the similarities among participants in the determinants of post-vaccine infection risk (same vaccine dose and a short period between vaccination and infection). The largest epidemic wave of Omicron BA.5 occurred between July and September 2022 in Japan, approximately 6 months after the onset of the third vaccination campaign, and recorded the highest weekly number of cases worldwide.5 This may be attributed to the high transmissibility of this variant6, 7 and the waning of third-dose vaccine immunogenicity over time.7, 8 We performed a serosurvey among the staff of the National Center for Global Health and Medicine (NCGM), Tokyo, Japan in June 2022 (1 month before the start of the Omicron BA.5 epidemic) and stored blood samples. This situation prompted us to test the hypothesis that pre-infection humoral immunity, which may vary considerably among participants 6 months after the third vaccination, can predict the risk of Omicron BA.5 infection. Beyond the role of pre-infection humoral immunity in preventing SARS-CoV-2 infection, its role for the development of long COVID is largely unknown. A low humoral immune response in the acute phase of SARS-CoV-2 infection has been linked to an increased risk of long COVID in non-vaccinated patients.9, 10 However, whether pre-infection vaccine-induced humoral immunity is associated with the risk of long COVID is unclear. Here, we compared the live-virus and pre-infection neutralizing antibody titers among staff members infected with SARS-CoV-2 during the Omicron BA.5 epidemic and their rigorously matched controls in a well-defined cohort of third-vaccine recipients. Additionally, we investigated the association between the pre-infection neutralizing capacity and long COVID. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 6 Methods Study setting A repeat serological study was conducted at the NCGM in Japan in July 2020 to monitor the spread of SARS-CoV-2 infection among staff during the COVID-19 epidemic. The details of this study have been reported elsewhere.3, 4 In summary, we have completed seven surveys as of December 2022, where we measured anti-SARS-CoV-2 nucleocapsid- (all surveys) and spike-protein antibodies (from the second survey onward) for all the participants using both Abbott and Roche assays, stored serum samples at -80, and collected information on COVID-19–related factors (vaccination, occupational infection risk, infection prevention practices, behavioral factors, etc.) via a questionnaire. We collected information on long COVID from participants with a history of COVID-19 via the seventh questionnaire survey conducted in December 2022. The self-reported vaccination status was validated using objective information provided by the NCGM Labor Office. We identified COVID-19 cases among the study participants from the COVID-19 patient records documented by the NCGM Hospital Infection Prevention and Control Unit, which provided information on the date of diagnosis, diagnostic procedures, possible route of infection (close contact), symptoms, hospitalizations, return to work for all cases, and virus strain and cycle threshold (Ct) values for those diagnosed at the NCGM. Written informed consent was obtained from all the participants. This study was approved by the NCGM Ethics Committee (approval number: NCGM-G-003598). Case-Control Selection We conducted a case-control study among the staff who participated in the sixth survey conducted in June 2022 and completed three doses of the monovalent mRNA COVID-19 vaccine (any dose pattern of BNT162b2 or mRNA-1273) (Figure 1). Of the 2,727 participants, 2,360 received three doses of the mRNA COVID-19 vaccines and donated blood samples. Of those, we identified 262 (11%) breakthrough infection cases, defined as those diagnosed at least 14 days after the third dose by September 21, 2022, using the in-house COVID-19 registry. We selected 243 patients with symptomatic SARS-CoV-2 infections as cases for analysis, after excluding 19 patients with asymptomatic infections. Finally, 243 cases and 2,098 undiagnosed participants formed the basis for the case-control study. We selected a control for each case using propensity score matching, and 243 matched pairs were selected and included in the analysis to compare pre-infection anti-spike antibody titers between cases and controls. The details of the case-control matching algorithm are described in eText 1. Of the 243 matched pairs, we randomly selected 50 pairs and measured live virus–neutralizing antibody titers to compare neutralizing antibodies between the groups. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 7 Antibody Testing Neutralizing activity against Wuhan and Omicron BA.5 in the sera of patients and controls was determined by quantifying the serum-mediated suppression of the cytopathic effect of each SARS-CoV-2 strain in HeLahACE2-TMPRSS2 cells.11, 12 The details of the measurement methods are described in eText 2. We assessed anti–SARS-CoV-2 antibodies in all the participants at baseline and retrieved data for the case-control pairs. We quantitatively measured the levels of antibodies against the receptor-binding domain of the SARS-CoV-2 spike protein using the AdviseDx SARS-CoV-2 IgG II assay (Abbott) (immunoglobulin [Ig] G) and Elecsys® Anti-SARS-CoV-2 S RUO (Roche) (including IgG). We also qualitatively measured antibodies against the SARS-CoV-2 nucleocapsid (N) protein using the SARS-CoV-2 IgG assay (Abbott) and Elecsys® Anti-SARS-CoV-2 RUO (Roche) to determine those with a possible infection before the baseline survey. Long COVID We defined long COVID as reporting SARS-CoV-2-related symptoms for ≥4 weeks after the SARS-CoV-2 infection, according to the definition of the Centers for Disease Control and Prevention (CDC).15 We asked patients with SARS-CoV-2 infections the following question in a follow-up survey conducted in December 2022: “Have you had any symptoms persisting for 4 weeks (28 days) or more since you were infected with COVID-19?” The participants who answered yes were asked about their symptoms in detail. The participants were informed that symptoms persisting for 4 weeks while resolving or returning should also be included, whereas those that were apparently due to other illnesses should not be included. We created the following six categories for the analysis, in accordance with the CDC guidelines15: (1) any of the long COVID symptoms; (2) general symptoms (tiredness/fatigue or fever); (3) respiratory and cardiac symptoms (difficulty breathing, cough, chest pain, or heart palpitations); (4) neurological symptoms (difficulty thinking/concentrating, headache, sleep problems, changes in smell/taste, or depression/anxiety); (5) digestive symptoms (diarrhea or stomach pain); and (6) other symptoms (joint/muscle pain, rash, or others). Among the 243 cases with symptomatic breakthrough infections included in the case-control analysis, we included 166 cases who participated in the follow-up survey and answered the questionnaire regarding long COVID in the analysis of the association between pre-infection antibody titers and long COVID development. Statistical Analyses We compared the log-transformed titers of neutralizing (Wuhan and Omicron BA.5) and anti-spike antibodies between . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 8 matched pairs using a generalized estimating equation (GEE) with group assignment (case or control) and a robust variance estimator to examine the difference in pre-infection antibody levels between cases and controls. Next, we back-transformed and presented these values as geometric mean titers (GMTs) with 95% confidence intervals (CIs). We repeated the analysis by restricting those who were infection-naïve at baseline (no history of COVID-19 and negative anti-N assays at baseline) for sensitivity analysis. We compared the titers between infection-naïve, previously diagnosed infection (history of COVID-19), and previously undiagnosed infection (anti-SARS-CoV-2 N seropositive on Abbott or Roche assays at baseline without a history of COVID-19) using a linear regression model adjusted for age, sex, and the interval between vaccination and blood sampling to examine the association between previous SARS-CoV-2 infection status and baseline antibody titers. We ran a linear regression model with adjustments for age, sex, previous SARS-COV-2 infection status, and the interval between vaccination and blood sampling to examine the association between pre-infection antibody titers and long COVID. We used the Kruskal–Wallis test with Dunn’s multiple comparisons to compare the neutralizing ratio of Omicron BA.5 to Wuhan NT50 between those with and without previous infection. Statistical analyses were performed using Stata version 17.0 (StataCorp LLC), and graphics were generated using GraphPad Prism 9 (GraphPad, Inc.). All P-values were 2-sided, and the statistical significance was set at P<0.05. Results Baseline characteristics in the unmatched and matched cohorts We ascertained 243 symptomatic breakthrough infection cases during the follow-up in the unmatched cohort of third-dose recipients, with an incidence rate of 13.0 per 10000 person-days. All patients had mild symptoms, and only three were admitted to the hospital. All 94 cases with available information on the SARS-CoV-2 strain type were Omicron variants. Of these, 2% were estimated to be Omicron BA.1; 5%, Omicron BA.2; and 85%, Omicron BA.4/BA.5, whereas the subvariants of the remaining 7% could not be determined. Patients were more likely to be younger, nurses, and at moderate or high risk of occupational SARS-CoV-2 exposure than were the controls in the unmatched cohort (Table 1). The 243 matched pairs were well-balanced regarding all the baseline characteristics after propensity matching with a 1:1 ratio. Patients were less likely than controls to have had a previously diagnosed infection during the Omicron BA.1/BA.2 waves (1.2% vs. 11.1%) and a previously undiagnosed infection (2.9% vs. 8.2%) (Table 2). The interval between the third dose and blood sampling did not show a significant difference among the groups; the median intervals for cases and controls were 174 days (interquartile range [IQR]:153–184) and 173 days (IQR:153–183), respectively. The type of . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 9 mRNA vaccine received did not differ between cases and controls, and most cases (91.4%) and controls (93.0%) received three doses of BNT162b2. Correlation between anti-spike and neutralizing antibodies Anti-spike antibody titers with the Abbott and Roche assays highly correlated with neutralizing antibody titers against Wuhan (Spearman’s ρ:0.82 and 0.84, respectively) and those against Omicron BA.5 (Spearman’s ρ:0.80 and 0.85, respectively) (eFigure 1). Pre-infection antibody titers between the matched cases and controls Pre-infection anti-spike and neutralizing antibody titers were lower in patients than in controls. The GEE-predicted GMTs (95% CI) of the anti-spike antibody on Abbott assay (AU/ml) was 4711 (4249–5223) for cases and 8045 (7003–9242) for controls with a predicted case-to-control ratio of the titers of 0.59 (95% CI:0.49–0.69) (Table 2 and Figure 1). The GMTs (95% CI) of the anti-spike antibody on Roche assay (U/mL) were 5029 (4578–5525) for cases and 8297 (7291–9442) with a ratio of 0.61 (95% CI:0.52–0.71). The predicted neutralizing antibody GMTs (95% CI) against Wuhan (NT50) were 700 (506–969) for cases and 1933 (1325–2822) for controls, with a ratio of 0.36 (95% CI:0.23–0.58). Those against Omicron BA.5 were 65 (50–84) for cases and 228 (152–342) for controls, with a ratio of 0.28 (95% CI:0.17–0.47). The difference in pre-infection antibody titers between cases and controls was attenuated in the sensitivity analysis restricting cases and controls to infection-naïve pairs; however, titers were still statistically lower in cases, except for neutralizing titers against Wuhan (Table 2). Previous SARS-CoV-2 infection status and antibodies Baseline antibody titers were higher in previously diagnosed and undiagnosed infected individuals than in infection-naïve individuals, whereas the diagnosed and undiagnosed groups did not show significant differences in the titers (Figures 2A-D). Compared with infection-naïve individuals, diagnosed and undiagnosed infected individuals had 4.8- and 4.4-fold higher anti-spike antibody titers using the Abbott assay, 4.2- and 3.5-fold higher titers using the Roche assay, 8.2- and 4.5-fold higher neutralizing titers against Wuhan, and 12.8- and 6.7-fold higher titers against Omicron BA.5, respectively. The neutralizing antibodies titers against Omicron BA.5 were considerably lower than those against Wuhan in the total samples, with a GMT ratio of 0.10 for Omicron BA.5 to Wuhan (95% CI:0.08–0.13). The GMT ratio was greater for diagnosed (0.15, 95% CI:0.09–0.24) and undiagnosed (0.16, 95% CI:0.09–0.28) infection groups than for the infection-naïve group (0.10, 95% CI:0.08–0.12) (Figure 2E). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 10 Pre-infection antibody titers and long COVID The proportion of long COVID cases was 26.5% (95% CI:20.0–33.9). Respiratory and cardiac (18.7%), general (12.7%), and neurological (7.8%) symptoms were the most frequent. None of the patients reported digestive symptoms. Pre-infection antibody titers did not show material differences between those who reported any of the long-term COVID symptoms and those who did not (Table 3). Regarding the long-COVID symptoms, general, respiratory, cardiac, neurological, gastrointestinal, and other symptoms did not show differences between the groups. Discussion The pre-infection neutralizing antibodies against Omicron BA.5 were lower in infected cases during the Omicron BA.5 wave than in the matched controls in this nested case-control study of a cohort of healthcare workers approximately 6 months after the third vaccination. The pre-infection neutralizing capacity did not show material differences between the breakthrough cases who reported long COVID and those who did not. This is the first study to investigate the association of pre-infection neutralizing antibody titers with the risk of Omicron BA.5 infection and development of long COVID. The significant association of pre-infection antibody titers with Omicron BA.5 infection in this study contrasts with the findings of our previous studies, where no associations were found with the risk of Delta infection among individuals approximately 2 months after the second dose3 and with the risk of Omicron BA.1/BA.2 infection among those approximately 1 month after the third dose.4 This may be attributed to the difference in variability of humoral immunity levels at the time of blood sampling across the surveys. Specifically, the coefficient of variation (CV) of anti-spike (Abbott) and Wuhan neutralizing antibody titers in blood samples in the present study (133% and 233%, respectively) were larger than those for Delta (69% and 69%, respectively) and Omicron BA.1/BA.2 (56% and 90%, respectively) infections (eTable 3). Among the third-dose recipients without a history of COVID-19, those approximately 6 months after vaccination (present study) had 4.5-fold lower mean antibody titers (Abbott) but a 1.9-fold greater CV than those approximately 1 month after vaccination (data from a previous study on Omicron BA.1/BA.2 infection). These results are compatible with longitudinal studies showing a large variation in the speed of waning of anti-SARS-CoV-2 antibodies . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 11 across vaccine recipients with different characteristics.16-18 Antibody levels can be used as a predictor for infection risk in populations with large variations in humoral immunity across individuals. In this study, those with previous SARS-CoV-2 infection (mostly Omicron BA.1/BA.2) among three-dose vaccine recipients had higher neutralizing antibody titers against Omicron BA.5 and a greater neutralizing ratio of Omicron BA.5 to Wuhan than those who were infection-naïve, in agreement with the findings of previous studies.18, 19 The proportion of previous Omicron BA.1/BA.2 infection was higher in controls than in breakthrough cases, consistent with the results of a previous study.20 These results suggest that the Omicron BA.5 neutralizing capacity induced by previous Omicron BA.1/BA.2 infections lowers the risk of Omicron BA.5 infection among the three-dose recipients and is compatible with the findings of observational studies indicating that hybrid immunity (three vaccinations plus previous Omicron BA.1/BA.2 infection) has higher effectiveness against Omicron BA.5 infection than only three vaccinations.20 We confirmed that Omicron BA.5 neutralizing antibody titers were modestly but significantly lower in cases than in controls in the analysis of infection-naïve vaccine recipients, suggesting that the level of humoral immunity induced by historical mRNA vaccine alone can also predict, albeit to a lesser extent than hybrid immunity, the infection risk of Omicron BA.5 with a high immune-evasive nature. In this study, 26.5% (95% CI:20.0–33.9) of patients who received three vaccinations and were infected during the Omicron BA.5-predominate wave experienced long COVID, which is somewhat higher the proportion among US adults who were infected with Omicron BA.4/BA.5 after receiving the third vaccine dose (20.9%, 95% CI:16.4–26.2).21 We found no association between pre-infection antibody titers and the risk of long COVID. An Itarian study reported that anti-spike IgG titers measured during the acute infection phase did not predict long COVID in vaccinated patients with or without hospitalization.22 Evidence for the association between vaccination status and long COVID risk is also inconsistent.21, 23 Our results and previous reports21-23 suggest that vaccine-induced immunity has no apparent protective role against post–COVID-19 symptoms. This study had several strengths. We rigorously matched cases and controls using a propensity score estimated by several factors potentially associated with SARS-CoV-2 infection risk, including occupational SARS-CoV-2 exposure risk, living arrangements, comorbidities, infection prevention practices, and high infection risk–behaviors. Blood samples for antibody testing were obtained before infection (1 month before the Omicron BA.5 epidemic onset). Previous SARS-CoV-2 infection was determined according to the history of COVID-19 infection and results of anti-N assay tests, . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 12 allowing us to identify undiagnosed infections. We measured the neutralizing antibody titers against Wuhan and Omicron BA.5 using live viruses. This study had some limitations. We did not conduct active surveillance to detect SARS-CoV-2 infection during the follow-up period. Nonetheless, we confirmed that the results were virtually unchanged in the sensitivity analysis, which excluded individuals who tested seropositive in anti-N assays in the follow-up survey from the control group (eTable 2). Data on virus strain were available for 39% of the cases; however, the remaining cases of breakthrough infections were most likely due to the Omicron BA.5 variant, which accounted for more than 90% of sequenced COVID-19 samples in Japan during the follow-up (July to September 2022).24 Lower levels of pre-infection humoral immunity have been linked to severe forms of COVID-19,25, 26 which may increase the risk of long COVID.27 Our results regarding long-COVID symptoms cannot be applied to patients with severe symptoms, since all the included patients with COVID-19 had mild symptoms. The sample size for the analysis of the relationship between antibody titers and long COVID (n=166) may be insufficient to detect a significant effect. Conclusion Pre-infection and live-virus neutralizing antibody titers against Omicron BA.5 were lower in breakthrough infection cases than in their matched controls during the Omicron BA.5– dominant wave among the third-dose recipients (mainly three doses of the BNT162b vaccine) approximately 6 months post-vaccination. The high neutralizing capacity of individuals with a history of Omicron BA.1/BA.2 infection was a substantial cause of these differences. Pre-infection neutralizing antibody titers were not associated with the risk of long COVID. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 13 Acknowledgments: Author Contributions Drs. Yamamoto and Mizoue had full access to all data in the study and took responsibility for the integrity of the data and the accuracy of the data analysis. Drs Yamamoto and Matsuda contributed equally to this article. Concept and design: Yamamoto S, Matsuda K, Maeda K, Mizoue T, Sugiyama H, Mitsuya H, Sugiura W, Ohmagari N. Acquisition, analysis, or interpretation of data: Yamamoto S, Matsuda K, Maeda K, Horii K, Okudera K, Oshiro Y, Inamura N, Nemoto T, Takeuchi S. J, Li Y, Konishi M, Ozeki M, Mizoue T. Drafting of the manuscript: Yamamoto S, Matsuda K, Mizoue T. Critical revision of the manuscript for important intellectual content: Yamamoto S Matsuda K, Maeda K, Takeuchi S. J, Mizoue T, Sugiyama H, Aoyanagi N, Mitsuya H, Sugiura W, Ohmagari N. Statistical analysis: Yamamoto S, Matsuda K, Konishi M. Administrative, technical, or material support: Yamamoto S, Matsuda K, Maeda K, Horii K, Okudera K, Oshiro Y, Inamura N, Nemoto T, Takeuchi S. J, Konishi M, Ozeki M, Tsuchiya K, Gatanaga H, Oka S, Mizoue T, Sugiyama H, Aoyanagi N, Mitsuya H, Sugiura W, Ohmagari N. Supervision: Mizoue T, Ohmagari N. Conflict of Interest Disclosures All authors: No reported conflicts of interest. Funding/Support This work was supported by the NCGM COVID-19 Gift Fund (grant number 19K059), Japan Health Research Promotion Bureau Research Fund (grant number 2020-B-09), and National Center for Global Health and Medicine (grant number 21A2013D). Abbott Japan and Roche Diagnostics provided reagents for the anti-SARS-CoV-2 antibody assays. Role of the Funder/Sponsor The above entities had no role in the design or conduct of the study; collection, management, analysis, and interpretation of the data; preparation, review, or approval of the manuscript; or the decision to submit the manuscript for publication. Additional Contributions . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 14 We thank Mika Shichishima for her contribution to data collection and the staff of the Laboratory Testing Department for their contribution to antibody testing. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288162
doi: 
medRxiv preprint 15",1
"We measured brain injury markers, inflammatory mediators, and autoantibodies in 203 participants with COVID-19; 111 provided acute sera (1-11 days post admission) and 56 with COVID-19-associated neurological diagnoses provided subacute/convalescent sera (6-76 weeks post-admission).  Compared to 60 controls, brain injury biomarkers (Tau, GFAP, NfL, UCH-L1) were increased in acute sera, significantly more so for NfL and UCH-L1, in patients with altered consciousness. Tau and NfL remained elevated in convalescent sera, particularly following cerebrovascular and neuroinflammatory disorders. Acutely, inflammatory mediators (including IL-6, IL-12p40, HGF, M-CSF, CCL2, and IL-1RA) were higher in participants with altered consciousness, and correlated with brain injury biomarker levels.  Inflammatory mediators were lower than acute levels in convalescent sera, but levels of CCL2, CCL7, IL-1RA, IL-2Rα, M- CSF, SCF, IL-16 and IL-18 in individual participants correlated with Tau levels even at this late time point. When compared to acute COVID-19 patients with a normal GCS, network analysis showed significantly altered immune responses in patients with acute alteration of consciousness, and in convalescent patients who had suffered an acute neurological complication. The frequency and range of autoantibodies did not associate with neurological disorders.  However, autoantibodies against specific antigens were more frequent in patients with altered consciousness in the acute phase (including MYL7, UCH-L1, GRIN3B, and DDR2), and in patients with neurological complications in the convalescent phase (including MYL7, GNRHR, and HLA antigens).  In a novel low-inoculum mouse model of SARS-CoV-2, while viral replication was only consistently seen in mouse lungs, inflammatory responses were seen in both brain and lungs, with  significant increases in CCL4, IFNγ, IL-17A, and microglial reactivity in the brain. Neurological injury is common in the acute phase and persists late after COVID-19, and may be driven by a para-infectious process involving a dysregulated host response. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 3 Introduction At the beginning of the COVID-19 pandemic, neurological complications occurred in approximately one-third of hospitalised patients1 and even in those with mild COVID-19 infection2.  Whilst these neurological ‘complications’ were often mild (headache and myalgia), it became clear that more significant neurological sequelae were observed, including encephalitis/encephalopathies, Guillain Barre Syndrome, and stroke3–5. Although in vitro studies show that SARS-CoV-2 can infect neurons and astrocytes6,7, autopsy studies indicate that direct viral invasion is unlikely to be a cause of neurological dysfunction in vivo.  Post-mortem studies failed to detect viral infection by immunohistochemistry in the majority of cases, and viral qPCR levels were often low and may simply have  reflected viraemia8–10. In addition, virus and/or anti-viral antibodies were rarely found in cerebrospinal fluid (CSF)11.  Thus, it seems more likely that the virus affects the brain indirectly. This could be through peripherally generated inflammatory mediators, immune cells, autoantibodies and/or blood brain barrier changes associated with endothelial damage12,13.  Immune infiltrates have been found in autopsy studies, including neutrophils and T cells, although agonal effects could not be excluded14. Elevated IL-6 and D-dimer levels at admission are associated with neurological complications, including thrombosis, stroke, cognitive and memory deficits, regardless of respiratory disease severity15–17. The brain injury markers NfL and GFAP, and inflammatory cytokines are elevated in COVID-19 and scale with severity, but their relationship to neuropathology has not been reported18–21.  Finally, specific neuronal autoantibodies have been reported in some neurological patients raising the possibility of para- or post-infectious autoimmunity12,22. To assess the relationship between host response and biomarkers of neurological injury, we studied two large, multisite cohorts which, in combination, provided acute, subacute and convalescent sera from COVID-19-positive (COVID+ve) participants.  We measured brain injury markers, a range of cytokines and associated inflammatory mediators, and autoantibodies, in these samples, and related them to reduced levels of consciousness (defined as a Glasgow Coma Scale Score [GCS] GCS≤14) in the acute phase, or the history of a neurological complication of COVID-19  in convalescent participants.  We further explored these relationships between host response and brain injury in a novel mouse model of low-inoculum intranasal SARS-CoV-2 infection. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 4 Results COVID-19 results in acute elevation of serum brain injury biomarkers, more so in participants with abnormal Glasgow coma scale (GCS) score. We used sera from the ISARIC study, obtained 1-11 days post admission, including 111 participants with COVID-19 of varying severity and 60 COVID-19-ve healthy controls (labelled Control).  Participants were stratified by normal (n = 76) or abnormal (n = 35) Glasgow coma scale (labelled GCS=15 or GCS≤14, respectively) scores to provide a proxy for neurological dysfunction (Fig. 1a).  GFAP (glial fibrillary acidic protein, marker of astrocyte injury), UCH-L1 (a marker of neuronal cell body injury), and NfL (neurofilament light) and Tau (both markers of axonal and dendritic injury) were measured. Overall, serum levels of NfL, GFAP, and Tau were significantly higher in COVID-19 participants compared to the COVID-19–ve healthy controls, but as shown in Fig. 1b-e, those participants with abnormal GCS scores had higher levels of NfL and UCH-L1 than those with normal GCS scores. Thus, all four biomarkers were raised in COVID-19 participants (both GCS=15 and GCS≤14) but, in addition, axonal and neuronal body injury biomarkers discriminated between participants with and without reduced GCSs. Brain injury biomarkers remain elevated in the subacute and convalescent phase in participants who have had a CNS complication of COVID-19 To ask whether these findings persisted in participants recovering from COVID-19-related neurological complications, ninety-two COVID-19+ve subjects were recruited to the COVID- Clinical Neuroscience Study (COVID-CNS), 56 who had had a new neurological diagnosis that developed as an acute complication of COVID-19 (group labelled “neuro-COVID”), and 36 with no such neurological complication (group labelled “COVID”, Fig. 1f, Table 1, Extended Data Table 1 and 2). When compared to the same healthy controls (n = 60), across all time-points, both COVID-19+ve subgroups (COVID and neuro-COVID) showed increased levels of NfL and Tau (but not UCH-L1 or GFAP (Fig. 1g-j, Extended Data Table 1). Furthermore, participants recovering from neuro-COVID had significantly higher levels of NfL, and a trend towards higher levels of Tau, than the COVID participants (Fig. 1g,j). Highest NfL serum levels were present in . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 5 participants with cerebrovascular conditions, whereas Tau was elevated in participants with cerebrovascular, CNS inflammation and peripheral nerve complications (Fig. 1k,l). We then separately compared the two cohorts at subacute and convalescent follow up periods (less than and over six weeks respectively).  NfL and GFAP levels remained elevated in all COVID-19 participants in the subacute period, but only remained elevated beyond 6 weeks in participants who had suffered an acute neurological complication (neuro-COVID, Fig. 1 m-p; Extended Data Fig. 1a).  The presence of elevated brain injury biomarkers in the acute phase of COVID-19 confirms previous findings,12 but the elevated levels of NfL and GFAP in those who had acute neurological complications suggests ongoing neuroglial injury, months after the acute illness. Clinical and biomarker evidence of neurological insult levels are associated with levels of innate inflammatory mediators in the acute phase of COVID-19 To explore whether the acute and persistent elevation of brain injury biomarkers we observed in participants with COVID-19 was associated with an acute inflammatory response, we measured a panel of 48 inflammatory mediators in serum at the same time points. In the ISARIC samples, six mediators were significantly higher in participants with an abnormal GCS than in those with a normal GCS (interleukin [IL]-6, hepatocyte growth factor [HGF], IL-12p40, IL-1RA, CCL2 and macrophage colony stimulating factor [M-CSF]), indicating increased innate inflammation (Fig. 2a, Extended Data Fig. 2a). Pearson’s correlation tests identified correlations between these significant biomarkers in an interrelated pro-inflammatory network (Fig. 2b,c), and unsupervised Euclidean hierarchical cluster analysis revealed clusters of pro-inflammatory mediators elevated together (Fig. 2d). The first cluster incorporated the IL-1 family (including IL-1RA), interferons and M-CSF, and the second cluster included IL-6, CCL2, CXCL9, HGF, and IL-12p40 (boxes in Fig. 2d).  Brain injury biomarkers correlated with elevations in these inflammatory mediators: GFAP and UCLH-1 correlated with a number of mediators, while Tau and NfL correlated strongly with HGF and IL-12p40 in the second cluster (Extended Data Table 3). A more stringent analysis of median-centred cytokine data (which corrected for within-participant skewing of mediator levels) confirmed that HGF and IL-12p40 were higher in the abnormal GCS COVID-19 participants, and correlated with cognate NfL levels (Extended Data Table 4). Taken . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 6 together these data suggest that activation of the innate immune system was related to both clinical and blood biomarker evidence of CNS insult. Inflammatory mediators are not elevated across the participant cohort at late timepoints after COVID-19; but late Tau elevations correlate with levels of several inflammatory mediators In contrast to the acute data, the levels of cytokines and associated mediators were lower when measured during the subacute and convalescent periods even in those who had suffered neurological complications of COVID-19 (group labelled “neuro-COVID”. Extended Data Fig.2b). Indeed IL-18 was downregulated in the neuro-COVID cases compared to controls (Extended Data Table 1, Extended Data Fig. 2c; also with median-centred data analysis), and the correlations between cytokines and associated mediators no longer displayed the same tight clusters (Fig. 2e,f).  GFAP remained elevated during the convalescent phase of neurological complications (Fig. 1p) but did not show correlations with the inflammatory mediators. Similarly NfL was higher overall in those with neurological complications (Fig. 1n) but there were no significant correlations with inflammatory mediators (Fig. 2f). However, Tau remained elevated overall in those with neurological complications ((1.7 (1.3, 2.2) pg/mL versus 1.3  (1.1, 1.9) pg/mL)) and levels correlated with eight immune mediators including CCL2, IL-1RA, IL-2Rα and M-CSF along with CCL7, stem cell factor (SCF), IL-16 and IL-18 (Fig. 2f, Extended Data Table 5, Extended Data Fig. 2d).  This last association was specific to the late phase of the illness and was not found in acute COVID-19. Cytokine networks are significantly altered in patients with neurological complications of COVID- 19: both acute encephalopathy, and those recovering from a neurological complication We used graph theoretical approaches to characterise cytokine networks in our three groups of patients: acute COVID-19 with a normal GCS; acute COVID-19 with altered consciousness (GCS≤14), and convalescent patients recovering from a neurological complication of COVID-19 (neuro-COVID).  Participants with both acute and chronic neurological consequences of COVID- 19 (GCS≤14) and Neuro-COVID both showed cytokine networks that were different from COVID-19 patients with no neurological problems (Fig. 2 b,c,e; p<0.001, Steiger test), suggesting a specific dysregulated innate immune response that is associated with both acute and chronic neurological complications of the illness. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 7 COVID-19 is associated with an acute polyclonal inflammatory response overall and with autoantibodies to viral antigen and some CNS autoantigens in those with an abnormal GCS score. Given past reports of autoantibody responses following COVID-1912,22, we sought evidence of similar dysregulated adaptive immune responses in our participant cohorts. We used a bespoke protein chip array of 153 viral and tissue proteins to measure IgM and IgG reactivity in the acute phase ISARIC sera.  The data from individual participants were normalized and the median fluorescence intensity per participant and Z-scores of each participant were compared to healthy control data, to determine positive reactivity to the different antigens (with a threshold for detection set at three standard deviations above COVID-ve controls for each antigen; see Extended Data Table 6, Extended Data Fig.3a, 4a).  IgM and IgG responses in COVID+ve participants showed greater reactivity overall (both GCS=15 and GCS≤14), compared to the controls, with no difference in normalised fluorescence Z scores or the number of participants with IgG ‘hits’ (a Z- score >3) between those with normal or abnormal GCS score (Fig. 3a,b, Extended Data Fig. 3a). However, several IgM and IgG autoantibodies, including against the CNS antigens UCH-L1, GRIN3B and DRD2, along with the cardiac antigen, myosin light chain (MYL)-7, were present in a greater proportion of participants with an abnormal GCS score, as were antibodies to spike protein (Fig. 3c, Extended Data Fig. 3b).  None of the antibodies correlated significantly with levels of brain injury markers (Extended Data Fig. 3c), but they did show correlations with each other (Fig. 3d), suggesting a non-specific antibody response in some individuals during the acute phase. Normalized fluorescence Z scores of serum autoantibodies in the subacute and convalescent samples were similar (Fig. 3e) to those in the acute samples (Fig. 3a), and the IgG ‘hits’ were more frequent than in controls, but were not different between COVID-19 patients with or without neurological complications (COVID and neuro-COVID, Fig. 3f, Extended Data Fig. 4a). However, specific autoantibody responses to MYL7, gonadotrophin releasing hormone receptor (GNRHR) and several HLA antigens had a frequency difference in the neuro-COVID participants (Fig. 3g, Extended Data Fig. 4b). As in the acute phase, autoantibody responses did not show significant associations with brain injury markers, but did tend to correlate with each other (Fig. 3h, Extended Data Fig. 4c)). Sera from acute COVID-19 participants with CNS antigen reactivity were incubated with sections of rat brain, neurons and antigen-expressing cells to explore binding . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 8 to neuronal antigens. Binding to rat brain sections identified a small number of participants with strongly positive and characteristic immunohistochemical staining who might have CNS autoreactivity (Fig. 3i) and overall, sera from neuro-COVID participants showed more frequent binding to brainstem regions than control sera, but this did not relate to the GCS of the participants (Fig. 3j, Extended Data Fig. 5a). Infection with low dose SARS-CoV-2 in a mouse model results in acute parainfectious brain pathology in the absence of active viral replication Our results from the clinical samples suggested immune responses in COVID-19, some of which were associated with neurological injury (as defined by brain injury biomarker levels) or neurological dysfunction (defined by a reduced GCS or an acute COVID-19-related neurological complication).   In order to further elucidate the mechanisms responsible for the acute neuroglial insult in COVID-19, we established a mouse model of para-infectious brain injury using intranasal infection with a low inoculum of SARS-CoV-2 (1x103 PFU) and compared this to a high inoculum (1x104 PFU); with assessment at day 5 post-infection (Fig. 4a). Both levels of infection caused pathology in the lung, with viral loads evidenced by qPCR of N1 in both low and high inocula of infection, but no weight loss (Fig. 4b, Extended Data Fig. 6a). Also, both resulted in evidence of active viral replication in the lung, as defined by qPCR of subgenomic E (Fig. 4c). Lung tissue from both high and low inoculum mice showed mononuclear cell infiltration which scaled with the severity of infection (Fig. 4d). When lung tissue from low-inoculum mice was examined by immunofluorescent staining and confocal microscopy, large amounts of viral protein were present and Iba1 expression was increased (Fig. 4e). These animals also showed increases in CD45 and CD11b staining in the lungs as opposed to uninfected animals (Fig. 4f, Extended Data Fig. 6b). H&E staining of brain sections from infected mice showed clusters of mononuclear cells in the frontal cortex, which scaled in proportion to the dose of inoculum of SARS-CoV-2 (Fig. 4g; Extended Data Fig. 6c). SARS-CoV-2 N1 transcript was detected in the brain of mice which received high and low inoculum of SARS-CoV-2 in four out of five and six out of nine animals, respectively (Fig. 4h shows experiment one). However, whilst three of four mice which received the high inoculum showed active viral transcription in the brain, as evidenced by the detection of subgenomic E, this was only detectable in two of nine at the low inoculum (tissue from these two . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 9 animals were not included in subsequent ex vivo experiments) (Fig. 4i). Confocal microscopy of ex vivo brain sections from mice following the low inoculum demonstrated Iba-1 staining but, in contrast to lung tissue, showed no evidence of staining for SARS-CoV-2 spike protein (Fig. 4j). Brains from low inoculum SARS-CoV-2 infected mice show increases in microglial and astrocyte activation despite the absence of active SARS-CoV-2 replication In order to understand the mechanisms driving this para-infectious neuropathology, we assessed transcription and protein levels of inflammatory mediators in brains and lung from the seven low- inoculum infected mice that showed no evidence of local viral replication in the brain. Lung tissue revealed four out the six mediators of interest, as assessed by the clinical studies, to be increased with low-inoculum infection; IL-RA, IL-6, CCL2, and IL-12p40 (Fig. 5a,b). Interestingly, even though brains from these mice showed no detectable viral proteins (checked by spike staining) and no detectable viral transcription (checked using subgenomic E), these brains showed reactive microglia, with increased Iba1 expression and ramification indices (Fig. 5c-g). These brains showed no increases in numbers of CD45+, CD11b+, CD3+, and or NK1.1+ cells (Extended Data Fig. 6d,e,f), or apoptotic cells (Extended Data Fig. 6g). Interestingly, clusters of GFAP+ astrocytes were found in the regions of high Iba1 expression in brain, suggesting concomitant microgliosis and astrogliosis, as has been reported in human post-mortem samples (Extended Data Fig. 7a). Assessment of changes in inflammatory mediators by transcription, protein levels, and immunostaining yielded mixed results. The brains from low dose infected mice showed increased transcripts of CCL4 and decreased levels of IL-1RA and IL-12p40 (Fig. 5h,i) while both IFNγ and IL-17A were increased at the protein level in the low-inoculum infected mice (Fig. 5j,k). The brain/serum albumin ratio in low dose infected animals was higher than controls, but this did not achieve significance (Extended Data Fig. 7b). There were no significant differences in NfL, which may potentially be due to the early assessment timepoint (day 5 post infection) (both by ELISA and Simoa, Extended Data Fig. 7c). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 10 Discussion 
 
We used several approaches to study neurological complications of COVID-19 infection. These included assessment of pathophysiology in participants with and without neurological complications, both in the acute and convalescent phases after COVID-19 infection; and examining systemic and neurological consequences in a para-infectious mouse model of SARS- CoV-2- associated encephalopathy.  These complementary approaches allowed characterisation of pathophysiology and insights into pathogenesis. We demonstrated increased levels of brain injury biomarkers following COVID-19 infection, which showed specific patterns with disease phase (acute or convalescent), and varied with the presence or absence of neurological injury or dysfunction.  In the acute phase, all four brain injury biomarkers (GFAP, NfL, Tau and UCH-L1) were elevated in participants when compared to controls, and specific markers of dendritic and axonal injury (Tau and NfL) were significantly higher in participants who showed a reduced level of consciousness (GCS≤14).  In the subacute phase (<6 weeks post-infection), GFAP, NfL, and Tau were elevated in participants recovering from COVID-19, with no differences between those who had and had not sustained a neurological complication of disease.  However, at late time points (>6 weeks) elevations NfL and GFAP were only seen in participants who had sustained a neurological complication of COVID-19 in the acute phase of their illness. In the acute phase, when compared to controls, we also observed increases in a range of inflammatory mediators (IL-6, HGF, IL-12p40, IL-1RA, CCL2, and M-CSF) in the overall cohort of COVID-19 participants, with HGF and IL-12p40 showing robust differentiation between participants with and without alterations in consciousness. In contrast, participants at the late phase after COVID-19 infection showed no group level elevation of inflammatory mediators. However, late elevations in Tau correlated with levels of CCL2, CCL7, IL-1RA, IL-2Rα, M-CSF, SCF, IL- 16, and IL-18, suggesting that these markers of the late innate host response were associated with persisting markers of dendritic/axonal injury markers. A network analysis showed that the repertoire of cytokine responses was different in participants both with acute reductions in GCS, or those recovering from a neurological complication of COVID-19 when compared to the GCS=15 group. Participants with acute COVID-19 also developed IgG autoantibody responses to a significantly larger number of both neural and non-neural antigens, than seen in controls.  These increased IgG . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 11 responses persisted into the late phase.  While the diversity of autoantibody response did not differ between participants with and without neurological dysfunction, autoantibody responses to specific antigens, including the neural antigens UCH-L1, GRIN3B, and DRD2, were more common in participants with abnormal GCS at presentation.  In the late phase, participants who had and had not experienced a neurological complication of COVID-19 were distinguished by the presence of autoantibodies to HLA antigens in those with complications. To better study these acute host response that drove these processes (and potentially resulted in neurological injury, dysfunction, and disease), and given that SARS-CoV-2 is rarely identified in the brain parenchyma in clinical samples, we developed a low-inoculum mouse model of COVID- 19 which induced pulmonary infection in the absence of brain infection. In mice with pulmonary infection there was production of inflammatory mediators, including CCL2, IL-6, IL-12p40 and IL-1RA, in the lung. Despite the absence of viral replication in the brain parenchyma, there was some, albeit limited, production of inflammatory mediators in the brain, including CCL4, IFNγ and IL-17A. In addition to this, there was an increase in microglial Iba1 staining and colocalization of microglial and astrocyte clusters reflecting clinical disease driven by parainfectious processes. Taken together these findings suggest that a primarily pulmonary inflammatory process can drive parainfectious immune activation in the brain and the signature of an NK cell and/or T cell response, indicates a cascade of inflammation potentially amenable to treatment. These data from clinical disease and a mouse model of low dose SARS-CoV-2 infection provide important insights regarding the pathophysiology and pathogenesis of neurological injury, dysfunction, and disease in COVID-19.  The clinical characteristics of our participant cohorts, and the elevation in brain injury biomarkers, provide evidence of both acute and ongoing neurological injury.  Further, the literature data on the rarity of direct CNS infection by the virus and the evidence from our para-infectious model of SARS-CoV-2 encephalopathy, suggest that the innate and adaptive host responses that we document should be explored as pathogenic mechanisms. The incidence of neurological cases has decreased since the first wave of the pandemic, possibly due to the use of immunosuppressants, such as dexamethasone, although this may also reflect vaccines attenuating disease and changes in the prevalence of different strains of SARS-CoV-223. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 12 The inflammatory mediators that we found to be elevated in the acute phase are broadly concordant with many other publications that have examined innate immune responses in COVID-1918,19 but there are limited data addressing associations between such responses and the development of neurological complications.  It is possible that some of the risk of developing such complications is simply related to the severity of systemic infection and the host response, and it would be surprising if this was not a strong contributor.  However, our data suggest that acute neurological dysfunction in COVID-19 is also associated with a different repertoire of cytokine responses, with HGF and IL-12p40 showing the statistically most robust discrimination between participants with and without an abnormal GCS.  HGF has important roles in brain development and synaptic biology24 and its elevation may represent a protective/reparative response in participants with neurological injury.  IL-12p40 has a core role in orchestrating Th1 responses, and has been reported to be central in the development of central and peripheral neuroinflammation, with p40 monomer subunits perhaps acting as inhibitors of the process25–27. Interestingly, the cytokine network that was activated in the late convalescent phase was different, potentially indicating differential drivers of neurological injury throughout the disease course. Though group level comparisons with controls showed some commonalities in inflammatory mediator increase, most notably in IL-1RA, CCL2, and M-CSF there were many differences.  The late Tau elevation that we demonstrated was significantly associated with elevations in these three mediators, but also CCL7, IL-2Rα, SCF, IL-16, and IL-18.  These are all important pro-inflammatory mediators, and their association with Tau levels may reflect the persistence of a systemic inflammatory response that can enhance neuroinflammation25,27,28. We found a polyclonal increase in antibody production following COVID-19 infection and only a few autoantibody frequencies were different when compared by GCS or COVID versus neuro- COVID cases. Of note, absolute levels of autoantibodies were low in comparison to anti-viral antibodies that developed over the course of the acute illness, with the exception of TROVE2, TUBA1B and SFTPA1. TROVE2, also known as Ro60 or SS-A is an RNA-binding protein and antibodies against it are present in autoimmune diseases including systemic lupus erythematosus and Sjögren’s syndrome, and its role in disease pathogenesis is an active area of research29–31. TUBA1B, a component of microtubules, has protein similarity to viral nucleocapsid and has been found to be recognized by antibodies from COVID-19 participant sera32. SFTPA1 is a lung surfactant protein and antibodies against it have been found to correlate with COVID-19 severity12. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 13 HLA alleles and even blood groups have been associated with COVID severity, but less is known about antibodies against them.33,34.  However, overall, we did not find a significant association of autoantibody levels with markers of brain injury, a finding that is against a broad causal role for these adaptive immune responses. Further analysis by screening the antibodies against brain antigens ex vivo revealed sporadic reactivity in both cases and controls with only the brainstem showing increased reactivity in COVID+ve participants but no difference between COVID and neuro-COVID. Our studies have several limitations including: limited clinical information on the acute participants and lack of longitudinal blood samples; in addition, the low GCS could indicate sedation for intubation, rather than CNS disease, in the acute cohort. Although we did not have COVID-19 severity scores, we did know whether participants had required oxygen or not; when data were analysed within the cohorts just comparing participants who had required oxygen, 5 out of 6 cytokines remained significantly elevated in the abnormal GCS group. In the COVID-CNS study where we did have in-depth clinical information, again we were limited by not having prospective and longitudinal blood samples. Nevertheless, several cytokines showed significant positive correlations with the brain injury marker Tau, and interestingly, three of them were cytokines that were significantly associated with abnormal GCS in the acute cohort (IL-1RA, CCL2, and M-CSF) highlighting a network of co-upregulated biomarkers associated with neurological complications.  This commonalities in innate immune response in patients who suffered neurological dysfunction/complications, both in the acute phase and at convalescence, is underlined by the results of network analysis.  Pro-inflammatory cytokines are expected to be increased in the anti-viral response, but we found that they not only correlate with COVID-19 severity, but with GCS, as well. Strengths of our study include the large cohort of participants studied with well-characterized neurological syndromes and a known range of timings since COVID-19 infection. We studied aspects of the innate and adaptive immune response as well as brain injury markers in order to discover useful biomarkers of neurological complications over time. Our mouse model is novel in using a low inoculum of SARS-CoV-2 virus for infection which does not involve lethal brain pathology, allowing us to study the immune activation in the brain in the absence of direct viral invasion. Our mouse model is congruent with the majority of human autopsy results which show . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 14 limited virus in the brain, but nevertheless demonstrate inflammation and microglia reactivity9,35. It is worth noting that a hamster model which examined neuropathology at longer term follow-up (31 days post-SARS-CoV-2 infection) found that Iba1 expression remained elevated36. Studies of brain organoids have found that Iba1+ microglia engulf post-synaptic material contributing to synapse elimination37. Several hypotheses for how SARS-CoV-2 causes neuropathology have been tested. A prospective study of hospitalised patients showing IL-6 and D-dimer as risk factors implicates the innate immune response and coagulation pathways16. The complement pathway and microthrombosis have been associated with brain endothelial damage from SARS-CoV-2 infection, and this phenotype persists months after COVID-1935,38. Animal models have provided key insights into COVID-19 neuropathology that warrant discussion. There have been at least two reports of viral encephalitis and neuron degeneration and apoptosis observed in non-human primates39,40. It is important to note that in these studies the virus was present at low amounts in the brain and predominantly in the vasculature as visualized by co-localization with Von Willebrand Factor40. Similar to the clinical scenario, there was no correlation of neuropathology with respiratory disease severity33. A recent mouse study is particularly relevant to our work and involved assessment of a mouse model that also lacked direct viral neural invasion by infecting mice that were intratracheally transfected with human ACE2. This study reported increased CXCL11 (eotaxin) in mouse serum and CSF which correlated with demyelination and this was recapitulated by giving CXCL11 intraperitoneally41. This was linked to clinical studies which showed elevated CXCL11 in patients with brain fog41. We did not observe elevated CXCL11 in our studies and this could be due to model and timepoint differences. Indeed, we did not observe any elevation of cytokines or brain injury markers in the sera of low dose infected mice (Extended Data Fig. 7c,d). However, it is very likely that these negative results are due to the protocols for heat-inactivation of experimental samples, which were part of safety protocols the CL3 lab42 at the point when these experiments were conducted. A combined analysis of hamster and clinical studies showed that COVID-19 infection led to IL-1β and IL-6 expression within the hippocampus and medulla oblongata and decreased neurogenesis in the hippocampal dentate gyrus which may relate to learning and memory deficits43. This was also borne out during in vitro studies that showed that serum from COVID+ve patients with delirium lead to decreased proliferation and increased apoptosis of a human hippocampal progenitor cell line mediated by elevated IL-644. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 15 In conclusion, we found that brain injury markers remain elevated in the sera of COVID-19 participants who have experienced acute neurological complications, with the highest levels observed in cerebrovascular and brain inflammation cases, with elevated levels persisting months after SARS-CoV-2 infection. If autoimmunity is involved in the neurological complications, it is not detectable in the sera at later timepoints by checking the pro-inflammatory cytokines elevated acutely, implicating local damage may be key. Our low inoculum SARS-CoV-2 mouse model highlights a way to study parainfectious effects on the brain and enables characterization of the brain tissue itself. The cytokine signature and microglia reactivity at day 5 post infection indicates an acute immune response including initial inflammation in the absence of active viral replication that could be amenable to targeted immunosuppression which can direct future studies. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 16 Methods Human participant studies/healthy controls and ethics information The ISARIC WHO Clinical Characterization Protocol for Severe Emerging Infections in the UK (CCP-UK) was a prospective cohort study of hospitalised patients with COVID-19, which recruited across England, Wales, and Scotland (National Institute for Health Research Clinical Research Network Central Portfolio Management System ID: 14152) The protocol, revision history, case report form, patient information leaflets, consent forms and details of the Independent Data and Material Access Committee are available online37 For the ISARIC participants, we examined those participants with a normal vs. abnormal Glasgow coma score. In the COVID-CNS study (REC reference# 17/EE/0025), we could stratify by participants with neurological complications and specific conditions. Serum samples were collected at different timepoints from COVID-19 diagnosis. ISARIC samples were collected during the acute phase (1-11 days from admission). COVID-CNS samples were collected at outpatient follow up during the sub-acute (<6 weeks from admission) or convalescent (>6 weeks) phases. The samples were aliquoted, labelled with anonymised identifiers, and frozen immediately at -70°C. Human brain injury markers measurements Brain injury markers were measured in sera using a Quanterix Simoa kit run on an automated HD- X Analyser according to the manufacturer’s protocol (Quanterix, Billerica, MA, USA, Neurology 4-Plex B Advantage Kit, cat#103345). We assessed neurofilament light chain (NfL), Ubiquitin C- Terminal Hydrolase L1 (UCH-L1), Tau, and glial fibrillary acidic protein (GFAP) in sera diluted 1:4 and used the manufacturer’s calibrators to calculate concentrations. Human serum cytokine measurements Analytes were quantified using the BioRad human cytokine screening 48-plex kit (Cat# 12007283) following manufacturer’s instructions on a Bioplex 200 using Manager software 6.2. This involved incubation of 1:4 diluted sera with antibody-coated magnetic beads, automated magnetic plate washing, incubating the beads with secondary detection antibodies, and adding streptavidin-PE. Standard curves of known protein concentrations were used to quantify analytes. Samples that . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 17 were under the limit of detection were valued at the lowest detectable value adjusted for 1:4 dilution factor. Median-centred normalization of human serum cytokine measurements To minimise any potential impact of any possible variation in sample storage and transport, concentrations were median-centred and normalised for each participant, using established methodology45–47. The pg/mL of cytokines were log-transformed and the median per participant across all cytokines was calculated. The log-transformed median was subtracted from each log- transformed value to generate a normalized set. Protein microarray autoantibody profiling Autoantibodies were measured from sera as previously described in Needham et al., 202212. Briefly, a protein array of antigens (based on the HuProt™ (version 4.0) platform) was used to measure bound IgM and IgG from sera, using secondary antibodies with different fluorescent labels detected by a Tecan scanner. As developed in previous studies12,48, antibody positivity was determined by measuring the median fluorescence intensity (MFI) of the four quadruplicate spots of each antigen. The MFI was then normalized to the MFI of all antigens for that participants’ sample by divided each value by the MFI. Z-scores were obtained from these normalized values based on the distribution derived for each antigen from the healthy control cohort. A positive autoantibody ‘hit’ was defined as an antigen where Z ≥ 3,. Immunohistochemistry Immunohistochemistry was performed on sagittal sections of female Wistar rat brains. Brains were removed, fixed in 4% paraformaldehyde (PFA) at 4°C for 1 h, cryoprotected in 40% sucrose for 48 h, embedded in freezing medium and snap-frozen in isopentane chilled on dry ice. 10µm thick sections were cut and mounted on slides in a cryostat. A standard avidin-biotin peroxidase method was used, as reported previously49,50, where sera were diluted 1:200 in 5% NGS and incubated at 4°C overnight, and secondary biotinylated goat anti-human IgG Fc was diluted (1:500) and incubated at room temperature for 1 h. Finally, slides were counter-stained using cresyl violet. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 18 Mouse studies of infection with SARS-CoV-2 An AWERB-approved protocol was followed for the two independent mouse studies. Male and female 2-4 month old heterozygote hACE2-transgenic C57BL/6 mice were intranasally infected with 1x103 or 1x104 plaque-forming units (PFU) of a human isolate of SARS-CoV2 (Liverpool Pango B). Mice were euthanized on day 5 post infection. Brains were perfused with 30mL PBS w/ 1mM EDTA, then one hemisphere fixed in formaldehyde-containing PLP buffer overnight at 4°C. Brains were then subjected to a sucrose gradient-- 10 and 20% sucrose for 1hr each and then 30% sucrose O/N at 4°C. Brains were then frozen in OCT by submerging moulds in a beaker of 2-methylbutane on dry ice. The other hemisphere was divided in two sagittal pieces and half preserved in 4%PFA for histology and half in trizol for RNA and protein extraction. Sera was collected and frozen and then heat-inactivated at 56°C for 30 mins prior to be moved from CL3 to CL2 lab. Lung tissue was preserved in 4%PFA for histology, in trizol for RNA and protein extraction, and in PLP for cryosectioning. qPCR of SARS-CoV-2 genes and mouse cytokines Gene expression was measured from trizol isolated RNA (Invitrogen cat# 15596018, manufacturer’s protocol) using Promega’s GoTaq Probe 1-Step RT-qPCR system (cat#A6120, manufacturer’s protocol) on an Agilent AriaMx. Primers and FAM probes for SARS-CoV2, cytokines, and housekeeping genes were purchased from IDT (Extended Methods Table 1) with standard IDT qPCR primer/probe sets for the mouse cytokines. The thermal cycle for N1 and the mouse cytokines was: 45°C for 15min 1x, 95°C for 2 min, then 45 cycles of 95°C for 3 secs followed by 55°C for 30 sec. For subgenomic E: 45°C for 15 min 1x, 95°C for 2 min, then 45 cycles of 95°C for 15 secs followed by 58°C for 30 sec. For 18S it was: 45°C for 15 min, 95°C for 2 min and 40 cycles of 95°C for 15s, 60°C for 1 min . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 19 Extended Methods Table 1: Primers and Probes for viral genes and normalization Gene 
Reagent 
IDT Cat# 
Sequence (5′–3′) N1 
Forward primer: 
10006830 
GACCCCAAAATCAGCGAAAT Reverse primer: 
10006831 
TCTGGTTACTGCCAGTTGAATCTG FAM probe: 
10006832 
ACCCCGCATTACGTTTGGTGGACC subgE 
Forward primer: 
10006889 
CGATCTCTTGTAGATCTGTTCTC Reverse primer: 
10006891 
ATATTGCAGCAGTACGCACACA FAM probe: 
10006893 
ACACTAGCCATCCTTACTGCGCTTCG 18S 
Forward primer: 
Custom 
ACCTGGTTGATCCTGCCAGTAG Reverse primer: 
Custom 
AGCCATTCGCAGTTTCACTGTAC FAM probe: 
Custom 
TCAAAGATTAAGCCATGCATGTCTAAGTACGCAC Extended Methods Table 2: Primers and Probes for Mouse cytokines Gene 
IDT Primetime Cat# IL-1RN 
Mm.PT.58.43781580 IL-6 
Mm.PT.58.10005566 IL-12p40 
Mm.PT.58.12409997 M-CSF 
Mm.PT.58.11661276 CCL2 
Mm.PT.58.42151692 HGF 
Mm.PT.58.9088506 CCL4 
Mm.PT.58.5219433 IL-1RN (Exon 1-3) 
Mm.PT.58.43781580 IFNG 
Mm.PT.58.41769240 IL-17A 
Mm.PT.58.6531092 For mouse cytokines, the primer/probe sets listed in Extended Methods Table 2 were used and 
the cycle was:  45°C for 15 min; 95°C for 2 min; 45 cycles of 95°C for 3 sec and 55°C for 30 
sec. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 20 Mouse histology and confocal microscopy The brain specificity of autoantibodies in participant sera were examined by assessing binding to mouse brain sections.  Paraffin-embedded formalin-fixed tissue was sectioned to 4 um sections. Slides were baked at 60°C for 30 minutes and then stained with H&E in an autostainer. H&E slides were imaged on a Leica microsope (model details). For immunofluorescent staining and confocal microscopy, OCT-embedded frozen tissue was sectioned to 12 um sections. 100% acetone was used for antigen retrieval (10 mins at room temperature). After air-drying, then PBS washing, tissue sections were permeabilized with 0.1% Triton X-100/PBS (20 mins at room temperature). After rinsing with PBS, tissue sections were blocked with Dako block (5 minutes at room temperature). After another PBS wash, primary antibodies were added at dilutions listed in table for an overnight incubation at 4°C in a humidified chamber. Tissue sections were washed twice with PBS for 5 minutes each wash. Secondary antibody (as described in Extended Methods Table 3) was added for a 2 hr room temperature incubation, followed by two 5-minute PBS washes. DAPI-mounting medium was used for coverslipping. Imaging was performed with Andor Dragonfly spinning disk confocal microscope. Marker fluorescence and microglia counts, intensity, and ramification indices (method based on previous work51) were quantified with Fiji (confocal microscope set up in Extended Methods Table 4 and macros downloadable from the public Github repository). Ramification indices of 0-1 of objects with a threshold size of 19 mm2 were quantified from three Z-stack images/mouse and two mice/group. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 21 Extended Methods Table 3: Antibodies for immunofluorescent stain and confocal imaging Antibody target-
fluorochrome 
Company 
Cat# 
Host Dilution Factor  
(for 200 µL per 
section) CD45-PE 
Invitrogen 
12-0451-82 
Rat 
50 CD11b-AF647 
BDBioscience 
557686 
Rat 
200 NK1.1-AF488 
BioLegend 
108718 
Mouse 
100 CD3-FITC 
Abcam 
ab34722 
Rat 
50 GFAP 
Invitrogen 
41-9892-82 
Mouse 
100 NeuN 
Merck 
MAB377X 
Mouse 
100 CD68 unconjugated 
Abcam 
ab53444 
Rat 
200 Anti-rat AF488 
Jackson 
Immunoresearch 712-546-
153 
Donkey 500 Iba1 unconjugated 
Wako 
019-19741 
Rabbit 
500 Anti-rabbit IgG-AF647 
Invitrogen 
A-31573 
Donkey 500 Spike unconjugated 
Invitrogen 
703958 
Human 
100 Anti-human AF568 
Fisher 
A-21090 
Goat 
500 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 22 Extended Methods Table 4: Confocal microscopy settings Microscope 
component Parameters Microscope 
Leica DMi8 with Andor Dragonfly Light source  
7-line integrated laser engine equipped with: 
Solid state 405 smart diode laser at 100mW: set to 10% 
Solid state 488 smart diode laser at 50mW: set to 35%  
OBIS LS 561 smart OPSS laser at 50mW: set to 2.0% 
OBIS LX Solid state 637 smart diode laser at 140mW: set to 7.0% Excitation/emission 
optics Dichroic mirror: Quad EM filter 405-488-561-640 
 
Dual camera beam splitter: Dual camera dichroic 565nm long pass 
 
Emission filters:  
450/50nm bandpass filter 
525/50nm bandpass filter 
600/50nm bandpass filter 
700/75nm bandpass filter 
 
spinning disk with 40 µm pinholes Objective lenses 
Leica objectives:  
11506358 HC PL APO 40×/1.30 OIL CS2 Detector 
Andor iXon Ultra 888 Ultra EMCCD Camera 
1024 × 1024;  
 
405: 500 ms exposure; 65 EM gain 
488: 500 ms exposure; 65 EM gain  
561:  30 ms exposure; 156 EM gain  
637: 30 ms exposure; 156 EM gain 
Averaging: 1; Binning: 1; camera magnification 1x  
Nyquist Z sampling . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 23 Statistical Analyses Prism software (version 9.4.1, GraphPad Software Inc.) was used for graph generation and statistical analysis. The Shapiro-Wilk normality test used to check the normality of the distribution. Data are expressed as mean ± S.E.M. Heatmaps, volcano plots and Chord diagrams were made using R studio (version 4.1.1 RStudio, PBC). The 2D cytokine network analyses were created using the qgraph package in R software and matrices differences were assessed by Steiger test52. Univariate analyses was conducted to test for differences between two groups. Differences between two normally distributed groups were tested using the paired or unpaired Student’s t test as appropriate.  The difference between two non-normally distributed groups was tested using Mann-Whitney U test.  Volcano plots used multiple Mann-Whitney U tests with a false discovery rate set to 5%, and heatmaps show Pearson’s correlations adjusted for a false discovery rate of 5%. Group comparisons were by Kruskal-Wallis test. Frequency differences of antibodies were measured by Fisher’s exact tests. P ≤ 0.05 was considered statistically significant. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 24 Acknowledgements BDM is supported by the UKRI/MRC (MR/V03605X/1), the MRC/UKRI (MR/V007181/1), MRC (MR/T028750/1) and Wellcome (ISSF201902/3).  CD is supported by MRC (MC_PC_19044). We would like to thank the University of Liverpool GCP laboratory facility team for Luminex assistance and the Liverpool University Biobank team for all their help, especially Dr Victoria Shaw, Lara Lavelle-Langham, and Sue Holden. We would like to acknowledge the Liverpool Experimental Cancer Medicine Centre for providing infrastructure support for this research (Grant Reference: C18616/A25153). We acknowledge the Liverpool Centre for Cell Imaging (CCI) for provision of imaging equipment (Dragonfly confocal microscope) and excellent technical assistance (BBSRC grant number BB/R01390X/1). DKM and EN are supported by the NIHR Cambridge Biomedical Centre and by NIHR funding to the NIHR BioResource (RG94028 and RG85445), and by funding from Brain Research UK 201819- 20. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23287902
doi: 
medRxiv preprint 25",1
"For many cancers there are few well-established risk factors. Summary data from genome-wide association studies (GWAS) can be used in a Mendelian randomisation (MR) phenome-wide association study (PheWAS) to identify causal relationships. We performed a MR-PheWAS of breast, prostate, colorectal, lung, endometrial, oesophageal, renal, and ovarian cancers, comprising 378,142 cases and 485,715 controls. To derive a more comprehensive insight into disease aetiology we systematically mined the literature space for supporting evidence. We evaluated causal relationships for over 3,000 potential risk factors. In addition to identifying well-established risk factors (smoking, alcohol, obesity, lack of physical activity), we provide evidence for specific factors, including dietary intake, sex steroid hormones, plasma lipids and telomere length as determinants of cancer risk. We also implicate molecular factors including plasma levels of IL-18, LAG-3, IGF-1, CT-1, and PRDX1 as risk factors. Our analyses highlight the importance of risk factors that are common to many cancer types but also reveal aetiological differences. A number of the molecular factors we identify have the potential to be biomarkers. Our findings should aid public health prevention
strategies
to
reduce
cancer
burden.
We
provide
a
R/Shiny
app (https://mrcancer.shinyapps.io/mrcan/) to visualise findings. 2 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint INTRODUCTION Cancer is currently the third major cause of death with an estimated 18.1 million new cases and nearly 10 million cancer deaths in 20201. By 2030 it is predicted there are likely to be 26 million new cancer cases and 17 million cancer-related deaths annually2. Such projections have renewed efforts to identify risk factors to inform cancer prevention programmes. For many cancers, despite significant epidemiological research, there are few established risk factors. Although randomised-controlled trials (RCTs) are the gold standard for establishing causal relationships, they are often impractical or unfeasible because of cost, time, and ethical issues. Conversely, case-control studies can be complicated by biases such as reverse causation and confounding. Mendelian randomisation (MR) is an analytical strategy that uses germline genetic variants as instrumental variables (IVs) to infer causal relationships (Fig. 1A)3. The random assortment of these genetic variants at conception mitigates against reverse causation bias. Moreover, in the absence of pleiotropy (i.e. the presence of an association between variants and disease through additional pathways), MR can provide unconfounded disease risk estimates . Elucidating disease causality using MR is gaining popularity especially given the availability of data from large genome-wide association studies (GWAS) and well-developed analytical frameworks3. Most MR studies of cancer have been predicated on assumptions about disease aetiology or have sought to evaluate purported associations from conventional observational epidemiology3,4. A recently proposed agnostic strategy, termed MR-PheWAS, integrates the phenome-wide association study (PheWAS) with MR methodology to identify causal relationships using hitherto unconsidered traits5. To identify causal relationships for eight common cancers (breast, prostate, colorectal, lung, endometrial, oesophageal, renal and ovarian), and reveal intermediates of risk, we conducted a MR-PheWAS study. We integrated findings with a systematic mining of the literature space to provide supporting evidence and derive a more comprehensive description of disease aetiology (Fig. 1B)6. 3 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint RESULTS Phenotypes and genetic instruments After filtering we analysed 3,661 traits, proxied by 336,191 genetic variants in conjunction with summary genetic data from published GWAS of breast, prostate, colorectal, lung, endometrial, oesophageal, renal, and ovarian cancers (Table 1; Supplementary Table 17). The number of SNPs used as genetic instruments for each trait ranged from one to 1,335. Figure 2 shows the power of our MR study to identify causal relationships between each of the genetically defined traits and each cancer type. The median PVE by SNPs used as IVs for each of the 3,661 traits evaluated as risk factors was 3.4% (0.01–84%). Our power to demonstrate causal relationships a priori for each cancer type reflects in part inevitably the size of respective GWAS datasets (Supplementary Table 1). Causal associations identified by MR To aid interpretation we grouped traits related to established cancer risk factors (i.e. smoking, obesity and alcohol) and those for which current evidence is inconclusive into the following categories: cardiometabolic; dietary intake; anthropometrics; immune and inflammatory factors; fatty acid (FA) and lipoprotein metabolism; lifestyle, reproduction, education and behaviour; metabolomics and proteomics; miscellaneous. Out of the 27,066 graded associations, MR analyses provided robust evidence for a causal relationship with 123 phenotypes (0.5% of total MR analyses), 174 with probable evidence (0.6% of total), 1,652 with suggestive evidence (6% of total). Across the eight cancer types, the largest number of robust associations were observed for endometrial cancer with 37 robust associations, followed by renal cancer (n = 32), CRC (n = 21), lung (n=20), breast (n=10), oesophageal (n=3) and prostate cancer (n=1). No robust MR associations were observed for ovarian cancer (Supplementary Table 3). Across all of the cancer types anthropometric traits showed the highest number of robust MR defined causal relationships (N=32; 0.1%), followed by lifestyle, reproduction, education and behaviour
(N=17;
0.06%).
No robust associations were observed for dietary intake or cardiometabolic categories (Supplementary Table 3). 4 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint To visualise the strength and direction of effect of the causal relationship between each of the traits examined and risk of each cancer type and, where appropriate, their respective subtypes we provide a R/Shiny app (https://mrcancer.shinyapps.io/mrcan/). Fig. 3 shows a screenshot of the app for selected traits across the eight different types of cancer. Many of the identified causal relationships, especially those that were statistically robust or probable, have been reported in previous MR studies and are related to established risk factor categories4,7,8. Notably: (i) the relationship between metrics of increased body mass index (BMI) with an increased risk of colorectal, lung, renal, endometrial and ovarian cancers9; (ii) cigarette smoking with an increased risk of lung cancer10; (iii) higher alcohol consumption and increased risk of oesophageal, colorectal, lung, renal, endometrial and ovarian cancers11; (iv) reduced physical activity and sedentary behaviour with an increased risk of multiple cancers, including breast, lung, colorectal and endometrial12. As anticipated, exposure traits pertaining to cigarette smoking were not causally related to lung cancer in never smokers. Paradoxically, but as reported in previous MR analyses, increased BMI was associated with reduced risk of prostate and breast cancer, and an inverse relationship between smoking and prostate cancer risk was observed9,13. Our analysis also supports the reported relationship between higher levels of sex hormone-binding globulin with reduced endometrial cancer risk and a relationship between testosterone with risk of endometrial cancer and breast cancers14,15. Notably, exposure traits related to testosterone levels were only causally associated with luminal-A and luminal-B breast cancer subtypes. With respect to dietary intake our analysis demonstrated probable associations between genetically predicted higher levels of coffee, oily fish, and cheese intake with reduced CRC risk and suggestive associations between genetically predicted beef and poultry intake and elevated CRC risk. We found suggestive associations between genetically predicted high serum vitamin B12 with colorectal and prostate cancer, serum calcium and 25-hydroxyvitamin-D with RCC, low blood selenium with colorectal and oesophageal cancers and methionine and zinc with reduced CRC risk. We observed no association between genetically predicted blood levels of circulating carotenoids or vitamins B6 and E for any of the cancers. In terms of glucose homeostasis, no relationship between genetically predicted blood glucose or glycated haemoglobin was shown for any of the eight cancers. However, higher levels of genetically predicted levels of fasting insulin and insulin growth factor 1 (IGF-1) and lower proinsulin showed 5 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint associations with CRC. Additionally, a suggestive association between proinsulin and renal cancer, fasting insulin and lung and endometrial cancers, and IGF-1 levels and breast cancer was observed. Amongst genetically predicted higher levels of lipoproteins, the only associations were a probable association between high density lipoprotein cholesterol (HDL-C) and breast cancer and suggestive associations between low density lipoprotein cholesterol (LDL-C) with CRC, and total cholesterol and ovarian cancer. Genetically predicted levels of plasma FAs showed an association with reduced cancer risk. Specifically, for the omega-6 polyunsaturated FAs, lower levels of arachidonic acid (20:4n6) and gamma-linoleic acid (18:3n6) and higher levels of linoleic acid (18:2n6) and adrenic acid (22:4n6)) with
reduced
risk
of
CRC;
for
the
omega-3
polyunsaturated
FAs
(alpha-linoleic
acid, eicosapentaenoic acid, docosahexaenoic acid) and breast cancer risk, and arachidonic acid and endometrial cancer. A relationship between longer lymphocyte telomere length (LTL) and an increased risk of six of the eight cancer types was identified - robust with respect to renal and lung cancers, probable for breast and prostate cancers and suggestive for colorectal and ovarian cancers. In addition to a robust association between higher HLA-DR dendritic plasmacytoid levels and risk of prostate cancer, 26 probable associations between genetically predicted levels of other circulating immune and inflammatory factors were shown across the cancers studied. These included higher levels of IL-18 with reduced risk of lung cancer, with specificity for lung cancer in never smokers. Our MR analysis provides support for the known relationship between colonic polyps and CRC16, benign breast disease and breast cancer17, oesophageal reflux with risk of oesophageal cancer (Supplementary Table 13)18. Other associations included possible relationships between pulmonary fibrosis and lung cancer19, as well as the relationship between a diagnosis of schizophrenia and lung cancer, which has been observed in conventional epidemiological studies20. It was noteworthy, however, that we did not find evidence to support the purported relationship between hypertension and risk of developing RCC. Similarly, our analysis did not provide evidence to support a causal relationship between either type 1 or type 2 diabetes and an increased cancer risk. 6 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Literature-mined support for MR causal relationships To provide support for the associations and to gain molecular insights into the underlying biological basis of relationships we performed triangulation through systematic literature mining. We identified 55,105 literature triples across the eight different cancer types and 680,375 literature triples across the MR defined putative risk factors (Supplementary Table 18). Overlapping risk factor-cancer pairings from our MR analysis yielded on average 49 potential causal relationships. Supplementary Table 19 stratifies the literature space size by trait category while recognising that causal relationships with a small literature space could be reflective of deficiencies in semantic mapping relationships with large literature spaces support triangulation. Supplementary Table 20 provides the complete list of potential mediators for each trait. Illustrating the use of triangulation using a large literature space (defined herein as >50 triples) to support causal relationships, Fig. 4 highlights four notable examples (IGF-1, LAG-3, IL-18, and PRDX1). IGF-1, which is reported to play a role in multiple cancers, appears to mediate its effect in part through beta-catenin and BRAF signalling, modulating CRC and breast cancer risk21,22. Whilst LAG-3 inhibition is an attractive therapeutic target in restoring T-cell function, we demonstrate genetically elevated LAG-3 levels as being associated with reduced CRC, endometrial and lung cancer. In all three of these cancers, the association appears to be at least partly mediated through IL-10 and the seemingly paradoxical relationship between LAG-3 levels and tumourgenesis suggests potentiation of T-cell function by serum LAG-3 rather than cell membrane expressed LAG-323. We identify genetically predicted IL-18 levels as being associated with an increased risk of lung cancer. Our literature mining also supports a role for the decoy inhibitory protein, IL-18BP as being a mediator of lung cancer risk as well as IL-10, IL-12, IL-4 and TNF24. Finally, PRDX1, a member of the peroxiredoxin family of antioxidant enzymes, interacts with the androgen receptor to enhance its transactivation resulting in increased EGFR-mediated signalling and an increased prostate cancer risk21. 7 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint DISCUSSION By performing a MR-PheWAS we have been able to agnostically examine the relationship between multiple traits and the risk of eight different cancer types, restricted only by the availability of suitable genetic instruments. Importantly, many of the traits we examined have not previously been the subject of conventional epidemiological studies or been assessed by MR. Even for risk factors that were examined in many previous analyses, the number of cases and controls in our study has afforded greater power to identify potential causal associations. This has allowed us to exclude large causal effects on cancer risk for the majority of exposure traits examined in our study. In addition to identifying causal relationships for the well-established lifestyle traits, which validates our approach, we implicate other lifestyle factors that have been putatively associated by observational epidemiology contributing to cancer risk. For example, the protective effects of physical activity, coffee, oily fish, fresh/dried fruit intake for CRC risk. A number of the causal relationships we identify have been the subject of studies of individual traits and include the association between longer LTL with increased risk of RCC and lung cancers; sex steroid hormones and risk of breast and endometrial cancer; and circulating lipids with CRC and breast cancer. Using genetic instruments for plasma proteome constituents has allowed us to identify hitherto unexplored potential risk factors for a number of the cancers, including: the cytokine like molecule, FAM3D, which plays a role in host defence against inflammation associated carcinogenesis with lung cancer25; the autophagy associated cytokine cardiotrophin-1 with lung, endometrial, prostate and breast cancer and the tumour progression associated antigen CD63 with endometrial cancer26,27. Levels of these and other plasma proteins potentially represent biomarkers worthy of future prospective studies. Clustering of MR causal effect sizes for each trait cancer relationship highlights the importance of risk factors common to many cancers but also reveal differences in their impact in part likely to be reflective of underlying biology (Fig. 5). A principal assumption in MR is that variants used as IVs are associated with the exposure trait under investigation. We therefore used SNPs associated with exposure traits at genome-wide significance. Furthermore, only IVs from European populations were used to limit bias from population stratification. Our MR analysis does, however, have limitations. Firstly, we were limited to studying phenotypes with genetic instruments available, moreover traits such as food intake or television watching can be highly correlated with other exposures making deconvolution of the 8 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint causal risk factor problematic28,29. Secondly, correcting for multiple testing guards against false positives especially when based on a single exposure outcome. However, the potential for false negatives is not unsubstantial. Since we have not adjusted for between trait correlations, our associations are inevitably conservative. Thirdly, for a number of traits, we had limited power to demonstrate causal associations of small effect. Fourthly, not unique to our MR analysis, is the inability of our study to deconvolute time-varying effects of genetic variants as evidenced by the relationship between obesity and breast cancer risk30. Finally, as with all MR studies, excluding pleiotropic IVs is challenging. To address this, we incorporated information from weighted median and mode-based estimate methods, to classify the strength of causal associations. However, there are inevitably limitations to such modelling as exemplified by the strong relationship between plasma FA and risk of CRC which has been shown to be driven by the pleiotropic FADS locus which has a profound effect on the metabolism of multiple FA through its gene expression31. A major concern articulated regarding any MR-PheWAS is the need to provide supporting evidence from alternative sources. Herein we have sought to address this by conducting a systematic interrogation of the literature space and potentially identify intermediates to explain relationships. Although literature mined data is inevitably noisy and driven by publication bias, we have been able to provide a narrative of the causal relationships for a number of risk factors, which are attractive candidates for molecular validation. Complementary studies are required to delineate the exact biological mechanisms underpinning associations. Our analysis does however highlight important targets for primary prevention of cancer in the population. The limited power to robustly characterise relationships between exposure traits and cancer in this study, provides an impetus for larger MR studies. Finally, we recognise that MR is not infallible and replication and triangulation of findings using different data sources, and if possible, benchmarking against RCTs is highly desirable. Such efforts could identify additional factors as targets to reduce the overall burden of cancer. 9 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint METHODS Ethics approval The analysis was undertaken using published GWAS data, hence ethical approval was not required. Study design Our study had four elements. Firstly, the identification of genetic variants serving as instruments for exposure traits under investigation; secondly, the acquisition of GWAS data for the eight cancers; thirdly, MR analysis; fourthly, triangulation through literature mining to provide supporting evidence for causal relationships (Fig. 1B). Genetic variants serving as instruments Single nucleotide polymorphisms (SNPs), considered genetic instruments, were identified from published studies or MR-Base (Supplementary Table 1). For each SNP, the corresponding effect estimate on a trait expressed in per standard deviation (SD) units (assuming a per allele effect) and standard error (SE) was obtained. Only SNPs with a minor allele frequency >0.01 and a trait association of P-values <5 × 10−8 in a European population GWAS were considered as instruments. We excluded correlated SNPs at a linkage disequilibrium threshold of r2 >0.01, retaining SNPs with the strongest effect. For binary traits we restricted our analyses to traits with a medical diagnosis, excluding cancer. We removed duplicate exposure traits based on manual curation. Cancer GWAS summary statistics To examine the association of each genetic instrument with cancer risk, we used summary GWAS effect
estimates
from:
(1)
Online
consortia
resources,
for
breast
(BCAC; https://bcac.ccge.medschl.cam.ac.uk/, accessed July 2022) and prostate cancer (PRACTICAL; http://practical.icr.ac.uk/;
accessed
July
2022)32,33;
(2)
GWAS
catalogue (https://www.ebi.ac.uk/gwas/), for ovarian, endometrial and lung cancers (accessed September 2022)34–36; (3) Investigators of published work, for colorectal cancer (CRC), renal cell carcinoma (RCC) and oesophageal cancer37–39. Cancer subtype summary statistics were available for lung, breast and ovarian cancers. As the UK Biobank was used to obtain genetic instruments for many traits investigated, the CRC and oesophageal GWAS association statistics were recalculated from primary data excluding UK Biobank samples to avoid sample overlap bias (Table 1). Single nucleotide 10 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint polymorphisms were harmonised to ensure that the effect estimates of SNPs on exposure traits and cancer risk referenced the same allele (Supplementary Table 2)40. Statistical analysis For each SNP, causal effects were estimated for cancer as an odds ratio (OR) per SD unit increase in the putative risk factor (ORSD), with 95% confidence intervals (CIs), using the Wald ratio41. For traits with multiple SNPs as IVs, causal effects were estimated under an inverse variance weighted random-effects (IVW-RE) model as the primary measurement as it is robust in the presence of pleiotropic effects, provided any heterogeneity is balanced at mean zero (Supplementary Table 3-6)42. Weighted median estimate (WME) and mode-based estimates (MBE) were obtained to assess the robustness of findings (Supplementary Table 7)43,44. Directional pleiotropy was assessed using MR-Egger regression (Supplementary Table 8)45. The MR Steiger test was used to infer the direction of causal effect for continuous exposure traits (Supplementary Table 9)46. For this we estimated the proportion of variance explained (PVE) using Cancer Research UK lifetime risk estimates for each tumour type (Supplementary Table 10)47. A leave-one-out strategy under the IVW-RE model was employed to assess the potential impact of outlying and pleiotropic SNPs (Supplementary Table 11)48. Because two-sample MR of a binary risk factor and a binary outcome can be biased, we primarily considered whether there exists a significant non-zero effect, and only report ORs for consistency49. Statistical analyses were performed using the TwoSampleMR package v0.5.6 (https://github.com/MRCIEU/TwoSampleMR) in R (v3.4.0)40. Estimation of study power The power of MR to demonstrate a causal relationship depends on the PVE by the instrument50. We excluded instruments with a F-statistic <10 since these are considered indicative of evidence for weak instrument bias51. We estimated study power, stipulating a P-value of 0.05 for each target a priori across a range of effect sizes as per Brion et al. (Supplementary Table 1)52. Since power estimates for binary exposure traits and binary outcomes in a two-sample setting are unreliable, we did not estimate study power for binary traits49. Assignment of statistical significance The support for a causal relationship with non-binary traits was categorised into four hierarchical levels of statistical significance a priori: robust (PIVW-RE <1.4×10-5; corresponding to a P-value of 0.05 after Bonferroni correction for multiple testing (0.05/3,500), PWME or PMBE <0.05, true causal 11 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint direction and >1 IVs), probable (PIVW-RE <0.05, PWME or PMBE <0.05, true causal direction and >1 IVs), suggestive (PIVW-RE <0.05 or PWALD <0.05), and non-significant (PIVW-RE ≥0.05 or PWALD ≥0.05) (Supplementary Table 12). While non-significant associations can be due to low statistical power, they also indicate that a moderate causal effect is unlikely. For binary traits we classified associations as being supported (P <0.05) or not supported (P >0.05; Supplementary Tables 13-16). Support for causality To strengthen evidence for causal relationships identified from the MR analysis we exploited the semantic predications in Semantic MEDLINE Database (SemMedDB), which is based on all PubMed citations53. Within SemMedDB pairs of terms connected by a predicate which are collectively known as ‘literature triples’ (i.e. ‘subject term 1’ – predicates – ‘object term 2’). These literature triples represent semantic relationships between biological entities derived from published literature. To interrogate SemMedDB we queried MELODI Presto and EpiGraphDB to facilitate data mining of epidemiological relationships for molecular and lifestyle traits54–56. For each putative risk factor-cancer pair the set of triples were overlapped and common terms identified to reveal causal pathways and inform aetiology. Based on the information profile of all literature mined triples, we considered literature spaces with >50 literature triples as being viable, corresponding to 90% of the information content57. We complemented this systematic text mining by referencing reports from the World Cancer Research Fund/American Institute for Cancer Research, and the International Agency for Cancer Research Global Cancer Observatory, as well as querying specific putative relationships in PubMed58,59. 12 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint ACKNOWLEDGMENTS R.S.H. acknowledges grant support from Cancer Research UK (C1298/A8362), the Wellcome Trust (214388) and Myeloma UK. A.S. is in receipt of a National Institute for Health Research (NIHR) Academic Clinical Lectureship, funding from the Royal Marsden Biomedical Research Centre, a Starter Grant from the Academy of Medical Sciences and is the recipient of the Whitney-Wood Scholarship from the Royal College of Physicians. M.K. is supported by a fellowship from the David Forbes-Nixon Foundation. We acknowledge pump-priming funding from the Royal Marsden Biomedical Research Centre Early Diagnosis, Detection and Stratified Prevention Theme. This is a summary of independent research supported by the NIHR Biomedical Research Centre at the Royal Marsden NHS Foundation Trust and the Institute of Cancer Research. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health. Support from the DJ Fielding Medical Research Trust is also acknowledged. A.H. was in receipt of a summer studentship from the Genetics Society. We thank Alex Cornish for providing code and critically appraising the manuscript. The breast cancer genome-wide association analyses for BCAC and CIMBA were supported by Cancer Research UK (PPRPGM-Nov20\100002, C1287/A10118, C1287/A16563, C1287/A10710, C12292/A20861, C12292/A11174, C1281/A12014, C5047/A8384, C5047/A15007, C5047/A10692, C8197/A16565) and the Gray Foundation, The National Institutes of Health (CA128978, X01HG007492- the DRIVE consortium), the PERSPECTIVE project supported by the Government of Canada through Genome Canada and the Canadian Institutes of Health Research (grant GPH-129344) and the Ministère de l’Économie, Science et Innovation du Québec through Genome Québec and the PSRSIIRI-701 grant, the Quebec Breast Cancer Foundation, the European Community's
Seventh
Framework
Programme
under
grant
agreement
n°
223175 (HEALTH-F2-2009-223175) (COGS), the European Union's Horizon 2020 Research and Innovation Programme (634935 and 633784), the Post-Cancer GWAS initiative (U19 CA148537, CA148065 and CA148112 - the GAME-ON initiative), the Department of Defence (W81XWH-10-1-0341), the Canadian Institutes of Health Research (CIHR) for the CIHR Team in Familial Risks of Breast Cancer (CRN-87521), the Komen Foundation for the Cure, the Breast Cancer Research Foundation and the Ovarian Cancer Research Fund. All studies and funders are listed in Zhang H et al (Nat Genet, 2020). The colorectal cancer genome-wide association analysis was supported by Ulrike Peters (GECCO), Stephanie Schmit (CCFR), Stephen Gruber (CORECT), Ian Tomlinson (CORGI, SCOT), and Malcolm 13 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Dunlop (SOCCS). Full study details and funders are listed in Fernandez-Rozadilla C et al (Nat Genet, 2023). The Prostate cancer genome-wide association analyses are supported by the Canadian Institutes of Health Research, European Commission’s Seventh Framework Programme grant agreement n° 223175 (HEALTH-F2-2009-223175), Cancer Research UK Grants C5047/A7357, C1287/A10118, C1287/A16563, C5047/A3354, C5047/A10692, C16913/A6135, and The National Institute of Health (NIH) Cancer Post-Cancer GWAS initiative grant: No. 1 U19 CA 148537-01 (the GAME-ON initiative). We would also like to thank the following for funding support: The Institute of Cancer Research and The Everyman Campaign, The Prostate Cancer Research Foundation, Prostate Research Campaign UK (now PCUK), The Orchid Cancer Appeal, Rosetrees Trust, The National Cancer Research Network UK, The National Cancer Research Institute (NCRI) UK. We are grateful for support of NIHR funding to the NIHR Biomedical Research Centre at The Institute of Cancer Research and The Royal Marsden NHS Foundation Trust. The Prostate Cancer Program of Cancer Council Victoria also acknowledge grant support from The National Health and Medical Research Council, Australia (126402, 209057, 251533, , 396414, 450104, 504700, 504702, 504715, 623204, 940394, 614296,), VicHealth, Cancer Council Victoria, The Prostate Cancer Foundation of Australia, The Whitten Foundation, PricewaterhouseCoopers, and Tattersall’s. EAO, DMK, and EMK acknowledge the Intramural Program of the National Human Genome Research Institute for their support. Genotyping of the OncoArray was funded by the US National Institutes of Health (NIH) [U19 CA 148537 for ELucidating Loci Involved in Prostate cancer SuscEptibility (ELLIPSE) project and X01HG007492 to the Center for Inherited Disease Research (CIDR) under contract number HHSN268201200008I] and by Cancer Research UK grant A8197/A16565. Additional analytic support was provided by NIH NCI U01 CA188392 (PI: Schumacher). Funding for the iCOGS infrastructure came from: the European Community’s Seventh Framework Programme under grant agreement n° 223175 (HEALTH-F2-2009-223175) (COGS), Cancer Research UK (C1287/A10118, C1287/A 10710, C12292/A11174, C1281/A12014, C5047/A8384, C5047/A15007, C5047/A10692, C8197/A16565), the National Institutes of Health (CA128978) and Post-Cancer GWAS initiative (1U19 CA148537, 1U19 CA148065 and 1U19 CA148112 – the GAME-ON initiative), the Department of Defence (W81XWH-10-1-0341), the Canadian Institutes of Health Research (CIHR) for the CIHR Team in Familial Risks of Breast Cancer, Komen Foundation for the Cure, the Breast Cancer Research Foundation, and the Ovarian Cancer Research Fund. The BPC3 was supported by the U.S. National Institutes of Health, National Cancer Institute (cooperative agreements U01-CA98233 to D.J.H., U01-CA98710 to S.M.G., U01-CA98216 toE.R., and U01-CA98758 to B.E.H., and Intramural Research Program of NIH/National Cancer Institute, Division of Cancer Epidemiology and Genetics). CAPS 14 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint GWAS study was supported by the Swedish Cancer Foundation (grant no 09-0677, 11-484, 12-823), the Cancer Risk Prediction Center (CRisP; www.crispcenter.org), a Linneus Centre (Contract ID 70867902) financed by the Swedish Research Council, Swedish Research Council (grant no K2010-70X-20430-04-3, 2014-2269). PEGASUS was supported by the Intramural Research Program, Division of Cancer Epidemiology and Genetics, National Cancer Institute, National Institutes of Health. AUTHORSHIP Contribution: M.W, A.S, C.M. and R.S.H designed the study. M.W, A.S., C.M., A.H., R.C., and P.L. performed statistical analyses; M.W, A.S., C.M., and R.S.H. drafted the manuscript; all authors reviewed, read, and approved the final manuscript. CONFLICT-OF-INTEREST DISCLOSURE The authors declare no competing financial interests. AVAILABILITY OF DATA AND MATERIAL Genetic instruments can be obtained through MR-Base or from published work (Supplementary Table
1).
Summary
GWAS
cancer
data
are
available
from: https://bcac.ccge.medschl.cam.ac.uk/bcacdata/
(breast
cancer); http://practical.icr.ac.uk/blog/?page_id=8088 (prostate cancer); GWAS Catalogue ID: GCST004481 (ovarian cancer); GWAS Catalogue ID: GCST006464 (endometrial cancer); GWAS Catalogue ID: GCST004748 (lung cancer); direct communication with consortia (renal and esophageal cancers); - phs001415.v1.p1, phs001315.v1.p1, phs001078.v1.p1, phs001903.v1.p1, phs001856.v1.p1 and phs001045.v1.p1 (US based studies) and GWAS Catalog ID: GCST90129505 (European based studies) colorectal cancer. 15 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint TABLES AND FIGURES LEGENDS Figure 1. Principles of Mendelian randomisation (MR) and study overview: (a) Assumptions in MR that need to be satisfied to derive unbiased causal effect estimates. Dashed lines represent direct causal and potential pleiotropic effects that would violate MR assumptions. A, indicates genetic variants used as IVs are strongly associated with the trait; B, indicates genetic variants only influence cancer risk through the trait; C, indicates genetic variants are not associated with any measured or unmeasured confounders of the trait-cancer relationship. SNP, single-nucleotide polymorphism; (b) Study overview. Created with BioRender.com. Figure 2. Power to demonstrate causal relationship in the Mendelian randomisation analysis across the eight different cancers. Each line represents one trait
with
line colour
indicating F-statistic,
a
measure
of
instrument
strength. The analysis of most traits is well powered across a modest range of
odds ratios and this generally corresponds to those with a higher F-statistic. F-stat, F-statistic Figure 3. Bubble plot of the causal relationship between selected traits and risk of different cancers. Each column corresponds to cancer type. Colours on the heatmap correspond to the strength of associations (odds ratio) and their direction (red positively correlated, blue negatively correlated), the size of each node corresponding to the -log10 P-value, with increasing size indicating a smaller P-value. In the available R/Shiny app (https://mrcancer.shinyapps.io/mrcan/), moving the cursor to each bubble will reveal the underlying MR statistics. Figure 4. Sankey diagram of literature spaces for exemplar cancer risk factors. Relationship between: (a) IGF-1 and colorectal cancer; (b) IL-18 and lung cancer; (c) LAG-3 and endometrial cancer; (d) PRDX1 and prostate cancer. Figure 5. Heatmap and dendrogram showing clustering of causal associations between traits and cancer risk. Heatmap based on Z-statistics using the clustering method implemented in the pheatmap function within R. Colours correspond to the strength of associations and their direction (red positive association with risk, blue inverse association with risk). Trait classes are annotated on the left. Only traits showing an association for at least one cancer type are shown. 16 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Table 1. Details of cancer genome-wide association studies used in the Mendelian randomisation analysis. 17 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint SUPPLEMENTARY TABLES LEGENDS Supplementary Table 1. List of traits examined in the Mendelian randomisation analysis and estimate of power for each trait and cancer type. Supplementary Table 2. Effect allele, frequency, effect on trait and strength of association with each cancer type for SNPs used as instrumental variables. Supplementary Table 3. Causal estimates from the Mendelian randomisation analysis for continuous traits and cancer risk. Supplementary Table 4. Causal estimates from the Mendelian randomisation analysis for continuous traits and breast cancer subtype. Supplementary Table 5. Causal estimates from the Mendelian randomisation analysis for continuous traits and lung cancer subtype. Supplementary Table 6. Causal estimates from the Mendelian randomisation analysis for continuous traits and ovarian cancer subtype. Supplementary Table 7. Weighted median estimate and mode-based estimates for each trait and cancer type. Supplementary Table 8. MR-Egger regression analysis for each trait and cancer type. Supplementary Table 9. MR Steiger analysis for each continuous trait and cancer type. Supplementary Table 10. Lifetime risk of each cancer used to calculate the proportion of variance explained. Supplementary Table 11. Leave one out inverse variance weighted random-effects MR analysis for each exposure trait and cancer type. 18 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Supplementary Table 12. The hierarchical levels of statistical support used to classify associations. Supplementary Table 13. Causal estimates for each Mendelian randomisation method for each binary trait and cancer risk. Supplementary Table 14. Causal estimates for each Mendelian randomisation method for each binary trait and breast cancer subtype. Supplementary Table 15. Causal estimates for each Mendelian randomisation method for each binary trait and lung cancer subtype. Supplementary Table 16. Causal estimates for each Mendelian randomisation method for each binary trait and ovarian cancer subtype. Supplementary Table 17. Details of filtering applied to instrumental variables used in the Mendelian randomisation analysis. Supplementary Table 18. Literature triples identified across eight different cancer types and Mendelian randomisation defined risk factors using SemMedDB. Supplementary Table 19. Stratification of literature space size by trait category. Supplementary Table 20. List of potential mediators for each trait identified using SemMedDB. 19 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint TABLES Cancer
Cases
Controls
PubMed ID
Number of contributing studies
GWAS Catalogue ID Breast
133,384 Triple negative
2,006 20,815 (subtype
analysis) 113,789
32424353
82 GCST010098
GCST010099
GCST010100 Luminal A
7,325 Luminal B
1,682 HER2 enriched
718 HER2 negative luminal B
1,779 Colorectal
73,673
86,854
36539618
16
GCST90129505 Endometrial
8,758
46,126
30093612
17
GCST006465 Lung
29,266 Ever-smoked
23,223
16,964 56,450
28604730
26 GCST004744
GCST004746
GCST004747
GCST004748
GCST004749
GCST004750 Never-smoked
2,355
7,504 Adenocarcinoma
11,273
55,483 Squamous cell carcinoma
7,426
55,627 Small cell lung cancer
2,664
21,444 Oesophageal
16,790
32,476
35882562
5
NA 20 Ovarian
26,293 Invasive high grade serous
13,037 40,941 (subtype
analysis) 68,502
28346442
77 GCST004415
GCST004416
GCST004417
GCST004418
GCST004419
GCST004461
GCST004462
GCST004478
GCST004479
GCST004480
GCST004481 All serous
16,003 Invasive mucinous
1,417 All mucinous
2,566 All low malignant potential
3,103 Invasive low grade serous and low malignant potential serous 2,966 Invasive low grade serous cases
1,012 Endometrioid
2,810 Clear cell
1,366 Low malignant potential serous
1,954 Low malignant potential mucinous
1,149 Prostate
79,194
61,112
29892016
8
GCST006085 Renal
10,784
20,406
28598434
5
GCST004710 Table 1 21 FIGURES Figure 1 22 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Figure 2 23 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Figure 3 24 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Figure 4 25 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint Figure 5 26 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.02.15.23285952
doi: 
medRxiv preprint",1
"Background. Homeostatic physiologic systems are regulated to maintain a particular variable such as blood pressure within a restricted range with relatively low variation, and this variation is increased with disease. Fractal physiologic systems, on the other hand, are characterized by wide, complex variation of variables such as heart rate or the walking stride interval, and this variation is decreased with disease. The present report examines time-series recordings of esophageal pH from normal subjects and different GERD phenotypes to measure the distributions of esophageal pH values and the distributions of changes in esophageal acid concentrations. Methods. Using Lyon consensus definitions of symptomatic GERD phenotypes, I analyzed 24-hour esophageal pH recordings from normal subjects (n=20), Functional Heartburn subjects (n=20), Reflux Hypersensitivity subjects (n=20), and nonerosive esophageal reflux disease (NERD) subjects (n=20). For each subject I calculated the distribution of pH values as well as the distribution of changes in esophageal acid concentrations. Results. Esophageal pH values have a power law distribution in both normal and symptomatic GERD phenotypes, and esophageal acid concentrations vary over four orders of magnitude in each group. The variation in esophageal acid concentrations decreased progressively from normal subjects to Functional Heartburn subjects to Reflux Hypersensitivity subjects to NERD subjects. Conclusions. The decreased variation in esophageal acid concentration in symptomatic GERD phenotypes represents changes associated with disease in a fractal physiologic system that shares characteristic features with other fractal systems . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 3 where disease states have been associated with decreased variation of heart rate or walking stride interval. INTRODUCTION Frequently, disease or ageing are accompanied by changes in one or more variables in physiologic systems (1-4). Homeostatic physiologic systems are regulated to maintain a particular variable such as blood pressure within a restricted range with relatively low variation, and this variation is increased with ageing or disease (1, 2). Fractal physiologic systems, on the other hand, are characterized by wide, complex variation of variables such as heart rate or the walking stride interval, and this variation is decreased with ageing or disease (3, 4). The Lyon Consensus Conference (5, 6) proposed criteria for the clinical diagnosis of three different phenotypes of gastroesophageal reflux disease (GERD): nonerosive gastroesophageal reflux disease (NERD), Reflux Hypersensitivity, and Functional Heartburn. Previous analyses, before recognition of different phenotypes of GERD, found that time-series of esophageal pH showed a fractal pattern in normal and GERD subjects (7). In other analyses esophageal pH values had a power law distribution in normal and GERD subjects (8). The present report examines time-series recordings of esophageal pH from normal subjects and different GERD phenotypes to measure the distributions of esophageal pH values as well as the distributions of changes in esophageal acid concentrations. SUBJECTS Patients were identified by exploring the electronic database at the Royal London Hospital GI Physiology Unit that contains clinically indicated impedance-pH recordings (Sandhill Scientific, Highlands Ranch, CO) from patients with typical symptoms of gastroesophageal reflux. Using Lyon consensus definitions of symptomatic GERD phenotypes,(5, 6), I selected 24-hour esophageal pH recordings from normal subjects (n=20), Functional Heartburn . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 4 subjects (n=20), Reflux Hypersensitivity subjects (n=20), and nonerosive esophageal reflux disease (NERD) subjects (n=20). All subjects had a normal upper gastrointestinal endoscopy at the time of the impedance-pH study. The esophageal pH recordings from one normal subject were technically unsatisfactory and were omitted from the present analyses. For this retrospective analysis of clinically indicated tests with no identifiable patient data, the Stanford University Institutional Review Board determined that this research does not involve human subjects as defined in 45 CFR 46.102(f) or 21 CFR 50.3 (g) (9). Values from impedance-pH testing from subjects for the present analyses have been published previously (10-12). Normal subjects, Functional Heartburn subjects and Reflux Hypersensitivity subjects all had normal esophageal acid exposure time (AET) with esophageal pH <4 for less than 4% of the 24-hour esophageal pH recording. NERD subjects had increased esophageal  AET of pH <4 for greater than 6% of the 24-hour esophageal pH recording. Reflux Hypersensitivity subjects had a positive association of symptoms with reflux episodes (13, 14), whereas Functional Heartburn subjects had no association of symptoms with reflux episodes. Normal subjects had no symptoms during the impedance-pH testing. METHODS The impedance-pH catheter was inserted with an esophageal pH sensor positioned 5cm above the upper border of the lower esophageal sphincter. Six impedance channels were positioned 3, 5, 7, 9, 15 and 17cm above the upper border of the lower esophageal sphincter, respectively. Software provided by Sandhill to process pH recordings automatically adjusts all pH values for the difference between the calibration temperature of 25C and the recording temperature of 37C. Software provided by Sandhill  was also used to export pH data for every 4th second of the recording to an Excel file. All pH values below 0.5 were replaced by 0.5 and all pH values above 7.5 were replaced by 7.5. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 5 To determine cumulative acid concentration for a subject, all pH values were converted to acid concentration in mmol/L  Cumulative acid concentration was calculated as the sum of all values for acid concentration for that subject, and each value of acid concentration was then expressed as a percentage of the cumulative acid concentration. The distribution of the change in sequential values of acid concentration was calculated for esophageal acidity for each subject. The frequency distributions of changes in acid concentration for each group of subjects were calculated as the means of the values from all subjects in the group for each bin of the distribution. Expressing acid concentration as a percentage of cumulative acid concentration for a given subject makes it possible to examine differences in the distributions of changes in acid concentration that do not depend on the magnitude of the acid concentration per se. Others (1) have calculated the change in values in a time-series as a percentage difference from the mean of all values. Curve fitting and statistical analyses were performed using GraphPad Prism 9.4.1 software. Because the present analyses were exploratory, P-values were not adjusted for multiple comparisons. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 6 RESULTS Figure 1. Distributions of values of esophageal pH in normal subjects and 
GERD phenotypes. Values given on the Y-axis are mean frequencies for 
the bin indicated on the X-axis .The solid lines are from linear, least-
squares regression analyses. Abbreviations are NL – normal subjects; FX 
HB – Functional Heartburn subjects; RFX HYP – Reflux Hypersensitivity 
subjects; NERD – Nonerosive Reflux Disease subjects. The number of 
subjects in each group is given in parentheses after each abbreviation. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 7 Table 1. RESULTS FROM LINEAR REGRESSION ANALYSES OF THE 
DISTRIBUTIONS OF ESOPHAGEAL pH VALUES. NORMAL 
FUNCTIONAL HEARTBURN REFLUX HYPERSENSITIVITY 
NERD SLOPE 
-0.5958 
-0.5094 
-0.5154 
-0.2047 R2 
0.966 
0.936 
0.976 
0.900 P-VALUE SLOPE NON- ZERO <0.0001 
<0.0001 
<0.0001 
<0.0001 P-VALUE SLOPE NON-ZERO was determined with an F-test. Results are from linear, 
least-squares regression analyses of the data in Figure 1 Table 2. COMPARISON OF THE SLOPES OF DISTRIBUTIONS OF ESOPHAGEAL 
pH VALUES IN NORMAL SUBJECTS AND GERD PHENOTYPES. COMPARISON 
P-VALUE 
F-VALUE (DFn, DFd) ARE SLOPES DIFFERENT? 
<0.0001 
43.29 (3, 60) NL VS FX HB 
0.0633 
3.72 (1, 30) NL VS RFX HYP 
0.0305 
5.16 (1, 30) NL VS NERD 
<0.0001 
135.0 (1, 30) FX HB VS RFX HYP 
0.8823 
0.022 (1, 30) FX HB VS NERD 
<0.0001 
62.24 (1, 30) RFX HYP VS NERD 
<0.0001 
130.7 (1, 30) Abbreviations are NL – normal; FX HB – functional heartburn; RFX HYP – reflux 
hypersensitivity; NERD – non erosive reflux disease; DFn – degrees of freedom 
numerator; DFd – degrees of freedom denominator. The p-value is from an F-test. Figure 1 shows a linear relationship between the values of esophageal pH and the logarithm of the frequency of the values for Normal subjects and each GERD phenotype. Results in Table 1 show that each slope of the lines in Figure 1 is significantly different from zero and accompanying values of R2 are at least 0.90. The linear relationships illustrated in Figure 1 are described by a power law in that the frequency of what is being measured is a negative exponential function of the magnitude of what is being measured, and the value of the exponential is given by the slope. The term “power law” is used because the frequency of a particular pH value is a power of the pH value. Table 2 shows results from pairwise comparisons of the values of the slopes given in Table 1. The slope from normal subjects was significantly higher than that from Reflux Hypersensitivity subjects and NERD subjects. It seemed possible that the lack of . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 8 statistical significance comparing the slope from Functional Heartburn subjects to that from normal subjects might be due to an under-powered sample size. The slope from NERD subjects was significantly lower than that from normal subjects, from Functional Heartburn subjects, and from Reflux Hypersensitivity subjects.  The slope from Functional Heartburn subjects was not significantly different from that from Reflux Hypersensitivity subjects. Figure 2 shows that the distribution of changes in cumulative esophageal acidity is biphasic for each GERD phenotype and normal subjects, and each distribution spans at least 4 orders of magnitude. The distribution of changes was significantly better fit by a 6th order polynomial than by a 5th order polynomial (P<0.0001 by F-test). A 7th order polynomial did not give a significantly better fit than a 6th order polynomial by an F-test. The polynomial model has no physiologic significance. It is simply a model that creates Figure 2. Distributions of change in values for cumulative esophageal 
acidity for GERD phenotypes and normal subjects. Values given on the Y-
axis are mean frequencies for the bin indicated on the X-axis .Abbreviations 
are CUM – cumulative; ESO – esophageal. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 9 a curve that comes close to the data points and makes it possible to test the data for statistical differences. Table 3. COMPARISON OF THE DISTRIBUTIONS OF CHANGES IN 
ESOPHAGEAL ACIDITY IN GERD PHENOTYPES USING FITS TO A 6TH ORDER 
POLYNOMIAL. COMPARISON 
P-VALUE 
F-VALUE (DFn, DFd) DIFFERENT CURVE FOR AT LEAST 1 DATASET <0.0001 
49.03 (21, 138) NL VS FX HB 
<0.0001 
33.25 (7, 67) NL VS RFX HYP 
<0.0001 
69.32 (7, 66) NL VS NERD 
<0.0001 
90.87 (7, 67) FX HB VS RFX HYP 
<0.0001 
9.827 (7, 71) FX HB VS NERD 
<0.0001 
47.94 (7, 72) RFX HYP VS NERD 
<0.0001 
29.01 (7, 71) Abbreviations are NL – normal; FX HB – functional heartburn; RFX HYP – reflux 
hypersensitivity; NERD – non erosive reflux disease; DFn – degrees of freedom 
numerator; DFd – degrees of freedom denominator. The p-value is from an F-test. Table 3 gives results from statistical comparisons of the data for distributions of changes in esophageal acid concentration in Figure 2 and indicates that all pairwise comparisons are significantly different at P<0.0001 by an F-test. Furthermore, the variation in esophageal acid concentrations is indicated by the range of values for change in esophageal acid concentration and is greatest in normal subjects, less in Functional Heartburn, still less in Reflux Hypersensitivity and least in NERD. DISCUSSION The present analyses of time-series of esophageal pH measurements show that esophageal pH values have a power law distribution in both normal and symptomatic GERD phenotypes, and that esophageal acid concentrations vary over four orders of magnitude in each group. Also, the variation in esophageal acid concentration decreased progressively from normal subjects to Functional Heartburn subjects to Reflux Hypersensitivity subjects to NERD subjects. Thus, the different GERD . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 10 phenotypes based on clinical characteristics identified by the Lyon Consensus Conference (5, 6) may actually share a common pathophysiology of varying severity. The decreased variation in esophageal acid concentrations in symptomatic GERD phenotypes represents changes associated with disease in a fractal physiologic system that is characterized by wide, complex variation under normal circumstances.  Thus, GERD phenotypes share characteristic features with other fractal systems where disease states have been associated with decreased variation of heart rate or walking stride interval (3, 4). All GERD phenotypes have symptoms of gastroesophageal reflux such as heartburn, regurgitation, or chest pain (5, 6) even though Functional Heartburn subjects and Reflux Hypersensitivity subjects have normal esophageal acid exposure times. It seems possible that regardless of the value of esophageal acid exposure time, the decreased variation in esophageal acid concentration, which can result in runs of self-similar esophageal acid concentrations, may represent a pathophysiologic signal to esophageal mucosa that triggers a symptom. Previous analyses of the same subjects used for the present study found that esophageal acid sensitivity appears to oscillate in each GERD phenotype, and for a given value of esophageal acid sensitivity, Reflux Hypersensitivity subjects have significantly more sequential symptoms associated with this sensitivity than do Functional Heartburn subjects (11). This difference between Functional Heartburn subjects and Reflux Hypersensitivity subjects might indicate that the lower variation of values of esophageal acid concentrations in Reflux Hypersensitivity subjects can cause an increase in GERD symptoms. Acknowledgement: I am grateful to Dr. Daniel Sifrim, Director of Upper GI Physiology Unit, Royal London Hospital for providing the impedance-pH records and for stimulating, helpful discussions. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288021
doi: 
medRxiv preprint 11",1
"Objectives A pitching motion involves three-dimensional whole body movement. Proper pelvic and trunk rotation movement are important for the prevention of throwing injuries. Given that throwing is not a simple rotation movement, the evaluation of proper motion should reflect muscle strength as well as coordination and pitching motion characteristics. We have devised a throwing rotational assessment (TRA) as a new evaluation of the total rotation angle required for throwing. The purpose of this study was to examine the characteristics of players with throwing disorders compared to a pain-free group using TRA. Materials and methods The subjects consisted of 164 high school baseball pitchers who participated in a medical check. Pain- induced tests included an elbow hyperextension test and an intra-articular shoulder impingement test. Pitchers who felt pain in either test were classified into a disorder group (n=61).  With the subjects in a position similar to the foot contact phase of throwing, the rotation angles of the pelvis and trunk were measured. All tests were performed in the throwing and opposite directions. Results The disorder group had significantly lower average rotation angles of the pelvis and trunk in the throwing direction and the rotation angle of the trunk in the opposite direction compared to the healthy . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint group. Conclusion TRA reflects the complex whole body rotation movement. Reductions in rotational angles as assessed in TRA may be associated with throwing disorders. TRA is a simple method that may be useful in the early detection of a throwing disorder and could be used in the systematic evaluation during a medical check, as well as during self-check in the sports field. Keywords： Throwing disorder, evaluation, pitching, range of motion, rotation . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Introduction In the execution of a throwing motion with a high degree of performance, many factors— such as the range of motion, muscle strength, timing of muscle contractions, and lower limb/trunk/upper limb kinetic chains—are involved [1]. Furthermore, three-dimensional movements (sagittal plane, coronal plane, horizontal plane) are required, with the trunk and pelvis rotation playing a particularly important role in movements along the horizontal plane. Studies of the relationship between rotation during the throwing motion and performance have shown that there is a correlation between the rotation angular velocity of the trunk in the throwing direction and speed [2]. In addition, decreased pelvic rotation velocity [3] and poor pelvic and trunk rotation timing [4] cause a decrease in throwing velocity. With regard to throwing disorders, research has shown that limitations in trunk rotation affect the shoulder and elbow joints [5]. If there is excessive rotation of the trunk (the body rotates to face forward) immediately before foot contact, the external rotation (valgus) torque in the elbow joint increases [6]. Poor timing in the pelvic and trunk rotation increases shoulder internal rotation torque [7]. Moreover, limitations in the range of motion of the lower limbs and trunk, as well as decreased stability and support, cause impingement of the rotator cuff and labrum in the late cocking phase due to an excessive external rotation and horizontal abduction of the shoulder joint of the throwing arm [8, 9]. Additional research has shown that a glenohumeral internal rotation deficit (GIRD) increases . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint the risk of shoulder and elbow joint disorders four-fold [10]. However, some reports indicate that there is no relationship between the range of motion and ulnar collateral ligament damage in the elbow joints [11]. As such, there is currently no consensus on this issue. Rather than considering the range of motion of each individual joint, it is necessary to consider a composite of factors including muscle strength and flexibility as a whole, in order to make an objective evaluation of the rotational motion of the entire body. Analyses of the throwing motion using a three-dimensional motion analysis system [2, 6] have shown that it is possible to calculate the detailed joint angle and joint torque, and is a useful method in the evaluation of performance and disorder prevention. However, due to the cost and the time required to perform such an analysis, it is not feasible in a clinical setting for most athletes. In this study, we recreated the foot contact phase of the throwing motion and devised a throwing rotational assessment (TRA) that indicates the rotation motion of the entire body and the composite elements, which can easily be utilized in clinical settings. The objective of this study was to investigate the reliability and usefulness of TRA in elucidating throwing disorders. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Subjects and methods Subjects Baseball skill camps are held annually for all high school baseball clubs in Kyoto Prefecture during the off-season. Study participants were recruited at these camps between 2010 and 2011 (Table 1). Table 1. The baseline characteristics of participants. Data presented as mean ± standard deviation Age (years)
1.63 ± 0.6 Height (cm)
173.2 ± 5.4 Weight (kg)
64.5 ± 7.0 Years of baseball experience (years)
7.7 ± 1.9 We enrolled 164 male pitchers, aged 16 to 17 years old (mean, 16.3±0.6 years). Prior to the start of this study, descriptions of the study were provided to all subjects and their parents/guardians and consent was obtained from all subjects. This study was performed in accordance with the declaration of Helsinki after obtaining approval from the ethics committee at our institution (Raku-gaku-Rin-01-000100). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Methods Diagnosis of the disorder group An orthopedic surgeon examined the subjects' shoulder and elbow joints. Players who were determined to require secondary observation in the hospital were placed into the disorder group. All others were placed into the healthy group. Those players who experienced limitations in movement due to leg pain or lower back pain were excluded. TRA movement method TRA of the foot contact phase of the throwing motion (Fig 1) was investigated by a group of three physiotherapists. Fig 1. Phase of throwing motion The foot contact phase is depicted in relation to other phases of the throwing motion. The physician performing the examinations and the physiotherapists were blind to the participant’s group status. The subjects were asked to stand with their feet the same distance apart as when throwing. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Their lower limb on the stance leg side was placed in a position that abducted the hip joint, and the subject took a stance that was neutral between internal and external rotation with their knee joint extended. The lower limb on their lead leg side was positioned so that the hip and knee joints were flexed and the hip joint was abducted and externally rotated. Both lower limbs bore the same amount of weight. From this initial posture, the participant’s hips and thoraxes rotated actively in either the throwing direction or in the opposite direction from the throwing direction. The angle of the hip rotation was calculated by placing step and horizontal lines from the stationary arm and the movable arm so that it connected both posterior superior iliac spines on either side. We also designated the step direction and horizontal line between the stationary arm and a line connecting the inferior angles of the scapulae on both sides as the movable arm in order to calculate the trunk rotation angle (Fig 2). Fig 2. Rotation during the foot contact phase In our evaluation of the foot contact phase, the step width was equivalent to actual pitching. The knee joint on the pivoting side is in an extended position and the step side is flexed. The weight bearing is equal for both legs. The pelvis and thorax are rotated in the throwing direction and in the opposite direction. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint A standard goniometer was used to measure the angles, which were measured in one-degree increments. In order to prevent trick motion by the feet, lower legs, or via flexion or lateroflexion of the hip or trunk, a researcher held the lower limb on the stance leg side in place. Visual observations were also made by all the researchers to ensure that compensatory movements did not affect the assessments. Statistical analysis The average pelvic and thoracic angle of rotation was calculated and expressed as the average ± standard deviation. A two-way factorial analysis of variance (ANOVA) with the presence vs. absence of a disorder as the intra-subject factor and throwing vs opposite direction as the intra- subject factor. The disorder group was further subdivided into a shoulder joint disorder group and an elbow joint disorder group and an additional one-way analysis of variance on the resulting three groups (shoulder, elbow, healthy) was performed. Statistical significance was set to p≤0.05. In order to investigate the utility of TRA as a screening method for throwing disorders, a receiver operating-characteristic (ROC) curve was created for items that showed a significant difference in our comparison of the healthy group and disorder groups. After determining that the area under the curve (AUC) was valid, sensitivity and specificity were calculated as well as the cutoff value using the Youden index (sensitivity + specificity - 1). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint In order to investigate the reliability of the measurements, we calculated the intra-class correlation coefficient (ICC) to assess intra-subject reliability (1, 1) and inter-subject reliability (2, 1), as well as the measurement error for the 20 healthy subjects. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Results Detailed description case of the disorders A total of 26 players were determined by the physician to have shoulder joint disorders and a total of 35 players were determined to have elbow joint disorders (Table 2). Table 2. Disorder ratios and details Disorder ratios and details Disorder ratio
103 Healthy cases (68.7%), 61 Disorder cases (31.3%) Shoulder joint disorder
26 cases (15.9%) Elbow joint disorder
35 cases (21.3%) All the players underwent follow-up examinations including X-rays, CT scan or MRI at a hospital, whereupon, they were diagnosed with throwing shoulder or throwing elbow disorder as determined by the examination findings. Pelvic and thoracic angle of rotation . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Since we observed a significant interaction for the pelvic rotation angle in the 2-way ANOVA (Table 3), we tested the simple main effects for movement direction. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint 1
Table 3. Pelvic and trunk rotation angles Healthy n=103
Disorder n=61
Main effect
Interaction Throwing direction Opposite direction Throwing direction Opposite direction Attribute
Direction Mean
SD
Mean
SD
Mean
SD
Mean
SD
F value
F value
F value Pelvis
41.21 
11.49 
37.23 
10.80 
34.67 
7.05 
34.39 
7.19 
12.14**
6.29*
4.75* Trunk
82.32 
13.81 
76.93 
14.73 
76.05 
12.57 
72.38 
11.26 
7.36**
28.74**
1.03 2
**, p < 0.01 3
*, p < 0.05 The results indicated that subjects in the healthy group had significantly higher values for rotation in the throwing direction compared to the opposite direction, but that there were no significant differences in the disorder group. Moreover, the average pelvic rotation angle was significantly higher in the healthy group. No significant interaction for thoracic rotation angle was observed. However, the healthy group had significantly higher values than those in the disorder group. The comparison of shoulder and elbow joint disorders with the healthy group indicated significantly higher pelvic rotation angle in the healthy group compared to both throwing disorders. In addition, the thoracic rotation angle was significantly higher in the healthy group compared to the elbow joint disorder group (Table 4). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Table 4. Pelvic and trunk rotation angles Healthy (n=103) Shoulder joint disorder (n=26) Elbow joint disorder (n=35) Main effect Mean
SD
Mean
SD
Mean
SD Pelvis
41.21 
11.49 
33.65 
7.61 
35.29 
6.64 
8.19 Trunk
82.32 
13.81 
76.54
14.86 
75.69 
10.77 
4.22 ** p < 0.01 * p < 0.05 Cutoff values The cutoff value calculated from the ROC curve was 34.5° for the pelvic angle, with sensitivity of 74.8%, specificity of 52.5%, and an AUC of 0.69 (Fig 3). Fig 3. ROC curve with pelvic angle for throwing disorders AUC, area under the curve The cutoff value calculated from the ROC curve for the thoracic angle was 81°, with sensitivity of 66.0%, specificity of 52.4%, and an AUC of 0.67 (Fig 4). Fig 4. ROC curve with thoracic angle for throwing disorders AUC, area under the curve Reliability ICC (1, 1) was 0.92 for the pelvis and 0.94 for the thorax. ICC (2, 1) was 0.82 for the pelvis and 0.85 for the thorax, indicating high reliability. Measurement error was 2.27° for the pelvis and 2.46° for the thorax. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Discussion Common throwing disorders involving the shoulders include proximal humerus epiphysiolysis in elementary and junior high school students who have not yet undergone epiphyseal closure, and internal impingement syndrome in high school and university students who have undergone epiphyseal closure [8, 12]. Internal impingement syndrome is a common throwing shoulder disorder and was first reported by Walch in 1992. It is characterized by excessive abduction and horizontal external rotation of the throwing shoulder joint during the late cocking phase of the throwing motion, which causes impingement of the rotator cuff and glenoid rim within the shoulder joint [9]. If the impingement is repeated with continuous throwing, it may lead to lateral rotator cuff tear and damage to the glenoid rim. Thus, early detection and prevention are important. The prevalence of posterior and lateral elbow joint disorders is high. Posterior disorders are divided into the following three types: posteromedial elbow impingement (malacia, synovitis, and other types of early osteoarthrosis due to sports, olecranon spur), incomplete olecranon epiphyseal closure, and olecranon stress fracture [13-15]. The mechanism of onset it thought to be a valgus extension overload from the late cocking phase to the acceleration phase and a mechanical door stop action in the follow-through phase. Internal disorders are caused by excessive external rotation of the elbow during the acceleration phase and traction stress on the inner tissue. These disorders are highly prevalent, with 70% of pitchers experiencing pain on the inner side [16]. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Risk factors for throwing disorder include the number of pitches, the type of pitches, the height of the pitcher, and the pitching form [17, 18]. Pitching form in particular is affected by physical functions such as the range of motion and muscle strength [19]. Investigation into differences for the range of motion for the right and left sides in baseball pitchers has elucidated that the shoulder on the pitching side has an increased external rotation angle and decreased internal rotation angle due to bone-related factors such as reduced flexibility of the posterior tissue of the shoulder, elongation of the anterior capsule of the shoulder, and increased humeral retroversion angle [20-24]. In addition, investigation into lower limb and trunk range of motion has shown that in healthy junior high and high school baseball pitchers, the rotation angle of the neck and trunk was significantly larger and that during hip joint internal rotation, the angle on the lead leg side was significantly larger than on the stance leg side [25]. The TRA used in this study also indicated that there was significant rotation toward the pitching direction, but there was no difference between right and left hip rotation angles in the disorder group. Hip rotation angle requires hip joint abduction and rotation angles, and therefore the absence of a difference between the right and left sides may indicate a correlation to the disorder. TRA calls for the recreation of the foot contact phase because during this phase, translational motion shifts to rotational motion. This is an important phase during which potential energy is converted to propulsive force and the lower limb muscle groups of the axis foot that is . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint fixed in place produce maximum momentum [26]. Regarding kinematics, it is performed by eccentric contraction on hip joint external rotation group of muscles and the adductors of the stance leg. However, dysfunctions, such as a restricted range of motion or reduced muscle strength, cause the throwing form to become disordered [27] and as a result, excessive shoulder abduction, reduced step width, and poor shoulder rotation timing occur, causing poor performance [3, 4, 28]. In addition, poor hip and trunk rotation timing and facing the body toward the front during this phase lead to greater stress on the shoulder and elbow joints [5-7], which may lead to throwing disorder. TRA is thought to involve abduction and external rotation of the hip joint on the stance leg side and the angles of flexion, rotation, and abduction of the hip joint on the lead leg side. Moreover, the muscle strength of the adductor on the stance leg side as well as the hamstring and gluteus maximus on the lead leg side may be involved. Our investigation of trunk rotation indicates that, in addition to the aforementioned lower limb range of motion and muscle strength, the trunk rotation range of motion is involved in TRA. The results of this study indicated significantly lower values for both hip and trunk rotation in the disorder group as compared to the healthy group. These lower limb and trunk dysfunctions are factors that can cause dropped elbow, excessive external rotation of the shoulder joint, and horizontal external rotation during the subsequent late cocking phase, which we believe may lead to a throwing disorder. Subjects with elbow joint disorder in particular showed lower values than . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint healthy subjects for both the hip and trunk rotation angles, which may indicate that the test is a better suited to screen for elbow joint disorder. Further study of this issue with larger numbers of subjects is required. This assessment method is reproducible, can be easily performed, and assesses complex elements in a short time. Its sensitivity in detecting hip rotation angle disorders is relatively high at 74.8%, which makes it useful as a screening method during medical checkups. It may also be useful as a self-check and partner-check at sporting events. However, because the present study was a pilot study, future detailed assessments of the range of motion for a variety of joints and muscle strength levels using TRA from the point of view of kinetics and dynamics, as well as elucidation of the association between actual throwing motion and TRA are required. Moreover, the investigation of whether improving the range of motion as assessed by TRA can prevent the future onset of shoulder internal impingement and posterior elbow disorders is needed in addition to the development of training methods that improve TRA. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint Conclusion We devised an assessment method that evaluates complex rotation motions similar to the throwing motion and used it to compare high school pitchers with a throwing disorder to those without (healthy). Our comparison of the healthy and disorder groups indicated that the disorder group had significantly lower values for both the hip and trunk. Disorder was determined by the hip cutoff value of 42.5°, sensitivity of 89.3%, and specificity of 45.6%. The trunk cutoff value was 81°, sensitivity was 63.8%, and specificity was 65%. Our method is reproducible, can be easily used to assess complex elements in a short time, and is useful as a screening method during medical checkups. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287922
doi: 
medRxiv preprint",1
"Background: The COVID-19 pandemic has led to many studies of seroprevalence. A
number of methods exist in the statistical literature to correctly estimate disease prevalence
or seroprevalence in the presence of diagnostic test misclassification, but these methods
seem to be less known and not routinely used in the public health literature. We aimed to
examine how widespread the problem is in recent publications, and to quantify the magnitude
of bias introduced when correct methods are not used. Methods: A systematic review was performed to estimate how often public health re-
searchers accounted for diagnostic test performance in estimates of seroprevalence. Using
straightforward calculations, we estimated the amount of bias introduced when reporting
the proportion of positive test results instead of using sensitivity and specificity to estimate
disease prevalence. Results: Of the seroprevalence studies sampled, 78% (95% CI 72% to 82%) failed to
account for sensitivity and specificity. Expected bias is often more than is desired in practice,
ranging from 1% to 12%. Conclusions: Researchers conducting studies of prevalence should correctly account for
test sensitivity and specificity in their statistical analysis. Keywords: Prevalence, Seroprevalence, Diagnostic Tests, Statistical Methods, Rogen-
Gladen, Bayesian, Sensitivity, Specificity 1 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Introduction Since the beginning of the SARS-CoV-2 pandemic, thousands of papers have been pub- lished detailing seroprevalence estimates in various populations (1). A glance into recent publications indicates that while some researchers used simple approaches such as pro- portions or logistic regression, others used complicated methods like Bayesian hierarchical models. An important question is therefore how often these methods are used in epidemio- logical studies and what, if any, degree of bias was introduced by using one method or the other. As diagnostic tests are not 100% accurate, it is expected that some small number of test results will be either false positives or false negatives. Using a simple proportion of the num- ber of positive diagnostic tests over the total number of tests ignores any misclassification inherent to the test. In the case where there are similar numbers of true positives and true negatives in the population, the bias introduced by using the proportion of positive tests to estimate the proportion of subjects with the disease may not be very high. However, if the rate of false positives differs greatly from that of false negatives, the bias may be quite large. Table 1: Typical example of 2x2 table comparing diagnostic test results and disease status. Test neg
Test pos
total Disease neg
724
80
804
Disease pos
20
176
196
total
744
256
1000 For example, in Table 1, 35.5% (355/1000) of subjects had a positive test result, but the true disease prevalence in this population is 30.0% (196/1000). So there is a bias of 5.5% because there are many more false positive test results (n = 70) than false negatives (n = 15). Other examples of this phenomenon are found in the literature (2,3). Statisticians often talk about sensitivity, 95% in this example, and specificity, 90%, of the diagnostic in relation to these quantities (described in more detail below), but it is accepted that without a “gold standard” diagnostic tool, it is difficult to accurately assess disease prevalence. Accounting for such misclassification in the interpretation of diagnostic tests is certainly not 2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint new in the literature. A straightforward method of adjusting observed prevalence is available (4,5), which gives a maximum likelihood estimate of true prevalence assuming predefined test sensitivity and specificity. The Rogan-Gladen correction has been extended to compute confidence intervals (6,7). Recently, an adaptation of the Rogan-Gladen correction that accounts for sampling bias, for example if only hospitalized subjects as opposed to the general population have been tested, has been proposed (8–10). Bayesian approaches have also been developed (3,11,12). A comparison of Bayesian and frequentist methods (13) showed that Bayesian methods are to be preferred, or the method of (4) with confidence intervals of (7). Despite this extensive treatment of the misclassification problem in the statistical literature, many public health researchers appear to not realize they may be publishing biased results or know what to do about it. In what follows, a systematic review quantifies the proportion of recent publications estimating seroprevalence that do not correct for diagnostic test performance. We will describe key concepts, and derive an estimate of the bias, as well as a range of prevalences where such naive estimates show low bias. Bias estimates will be described according to test sensitivity and specificity, and we will apply these results to a real example of SARS-CoV-2 seroprevalence in children. Methods To start, we introduce some notation. Disease status, D, is denoted 1 if a subject has the disease in question (or for the case of seroprevalence, has antibodies for it), and 0 otherwise. Similarly, the result of the diagnostic test, Y, is given as 1 if the subject tests positive for the disease, and 0 otherwise. FP is often used to refer to false positive test results, and similarly FN for false negatives, TN for true negatives and TP for true positives. Prevalence is the probability of having the disease of interest, P = Pr(D = 1). Often in prevalence studies, this probability is studied at a specific point in time, giving so-called point prevalence (14). Seroprevalence, a related concept, looks at the proportion of individuals in the population have antibodies for a specific disease, for example, SARS-CoV-2 (15). Sensi- 3 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint tivity, denoted Se, sometimes also called the true positive fraction (TPF), is the probability of having a positive test result, given that the subject has the disease, Pr(Y = 1|D = 1) (2). On the other hand, specificity, Sp is the probability of having a negative test result when a subject does not have the disease, Pr(Y = 0|D = 0) (sometimes 1 - specificity is discussed, which is often referred to as false positive fraction, or FPF (16)). In real settings where true disease status is known via another method, sometimes referred to as the “gold standard”, Se can be computed as TP/(TP + FN), where TP is the number of true positives and FN is the number of true negatives. Similarly, Sp can be computed as 1 −FP/(FP + TN). The proportion of positive tests can be expressed as Pr(Y = 1) = (FP + TP)/(FP + TP + TN + FN), while the disease prevalence in the sample can be expressed as Pr(D = 1) = (FN + TP)/(FP + TP + TN + FN). The difference between these two quantities is simply (FP −FN)/(FP + TP + TN + FN), that is, the proportion of false positives minus the proportion of false negatives. According to the definition of joint probability Pr(A, B) = Pr(A|B)Pr(B), the proportion of false positives can be written as Pr(Y = 1, D = 0) = Pr(Y = 1|D = 0)Pr(D = 0), which simplifies to (1 −P)(1 −Sp). In a similar fashion, the proportion of false negatives can be written as Pr(Y = 0, D = 1) = Pr(Y = 0|D = 1)Pr(D = 1), which simplifies to P(1 −Se). The bias when using the proportion of positive tests, Pr(Y = 4 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint 1), to estimate the proportion with disease, Pr(D = 1), is therefore (1 −P)(1 −Sp) − P(1 −Se) or equivalently 1 −Sp + P(Sp + Se −2). Suppose we want to guarantee that the bias is no larger than, say, δ = 0.02, that is ±2% in either direction. We can solve −δ ≤1 −Sp + P(Sp + Se −2) ≤δ for P, to get: max
 δ + Sp −1 Sp + Se −2, 0

≤P ≤min
−δ + Sp −1 Sp + Se −2 , 1

. The lower bound will be 0 if δ ≥1 −Sp, while the upper bound will be 1 if δ ≥1 −Se. Therefore, if both Se and Sp are very high, say 99% or higher, then the proportion of positive tests is a good estimate of the true prevalence. If only Se is that high, this is will be true only when the true prevalence is quite high, and conversely if only Sp is very high, this will be true only when true prevalence is quite low. When neither Se nor Sp is high, the proportion of positive tests may or may not be a good estimate of the true prevalence. One simple way to reduce this bias, if no dependence on covariates is assumed, is to use the Rogan-Gladen correction (4). Assuming an observed fraction Pobs of positive test results, the corrected prevalence is PRG = Pobs + Sp −1 Se + Sp −1 . In a small number of cases, primarily when the sample size and the prevalence are both small (17,18), the Rogan-Gladen correction will yield values less than 0 or greater then 1. However, even if this “clipped” version has some bias, the variance will be smaller. The systematic review of recent studies of seroprevalence in the literature started with a pubmed (https://pubmed.ncbi.nlm.nih.gov/) search for “covid-19 seroprevalence”, which 5 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint yielded 637 publications published in 2022. Publications were included in the systematic review if they assess COVID-19 seroprevalence in humans, and were published in 2022 in English or German. Exclusion criteria included: 1) studies comparing seroprevalence in different subgroups, 2) studies examining risk factors for seropositivity, 3) studies in animals, 4) reviews, 5) methodological papers, 6) studies with possible conflict of interest, 7) if the full text was not available or 8) if the publication was a research letter. The following information was extracted: 1) whether the aim of the study was to assess COVID-19 seroprevalence in humans, 2) the sensitivity and 3) specificity of the diagnostic test, 4) the reported seroprevalence estimate (the first mentioned value, and if unadjusted was reported before adjusted, we extracted the most adjusted value of the first mentioned seroprevalence), and 5) which statistical methods were used to calculate seroprevalence. A protocol for the systematic review was developed using the PRISMA-P checklist (https://osf.io/b59x2/). Two independent reviewers (SRH and DK) screened the publications using the rayyan.ai web-based tool, and performed data extraction in parallel using a structured spreadsheet. Discrepancies were resolved by discussion. Summary statistics were computed for the methods used (n (%)), reported sensitivity and specificity (median [range]) and estimated bias (median [range]). To provide a concrete example of this problem, we use the Ciao Corona study (19), a school-based longitudinal study of seroprevalence in Swiss school children with 5 rounds of SARS-CoV-2 antibody testing between June 2020 and June 2022, covering a range of seroprevalences in the population (Trial Registration: ClinicalTrials.gov NCT04448717). The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the Canton of Zurich, Switzerland (2020-01336). All participants provided written informed consent before being enrolled in the study. Results To examine the methods actually used in seroprevalence studies in the literature, we performed a systematic review of publications from 2022 which estimated COVID-19 sero- 6 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint Table 2: Key outcomes of systematic review. The main analysis included 291 publications
meeting all inclusion criteria. Characteristic
Overall
N = 291 (100%)
uncorrected
N = 226 (78%)
corrected
N = 65 (22%) Statistical method
Rogan-Gladen
39 (13%)
0 (0%)
39 (60%)
Bayesian
18 (6.2%)
0 (0%)
18 (28%)
unspecified method
8 (2.7%)
0 (0%)
8 (12%)
unclear
2 (0.7%)
2 (0.9%)
0 (0%)
uncorrected
224 (77%)
224 (99%)
0 (0%)
Reported test Se and Sp?
did not report
96 (33%)
93 (41%)
3 (4.6%)
partially reported
11 (3.8%)
11 (4.9%)
0 (0%)
reported
184 (63%)
122 (54%)
62 (95%)
Se
95.2 (60.2 - 100.0)
96.8 (60.2 - 100.0)
92.7 (66.9 - 100.0)
Sp
99.6 (82.4 - 100.0)
99.5 (86.5 - 100.0)
99.7 (82.4 - 100.0)
expected bias
0.0 (-12.2 - 9.1)
0.0 (-12.2 - 7.1)
-0.3 (-12.2 - 9.1)
expected bias (category)
[-15,-10)
5 (2.7%)
3 (2.5%)
2 (3.2%)
[-10,-5)
6 (3.3%)
3 (2.5%)
3 (4.8%)
[-5,-1)
29 (16%)
18 (15%)
11 (18%)
[-1,1)
113 (61%)
74 (61%)
39 (63%)
[1,5)
28 (15%)
22 (18%)
6 (9.7%)
[5,10]
3 (1.6%)
2 (1.6%)
1 (1.6%) 1 n (%); Median (Range) prevalence in humans (Table 2). Of the 640 publications identified in pubmed, 4 were duplicates, and 349 were excluded (5 represented possible conflicts of interest, 7 were published in languages other than English or German, 2 did not examine COVID-19, 9 were animal studies, 22 described secondary research, 233 did not assess seroprevalence, 41 compared subgroups or risk factors for seropositivity, 3 did not have full texts available, and 23 were published as research letters). Of the remaining 291 publications (Supplementary Material Table S1), 77.7% (n = 226, 95% CI 72.4% to 82.3%) did not adjust for diagnostic test performance, while 22.3% corrected for sensitivity and specificity of the diagnostic test (n = 65, 17.7% to 27.6%). Among the publications which adjusted for test characteristics, 39 (13.0%) used Rogan-Gladen correction, 18 (6.2%) used Bayesian approaches, and 8 (12%) mentioned adjustment but did not specify further. 7 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint Further, among those publications that did adjust for test performance, 122/226 (54.0%) reported sensitivity and specificity, the remaining publications either did not report test characteristics (41.0%, n = 93) or only reported partial test characteristics (4.9%, n = 11). Among all publications reviewed, it is therefore observed that 33% (99/291) neither adjusted for test performance nor reported sensitivity and specificity. Among those that did not correct for test performance but did report both sensitivity and specificity (n = 122), expected bias ranged from -12.2% to 7.1%. 74 (61%) of the publications reporting seroprevalence to within ±1% of the true value despite not using any adjustment, while the remaining 48 (39%) needed adjustment for test performance (8 of those were not even within ±5%). It could be inferred therefore that approximately 41 of the 104 publications not or partially reporting test performance are also in need of adjusted seroprevalence estimates to account for test performance, even though all of those publications reported naive estimates. These results did not change when including publications denoted “research letters” (Supplementary Material Table S2). While the need to adjust seroprevalence estimates for test performance is well known the the statistical literature, the vast majority of published analyses on this topic fail to account for it when they should have. This problem is also not restricted to “low quality” journals, as such analyses can be found also in many prominent journals (Supplementary Data). Next, we sought to characterize scenarios where expected bias would be minimal. Using the result bias = 1 −Sp + P(Sp + Se −2) described above, we calculated the expected bias for a range of reasonable combinations of sensitivity, specificity and disease prevalence (Table 2, Figure 1). When sensitivity and specificity were both 90%, bias was as high as 10%, especially near prevalences of 0% or 100% (bottom row of Table 2, solid line in leftmost panel of Figure 1). When specificity was 90%, a bias of 10% could be expected with small prevalences near 0% even if sensitivity was 99% (e.g. 3rd line of Table 2). The least bias, 1%, could be expected where sensitivity and specificity were both 99% (1st line of Table 2). Using the bounds of prevalence as derived above, we explored where the maximum tolerated bias is limited to 1%, 2.5% and 5% (Figure 2). When Se and Sp are each 90%, bias is within 8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint Table 3: Estimated bias (in percentage points) for selected combinations of sensitivity (Se),
specificity (Sp) and disease prevalence (P) Se
Sp
P=2
P=10
P=30
P=50
P=90
P=98 90
99
0.8%
-0.1%
-2.2%
-4.4%
-8.8%
-9.8%
90
95
4.7%
3.5%
0.5%
-2.2%
-8.5%
-9.7%
90
90
9.8%
8.0%
3.8%
0.1%
-7.6%
-9.6% 95
99
0.9%
0.4%
-0.8%
-2.1%
-4.4%
-4.9%
95
95
4.8%
4.0%
1.8%
0.0%
-4.0%
-4.8%
95
90
9.7%
8.7%
5.2%
2.7%
-3.6%
-4.7% 99
99
1.0%
0.8%
0.4%
0.0%
-0.8%
-1.0%
99
95
4.9%
4.4%
3.3%
1.9%
-0.3%
-0.9%
99
90
9.8%
9.0%
6.8%
4.6%
0.1%
-0.8% a tolerance of 1% only very close to 50% disease prevalence, within 2.5% tolerance in the range of 38% - 62% disease prevalence and to within 5% tolerance as long as disease prevalence is between 25% and 75%. When the desired tolerance is 1%, the range of disease prevalence where a naive approach will yield unbiased results is fairly narrow in all cases, unless Se and Sp are each at least 99%. Outside of these ranges, using the proportion of positive test results to estimate seroprevalence will be too biased, and more sophisticated analysis methods should be used. As an example of this, take the Ciao Corona study (19), a school-based longitudinal study of seroprevalence in Swiss school children with 5 rounds of SARS-CoV-2 antibody testing between June 2020 and June 2022. The antibody test used has a sensitivity of 94% in children, and a specificity of 99.2%. In June 2020, 98 / 2473 (4.0%) of subjects showed as seropositive, compared to 154 / 2500 (6.2%) in October 2021, 17.3% (426 / 2453) in March 2021, 48.5% (910 / 1876) in November 2021, and 94.5% (2008 / 2125) in June 2022. Given the diagnostic test characteristics, absolute bias can be expected to be less than 1% in the range of 0% - 26.5% disease prevalence, and less than 2% for disease prevalence of up to 41.2%. These results imply that reported seroprevalence estimates based on a naive logistic approach are likely relatively unbiased for the first 3 rounds of Ciao Corona antibody testing (0.5%, 0.4% and -0.4% respectively), but that after that any seroprevalence estimates that do not adjust for test characteristics are likely quite biased (-2.4% and -5.6%). In order 9 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint to adjust for covariates and survey sampling weights, we corrected the seroprevalence estimates using a Bayesian hierarchical model approach in all rounds of testing. Discussion We have demonstrated that average bias in prevalence estimates can be higher than desired, as high as 10%, when using a naive approach of calculation based on the proportion of positive test results, even if sensitivity and specificity are 90% or higher. Further, we have derived a range of disease prevalence values for which the naive approach gives reasonably unbiased prevalence estimates. A systematic review indicates that many public health researchers are not aware of methods for reducing this potential bias, and do not correct for this in their own studies of prevalence. Nor do peer reviewers and editors seem to notice this widespread problem. Taken together, the results emphasize the necessity in public health research to not sim- ply report raw proportions of positive tests, even if those are adjusted for demographic characteristics using logistic regression. Since disease prevalence is of course not known precisely prior to study conduct, the most straightforward approach is then to plan statistical methods so that sensitivity and specificity are accounted for. Even if other sources of bias (e.g. sampling bias, or sampling variation) are accounted for, the results of seroprevalence studies will continue to be biased if analyses do not also account for test sensitivity and specificity. Care should also be taken in reading publications reporting (sero)prevalence estimates to ensure that suitable statistical methods have been used. These results are based on the definitions of sensitivity and specificity only and require no complicated derivations. we have not adjusted for demographic characteristics, such as age and gender, or used weighting to approximate the target population, as is typical in surveys of disease prevalence. However, such adjustment cannot alleviate any general concerns of bias as presented here. The bias demonstrated here is also an average bias, and observed bias may vary more or less depending on the size of the sample. The results do not account for other possible issues with a diagnostic test (20–22), that can often not be corrected with 10 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint statistical methods (e.g. when the validation sample, on which the sensitivity and specificity estimates are based, is not similar to the population of interest). Average bias is given, which does not account for sampling bias or variation, as has been described elsewhere (9). The question remains as to how best to account for diagnostic test sensitivity and specificity when estimating disease prevalence. A nice outline of some appropriate methods along with implementation in R (23) code is given by (13,24,25). To calculate corrected confidence intervals for prevalence in studies where covariates do not need to be adjusted for, and no survey weights are needed, the R package bootComb (26) and website “epitools” (https: //epitools.ausvet.com.au/trueprevalence) are available, while Bayesian methods are available in prevalence (27). Using the Rogan-Gladen correction with bootstrap confidence intervals, or the Bayesian correction in the prevalence package are appropriate when there is no need to adjust for any other factors. Adjusting for covariates, adjusting for sampling bias or variation, or application of post-stratification weights (among other issues) may unfortunately need to be done without the use of such prepackaged code, e.g. as described by (28). Collaboration with experienced statisticians is invaluable in ensuring that correct analysis techniques are used so that unbiased prevalence estimates can be reported. The majority of publications, even if high impact journals, reporting seroprevalence estimates in the literature do not account for sensitivity and specificity of the diagnostic test. Bias introduced by reporting the proportion of positive tests rather than prevalence can be easily as high as 10%, or more if sensitivity or specificity are less than 90%. Public health researchers performing prevalence studies should consult experienced statisticians when analyzing such data, and be sure to account for test performance. However, researchers reviewing published prevalence studies also need to be aware of this issue. The results here will assist reviewers in determining the the magnitude of bias that can be expected, so that publications in the epidemiology literature can be interpreted properly. 11 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint Funding statement: The Ciao Corona study, used in our example, is part of Corona Immunitas research network, coordinated by the Swiss School of Public Health (SSPH+), and funded by fundraising of SSPH+ that includes funds of the Swiss Federal Office of Public Health and private funders (ethical guidelines for funding stated by SSPH+ will be respected), by funds of the Cantons of Switzerland (Vaud, Zurich, and Basel) and by institutional funds of the Universities. Additional funding, specific to this study is available from the University of Zurich Foundation. The EBPI at the University of Zurich provided funding for the systematic review. The funders had no involvement in the systematic review, writing of this report, or decision to submit the paper for publication. Competing interests: The authors declare no competing interests. Contributions: SRH initiated the analysis, developed the methodology, planned and con- ducted the systematic review, performed the statistical analysis, and wrote the manuscript. DK, in parallel with SRH, reviewed publications for the systematic review, extracted relevant data, and reviewed the manuscript. No others meeting criteria for authorship have been omitted. Acknowledgements: The authors thank Julia Braun, Thomas Radtke, and Milo Puhan for their critical comments. 12 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
perpetuity. 
 is the author/funder, who has granted medRxiv a license to display the preprint in
(which was not certified by peer review)
preprint 
The copyright holder for this
this version posted February 23, 2024. 
; 
https://doi.org/10.1101/2022.11.24.22282720
doi: 
medRxiv preprint",1
"The  
vaginal  
microbiome  
has  
been  
shown  
to  
be  
associated  
with  
pregnancy  
outcomes  
including  
preterm birth  
(PTB)  
risk.  
Here  
we  
present  
VMAP:  
Vaginal  
Microbiome  
Atlas  
during  
Pregnancy ( 
 
http://vmapapp.org 
 
) 
 
,  
an  
application  
to  
visualize  
features  
of  
3,909  
vaginal  
microbiome  
samples  
of  
1,416 pregnant  
individuals  
from  
11  
studies,  
aggregated  
from  
raw  
public  
and  
newly  
generated  
sequences  
via an  
open-source  
tool,  
MaLiAmPi.  
Our  
visualization  
tool  
(http://vmapapp.org)  
includes  
microbial  
features such  
as  
various  
measures  
of  
diversity,  
VALENCIA  
community  
state  
types  
(CST),  
and  
composition  
(via phylotypes  
and  
taxonomy).  
This  
work  
serves  
as  
a  
resource  
for  
the  
research  
community  
to  
further analyze  
and  
visualize  
vaginal  
microbiome  
data  
in  
order  
to  
better  
understand  
both  
healthy  
term pregnancies and those associated with adverse outcomes. Introduction The  
vaginal  
microbiome  
plays  
a  
significant  
role  
in  
birth  
outcomes,  
including  
spontaneous  
and  
preterm labor 
 
1–7 
 
.  
An  
increasing  
number  
of  
microbiome  
studies  
in  
the  
past  
decade,  
thanks  
to  
the  
advances  
in high-throughput  
sequencing,  
have  
permitted  
greater  
insights  
into  
the  
role  
of  
the  
microbiome  
in  
human health  
and  
disease 
 
8 
 
;  
however,  
efforts  
to  
aggregate  
microbiome  
datasets  
to  
better  
understand  
the microbiome  
through  
analysis  
of  
large,  
widely-representative  
data  
has  
been  
precluded  
by  
technical challenges 
 
9 
 
.  
In  
this  
work,  
we  
present  
an  
atlas  
of  
the  
vaginal  
microbiota  
during  
pregnancy  
leveraging data  
across  
11  
studies  
harmonized  
by  
the  
open-source  
tool,  
MaLiAmPi.  
This  
dataset  
represents  
one  
of the  
largest  
and  
most  
geographically  
diverse  
aggregations  
of  
the  
vaginal  
microbiome  
in  
pregnancy  
to date  
–  
3,909  
samples  
from  
1,416  
pregnant  
individuals  
–  
harmonized  
into  
a  
set  
of  
generalizable  
features suitable  
for  
integration  
of  
new  
microbiota  
data  
post-hoc.  
A  
rich  
set  
of  
paired  
metadata  
is  
included, including  
collection  
week  
during  
gestation  
(by  
specimen),  
week  
of  
delivery,  
and  
maternal  
NIH  
racial category.  
Approximately  
one-third  
of  
the  
pregnancies  
in  
this  
data  
set  
were  
of  
women  
delivering  
preterm, including  
just  
over  
1/8th  
who  
delivered  
early  
preterm.  
This  
dataset  
and  
features  
derived  
from  
these  
data can  
be  
interactively  
visualized  
via  
our  
Vaginal  
Microbiome  
Atlas  
During  
Pregnancy  
(VMAP)  
application, a resource for understanding the vaginal microbiome in the context of pregnancy ( 
 
http://vmapapp.org 
 
). Methods The  
dataset  
was  
constructed  
by  
aggregating  
and  
processing  
vaginal  
microbiome  
data  
from  
the  
public domain 
 
10–14  
in  
addition  
to  
including  
newly  
generated  
data  
not  
yet  
released  
to  
the  
public.  
The  
publicly available  
data  
were  
from  
nine  
studies,  
representing  
3,578  
samples  
across  
1,268  
individuals  
of  
whom 851  
delivered  
term  
and  
417  
preterm  
(before  
37  
weeks  
of  
gestation)  
including  
170  
whose  
deliveries were  
early  
preterm  
(before  
32  
weeks  
of  
gestation).  
Two  
additional  
unpublished  
datasets  
are  
included. One  
is  
from  
Wayne  
State  
University  
consisting  
of  
159  
samples  
across  
60  
individuals  
among  
whom  
40 (66.7%)  
had  
term  
deliveries  
and  
20  
(33.3%)  
had  
preterm  
deliveries 
 
,  
including  
5  
(8.3%)  
who  
had  
early preterm  
deliveries.  
A  
newly  
generated  
dataset  
which  
comprises  
172  
vaginal  
microbiome  
samples  
from 88  
individuals,  
up  
to  
three  
samples  
(one  
sample  
per  
trimester)  
for  
each  
individual,  
with  
48  
individuals (54.5%)  
having  
term  
deliveries,  
and  
40  
individuals  
(45.5%)  
having  
preterm  
deliveries  
including  
8  
(9.1%) having early preterm deliveries 
 
15 
 
. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.21.23286947
doi: 
medRxiv preprint To  
harmonize  
the  
raw  
amplicon  
sequence  
reads  
from  
such  
a  
diverse  
set  
of  
underlying  
approaches,  
we developed  
a  
novel  
phylogenetic-based  
approach  
implemented  
in  
an  
open  
source  
workflow  
called MaLiAmPi 
 
14 
 
.  
We  
further  
compute  
the  
alpha-diversity  
of  
communities  
(diversity  
measures  
include Shannon,  
Inverse  
Simpson,  
Balance  
weighted  
phylogenetic  
diversity  
(bwpd),  
phylogenetic  
entropy, quadratic,  
unrooted  
phylogenetic  
diversity,  
and  
rooted  
phylogenetic  
diversity),  
weighted  
phylogenetic (KR)  
distance  
between  
communities,  
provide  
taxonomic  
assignments  
to  
each  
ASV,  
and  
cluster  
ASVs into  
phylotypes  
which  
serve  
as  
a  
taxonomy-independent  
representative  
of  
(phylogenetically)  
closely related  
organisms.  
In  
addition,  
VALENCIA 
 
17  
was  
used  
to  
provide  
the  
community  
state  
type  
(CST)  
of each sample. VMAP, an interactive R Shiny application was developed to visualize the data. Results VMAP  
interactively  
visualizes  
vaginal  
microbiome  
features  
during  
healthy  
term  
or  
adverse-outcome associated  
pregnancies  
across  
11  
studies,  
representing  
3,909  
samples  
from  
1,416  
individuals,  
of  
whom 939  
delivered  
at  
term  
and  
477  
preterm  
,  
including  
183  
whose  
deliveries  
were  
early  
preterm.  
The demographics  
and  
information  
about  
the  
source  
studies  
are  
presented  
in  
Table  
1.  
A  
screen  
shot  
of  
the visualization  
tool  
is  
presented  
in  
Figure  
1.  
This  
tool  
allows  
customizable  
visualization  
of  
the  
data  
at several  
levels  
including  
demographics  
of  
the  
cohort  
at  
the  
sample  
or  
individual  
level  
organized  
by outcome  
(term,  
preterm,  
early  
preterm),  
race  
and  
ethnicity  
distributions,  
as  
well  
as  
breakdown  
per project.  
Demographics  
are  
shown  
as  
stacked  
barplots  
where  
the  
relative  
frequency  
of  
each  
type  
of category  
selected  
can  
be  
seen.  
Alpha-diversity  
measures  
are  
also  
shown  
in  
several  
ways  
including overall  
correlation  
plots  
of  
the  
individual  
measures  
and  
their  
distribution  
as  
well  
as  
box  
plots  
across various  
stratifications.  
Different  
diversity  
measures  
are  
customizable  
allowing  
users  
to  
select  
their measures  
of  
interest.  
CSTs  
are  
visualized  
longitudinally  
across  
trimesters  
allowing  
for  
various stratifications  
of  
interest.  
This  
kind  
of  
plot  
allows  
the  
observation  
of  
changes  
in  
the  
composition  
of  
CSTs across  
trimesters  
according  
to  
the  
feature  
selected,  
representing  
each  
stream  
as  
an  
individual.  
Finally, dimensionality  
reduction  
plots,  
namely  
Uniform  
Manifold  
Approximation  
and  
Projections  
(UMAPs),  
are used  
to  
visualize  
phylotype  
data  
per  
sample  
or  
individual  
comparing  
among  
different  
features.  
The prevalence  
and  
relative  
abundance  
of  
closely  
related  
microbes  
(as  
represented  
by  
phylotypes)  
or  
taxa are displayed as heatmaps (for prevalence). Discussion VMAP  
provides  
a  
means  
to  
view  
a  
rich  
set  
of  
features  
derived  
from  
vaginal  
microbiota  
data  
(including alpha-diversity,  
CSTs,  
and  
the  
composition  
of  
specific  
microbes  
and  
taxa)  
associated  
with  
core outcomes  
(delivery  
week)  
and  
metadata  
(e.g.,  
NIH  
racial  
categories,  
maternal  
age)  
that  
can  
be  
sliced and  
viewed  
interactively.  
VMAP  
can  
be  
a  
resource  
for  
those  
wishing  
to  
relate  
and  
understand  
vaginal microbiota  
data  
within  
the  
context  
of  
pregnancy.  
The  
generalizability  
of  
these  
features  
across  
studies using  
distinct  
underlying  
techniques  
(e.g.,  
targeting  
different  
16S  
rRNA  
gene  
variable  
regions  
or  
novel sequencing  
platforms)  
and  
the  
ability  
to  
integrate  
new  
data  
into  
the  
existing  
set  
of  
features  
are significant  
advances  
for  
microbiome  
research.  
For  
example,  
this  
data  
set  
and  
approach  
were successfully  
employed  
as  
the  
basis  
for  
a  
machine  
learning  
(“DREAM”)  
challenge  
predicting  
preterm and  
early-preterm  
birth  
where  
over  
300  
teams  
participated  
with  
the  
goal  
of  
building  
machine  
learning models  
to  
predict  
which  
women  
would  
deliver  
preterm 
 
15 
 
.  
This  
challenge  
used  
this  
novel  
approach  
to . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.21.23286947
doi: 
medRxiv preprint integrate  
data  
post-hoc  
to  
validate  
machine  
learning  
models  
against  
datasets  
unavailable  
to participants. The  
resource  
has  
limitations  
that  
should  
be  
considered.  
It  
is  
based  
on  
publicly  
available  
data  
which might  
not  
have  
full  
clinical  
or  
demographic  
annotations  
of  
the  
samples  
in  
the  
metadata.  
While  
the sample  
size  
of  
the  
study  
is  
considerable,  
it  
may  
not  
be  
representative  
of  
the  
entire  
population  
of pregnant women from around the world. This  
work  
serves  
as  
the  
basis  
for  
several  
potential  
follow-up  
opportunities.  
Our  
visualization  
resource can  
further  
be  
extended  
to  
include  
non-pregnancy  
datasets  
as  
well  
as  
microbiome  
data  
across  
body sites.  
In  
addition,  
other  
adverse  
pregnancy  
outcomes  
such  
as  
recurrent  
pregnancy  
loss  
can  
be investigated  
and  
included  
in  
the  
expansion  
of  
the  
resource.  
Finally,  
the  
microbiome  
data  
can  
further  
be integrated  
with  
other  
omics  
measures  
to  
better  
understand  
healthy  
human  
pregnancy  
and  
those associated  
with  
adverse  
outcomes.  
VMAP  
will  
be  
valuable  
for  
more  
robust  
interpretation  
of  
novel datasets  
seeking  
to  
relate  
the  
vaginal  
microbiome  
to  
pregnancy  
outcomes  
–  
serving  
as  
a  
large  
and regularized set of data and metadata for comparison. Code and Data Availability The RShiny app is available:  
http://vmapapp.org Datasets  
were  
downloaded  
from  
ImmPort 
 
11  
via  
the  
March  
of  
Dimes  
Preterm  
Birth  
database 
 
10  
(Study SDY465),  
from  
the  
NCBI  
Sequence  
Read  
Archive 
 
13  
(BioProjects  
PRJNA242473,  
PRJNA294119, PRJNA393472,  
and  
PRJNA430482),  
the  
Sequence  
Read  
Archive  
of  
the  
European  
Nucleotide  
Archive 
 
14 (Projects  
PRJEB11895,  
PRJEB12577,  
PRJEB21325,  
and  
PRJEB30642),  
and  
the  
database  
of Genotypes and Phenotypes (dbGaP) 
 
12  
(accession number  
phs001739.v1.p1). Additional 
 
associated 
 
metadata  
were  
requested  
and  
obtained  
from  
the  
RAMS  
Registry ( 
 
https://ramsregistry.vcu.edu 
 
)  
(PRJNA430482)  
or  
from  
the  
senior  
author  
(Projects  
PRJEB11895, PRJEB12577,  
PRJEB21325,  
and  
PRJEB30642),  
or  
downloaded  
from  
the  
publication  
by  
the  
Kindinger et al.(ref) (PRJEB11895 and PRJEB12577). Data  
for  
accession  
number  
phs001739.v1.p1  
are  
exclusively  
available  
via  
dbGap  
after  
following  
the application procedures there. The  
aggregated  
vaginal  
microbiome  
dataset  
can  
be  
downloaded  
from  
the  
March  
of  
Dimes  
Prematurity Research  
Database  
excluding  
the  
datasets  
that  
we  
are  
unable  
to  
share  
due  
to  
legal  
restrictions ( 
 
SDY2187 
 
). The code for VMAP is available on Github:  
https://github.com/msirota/vmap.git Acknowledgements We  
thank  
members  
of  
the  
Sirota  
Lab,  
University  
of  
California,  
San  
Francisco,  
for  
useful  
discussion.  
This study  
was  
supported  
by  
the  
March  
of  
Dimes  
(ALP,  
TTO,  
AR,  
AST,  
CWYH,  
RJW,  
GA,  
IK,  
SN,  
YSLL, PRB,  
DAM,  
SVL,  
DKS,  
NA,  
JLG,  
MS);  
Spanish  
Ministry  
of  
Science,  
Innovation  
and  
Universities  
through . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.21.23286947
doi: 
medRxiv preprint FPU  
program  
FPU18/0177;  
EST22/00170  
(ALP),  
Instituto  
de  
Salud  
Carlos  
III  
(Spanish  
Ministry  
of Science  
and  
Innovation)  
through  
Miguel  
Servet  
program  
CP20/00118  
and  
co-funded  
by  
European Union  
(PGD),  
NIH  
grants  
R35GM138353  
(NA),  
1R01HL139844  
(NA),  
3P30AG066515  
(NA), 1R61NS114926  
(NA),  
1R01AG058417  
(NA),  
R01HD105256  
(NA,  
MS),  
and  
P01HD106414  
(NA);  
the Burroughs  
Welcome  
Fund  
(NA);  
the  
Alfred  
E.  
Mann  
Foundation  
(NA);  
and  
the  
Robertson  
foundation (NA). Conflict of Interest Antonio  
Parraga-Leo  
and  
Patricia  
Diaz-Gimeno  
are  
receiving  
hononaria  
from  
the  
IVI  
Foundation.  
All other authors declare no financial or non-financial competing interests. Author Contributions JLG  
and  
MS  
conceived  
the  
study.  
CWYH,  
RJW,  
and  
ALT  
generated  
and  
shared  
data  
for  
the  
validation dataset.  
JLG,  
TTO,  
AST,  
AR,  
and  
SSM  
aggregated  
the  
training  
datasets.  
JG  
and  
AR  
normalized  
the training  
and  
validation  
datasets.  
ALP  
and  
JG  
developed  
the  
application.  
TTO,  
JLG,  
and  
MS  
were  
major contributors in writing the manuscript. All authors read and approved the final manuscript. Ethics Statement This  
work  
was  
approved  
by  
the  
National  
Heart,  
Lung,  
and  
Blood  
Institute  
(NHLBI)  
Clinical  
Data  
Science Institutional  
Review  
Board  
(CDS-IRB)  
in  
study  
number  
2021-040,  
and  
reliance  
was  
granted  
to  
the NHLBI  
CDS-IRB  
by  
the  
University  
of  
California,  
San  
Francisco  
Institutional  
Review  
Board  
in  
study number 21-35274.",1
"Background 
We investigated which clinical and sociodemographic characteristics were associated with unhealthy patterns of 
weight gain amongst adults living in England during the pandemic. 
  
Methods 
With the approval of NHS England we conducted an observational cohort study of Body Mass Index (BMI) 
changes between March 2015 and March 2022 using the OpenSAFELY-TPP platform. We estimated individual 
rates of weight gain before and during the pandemic, and identified individuals with rapid weight gain 
(>0·5kg/m2/year) in each period. We also estimated the change in rate of weight gain between the prepandemic 
and pandemic period and defined extreme-accelerators as the ten percent of individuals with the greatest 
increase (>1·84kg/m2/year). We estimated associations with these outcomes using multivariate logistic 
regression. 
  
Findings 
We extracted data on 17,742,365 adults (50·1% female, 76·1% White British). Median BMI increased from 
27·8kg/m2 [IQR:24·3-32·1] in 2019 (March 2019 to February 2020) to 28·0kg/m2 [24·4-32·6] in 2021. Rapid 
pandemic weight gain (n=3,214,155) was associated with female sex (male vs female: aOR 0·76 [95%CI:0·76-
0·76]); younger age (50-59-years vs 18–29-years: aOR 0·60 [0·60-0·61]); White British ethnicity (Black 
Caribbean vs White British: aOR 0·91 [0·89-0·94]); deprivation (least-deprived-IMD-quintile vs most-deprived: 
aOR 0·77 [0·77-0·78]); and long-term conditions, of which mental health conditions had the greatest effect (e.g. 
depression (aOR 1·18[1·17-1·18])). Similar characteristics increased risk of extreme acceleration (n=2,768,695).  
  
Interpretation 
We found female sex, younger age, deprivation and mental health conditions increased risk of unhealthy 
patterns of pandemic weight gain. This highlights the need to incorporate sociodemographic, physical, and 
mental health characteristics when formulating post-pandemic research, policies, and interventions targeting 
BMI.  
  
Funding 
NIHR . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 3 Background Obesity and rapid weight gain1 are established risk factors for non-communicable diseases and have emerged as 
independent risk factors for severe disease following Coronavirus-19 (COVID-19) infection.2–5 In March 2020, 
restrictions imposed to reduce COVID-19 transmission resulted in profound societal changes that impacted 
many health behaviours, including physical activity and nutrition.3,6–8 However, the suspension of population 
health surveys, such as the Health Survey for England, during the pandemic9 resulted in a lack of quantitative 
data on weight changes.  
 
A systematic review of observational studies reported a modest increase in adult weight during the pandemic, 
but analyses were limited by small sample size and non-representative samples.10 Studies using routinely 
collected healthcare records have been limited to populations without universal access to healthcare,11–13 or 
individuals with long-term conditions (LTCs).14 In these settings, women,11,12 young adults,11,12 those living in 
deprivation,13 and those with LTCs,13,14 including depression,13 were at greatest risk of weight gain during the 
pandemic.7 However, these findings have not been replicated in a population-representative cohort. 
Additionally, groups such as young adults, were at increased risk of weight gain even prior to the pandemic.15 
Therefore, a comparison of individual rates of weight gain, before and after the onset of the pandemic is 
required to understand the specific impact of the pandemic and identify individuals who had the greatest 
acceleration in their rate of weight gain during the pandemic (extreme-accelerators). Body Mass Index (BMI) 
data, a measure of weight adjusted for height, recorded in English primary care Electronic Health Records 
(EHRs) were used to estimate rates of weight gain (BMI trajectories) before the pandemic,15 and provide a 
source of nationally representative data available during the pandemic.16  
 
Using routinely collected English primary care records we aimed to: (i) describe population-level changes in 
BMI during the pandemic and (ii) estimate individual-level rates of weight gain before and during the pandemic 
to identify how (iia) risk of rapid weight gain (>0·5kg/m2/year) in the pre-pandemic and pandemic periods and 
(iib) risk of extreme acceleration in the rate of weight gain between the prepandemic and pandemic period 
varied by sociodemographic and clinical characteristics. The results of these analyses will inform targeted policy 
to deliver weight loss interventions in the post-pandemic recovery. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 4 Methods Data Source All data were linked, stored and analysed securely within the OpenSAFELY-TPP platform, 
https://opensafely.org/, containing pseudonymised data on approximately 40% of the English population, 
including coded diagnoses, medications and physiological parameters. No free text data are included. Detailed 
pseudonymised patient data is potentially re-identifiable and therefore not shared. The study was approved by 
the London School of Hygiene & Tropical Medicine Ethics Board (reference 26536). An information 
governance statement is included (Appendix 3). Study population We extracted data on all male and female adults aged >18 to <90 years who had been registered with a primary 
care practice using TPP EHR software for at least one year prior to 1st March 2022. Study Outcomes We used changes in Body Mass Index (BMI, a measure of weight adjusted for height), as a proxy for weight 
change. BMI is recorded in primary care records during routine health checks, disease monitoring, and 
opportunistically. We extracted BMI data using recorded weight (in kilograms) and height (in metres), or 
directly from recorded BMI values (in kg/m2) between 1st March 2015 and 1st March 2022, from the point an 
individual reached 18 years of age. Individuals who joined the OpenSAFELY-TPP database after 1st March 
2015 had BMI data extracted between their date of first registration with the GP practice and 1st March 2022. 
Recordings taken within healthcare settings and self-reported values are included in EHRs and cannot be 
differentiated using clinical codes. We extracted monthly values per individual and took the most recent in 
instances where multiple values per calendar month were present. Extreme values (BMI<15kg/m2 and 
BMI>65kg/m2) were omitted from all quantitative analyses related to BMI values to censor erroneous results 
and exclude extremely overweight and underweight individuals. Population level trends in BMI recording activity and median BMI We examined the proportion and characteristics of the population having at least one BMI value recorded in the 
year beginning March 2019 (2019), March 2020 (2020), and March 2021(2021) to characterise how the 
pandemic had influenced BMI data availability. We then calculated the population-level median and 
interquartile range (IQR) of the recorded BMI values for each of the years above, using the median per year, 
where multiple values were available for a single individual. Individual level BMI trajectories in pre-pandemic and pandemic periods We first estimated individual level BMI trajectories as rates of BMI change per year (δ) in kg/m2/year, before 
the onset of COVID-19 pandemic (δ-prepandemic) and after the onset of the pandemic (δ-pandemic). To 
calculate δ-prepandemic we randomly selected one pair of BMI measurements from individuals who had a BMI 
value recorded in each of the following time windows (period-1: March 2015 - February 2018; and period-2: 
March 2018 - February 2020), and calculated the rate of BMI change/year assuming a linear trend following 
established methods.15 To calculate δ-pandemic we replicated this method, with a random pair of BMI measures 
from each of the following time windows (period-2: March 2018 - February 2020; and period-3: March 2020 - 
February 2022). (Appendix 1) Individuals with known cancer and those underweight (BMI <18·5kg/m2) in the 
prepandemic period were excluded from these analyses as their patterns of weight change were likely to differ 
from the general population. The most extreme 0·05% of values of δ-prepandemic and δ-pandemic (a positive or 
negative change of >6kg/m2/year) were censored to reduce the impact of erroneous results. Individuals gaining 
>0·5kg/m2/year were classified as experiencing rapid weight gain.1 We estimated associations between socio- . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 5 demographic and clinical characteristics and rapid weight gain before and during the pandemic using descriptive 
statistics and calculating odds adjusted for age, sex, ethnicity and IMD. Defining a population of extreme accelerators To identify individuals who had experienced the greatest acceleration in their rate of weight gain between the 
prepandemic and pandemic period, we first estimated individual-level changes in rate of weight gain (δ-change) 
as the difference in the prepandemic and pandemic rates of weight gain (δ-change = δ-pandemic - δ-
prepandemic) (see Appendix 1 for a more detailed description of the methods). A positive δ-change indicated 
the rate of weight gain increased or, if patients were losing weight prepandemic, the rate of weight loss slowed. 
We then defined the ten percent of the study population experiencing the greatest acceleration in their rate of 
weight gain (δ-change >= 1·84kg/m2/year) as ‘extreme accelerators’. We estimated associations between socio-
demographic and clinical characteristics and extreme acceleration using descriptive statistics and calculating 
odds adjusted for age, sex, ethnicity and IMD. Covariates Covariates were chosen by clinical consensus, guided by data availability and known potential predictors of 
BMI change. Covariates included age (18-29 years, 30-39 years, 40-49 years, 50-59 years, 60-69 years, 70-79 
years, 80 ≤ 90 years), sex (female or male); ethnicity, based on the 2001 UK Census definitions (White British, 
White Irish, Other White, Black African, Black Caribbean, Other Black, Indian, Pakistani, Bangladeshi, 
Chinese, Other Asian, Mixed White/Black African, Mixed White/Black Caribbean, Mixed White/Asian, Other); 
deprivation using the most recent patient postcode-derived Index of Multiple Deprivation (IMD; by quintiles 
from those living in the most deprived 20% of households to the least deprived 20%); and the presence or 
absence of the following LTCs ever-recorded: hypertension; type 1 diabetes (T1D), type 2 diabetes (T2D), 
asthma, chronic obstructive pulmonary disease (COPD), anxiety and depression, serious mental illness (SMI: 
psychosis or bipolar disorder), learning difficulties, dementia, cardiovascular disease (CVD: chronic cardiac 
conditions), and stroke and transient ischaemic attack (Stroke and TIA). Statistical Models We used complete case analysis (inclusion of individuals with all baseline covariate data) in all statistical 
models. This is consistent with previous studies using English primary care data.17 As this is a descriptive study, 
multiple imputation techniques were not employed as we were not trying to gain an unbiased estimate of a 
single exposure-outcome association, and therefore did not have an analytic model around which to build an 
imputation procedure. Additionally, the missing at random assumption (required for imputation) was unlikely to 
hold, e.g. individual IMD linked to residence is less likely to be recorded for individuals in unstable 
accommodation. All individuals had complete data for age and sex as these were part of the study inclusion 
criteria, and clinical covariates (such as T2D) which were identified based on the presence/absence of specified 
codes in the data. Therefore, ethnicity and IMD were the only variables with missing data.  
 
For each of the estimated individual-level outcomes (rapid weight gain prepandemic, rapid weight gain 
pandemic and extreme acceleration in rate of weight gain), we compared the baseline characteristics of the 
population with and without the outcome. We also compared baseline characteristics and outcomes between the 
complete case sample and the entire population, including those with missing ethnicity and deprivation data, to 
identify differences in individuals with and without missing data.  
 
We used logistic regression to explore associations between the sociodemographic and clinical covariates and 
the estimated outcomes. Models were adjusted separately for age, sex, IMD and ethnicity and then in 
multivariable models adjusted for age and sex; age, sex and IMD; age, sex and ethnicity; and age, sex, IMD and 
ethnicity. 
 
Subgroup Analyses . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 6 We investigated whether estimated associations between the covariates and extreme acceleration in rate of 
weight gain persisted in populations stratified by age group (18-39 years, 40-59 years, and 60-79 years), sex, 
IMD (IMD quintile 1 and IMD quintile 5) and ethnicity (Black and South Asian (Bangladeshi, Indian, 
Pakistani)). The ethnicities were grouped into broader groups (White, Black, South Asian, Chinese and other, 
and Mixed ethnicity) for subgroup analyses to reduce risk of disclosure from rare events.  
 
Data management was done with Python 3·8 and SQL, and analysis was done using R 4·0. Prevalence counts 
are rounded to the nearest five to reduce risk of disclosure. All code for data management and analysis, as well 
as codelists is shared openly for review and re-use under MIT open license, available at 
https://github.com/opensafely/BMI-and-Metabolic-Markers. Patient and public involvement OpenSAFELY has a publicly available website through which we invite patients or members of the public to 
contact us about this study or the broader OpenSAFELY project. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 7 Results Data were extracted for 17,742,365 adults meeting study inclusion criteria of whom 50·07% were female and 
76·1% were of white British ethnicity (Table 1). Figure 1 demonstrates the population contributing data to each 
stage of the analysis. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint Population level trends in median BMI The median recorded BMI increased from 27·8kg/m2 [IQR: 24·3-32·1kg/m2] in 2019 to 28·0kg/m2 [24·4-32·6] in 
2021. An increase in median BMI was seen in all subgroups, except the oldest age group (aged 80-89), people 
with dementia, and people with T2D. (Figure 2) . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 10 The proportion and characteristics of the population having a BMI recorded changed during the pandemic. BMI 
was recorded in 31·3% (95% CI: 31·3-31·3) of the total study population in 2019, dropping to 18·7% in 2020 
before recovering to 25·0% in 2021, with higher proportions seen amongst individuals with LTCs in all the 
study periods (e.g. T2D: 82·5% in 2019, 72·2% in 2021). (Supplementary Table 1) Individual level BMI trajectory analyses Rapid weight gain (> 0·5kg/m2/year) in prepandemic and pandemic time periods We investigated individual BMI trajectories to identify the groups at greatest risk of rapid weight gain amongst 
adults with BMI measures recorded in their EHRs. The average rate of weight gain was 0·06kg/m2/year 
(SD:1·20) during the pandemic and 0·08kg/m2/year (SD:1·05) prepandemic (Supplementary Table 2). We found 
29·2% of individuals gained weight rapidly during the pandemic, compared to 26·9% prepandemic. Figure 3 
demonstrates the estimated risk and adjusted odds of rapid weight gain before and after the onset of the 
pandemic. The risk of rapid weight gain during the pandemic (δ-pandemic) was associated with age, sex, 
deprivation, ethnicity and a history of LTCs. Adults aged 18-29 years had the highest risk (44·2%), all other age 
groups had lower risk and adjusted odds (e.g. 50-59-year-olds 30·1%: aOR 0·60 [0·60-0·61]). Male sex reduced 
the risk and adjusted odds (23·9% male vs 32·9% female: aOR 0·76 [ 0·76-0·76]), as did living in the least 
deprived quintile (26·1% IMD5 vs 32·6% IMD1; aOR 0·77 [0·77-0·78]). White British people had the highest 
adjusted odds of rapid weight gain compared to all other ethnicities; in comparison, Black Caribbean people had 
the greatest risk (32·4%) but a lower adjusted odds (aOR 0·91[0·89-0·94]). When comparing individuals with 
and without specific LTCs, T2D was the only LTC associated with a reduction in the estimated risk and adjusted 
odds (20·73%, aOR 0·71 [0·71-0·72]). Individuals with all the other studied LTCs had an increased estimated 
risk, with the greatest risk amongst those with learning difficulties (36·3%: aOR 1·10 [1·08-1·12]), SMI (35·2%: 
aOR 1·23[1·21-1·24]) and depression (33·4%: aOR 1·18 [1·17-1·18]). Risk factors for rapid weight gain before 
the pandemic were similar (Figure 3). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint Extreme acceleration in rate of weight gain between the prepandemic and pandemic To define the population experiencing the greatest increase in their rate of weight gain between the prepandemic 
and pandemic periods (extreme accelerators), we first estimated the change in rate of weight gain in 
n=2,768,695 individuals who had prepandemic and pandemic BMI trajectory data (δ-change = δ-pandemic - δ-
prepandemic). We found a wide distribution in δ-change (Supplementary Figure 2 in appendix 2), half the 
population had a slight reduction in their rate of weight gain (median δ-change = -0·02kg/m2/year), while 40% 
of the population experienced an acceleration of more than 0·27kg/m2/year. The ten percent of the study 
population with the greatest acceleration in their rate of weight gain after the onset of the pandemic (δ-change 
>= 1·84kg/m2/year, appendix 2) were classified as extreme accelerators. Consistent with our findings on risk of 
rapid weight gain, women, younger adults, and individuals living in the most deprived quintiles were at greatest 
risk of being an extreme accelerator. Living with T1D reduced the adjusted odds (9·4%, aOR 0·87 [0·85-0·89]). 
All other LTCs increased the adjusted odds, with mental health conditions, including depression, SMI and 
dementia, having the largest estimated effect. (Figure 4) . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint Sensitivity and Subgroup Analyses We compared our findings in the complete case sample to analyses conducted in the entire population, including 
those with missing ethnicity and deprivation data, and found our results to be consistent in both groups. We 
found that most of the estimated associations between the risk of extreme acceleration in rate of weight gain and 
age, sex, IMD and LTCs persisted in the stratified analyses, but there were some exceptions, e.g. amongst Black 
individuals, deprivation was not associated with risk (IMD5 vs IMD1: aOR 1·04[0·92-1·18]); and amongst those 
living in the least deprived quintiles, Black individuals were at higher risk than Whites (Black vs White: aOR 
1·24 [1·10-1·39]). There was consistent evidence that most LTCs increased the estimated risk of a rapid 
acceleration in rate of weight gain, with mental health conditions continuing to have the greatest effect. 
Associations with T2D were less consistent, a history of T2D continued to have an estimated protective affected 
amongst South Asian (aOR 0·84 [0·80-0·87]) and Black (aOR 0·88 [0·83-0·94]) individuals, but increased the . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 13 estimated risk in other subgroups including older adults (aged 60-79: aOR 1·11 [1·09-1·12]) and the least 
deprived IMD quintile (aOR 1·17 [1·15-1·20]) (Supplementary Table 2-6). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 14 Conclusion The three years since the onset of the COVID-19 pandemic has seen profound societal change in how 
individuals live, work and interact with each other. However, the impact of these changes on patterns of weight 
gain have not been well characterised. We are, to our knowledge, the first to use a nationally representative 
population database16 to describe patterns of weight gain during the pandemic. We report novel findings from 
our analyses of BMI data in the EHRs of over 17 million adults living in England. At population level there was 
a modest increase in the median BMI between the prepandemic year and the year beginning March 2021. At 
individual-level we showed that, amongst adults with BMI values recorded in the EHR, women, younger adults 
and those living in the most socioeconomically deprived areas were at greatest estimated risk of unhealthy 
patterns of weight gain, namely rapid weight gain during the pandemic and an extreme acceleration in rate of 
weight gain between the prepandemic and pandemic periods. Individuals with a history of LTCs were also at 
greatest risk of unhealthy weight gain, and people with mental health conditions, including depression, SMI and 
learning difficulties, were at disproportionately greater risk than those with physical health conditions such as 
hypertension.  
 
Previous research in this area has been limited to small voluntary surveys,10 or routine health data from non-
representative populations.11–14 By using a large-scale, nationally representative and contemporaneous EHR 
dataset, we have been able to robustly reproduce prior observations that women,11,12 young adults,11,12 those 
living in deprivation13 and those with LTCs13,14 may be the most affected by unhealthy patterns of pandemic 
weight gain and demonstrate this at population scale. Our findings suggest the increased risk of weight gain seen 
amongst young adults prepandemic15 was exacerbated during the pandemic. The increased risk seen amongst 
women may reflect gender disparities in the impact of the pandemic on social factors such as employment loss 
and caring responsibilities, with a consequent influence on health behaviours.18 The increased risk amongst 
those living in areas of greater deprivation reflects prepandemic trends and is likely to have multifactorial 
aetiology including food poverty, reduced opportunity for physical activity and an increased burden of physical 
and mental health conditions.19 In our analysis, instead of controlling for LTCs as prior studies have done,1,15 we 
characterised the associations between patterns of weight gain and clinical and mental health characteristics. 
This approach uncovered important inequality between people with prior mental health versus physical health 
conditions, with the former having a greater risk of unhealthy patterns of weight gain. These differences may 
reflect associations between disordered eating, reduced physical activity and poor mental health exacerbated by 
the pandemic.20,21 Our finding highlights the need for health services to ensure parity of esteem22 between 
physical and mental health conditions when prioritising groups to be supported by weight loss interventions.  
 
The key strengths of this study are the quality, scale and representativeness of the health record data used.16 
OpenSAFELY-TPP provides access to the primary care records of roughly 40% of the population.16 In England, 
primary care hosts and records data from 90% of all patient consultations in the National Health Service, with 
particular responsibility for preventative care and routine management of LTCs.23 Therefore, our study has been 
able to make robust observations that have immediate relevance to the delivery of health care in the post-
pandemic period. These findings could be rapidly implemented in the routine clinical care that has generated 
them, for example through the practice reorganisation or redistribution of care to target those at most need.  
 
We recognise the limitations of our work. We restricted our analyses to individuals registered with a GP practice 
for at least one year to minimise the impact of incomplete data. However, this may have introduced survival 
bias, e.g. if individuals with the most unhealthy patterns of weight gain were more likely to die during the 
pandemic. BMI is recorded in EHRs when clinically indicated, this may be systematically as part of the routine 
monitoring of LTCs such as T2D or opportunistically when relevant to the clinical consultation. Despite this risk 
of information bias, prepandemic EHRs produced BMI trajectory estimates comparable to representative 
national surveys.24 However, we report changes in BMI recording activity that may have introduced new bias, 
with a decline in BMI recording after the onset of the pandemic which had only partly recovered by the year 
beginning March 2021, reflecting wider patterns of primary care activity during the pandemic.25 Notably, the 
greatest recovery in BMI recording was amongst individuals with LTCs requiring annual BMI recording, e.g. 
T2D. This leads to a higher prevalence of LTCs amongst individuals contributing pandemic BMI data (Table 1) . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 15 and the possibility that the protective effect of T2D against weight gain is in fact due to collider bias.26 
Alternatively this may be a true effect arising from the awareness that COVID-19 infection confers greater risk 
to people with obesity and LTCs such as T2D, leading to more proactive and intensive weight management in 
primary care, or adoption of healthy behaviours. The observed protective effect of T1D against extreme 
acceleration may also reflect bias, however there is evidence of improved blood glucose control amongst people 
living with T1D during the pandemic.27 Pandemic-associated changes in clinical activity, such as BMI self-
reporting during remote consultations, may have introduced biases that are harder to characterise.28 Further 
research, including representative population health surveys, would be required to investigate this further.  
 
Another limitation relates to missing data. We have undertaken a complete case analysis, removing individuals 
with missing ethnicity and IMD data. This approach precludes generalisability of findings to those missing data 
in these fields, e.g. those missing IMD data due to unstable accommodation, but gives robust evidence in 
everybody else. Multiple imputation models would not overcome this limitation, as the Missing at Random 
assumption is unlikely to hold.29  Some prepandemic analyses of BMI trajectories have used imputation, 
adjusting estimates with population-survey data to account for the MNAR assumption.15 We could not replicate 
this approach due to the suspension of such surveys during the pandemic.  
 
In conclusion, our study is the largest and most representative analysis of weight changes associated with the 
pandemic to date. We identify that women, young adults, those living in the most deprivation, as well as those 
with mental health conditions were at increased risk of unhealthy patterns of weight gain during the pandemic. 
We demonstrate the value of routinely collected data to understand the impact of a pandemic at population scale 
and identify health inequalities and areas of focus. Overall, our findings identified clear targets for health 
services and policy makers to target with preventative interventions to mitigate the lasting and inequitable 
impact of the pandemic. Future work is needed to continue the monitoring of these trends to understand whether 
these effects are lasting or even exacerbated. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 16 Role of Funding Source This study was undertaken by MS as part of her National Institute of Health Care Research (NIHR) funded 
academic clinical fellowship in primary care.  There was no other direct funding for this analysis. Declaration of Interests MS salary costs have been supported through a National Institute for Health and Care Research (NIHR) funded 
academic clinical fellowship in primary care and NIHR grant funding (NIHR AI-MULTIPLY Consortium 
NIHR203982).   RYP is supported by the EPSRC Centre for Doctoral Training in Health Data Science 
(EP/S02428X/1). RYP was previously employed as a data scientist for the Bennet Institute which is funded by 
grants from the Bennett Foundation, Wellcome Trust, NIHR Oxford Biomedical Research Centre, NIHR 
Applied Research Collaboration Oxford and Thames Valley, Mohn-Westlake Foundation.  SVE is funded by a 
Diabetes UK Sir George Alberti research training fellowship (grant number: 17/0005588).  FE salary cost is 
supported by MRC (MR/S027297/1) “Multimorbidity, clusters, trajectories and genetic risk in British south 
Asians, 2020-2023”.  DS is funded by the NIHR (NIHR203982).  AM is a senior clinical researcher at the 
University of Oxford in the Bennett Institute, which is funded by grants from the Bennett Foundation, Wellcome 
Trust, NIHR Oxford Biomedical Research Centre, NIHR Applied Research Collaboration Oxford and Thames 
Valley, Mohn-Westlake Foundation. AM has consulted for https://inductionhealthcare.com/. AM is a member of 
the RCGP health informatics group and the NHS Digital GP data Professional Advisory Group that advises on 
access to GP Data for Pandemic Planning and Research (GDPPR); payment direct to me for the GDPPR role. 
RM is supported by Barts Charity (MGU0504).  JV is National Clinical Director for Diabetes & Obesity at NHS 
England. BMK is also employed by NHS England.  KK is supported by the National Institute for Health 
Research (NIHR) Applied Research Collaboration East Midlands (ARC EM) and the NIHR Leicester 
Biomedical Research Centre (BRC).  KK has acted as a consultant, speaker or received grants for investigator-
initiated studies for Astra Zeneca, Bayer, Novartis, Novo Nordisk, Sanofi-Aventis, Lilly and Merck Sharp & 
Dohme, Boehringer Ingelheim, Oramed Pharmaceuticals, Roche and Applied Therapeutics. SF has received 
grants from the NIHR (NIHR 31672, NIHR 202635) and MRC (MR/W014416/1, MR/V004905/1, 
MR/S027297/1). SF, RM, CM are part of the Genes & Health programme, which is part-funded (including 
salary contributions) by a Life Sciences Consortium comprising Astra Zeneca PLC, Bristol-Myers Squibb 
Company, GlaxoSmithKline Research and Development Limited, Maze Therapeutics Inc, Merck Sharp & 
Dohme LLC, Novo Nordisk A/S, Pfizer Inc, Takeda Development Centre Americas Inc. 
 
This research used data assets made available as part of the Data and Connectivity National Core Study, led by 
Health Data Research UK in partnership with the Office for National Statistics and funded by UK Research and 
Innovation (grant ref MC_PC_20058). In addition, the OpenSAFELY Platform is supported by grants from the 
Wellcome Trust (222097/Z/20/Z); MRC (MR/V015757/1, MC_PC-20059, MR/W016729/1); NIHR 
(NIHR135559, COV-LT2-0073), and Health Data Research UK (HDRUK2021.000, 2021.0157). Data Sharing Access to the underlying identifiable and potentially re-identifiable pseudonymised electronic health record data 
is tightly governed by various legislative and regulatory frameworks, and restricted by best practice. The data in 
OpenSAFELY-TPP is drawn from General Practice data across England where TPP is the data processor.  
 
TPP developers initiate an automated process to create pseudonymised records in the core OpenSAFELY 
database, which are copies of key structured data tables in the identifiable records. These pseudonymised 
records are linked onto key external data resources that have also been pseudonymised via SHA-512 one-way 
hashing of NHS numbers using a shared salt. Bennett Institute for Applied Data Science developers and PIs 
holding contracts with NHS England have access to the OpenSAFELY pseudonymised data tables as needed to 
develop the OpenSAFELY tools. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 17 These tools in turn enable researchers with OpenSAFELY Data Access Agreements to write and execute code 
for data management and data analysis without direct access to the underlying raw pseudonymised patient data, 
and to review the outputs of this code. All code for the full data management pipeline—from raw data to 
completed results for this analysis—and for the OpenSAFELY platform as a whole is available for review at 
https://github.com/OpenSAFELY. The data management and analysis code for this paper was led by MS and 
RYP and is available for scientific review and re-use under MIT open licence. 
https://github.com/opensafely/BMI-and-Metabolic-Markers. Acknowledgements We are very grateful for all the support received from the TPP Technical Operations team throughout this work, 
and for generous assistance from the information governance and database teams at NHS England and the NHS 
England Transformation Directorate. 
 
 
Authors Contribution Statement 
All authors contributed to this manuscript.  MS, RYP, RM, CEM, BMK, KK, JV and SF contributed to the 
conceptualisation of the study.  MS, RYP, CEM, SB, AM, JM, ID, PI, WJH, RR, BMK and SF were involved in 
project administration underpinning this analysis.  CEM, SB, AM, JM, ID, PI, WJH, and BMK contributed to 
the development and maintenance of the OpenSAFELY-TPP platform. MS, RYP, CEM, SB, AM, ID, PI and 
WJH were involved in curation of the data used in the analysis.  MS, RYP, SVE, FE, DS, RM and SF 
contributed to the development and design of the statistical methodology used in these analyses.  MS conducted 
the formal analysis including the application of statistical techniques with technical support from RYP and 
further support and supervision from DS, SVE, RM and SF.  RYP, SVE, WJH, KK, RM, JV, BMK and SF 
contributed to supervision of different parts of the analysis including support to use the platform, support in the 
analyses and interpretation of the study findings.  MS wrote the original draft. All authors contributed to further 
review and editing of the draft. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 3, 2023. 
; 
https://doi.org/10.1101/2023.04.01.23287538
doi: 
medRxiv preprint 18",1
"Strategies to improve the immunogenicity of COVID-19 vaccines are necessary to optimise their protection against disease.  Fractional dosing by intradermal administration (ID) has been shown to be equally immunogenic as intramuscular (IM) for several vaccines, but the immunogenicity of ID inactivated whole-virus SARS-CoV-2 at the full dose is unknown.  This study (NCT04800133) investigated the superiority of antibody and T cell responses of full-dose CoronaVac by ID over IM in adolescents.  Participants aged 11-17 years received 2 doses IM or ID, followed by the 3rd dose 13-42 days later.  Humoral and cellular immunogenicity outcomes were measured post-dose 2 (IM-CC versus ID-CC) and post-dose 3 (IM-CCC versus ID-CCC).  Doses 2 and 3 were administered to 173 and 104 adolescents, respectively.  S IgG, S-RBD IgG, S IgG FcgRIIIa-binding, SNM-specific IL- 2+CD4+, SNM-specific IL-2+CD8+, S-specific IL-2+CD8+, N-specific IL-2+CD4+, N-specific IL- 2+CD8+ and M-specific IL-2+CD4+ responses fulfilled the superior and non-inferior criteria for ID-CC compared to IM-CC, whereas IgG avidity was inferior.  For ID-CCC, S-RBD IgG, surrogate virus neutralisation test (sVNT), 90% plaque reduction neutralisation titre (PRNT90), PRNT50, S IgG avidity, S IgG FcgRIIIa-binding, M-specific IL-2+CD4+, interferon- g+CD8+ and IL-2+CD8+ responses were superior and non-inferior to IM-CCC.  The estimated vaccine efficacies were 49%, 52%, 66% and 79% for IM-CC, ID-CC, IM-CCC and ID-CCC, respectively.  More in the ID groups reported local, mild adverse reactions.  This is the first study to demonstrate superior antibody and M-specific T cell responses by ID inactivated SARS-CoV-2 vaccination and serves as the basis for future research to improve immunogenicity of inactivated vaccines. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents INTRODUCTION The coronavirus disease 2019 (COVID-19) pandemic caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) remains of major global public health concern.  Although hospitalisations were rarer for adolescents, severe disease still occurred.1  During an outbreak by Omicron variants in Hong Kong (HK) in 2022, paediatric hospitalisations increased, with acute neurological and respiratory complications, multisystem inflammatory syndrome in children (MIS-C), long COVID and mental health issues being reported in children and young people, outcomes which can be ameliorated by vaccination.1-5 Initial landmark trials demonstrated that the nucleoside-modified mRNA vaccine, BNT162b2, and inactivated whole-virus vaccine, CoronaVac, had ~90-95% and ~50-85% efficacies against symptomatic COVID-19 in persons aged ≥16 and ≥18 years old, respectively.6-8  The efficacy for BNT162b2 in another phase 3 study for 12 to 15-year-old adolescents was 100%.9  These vaccines, with efficacies >50%, have been approved for emergency use since 2021.  In a phase 2 trial on CoronaVac for adolescents, 2 doses induced 100% seroconversion in those 12-17 years old.10  However, the real-life effectiveness for CoronaVac in the prevention of hospitalisation for adolescents was lower, at ~90% in Chile and HK, and it is further reduced against infection.5,11,12 As inactivated vaccines, namely CoronaVac, has been amongst the most widely used COVID-19 vaccines for individuals ³3 years old globally, our group performed a humoral and cellular immunobridging study during the early phase of vaccine availability and found non-inferior immunogenicity for CoronaVac in adolescents compared to adults.13,14 However, CoronaVac induced lower antibody responses than BNT162b2 in adolescents and performed similarly in adolescents as adults.14  These findings were consistent with the observation that CoronaVac’s efficacies against infection appeared lower than BNT162b2 in separate pivotal clinical trials.6-8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents Moreover, emerging VOCs, such as various Omicron subvariants, developed mutations at numerous sites that allow neutralising antibody escape for previously infected or vaccinated individuals, further raising concerns for reduced efficacies.15  In our recent study, the immunogenicity of CoronaVac against Omicron was markedly lower than the wild type (WT) strain in adolescents.16  As a result of immune evasion by these VOCs and waning antibodies, inclusion of the third dose as part of the primary series of CoronaVac had been recommended in Hong Kong and Singapore for most age groups.  It is apparent that all feasible strategies on optimizing immunological responses to CoronaVac need to be urgently explored. Intradermal vaccination (ID) has been shown to be safe and can enhance immunogenicity compared to the intramuscular route (IM).17  Introduction of viral antigens and adjuvant into the skin activates resident innate cells, including dermal CD14+ dendritic cells, Langerhans cells and mast cells that secrete cytokines, cross present to CD8+ T cells and prime CD4+ T cells to induce switching of naïve B cells into IgG- and IgA-producing isotypes.18-21  Fractional vaccine dosing has been studied extensively for ID to mitigate vaccine inequity during supply shortages or unaffordable costs, especially for the inactivated poliovirus (IPV), hepatitis B (HBV) and human papillomavirus (HPV) vaccines, which showed similar humoral immunogenicity as full doses of IM.22-24 Inactivated influenza vaccines, produced by similar technology as CoronaVac, also induced similar IgG titres with fractional dosing by the ID as the full IM dose.17  In 2 separate studies, our group showed children 6 months to 17 years old who received inactivated influenza vaccination intradermally at one-fifth of the IM dose developed similar antibody responses to the conventional IM dose.25,26  For full ID dosing, two trials found superior geometric mean (GM) haemagglutination inhibition antibody titres and seroprotection rates compared to IM of the inactivated influenza vaccines in older adults.27,28 Several investigators recently compared fractional ID ChAdOx1/AZD-1222 with BNT162b2 boosters after 2 IM injections of CoronaVac, which induced similar antibody and . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents T cell responses as IM boosters of the same respective vaccine.29,30  However, ID usage of full doses of COVID-19 vaccines has not been studied thus far.  Based on the collective available scientific data, we postulated that ID with the full dose of CoronaVac can induce greater immunogenicity against SARS-CoV-2 than the currently recommended IM.  This study aimed to compare the reactogenicity of 2 and 3 full doses of CoronaVac between ID and IM and show superior immunogenicity with ID for adolescents 11-17 years old.  The current study presents a pre-specified interim analysis of the immunogenicity against the WT and Omicron SARS-CoV-2, reactogenicity and safety results at 1 month after 2 and 3 doses of CoronaVac. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents RESULTS Study participants.  A total of 185 adolescents aged 11-17 years received at least 1 dose of IM or ID CoronaVac at the screening visit (V1) from 27 April 2021 to 06 August 2022 (see Methods; Supp. Fig. 1).  There were 185 and 178 who returned for subsequent follow-up visits 2 (V2) and 3 (V3), respectively, and those who attended V3 were included in the reactogenicity and safety analyses (healthy safety population; see Methods, and Protocol and Statistical Analysis Plan in Supplementary Information; Supp. Fig. 1).  The evaluable analysis population included those uninfected based on the clinical history obtained and negative ORF8 IgG (a serological marker of past natural SARS-CoV-2 infection) at every visit, who had negative baseline S-RBD IgG, no major protocol deviations and a valid immunogenicity result (see Methods; Supp. Fig. 1).  Of the 173 who completed the 2-dose series (IM-CC and ID-CC), 104 received 3 vaccine doses (IM-CCC and ID-CCC) and returned for the follow-up visit (V4), all within the evaluable intervals (Supp. Fig. 1).  A total of 119 IM-CC and 54 ID-CC were included in the evaluable analysis population, with 60 IM and 44 ID recipients for 3 doses.  We confirmed these findings by performing an analysis with the expanded analysis population that consisted of 119 IM-CC, 59 ID-CC, 82 IM-CCC and 43 ID-CCC who had wider time intervals of vaccination and blood sampling (see Methods; Supp. Fig. 1).  There was an even distribution of demographic characteristics between the IM and ID groups (Supp. Table 1). Humoral immunogenicity analyses between IM and ID administrations.  The primary humoral immunogenicity outcomes in this study were SARS-CoV-2 S IgG, S-RBD IgG by enzyme-linked immunosorbent assay (ELISA), surrogate virus neutralisation test (sVNT), plaque reduction neutralisation titres (PRNT), spike (S) IgG avidity and S IgG Fcg receptor IIIa (FcgRIIIa)-binding on ELISA performed for healthy, uninfected adolescents 13-42 days after dose 2 or 3 of CoronaVac by IM or ID (see Methods).  Evaluable IM achieved 96.6% S- RBD IgG seropositivity, with geometric mean (GM) optical density-450 (OD450) and sVNT . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents inhibition of 1.20 and 71.2% post-dose 2, respectively (Table 1).  100.0% of evaluable ID-CC had positive (S receptor-binding domain) S-RBD IgG, with GM OD450 value of 2.16 and sVNT inhibition of 78.4%.  GM for PRNT90 was 9.83 and 10.7 after IM-CC and ID-CC, respectively.  GM for PRNT50 against WT was 26.8 and 30.2 after IM-CC and ID-CC, respectively.  After IM-CC and ID-CC, GM avidity was 20.5% and 6.95%, and GM OD450 of S IgG FcgRIIIa-binding were 0.749 and 1.10, respectively. Compared to IM-CC, humoral responses when measured by S IgG (GM ratio (GMR) 1.18, 95%CI 1.02-1.38, P=0.028), S-RBD IgG (GMR 1.80, 95%CI 1.58-2.05, P<0.0001) and S IgG FcgRIIIa-binding (GMR 1.47, 95%CI 1.16-1.87, P=0.002) (Fig. 1A) satisfied the superior and non-inferior criteria for evaluable ID-CC.  ID-CC mounted non-inferior humoral responses by sVNT (GMR 1.10, 95%CI 0.99-1.22, P=0.065), PRNT90 (GMR 1.09, 95%CI 0.84-1.41, P=0.536), PRNT50 (GMR 1.13, 95%CI 0.85-1.49, P=0.397) and inferior S IgG avidity (GMR 0.34, 95%CI 0.27-0.43, P<0.0001). Since IM-CCC had been recommended as the primary vaccination series in HK and Singapore, this regimen was also compared with ID-CCC.  Evaluable IM-CCC achieved 100.0% S-RBD IgG seropositivity, with GM OD450 and sVNT inhibition of 1.77 and 84.9% post-dose 3, respectively (Table 1).  100.0% of evaluable ID-CCC had positive S-RBD IgG, with GM OD450 value of 2.45 and sVNT inhibition of 91.3%.  Neutralisation titres demonstrated GM of 17.8 and 38.7 for PRNT90 after IM-CCC and ID-CCC, respectively. GM for PRNT50 was 55.3 and 110 after IM-CCC and ID-CCC, respectively.  GM avidity was 38.5% and 53.3%, and the GM OD450 results of S IgG FcgRIIIa-binding were 1.41 and 1.79, after IM-CCC and ID-CCC, respectively. ID-CCC satisfied the superior and non-inferior criteria by S-RBD IgG (GMR 1.38, 95%CI 1.28-1.49, P<0.0001), sVNT (GMR 1.08, 95%CI 1.02-1.14, P=0.014), PRNT90 (GMR 2.17, 95%CI 1.43-3.30, P=0.0004), PRNT50 (GMR 1.98, 95%CI 1.36-2.87, P=0.0004), S IgG avidity (GMR 1.38, 95%CI 1.19-1.61, P<0.0001) and S IgG FcgRIIIa-binding (GMR 1.28, 95%CI 1.06-1.54, P=0.011), but not by S IgG (GMR 1.13, 95%CI 0.97-1.31, P=0.111), which . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents satisfied the non-inferiority criterion only (Fig. 1B).  Superiority and non-inferiority testing in the expanded analysis populations for post-doses 2 and 3 were analogous to these results (Supp. Table 2; Supp. Fig. 2). Cellular immunogenicity analyses between IM and ID administrations.  The primary cellular immunogenicity outcomes for this study were interferon-g (IFN-g)+ and interleukin-2 (IL-2)+ CD4+ and CD8+ T cells responses to S, N (nucleocapsid protein) and M (membrane protein) after IM-CC and ID-CC, which were analysed using intracellular cytokine staining by flow cytometry (see Methods).  For the 60 CD-IM and 48 CC-ID evaluable adolescents, more than half of participants had detectable responses for S-specific IFN-g+CD4+ or IL- 2+CD4+ T cells after 2 ID or IM doses (Table 2).  There were 45.8-52.1% of those with IFN- g+CD8+ and IL-2+CD8+ T cell responses to S after 2 doses of either administration routes. The remainder of the T cell responses to the peptide pools S, N, M and SNM (sum of individual S, N, and M peptide pools) after IM-CC, ID-CC, IM-CCC and ID-CCC also are shown in Table 2. After 2 doses of ID CoronaVac, SNM-specific IL-2+CD4+(GMR 2.83, 95%CI 1.67- 4.79, P=0.0002), SNM-specific IL-2+CD8+ (GMR 2.88, 95%CI 1.77-4.70, P<0.0001), S- specific IL-2+CD8+ (GMR 1.94, 95%CI 1.13-3.34, P=0.017), N-specific IL-2+CD4+ (GMR 2.24, 95%CI 1.14-4.38, P=0.019), N-specific IL-2+CD8+ (GMR 3.21, 95%CI 1.81-5.71, P=0.0001) and M-specific IL-2+CD4+ (GMR 1.88, 95%CI 1.08-3.28, P=0.027) T cell responses were superior and non-inferior to IM (Fig. 2A).  SNM-specific IFN-g+CD8+, S- specific IFN-g+CD4+ and S-specific IFN-g+CD8+ T cell responses were inconclusive, while other T cell responses were non-inferior.  Additionally, evaluable ID-CCC satisfied superior and non-inferior criteria for M-specific IL-2+CD4+(GMR 2.28, 95%CI 1.05-4.96, P=0.038), IFN-g+CD8+ (GMR 2.39, 95%CI 1.07-5.33, P=0.034) and IL-2+CD8+ (GMR 2.63, 95%CI 1.30- 5.32, P=0.008) T cell responses compared to IM-CCC (Fig. 2B).  SNM-specific IFN-g+CD8+, . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents S-specific IFN-g+CD8+, S-specific IL-2+CD8+ and N-specific IFN-g+CD8+ T cell responses were inconclusive, while the other T cell responses were non-inferior.  These results were consistent with superiority and non-inferiority testing in the expanded analysis populations (Supp. Table 3; Supp. Fig. 3).  Overall, none of the cellular immunogenicity outcomes tested for groups that received ID was inferior compared to IM. Longitudinal immunogenicity changes between doses over time within the same group of IM or ID.  Antibody and T cell responses were compared between post-doses 2 and 3 in the evaluable analysis populations.  Overall, dose 3 induced higher humoral responses for S IgG, S-RBD IgG, sVNT, PRNT90, PRNT50, S IgG avidity and S IgG FcgRIIIa-binding than dose 2 by the respective IM or ID routes (Supp. Fig. 4).  Additionally, there were higher SNM-specific IL-2+CD4+, SNM-specific IL-2+CD8+ and N-specific IL- 2+CD8+ T cell responses after IM-CCC compared to IM-CC (Supp. Fig. 5).  ID-CCC induced a higher M-specific IL-2+CD8+ T cell response than ID-CC. Estimation of vaccine efficacies from different doses and routes of administration of CoronaVac based on neutralisation titres.  Levels of neutralising antibodies have been regarded as a correlate of protection.  Hence, we extrapolated our PRNT50 results from evaluable adolescents who received 2 or 3 doses of IM or ID CoronaVac with vaccine efficacies against symptomatic COVID-19 by normalization to convalescent sera, as previously described.14,16,31  The mean neutralisation levels of IM-CC, ID-CC, IM-CCC and ID-CCC were 0.19, 0.22, 0.40 and 0.79, corresponding to 49%, 52%, 66% and 79% vaccine efficacies, respectively (Fig. 3).  Using PRNT90 instead of PRNT50 yielded similar findings (data not shown). Omicron-specific humoral and cellular immunogenicity post-dose 2 and post-dose 3 of ID CoronaVac.  For ID-CCC, humoral responses tested against the Omicron variant were . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents significantly lower than WT SARS-CoV-2 for S IgG (GM OD450 0.81 vs 1.05, P<0.0001) and S IgG FcgRIIIa-binding (GM OD450 1.33 vs 1.79, P<0.001), respectively (Fig. 4A).  S IgG avidity and all the cellular immunogenicity outcomes were similar between Omicron and WT (Fig. 4A-D). Reactogenicity and safety of IM or ID CoronaVac.  In the healthy safety population, pain at the injection site was the most reported AR for IM, which was similar to ID (Fig. 5A). Greater proportions of ID reported swelling, erythema and induration (IM-C vs ID-C: 1.7vs 52.5%, P<0.0001; IM-CC vs ID-CC: 0.8% vs 64.4%, P<0.0001; IM-CCC vs ID-CCC: 1.1% vs 65.1%, P<0.0001) and pruritis (IM-C vs ID-C: 0.8% vs 62.7%, P<0.0001; IM-CC vs ID-CC: 0.8% vs 50.9%, P<0.0001; IM-CCC vs ID-CCC: 1.1% vs 60.5%, P<0.0001) at the injection site than IM.  Most recipients developed these symptoms within minutes after ID, which progressed locally at the site of inoculation over 1-2 weeks and subsided over several weeks (Fig. 5B-D; Supp. Fig. 6).  Systemic ARs were similar between IM and ID (Fig. 6).  There were 13 adverse events (AEs) reported within 28 days after vaccination (8 for IM and 5 for ID) and no serious adverse event (SAE) for either administration route (Supp. Table 4). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents DISCUSSION This is the first study to assess the immunogenicity, reactogenicity and safety of intradermal administration of an inactivated COVID-19 vaccine with the full dose, which demonstrated superior antibody responses and T cell responses against the SARS-CoV-2 membrane protein in adolescents who received 3 injections.  ID-CCC elicited higher antibody responses across all parameters tested and M-specific IL-2+CD8+ T cell response than ID-CC.  These data estimated the vaccine efficacy of ID-CCC to be 79%, which was ~30% higher than 2 doses of IM observed in this study (49%), our recent immunobridging publication (50%) and CoronaVac’s initial study in Brazil (51%).7,14  Whether the observed superior immunogenicity and estimated efficacy from ID translate to actual higher clinical efficacy and effectiveness needs to be further studied. The long-term immune protection from different types and routes of vaccination, particularly against VOCs, is incompletely established.  In this study, the levels of anti-spike antibodies and its FcgRIIIa-binding against Omicron was lower than WT after ID-CCC. However, S IgG avidity and cellular immunogenicity against Omicron were maintained, which can possibly explain the persistently high real-life vaccine effectiveness even when it is known that there is waning of quantitative serum antibody concentrations against VOCs.5,12  Indeed, several human studies have shown that T cell immunity can persist for years after prior exposure and mitigate disease severity when neutralising antibodies are reduced, and the pre-existing antigen-specific T cells are protective against influenza viral infections, severity of symptoms and viral shedding.32-36  In addition, vaccine-induced T cell responses against the highly conserved structural SARS-CoV-2 membrane protein confers partial protection from lung pathology in a murine model.37 Interestingly, S IgG avidity for ID-CC was inferior compared to IM-CC.  However, this was reversed after 3 doses.  This was accompanied by a simultaneous shift from non- inferior to superior neutralising antibodies and sVNT, while the differences in S IgG and S- RBD IgG between IM and ID became less pronounced.  We speculate despite reduced . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents differences in the quantitative anti-S IgG concentrations after dose 3 between ID and IM, there was greater antibody function and quality from the higher avidity that correspondingly enhanced the neutralisation of SARS-CoV-2.38  The high-affinity antibodies that resulted in higher avidity binding are indicative of class switching in germinal centres, affinity maturation and longer lasting functional antibody responses.  Furthermore, FcgRIIIa functions were also increased by ID vaccination.  Our recent publication on BNT162b2 by IM demonstrated consistent increases in S IgG avidity across time, while the concentrations of IgGs waned before dose 3 was administered as a booster vaccination to adolescents.39  These findings support the notion that effective vaccination induces efficient immunity characterized by antibodies that have high avidity and increased effector functions against SARS-CoV-2 rather than large quantities of low-quality immunoglobulins.40  In this regard, S IgG avidity maturation in our cohort appears to be markedly boosted by dose 3 of ID CoronaVac, and the kinetics of this antibody response is significantly different from IM. This study had some limitations.  It was not possible to blind the participants because receipt of IM or ID injections was readily differentiable, and the technique of administration for the 2 routes was also noticeably distinct to the clinical staff.  It can be viewed that this unblinded, non-randomised study design is a limitation and has the potential for selection bias.  However, since the age, sex and ethnic distributions were similar between groups, the immunogenicity comparisons should be valid.  Importantly, this practical approach can be directly interpretable for real-life applicability.  For example, although the immunogenicity of ID appears greater than IM, ID injections were associated with more local reactions.  This known adverse effect can be a reason for some individuals to select IM rather than ID in the real world.  Although ARs were predominantly local symptoms and no SAEs occurred, participants who received ID endured weeks of induration and itching at the site of injection, most of whom described as tolerable.  These findings were consistent with the recent study on the novel mpox by ID route that was associated with acceptable local reactions, as both CoronaVac and 3rd generation mpox vaccines are incapable of replication.41  Larger . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents pharmacovigilance studies over longer periods of time will be required to delineate whether there are higher risks of rarer adverse effects from full-dose, intradermally administered inactivated COVID-19 vaccination.  Additionally, the strict social distancing policies that the HK Government mandated during the COVID-19 period kept infection numbers low so that it was not possible to investigate vaccine efficacy in this cohort during this study period, from April 2021 to August 2022 (see Methods).1,5,12  On the other hand, this provided the opportunity to determine the intradermal vaccine immunogenicity without the confounding effects of existing immunity acquired from past infections.1,5,12  Moreover, we were able to estimate actual vaccine efficacies using neutralising antibodies.14,16,31  The comprehensive, validated assays of antibody levels, avidity and binding and T cell responses were a major strength of this study, yet follow-up research on clinical efficacy and real-life effectiveness against COVID-19 from ID would be needed to further support the current findings.  The sample size for the ID group was slightly smaller than the initial target due to the unforeseeable wave of Omicron infections that swept across HK during the study period that doses 2 and 3 were administered.  This led to exclusion of more enrolled participants than expected, while a few others under quarantine as close contacts could not provide blood samples within the evaluable window, and these can lead to selection bias.  Despite the reduced sample size, detection of significant superiority in the ID group was still achieved in many of the major immunogenicity outcomes.  The larger sample size from the expanded analysis population confirmed the observed superior immunogenicity.  This study focused on adolescents, and thus the immunogenicity of ID CoronaVac will need to be explored for adults and young children as well. The specific populations that would likely benefit most from this enhanced immunogenicity of ID CoronaVac are the unvaccinated and high priority group that includes immunocompromised patients, young individuals with comorbidities, elderly, pregnant persons and frontline health workers.  This aligns with the World Health Organization’s most recent recommendation on 28 March 2023 that this high priority group should receive an . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents additional COVID-19 booster vaccination 6-12 months after the last dose.  Many older adults, young children or those with debilitating chronic diseases are hesitant towards receiving the novel monovalent or bivalent mRNA vaccines due to systemic adverse effects, risks of myocarditis and potentially higher association with ischaemic stroke.14,42-44  As this study demonstrated more frequent but tolerable local adverse effects only, it is possible these individuals would be more accepting of ID inactivated vaccines.  ID can offer a cost- effective option for areas with limited access to more immunogenic vaccines that have higher financial or storage demands.  A previous study revealed enhanced immunogenicity from ID against the hepatitis B virus in dialysis patients that was more striking 52 weeks after ID than IM.45  Whether such durability of heightened immunogenicity in healthy individuals is conferred by ID CoronaVac against SARS-CoV-2 and novel VOCs in the future is not yet certain, which we will track in these adolescents for the next 3 years.  Importantly, this study serves as the basis for further research on ID using full doses of vaccines, rather than fractional dosing, to optimise protection against COVID-19 and other infectious diseases, particularly for those at risk of vaccine failures. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents METHODS Study Design.  This registered study is part of the COVID-19 Vaccination in Adolescents and Children (COVAC) (Department of Health, HK, Clinical Trial Certificate 101894; clinicaltrials.gov NCT04800133) that investigates immunobridging for BNT162b2 and CoronaVac in adolescents and children, as previously described.14,16,39  The current pre- specified interim analysis aims to demonstrate the superiority in immunogenicity of the intradermal (ID) compared to intramuscular (IM) route of administration for CoronaVac in adolescents 11-17 years old and reports on the reactogenicity between ID and IM.  The University of Hong Kong (HKU)/HK West Cluster Hospital Authority Institutional Review Board (UW21-157) approved the research procedures, which were in compliance with the October 2013 Declaration of Helsinki principles. Participants.  Recruitment targeted 11- to 17-year-old adolescents residing in HK who were healthy or in stable condition.  Potential participants were recruited using advertisements posted in schools and mass media.  The exclusion criteria for this analysis included known history of COVID-19 (by self-reporting at any of the 4 study visits, or baseline S-RBD IgG or ORF8 IgG positivity at any visit), severe allergy, neuropsychiatric conditions, immunocompromised states, transfusion of blood products within 60 days, haemophilia, pregnancy or breastfeeding (see Protocol and Statistical Analysis Plan for details). Procedures.  Study doctors obtained informed assent from eligible participants and consent from their respective parents or legally acceptable representatives.  The skin superficial to the deltoid muscle was cleansed with 70% weight/volume isopropyl alcohol before using the standard 1 mL (KDL Medical, Shanghai, China) or MicronJet600 (NanoPass Technologies, Ness Ziona, Israel) needles for IM or ID, respectively, at a HK Community Vaccination Centre (CVC) (Supplementary Video).46  The dosage of CoronaVac was 0.5 mL (equivalent to 600 SU, or 3 µg, of the whole virus antigen of the inactivated SARS-CoV-2 CZ02 strain) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents for each injection, with a total of 3 separate doses given fo IM or ID.  Doses 2 and 3 were given 28-35 days and 84 days after dose 1, respectively.  Whole blood was obtained before doses 1 (baseline), 2 (C), 3 (CC) and post dose 3 (CCC) (see Analysis populations in Statistical Analyses of Methods). Safety and reactogenicity data collection Participants remained at the CVC for observation by the study nurse and doctor for at least 15 mins after each vaccine injection.  Participants were required to report pre-specified adverse reactions (ARs) in an online or handwritten diary for the following 7 days, as previously described.14,16  They were encouraged to capture photos of their sites of injection for at least 7 days and until resolution of local reactions, followed by uploading onto our online diary website.  Unsolicited adverse events (AEs), such as hospitalisation, life-threatening illnesses, disabilities, deaths, birth defects of offspring and breakthrough COVID-19 are monitored for 3 years. These AEs were reviewed by study physicians, who assessed the probability of causal relationship with the study vaccination. S-RBD IgG, surrogate virus neutralisation assay (sVNT), plaque reduction neutralisation titre (PRNT) Peripheral blood was collected into clot activator vacutainer tubes, separated into serum and stored at -80° C.  Sera were heat-inactivated at 56° C for 30 mins prior to testing.  The SARS-CoV-2 S receptor-binding domain (S-RBD) IgG enzyme- linked immunosorbent assay (ELISA) and PRNT have been validated in our previous publications.14,16,39,47  The sVNT was carried out according to the manufacturer’s instructions (GenScript Inc, Piscataway, USA) and our previous experiments, which have been validated.14,16,39,47 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents Briefly, S-RBD IgG ELISA plates were coated overnight with 100 ng/well of purified recombinant S-RBD in PBS buffer, and then 100 µL Chonblock Blocking/Sample Dilution (CBSD) ELISA buffer (Chondrex Inc, Redmond, USA) were added.  The incubation period of this mixture at room temperature (RT) was 2 hrs. Serum was tested at a dilution of 1:100 in CBSD ELISA buffer, then added to the wells for 2 hrs at 37°C.  After washing with PBS that contained 0.2% Tween 20, horseradish peroxidase (HRP)-conjugated goat anti-human IgG (1:5000, Thermo Fisher Scientific) was added for 1 hr at 37°C and then washed 5 times with PBS containing 0.2% Tween 20.  HRP substrate (Ncm TMB One, New Cell & Molecular Biotech Co. Ltd, China) of 100 µL was added for 15 mins.  This reaction was stopped by 50 µL of 2 M H2SO4.  The optical density (OD) was analysed in a Sunrise absorbance microplate reader (Tecan, Männedorf, Switzerland) at 450 nm wavelength.  Each OD reading subtracted the background OD in PBS-coated control wells with the participant’s serum.  Values at or above an OD450 of 0.5 were considered positive, while values below were imputed as 0.25.14,16,39 The sVNT was performed using 10 µL of each serum, positive and negative controls, which were diluted at 1:10 and mixed with an equal volume HRP conjugated to the wild type (WT) SARS-CoV-2 S-RBD (6 ng), and these were incubated for 30 mins at 37°C, then 100 μL of each sample was added to microtitre plate wells coated with angiotensin-converting enzyme-2 (ACE-2) receptor.14,16,39 This plate was sealed for 15 mins at 37°C, washed with wash-solution, tapped dry, and then 100 μL of 3,3',5,5'-tetramethylbenzidine (TMB) was added.  This mixture was incubated in the dark at RT for 15 mins.  The reaction was terminated with 50 µL of Stop Solution and the absorbance was read at 450 nm in a microplate reader. After confirmation that the positive and negative controls provided the recommended OD450 values, the % inhibition of each serum was calculated as 1 - sample OD value/negative control OD value x100%.  Inhibition (%) of at least 30%, the limit of . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents quantification (LOQ), was regarded as positive, while values below 30% were imputed as 10%.14,16,39 The PRNT was performed in duplicate in a biosafety level 3 facility.14,47  Serial dilutions of serum from 1:10 to at least 1:320 were incubated with approximately 30 plaque-forming units of the WT SARS-CoV-2 BetaCoV/Hong Kong/VM20001061/2020 virus in culture plates (Techno Plastic Products AG, Trasadingen, Switzerland) for 1 hr at 37°C.14-16,39  These virus-serum mixtures were added onto Vero E6 cell monolayers and incubated for 1 hr at 37°C in a 5% CO2 incubator.  The plates were overlaid with 1% agarose in cell culture medium and incubated for 3 days, and then the plates were fixed and stained.  Antibody titres were defined as the reciprocal of the highest serum dilution that resulted in the more stringent cut-off of >90% (PRNT90) or >50% (PRNT50) reduction in the number of plaques.  Values below the lowest dilution tested, which was 10, were imputed as 5, while those above 320 were imputed as 640.14,16,39 S IgG, avidity and FcgRIIIa-binding S IgG, avidity and FcgRIIIa-binding assays were performed as previously described, with the addition of Omicron BA.2.14,16,39,48  In brief, plates (Nunc MaxiSorp, Thermofisher Scientific) were coated with 250 ng/mL WT (AcroBiosystems) or Omicron BA.2 (AcroBiosystems) SARS-CoV-2 S protein for IgG and IgG avidity assessments, or 500 ng/mL WT (Sinobiological) or Omicron BA.2 (AcroBiosystems) S for FcγRIIIa-binding detection, or 300 ng/mL ORF8 (Masashi Mori, Ishiwaka University, Japan) at 37°C for 2 hrs.  The protein for S IgG was diluted in PBS.  The plates were blocked with 1% FBS in PBS for 1 hr, incubated with 1:100 heat-inactivated (HI) serum diluted in 0.05% Tween-20/ 0.1% FBS in PBS for 2 hrs at RT prior to rinsing.  For antibody avidity, plates were washed thrice with 8M Urea before incubation for 2 hrs with IgG-HRP (1:5000; G8-185, BD).  HRP was revealed . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents by stabilized hydrogen peroxide and tetramethylbenzidine (R&D systems) for 20 mins and stopped with 2N H2SO4 before analysis with an absorbance microplate reader at 450 nm wavelength (Tecan Life Sciences).  For those with a positive S IgG value, the IgG avidity index was calculated by the ratio of the OD450 values after to before washing of the plates, censored at 100%.  FcgRIIIa-binding antibodies were detected after incubation with HI serum at 1:50 dilution for 1 hr at 37°C and then with biotinylated FcgRIIIa-V158, which was expressed in-house (from Mark Hogarth and Bruce Wines, Burnet Institute, Australia), at 100 ng/mL for 1 hr at 37°C.  Streptavidin- HRP (1:10,000, Pierce) was added for detection of S specific FcgRIIIa-V158-binding antibodies.  OD450 values at or above the respective limits of detection (LODs) were considered positive, while values below were imputed as 0.5 of the LOD.14,16,39 T cell responses Density gradient separation was performed to isolate peripheral blood mononuclear cells (PBMCs) from whole blood, which was frozen in liquid nitrogen.14,16,39  Subsequently, thawed PBMCs were rested for 2 hrs in RPMI medium supplemented with 10% human AB serum.  The cells were stimulated with sterile double-distilled water (ddH2O) or 1 µg/mL overlapping peptide pools representing the WT SARS-CoV-2 S, N (nucleocapsid) and M (membrane) proteins, or Omicron B.1.1.529/BA.1 S mutation pool and WT reference pool, BA.1 N mutation pool and WT N reference pool, BA.1 M mutation pool and WT M reference pool (Miltenyi Biotec, Bergisch Gladbach, Germany) (synthesized by ChinaPeptides Co., Ltd, as previously described) for 16 hrs in 1 µg/mL anti-CD28 and anti-CD49d costimulatory antibodies (clones CD28.2 and 9F10, Biolegend, San Diego, USA).14,16,39  This mixture was stimulated for 2 hrs, followed by the addition of 10 µg/mL brefeldin A (Sigma, Kawasaki, Japan).14,16,39,49  The cells were then washed and subjected to immunostaining with a fixable viability dye (eBioscience, Santa Clara, USA, 1:60) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents and antibodies against CD3+ (HIT3a, 1:60), CD4+ (OKT4, 1:60), CD8+ (HIT8a, 1:60), IFN-g (B27, 1:15) and IL-2 (MQ1-17H12, 1:15) antibodies (Biolegend, San Diego, USA).  Flow cytometry (LSR II with FACSDiva version 8.0, BD Biosciences, Franklin Lakes, USA), analysed by Flowjo version 10 software (BD, Ashland, USA), was used for data acquisition.  Antigen-specific T cell results were finalised after subtracting the background (ddH2O) data and presented as the percentage of CD4+ or CD8+ T cells.14,16,39,50  The T cell response against a single peptide pool was considered positive when the frequency of cytokine-expressing cells was higher than 0.005% and the stimulation index was higher than 2, while negative values were imputed as 0.0025%.14,16,39  Total T cell responses against S, N and M peptide pools were added together, which used a cut-off of 0.01%.14,16,39 Outcomes.  The primary outcomes in this interim analysis were humoral immunogenicity (S IgG and S-RBD IgG levels, sVNT %inhibition, 90% and 50% PRNT titres, S IgG avidity and FcgRIIIa-binding) and cellular immunogenicity markers (S, N- and M-specific IFN-g+ and IL-2+ CD4+ and CD8+ T cell responses measured by the flow-cytometry-based intracellular cytokine staining assay) 13-42 days after doses 2 and 3 of CoronaVac.  The primary reactogenicity outcomes included pre-specified ARs and reported antipyretic use during the 7 days following each vaccine injection. Omicron humoral and cellular immunogenicity results were secondary outcomes. Regarding safety, the secondary outcomes were unsolicited AEs within 28 days after each vaccine injection and SAEs during the entire study period.  The Protocol and Statistical Analysis Plan (Supplementary Information) described other secondary outcomes that were not pertinent to this interim analysis, such as specific assessments for participants with known chronic illnesses. Statistical Analyses. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents Power analyses and sample size estimation For the primary immunogenicity objectives, when comparing the peak geometric mean (GM) immunogenicity outcomes of seroconversion rate or adverse events for CoronaVac ID with that of IM administration in adolescents aged 11-17 years, a sample size of 50 in each group would assure that a two-sided test with a=0.05 has 97% power to detect an effect size with a Cohen’s d value=0.78, or a difference of 0.51 after natural logarithmic transformation, between the 2 groups, with a standard deviation (SD) of 0.65 within each group.  We aimed to recruit 60 participants for each group of IM and ID administrations to accommodate for potential attrition or protocol deviation.  However, performance of assays requiring large blood volumes was omitted for a few younger, small-sized adolescents, who could provide limited amounts of blood.  In terms of the proportion of participants with a positive result in immunogenicity outcomes or ARs, 50 adolescents would yield a 95% chance to detect the true value within ±11 percentage points of the measured percentage, assuming a prevalence of 80%.  G*Power (Heinrich-Heine-Universität Düsseldorf, Düsseldorf, Germany) and Sampsize (sampsize.sourceforge.net) were used for these power analyses. Analysis populations The primary analysis of humoral and cellular immunogenicity outcomes was performed in the healthy adolescent participants who received IM or ID injections of CoronaVac on a per-protocol basis.  The evaluable analysis population included participants who were generally healthy and have remained uninfected during study visits (based on self-reporting, ORF8 IgG negativity and negative baseline S-RBD IgG), no major protocol deviations, received dose 3 at least 84 days after dose 1, blood sampling within the evaluable window for post-dose 1 (no more than 3 days earlier or later than day 28, and before dose 2), post-dose 2 (within day 13-42 post- . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents dose 2 and before any further doses), within days 13-42 post-dose 3 and had valid results for the relevant analysis and timepoints (see Protocol in Supplementary Information).  The expanded analysis population included similar criteria as the evaluable analysis population except the requirement of a valid immunogenicity result for the particular analysis at least 14 days post-dose 1 but before dose 2 and between 7-56 days post-dose 2 (see Protocol in Supplementary Information).  The superiority and non-inferiority hypotheses testing for primary immunogenicity outcomes included participants aged 11-17 years in the adolescent groups who received IM or ID injections of CoronaVac for doses 1-3. Statistical tests Immunogenicity outcome data below the cut-off were imputed with half the cut-off value.  GMs were calculated for each immunogenicity outcome, timepoint and group.  GM ratios (GMRs) were calculated as exponentiated differences between the means of the natural logarithmic-transformed immunogenicity outcomes between groups.  The GMRs were reported with a two-sided 95%CI for testing the superiority hypothesis with the lower bound of the 95%CI for GMR >1.  Additionally, confirmation of the superiority results was performed in the expanded analysis population. Simultaneously, we conducted a non-inferiority analysis at the non-inferiority margin of 0.60 for immunogenicity outcomes since it was possible that superiority for a few immunogenicity outcomes would not be satisfied.  By convention, results were regarded as inconclusive if both non-inferiority and inferiority were not met. Comparisons of immunogenicity outcomes between groups were performed with unpaired t test after natural logarithmic transformation.  The proportion of participants with a positive result was reported in percentages, with two-sided 95%CI derived from Clopper-Pearson method.  The Fisher exact test was used for comparisons of proportions between groups. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents Reactogenicity and safety were assessed in the participants who remained generally healthy, uninfected and contributed any AR or AE data after the 3 doses and before the study database was locked for the current interim analysis in these adolescent groups who received IM or ID CoronaVac that comprised the healthy safety population.  For the primary reactogenicity analysis, the proportions of participants reporting each AR at the maximum severity and antipyretic use within 7 days after each vaccine injection were reported in percentages, with the 95%CI derived using the Clopper-Pearson method.  ARs of all severity and antipyretic use were compared between vaccine routes of administration by the Fisher exact test. The incidences of AEs by severity and SAEs that were reported by the post-dose 3 study visit (28 days post-dose 3) were presented as counts and events-per- participant by the vaccine route of administration. Data analyses and graphing were performed using GraphPad Prism (version 9.4.0). Two-sided 95%CIs were presented for all outcomes unless otherwise stated. Vaccine efficacy estimates Vaccine efficacies were estimated as a secondary objective by correlations with neutralising antibody titres, as described in our previous publication.14,16,31  In brief, GMTs of PRNT50 in the evaluable analysis populations were divided from that of 102 convalescent sera collected on days 28-59 post-onset of illness in patients aged ³18 years to calculate the mean neutralising levels.  The best fit of the logistic model, generated from the online plot digitizer tool (https://automeris.io/WebPlotDigitizer/, version 4.5), was used to extrapolate a single point VE estimate for each route of vaccine administration. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288005
doi: 
medRxiv preprint Superior immunogenicity intradermal COVID-19 vaccine adolescents",1
"Objective: The study proposes a decision strategy for the choice of arm
for placement of an arteriovenous fistula (AVF) in patients with kidney failure
who undergo hemodialysis (HD). The possible surgical sites for placement of
the AVF are located in the arm and at two locations in each arm, the wrist
(distal AVF) and the elbow (proximal AVF). Recommendations are made for
the location of the first AVF and subsequent ones, as needed.
Method: A retrospective analysis of AVFs created between 2015 and
2022 in patients at five HD centers was performed. The study uses quality-
adjusted survival, parametric, and nonparametric survival methods. We con-
ducted two surveys to assess the HD patient’s quality of life. In addition, the
Cox proportional hazard model is used to suggest a patient-specific strategy.
The survival functions are compared using the log-rank and Tarone-Ware
tests.
Results: The results of the multivariate analysis showed that placing
AVFs on the non-dominant arm leads to superior patient quality of life com-
pared to the dominant arm. The quality-adjusted survival was also found to
be better when AVFs were located on the non-dominant arm. Considering
both these aspects and also clinical constraints on the sequence, the opti-
mal sequence is found to be non-dominant distal followed by non-dominant
proximal locations, followed by a similar sequence on the dominant hand if
required.
Conclusion: The study identifies criteria for data-driven decision-making
as to good locations for AVFs for Hemodialysis and supports clinical experi- Preprint submitted to Health Policy and Technology
April 6, 2023 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. ence and practice in this matter. Keywords:
Chronic Kidney Disease, Hemodialysis Management,
Quality-adjusted survival, Treatment Decision Policy, therapeutic delivery
PACS: 0000, 1111
2000 MSC: 0000, 1111 1. Introduction Patients with end-stage renal disease (ESRD) need dialysis on a regular
basis till transplant options become available. Hemodialysis (HD) is a com-
monly used procedure that requires stable vascular access. These could be
of several types, such as; a central venous catheter (temporary and perma-
nent catheter), arteriovenous fistula (AVF), and arteriovenous graft (AVG)
[1]. An arterio-venous fistula is a surgical procedure to connect an artery
with a vein in the arm of a patient requiring HD. Several studies have high-
lighted the use of arteriovenous fistulas for hemodialysis because they have
fewer complications and longer patency compared with other vascular access
options [1, 2].
It is observed that AVF sometimes matures sub-optimally and therefore
is unusable for dialysis. AVFs should mature fully to be able to be used for
prolonged periods. The average time for AVF maturation is four to six weeks.
Factors associated with AVF maturation have been reviewed in the literature
[3]. The optimal time to refer Chronic Kidney Disease (CKD) patients for
AVF placement is crucial and should be several months before the need to
start dialysis, as it takes several weeks to mature [4, 5]. A Markov model
with decision trees was presented to determine the cost-effectiveness of AV
fistula versus AVG [6].
The AVF can be created on any hand, at the wrist (radio-cephalic or
distal), or at the elbow, known as brachio-cephalic (proximal). A proximal
AVF is possible if the distal option has already been used for an earlier
AVF, but the reverse is not possible for clinical reasons. The blood flow
in the downstream vein at the wrist reduces after constructing a proximal
AVF. Therefore, nephrologists try to construct the first AVF at the wrist to
keep the upstream option open for subsequent AVFs if needed, as shown in
Figure 1. The patency of different sites, such as distal and proximal, has
been studied in the literature but is not specific to the arm used for fistula
placement [7, 8]. The suitability of the arm is assessed before performing 2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint the AVF surgery as per standard protocol. Most of the time, the patient’s
non-dominant hand is preferred over the dominant hand with the belief that
this approach would avoid injury to the AVF that a dominant arm can be
more prone to. However, the validation of this choice of the non-dominant
arm has not been established.
The AVF, as a vascular access option, has good survival rates. Failure
modes are (i) immediate failure after construction, which is rare, (ii) sub-
optimal maturation, called late primary failure; and (iii) secondary failure
after normal maturation. Analysis of survival statistics is in sections 4 and
6. One study highlighted the reasons and presented the guidelines for how to
intervene to avoid AVF failures [2]. A decision support model for predicting
graft survival in renal transplant recipients was presented [9]. Recently, a
patient-specific computational model has been developed and validated in a
clinical trial to predict pre-operatively the blood flow volume (BFV) in AVF
for different surgical configurations on the basis of demographic, clinical, and
doppler ultrasound data [10]. Although the authors have presented a model
to measure the BFV pre-operatively for different configurations, they have
not discussed the sequence of sites and the consequence of AVF placement
at a particular site.
Quality-adjusted survival is widely used to compare different disease treat-
ments [11, 12, 13, 14]. A widely used method is Quality-adjusted time with-
out disease symptoms and toxicity (Q-TWiST), where TWisT health state
will have a higher utility than other health states.
Various studies have been done to understand AVF patency, but none
talks about the difference in patency rates between the two arms and the
optimal sequence of creation of multiple AVFs. In this study, we attempt
a decision plan for the creation of multiple AVFs in a sequential manner.
The policy depends on the patency estimates for any AVF and the patient’s
quality of life. In addition to the optimal policy for the first AVF and the
optimal sequence for multiple AVFs, a contribution to this is a rigorous defi-
nition of a patient’s quality of life impacted by the AVF creation. Comparing
a patient’s quality of life becomes very important in decision-making when
the patency of two treatments is almost the same. 2. Problem Description Hemodialysis patients require stable vascular access for prolonged dial-
ysis. The various possible sites and their consequences have already been 3 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint discussed in the introduction section 1.
For an AVF placement, there are four potential locations, viz. Dominant
arm Distal, Dominant arm Proximal, Non-dominant arm Distal, and Non-
Dominant arm Proximal.
Part one of this study proposes a threshold policy to choose the dominant
or non-dominant arm for the first AVF, keeping in mind the patient’s quality
of life and survival of the fistula. Choice
of Arm Dominant (D) Non-Dominant (ND) Distal
Proximal
Distal
Proximal First Fistula Second and Subsequent Fistula Figure 1: Decision flow for arteriovenous fistula placement. The arrows show the possible
flow of decisions for AVF placement. The second part of the study extends the analysis to the sequences adopted when multiple AVFs were created, as hemodialysis patients usually need
more than one AVF during their lifetime on dialysis. The possible clinical
sequences are as follows and also depicted by the arrows in Figure 1: 1. D Distal →D Proximal →ND Distal →ND Proximal
2. D Distal →ND Distal →D Proximal →ND Proximal
3. ND Distal →ND Proximal →D Distal →D Proximal
4. ND Distal →D Distal →ND Proximal →D Proximal 4 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint A Distal AVF creation is not possible if a proximal AVF has been constructed
in the first instance on either of the arms. Currently, the clinical choice and
practice are to choose the ND arm distal location as the first choice for AVF.
However, it needs data-driven validation, which is in this paper. The risk
of (i) an accidental injury and (ii) inconvenience are both perceived to be
higher for D Distal over ND distal. The patency for any AVF is improved by
physical activity, which in turn enhances the blood circulation to that AVF.
As the dominant hand is involved in everyday physical activities, patients do
not require to undertake an additional physical effort to improve this blood
circulation, as compared to the non-dominant hand.
The choice of placement of the second fistula is less clearly defined in
current practice and is an open question. Therefore, we compare the four
sequences implied by Fig. 1 for cases where multiple AVFs are used.
Also, after AVF surgery, patients may have challenges in performing their
daily activities with that arm. Therefore, the AVF should be constructed
where it would have a higher chance of survival than other potential sites and
minimally impact the patient’s physical ability to perform daily activities.
One of the research problems we tackle in this article is how to measure
the effect of AVFs on a patient’s everyday physical activities. We define the
quality of life of a patient with AVF on either arm as the weighted quantity
of the effects of AVF on various activities.
Therefore, it is of interest to
hypothesize that the average quality of life of a patient is different in the
different arms. 3. Methodology This study has included various methodologies in computing quality-
adjusted survival for either arm.
We divide our computations into three
parts; quality of life when fistula was constructed on D and ND arm, patency
estimate of D and ND arm fistula, and quality-adjusted survivals. Each of
these parts is elaborated on in the following subsections. 3.1. Computation of quality of life
AVF affects a patient’s quality of life because he or she is limited in
some daily activities. We assume that the extent to which this occurs (as a
factor) remains constant throughout the life of the fistula. The quality of life
of a patient undergoing hemodialysis depends on many factors, such as the
patient’s and AVF’s health. Here, we attempt to quantify the factor by which 5 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint the quality of life of a patient is affected based on the site and location of
the AVF (whether it is on the D or ND hand). It is also important to record
how much difficulty a patient has in performing each activity. Moreover,
each activity has a different importance in our daily life.
Therefore, we
propose that quality of life is a composite quantity consisting of physical,
symptomatic, and psychosocial factors. Each of these factors consists of 5-
6 sub-factors, e.g., physical activities include holding/gripping, stretching,
rotation, pushing, constriction/pressure, and complex activities, as shown in
Table 3. The quality of life is defined as the weighted sum of the impact of
AVF on each activity. The weights reflect the relative importance of each
activity in our everyday life.
We define the quality of life, Q, as: Q = n
X i=1
wiqi
(1) Q is measured on a scale from 0 to 1, where 1 indicates the highest possible
quality for the patient. wi and qi are the relative importance and quality
gain of activity i, n is total activities considered in the study.
We hypothesized the following to shed light on the effects of AVF on each
activity: H0 : qD
i = qND
i
∀i
(2) H1 : qD
i ̸= qND
i
for some i
(3) where, qND/D
i
denotes the impact of ND/D AVF on ith activity.
For the quality of life with respect to ND/D hand, we designed our hy-
pothesis as follows: H0 : QD = QND
(4) H1 : QD ̸= QND
(5) 3.2. Survival Function
The survival function is that a subject of interest will survive past a cer-
tain time. In reliability studies, survival is a widely used term to estimate
lifespan, whereas, in nephrology, the term patency is often used [15, 16]. 6 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint Therefore, AVF patency is synonymous with survival in this article. The sur-
vival function can be estimated using non-parametric and parametric meth-
ods. The Kaplan-Meier survival estimate is a non-parametric method that
estimates the survival probabilities using the observed survival times [17].
Non-parametric methods have advantages over the parametric method, but
it is difficult to extrapolate the survival analysis beyond the study period.
Parametric methods make assumptions about the data distribution, and
frequently used standard distributions are exponential, Weibull, log-normal,
and log-logistic.
The best parametric distribution for survival estimation
is obtained based on the goodness of fit, which can be accessed using the
Akaike Information Criterion (AIC). In this study, we have used all these
four parametric distributions and selected the best survival function with
the lowest AIC score.
The different statistics, such as the log-rank test, Wilcoxon, Tarone-Ware,
and Peto-Peto, were used to compare two survival functions [18]. The median
survival statistics have been presented here for all the survival functions
computed in this study. 3.3. Quality-adjusted Patency
Quality-adjusted survival (SQ) of a subject is defined as the measure of
the effectiveness of an intervention with a trade-off between quality of life and
patency [13, 14]. Quality-adjusted survival in this article refers to quality-
adjusted AVF patency rates.
In our study, we have one state that is a patient under dialysis with AVF
vascular access where AVF could be on either hand. Therefore, we define
it as follows: Let Si(t) denote the survival function of an AVF constructed
on i = {D, ND}, represents dominant (D) and non-dominant (ND) hand, T
denote the time horizon for which we are planning the fistula placement, and
Qi denotes utility of fistula on the i arm. SD
Q(T) =
Z T 0
QDSD(t)dt = QDRMST D(T)
(6) SND
Q (T) =
Z T 0
QNDSND(t)dt = QNDRMST ND(T)
(7) where RMST(T) refers to restricted mean survival till time T. It esti-
mates the time spent in that health state. Policy 1. The decision policy for the planning horizon, T is: 7 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint 1. Dominant arm should be preferred if SD
Q(T) ≥SND
Q (T).
2. Non-Dominant arm should be preferred if SD
Q(T) ≤SND
Q (T). 4. Data Description For this study, the AVF survival data1 were collected from five dialysis
centers. The patient’s details are the arm used for the fistula and the time of
failure of the fistula. An AV fistula can have two types of failures: primary
and secondary. Primary failure is defined as the failure of the fistula within
three months of its construction. The secondary failure is defined as the
failure of AVF that survives beyond three months. Total AVF
n = 314 1st AVF
n = 280 2nd AVF
n = 56 3rd AVF
n = 8 Primary Failure
n = 43 (15%) Secondary Failure
n = 51 (18%) Figure 2: Data Description. Figure 2 shows the details of the data used in this analysis. Out of 43
primary AVF failures, 12 (28%) AVFs were on the D hand, and the rest were
on the ND hand.
The radio-cephalic (distal) fistulas had higher primary
failure than brachiocephalic (proximal) fistulas for both hands.
We conducted two different surveys for quality-of-life computation. One
included questions related to the relative importance of various activities in
our daily life, and the other one is related to the impact of AVF on everyday
activities. We designed our questionnaire to quantify the impact of AVF 1The Institutional Ethics committee approved this study. 8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint based on the previous research [19]. The questionnaire listed various physical
activities of daily living like rotation, pushing, and complex activities with
more than one type of motion and the degree of difficulty in performing them.
It also included clinical symptoms like pain, weakness, stiffness, and anxiety. 5. Quality Computation Results The survey for the impact of AVF on daily activities was conducted
and responses were received from 127 patients (25 had AVF on dominant
and 102 had it on non-dominant).
The multivariate analysis of variance
(MANOVA) test was performed to see the difference between the dominant
and non-dominant arm groups. We found a statistically significant differ-
ence (p ≈0.00) between the two groups, implying that our null hypothesis
(2) had to be rejected. Although the quality gain of each activity is higher
for the non-dominant arm group as compared to the dominant arm group,
the Bonferroni correction method showed that none of the activities were
statistically significant in a univariate analysis at a 5% significance level as
shown in Table 3. Though we do not have statistical evidence of AVF’s con-
sequences for each activity, it has statistical impact on the patient’s daily life
for hands.
This type of survey can not be administered to each patient who needs
dialysis. It is a one-time collection of the impact of different factors and the
consequence of constructing a fistula on either hand. The aggregate response
is used as a general guideline for fistula constructions. Any patient-specific
information would then be used over and above this.
The importance of one activity to a person may be higher (equal or lower)
than that of another. The relative importance of specific activities for a given
person may vary, which confuses a nephrologist’s decision-making process.
Our approach consolidates individual preferences in the order of activities to
derive relative importance.
All 18 activities were divided into physical, symptomatic, and psychoso-
cial factors. We conducted a survey for the relative importance of each of
the activities under physical factors. The respondents were requested to rate
these activities per their personal preference (scale of 1 to 9). We interviewed
the nephrologist for the activities under symptomatic and psychosocial fac-
tors and asked them to rate these on a scale of one to nine. On a rating scale,
one is “not important”, and nine is “very important.” We have also asked
nephrologists to rate these three categories among themselves on a scale of 9 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint Physical
Weights Holding/Gripping
0.13 Stretching
0.16 Rotation
0.15 Pushing
0.17 Constriction/pressure
0.18 Complex
0.19 Symptomatic
Constant Pain in the arm at rest
0.18 Pain at movement
0.11 Weakness in the arm
0.16 Stiffness in the arm
0.11 Difficulty in sleeping due to pain
0.18 Puncture site itchiness
0.3 Listlessness in the shoulder after dialysis
0.11 Psychosocial factors
Constant Feel less capable, confident
0.20 Fear of AVF shutdown
0.10 Fear of injury to AVF hand
0.11 Fear of needle insertion pain
0.10 Concern about cosmetic appearance
0.30 Table 1: Relative weights of factors
within the category Factors
Weights Physical Activities
0.3 Symptoms
0.5 Psychosocial
0.2 Table 2: Category weights one to three, where an ascending order of these numbers refers to increasing
importance. We randomly generated 1000 samples for each category using a
uniform distribution with the means taken from Table 2.
In the relative importance survey, we received 37 responses. The mean
importance of each activity was divided by the sum of the mean importance
of all activities of a category to compute their weights within that category.
The relative importance of each activity (wi) was computed after multiplying
the weights of each activity with its category weights.
We have obtained the reduction in “quality of life” by aggregating the
product of relative importance and impact of AVF shown in Table 3 for each
activity. Our data analysis showed that dominant arm AVF reduces the QOL
(1 −Q) of hemodialysis patients by 0.30 (±0.1), whereas the non-dominant
arm AVF reduces it by 0.27 (±0.09). We tested the hypothesis (4) using the
t-test that the mean reduction of quality of life when AVF constructed on the
dominant and non-dominant hand is the same. T-test resulted in p < 0.0001,
which means that the mean reduction of quality of life for dominant hand 10 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint Activities
D
Mean
D
Std
ND
Mean
ND
Std
p-value
p-adjusted Physical Holding/Gripping
0.26
0.12
0.25
0.12
0.993
1 Stretching
0.3
0.15
0.27
0.14
0.3
1 Rotation
0.34
0.18
0.27
0.14
0.066
1 Pushing
0.3
0.16
0.26
0.13
0.329
1 Constriction/pressure
0.31
0.16
0.29
0.14
0.509
1 Complex
0.39
0.17
0.3
0.16
0.005
0.1 Symptomatic Pain in the arm at rest
0.22
0.07
0.23
0.09
0.827
1 Pain in the arm at movement
0.25
0.1
0.24
0.1
0.645
1 Weakness in the arm
0.28
0.13
0.28
0.14
0.696
1 Stiffness in the arm
0.27
0.13
0.26
0.14
0.524
1 Difficulty in sleeping due to pain
0.27
0.13
0.23
0.08
0.093
1 Puncture site itchiness
0.27
0.11
0.29
0.17
0.967
1 Listlessness in the shoulder after dialysis
0.32
0.16
0.28
0.14
0.306
1 Psychosocial
factors Feel less capable, confident
0.34
0.15
0.27
0.13
0.009
0.1 Fear of AVF shutdown
0.3
0.24
0.25
0.12
0.641
1 Fear of injury to AVF hand
0.3
0.15
0.25
0.11
0.028
0.4 Fear of needle insertion pain
0.32
0.12
0.36
0.13
0.187
1 Concern about cosmetic appearance
0.25
0.13
0.26
0.17
0.782
1 Table 3: Impact of AVF on each factor. fistulas is higher than for the non-dominant hand. 6. Observations & Analysis The dataset was obtained from multiple centers, and four independent
distributions were fitted to estimate AVF survival. The event of interest was
considered a ‘secondary failure,’ and censoring data included the AVFs that
were either still working, patients who were lost to follow-up, and patients
who died with functional AVFs. Secondary failure corresponds to long-term
failure and is, therefore, appropriate for our decision policy. In our dataset,
a total of 51 AVFs had a secondary failure. This event occurred in 12 cases
for the dominant arm AVF, and 39 events occurred in the non-dominant arm
AVF. The parametric survival functions, listed in section 3.2, were estimated
on the above dataset.
The goodness of fit shows that all parametric models have equivalent
performance in the data considered in this study. The optimal parameter for
exponential for the dominant arm is λ = 98.37, whereas for the non-dominant
arm is λ = 148.63. Fig. (3a) shows the survival plot of the exponential
survival function.
The quality-adjusted survival has two terms corresponding to the qual-
ity of life and the AVF patency. An AVF patency for an individual patient
can be estimated by incorporating the patient’s characteristics. It helped us
to propose a patient-specific treatment plan, as in this study, the quality of
life is assumed to be independent of the patient. We attempted the multi-
variate survival analysis of AVFs on the dominant and non-dominant arms
by including characteristics such as arm used, patient’s age at the time of 11 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint (a) Exponential survival function of D and ND
hand.
(b) Cox-Proportional hazard model. Figure 3: Univariate and Multivariate survival of dominant and non-dominant hand. fistula construction, diabetes mellitus status, cardiovascular problems, and
smoking/tobacco use status. Fig. (3b) shows the log hazard of all these
variables, and none of them were statistically significant at a 5% significance
level. This means that we do not have statistical evidence of whether the
secondary failure of AVF depends on a patient’s characteristics.
The median patency of the dominant arm AVF was 72.4 months (CI2:
34.3 - ∞), and for the non-dominant arm AVF, it was 91.3 months (CI:
73.2 - 179.67). This was computed using the Kaplan-Meier estimate for the
first AVF. All the statistical tests discussed in the section 3.2 show that the
dominant arm AVF patency is not statistically different from that of the
non-dominant arm AVF patency (p > 0.1).
Fig. (4a) shows the quality-adjusted survival of either hand for a period
of five years. It is evident from Fig. (4a) that this quality-adjusted survival
is always higher for the non-dominant hand. Access Location
Failures (in %)
Median Survival (in months)
Left sided 95% CI D Distal
5/18 (27.8)
72.4
47.6
ND Distal
23/100 (23)
91.3
73.2
D Proximal
7/22 (38.9)
48.43
29.07
ND Proximal
16/85 (18.8)
84
48.2 Table 4: Median patency of AVF based on different locations. 2CI: Confidence Interval 12 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint (a) Quality-adjusted survival plots.
(b) Patency plots. Figure 4: Quality adjusted survivals for D and ND hand for first AVF and Kaplan Meier
estimates of different AVF based on locations. We showed that the patient’s QOL is affected less when the AVF fistula is
constructed on the ND arm and also scores over the dominant arm in terms of
AVF patency. This, therefore, indicates that the first AVF should be created
on the non-dominant arm to achieve better quality-adjusted AVF survival
(p < 0.05). Hence, the first and second sequences (discussed in the section 2)
can not be optimal for multiple AVF planning. We have tested the hypothesis
that the second AVF on the ND arm proximal location has the same hazard
ratio as the D distal arm fistula since the hand and the first AVF location
are fixed based on the above discussion. We used the Tarone-Ware test to
test our hypothesis because both the survival functions were intersecting.
We found the p-value between D distal and ND proximal to be 0.58. This
indicates that D distal and ND proximal have similar survivals. Figure (4b)
shows the AVF survival at different locations. Although patencies of D distal
and ND proximal AVFs are not statistically significant, the median patency
is higher for ND Proximal as compared to D Distal, as depicted in Table (4).
Based on the QOL and patency rates, the third sequence (3) of multiple AVF
should be preferred over the fourth sequence (4). 7. Conclusion Given the fact that patients on dialysis currently survive for a very long
time, and that their vascular access has a relatively shorter life span, and
that there are only a limited number of location options to insert a vascu-
lar access in a given patient, this paper addresses the issue of the optimum
sequence of locations where such a vascular access can be inserted in these 13 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288219
doi: 
medRxiv preprint hemodialysis patients. Studies on the quality adjusted vascular access sur-
vival favours the distal followed by proximal location on the non-dominant
arm, over the dominant arm. Our data showed that vascular access survival
statistics were comparable between the two arms and the two locations on
each arm. However, quality of life indicators were significantly better when
a vascular access was inserted on the non-dominant arm. Our quality assess-
ment is comparable to the detailed study done in [19], but adapted to the
patient base in India. It can be revised in other environments as needed. Statements and Declarations Competing Interests None declared Funding None Ethical approval: The institute ethics committee has approved the study.",1
"Stroke is the second leading cause of death worldwide. Nearly two-thirds of strokes are
produced by cardioembolisms, and half of cardioembolic strokes are triggered by Atrial Fibril-
lation (AF), the most common type of arrhythmia. A more recent cause of cardioembolisms is
Transcatheter Aortic Valve Replacements (TAVRs), which may onset post-procedural adverse
events such as stroke and Silent Brain Infarcts (SBIs), for which no definitive treatment exists,
and which will only get worse as TAVRs are implanted in younger and lower risk patients.
It is well known that some specific characteristics of elderly patients may lower the safety
and efficacy of anticoagulation therapy, making it a real urgency to find alternative therapies.
The device introduced in this paper offers an anticoagulant-free method to prevent stroke and
SBIs, imperative given the growing population of AF and elderly patients. This work ana-
lyzes a design based on a patented medical device, intended to block cardioembolisms from
entering the cerebrovascular system, with a particular focus on AF, and potentially TAVR
patients. The study has been carried out in two stages. Both of them use computational fluid
dynamics (CFD) coupled with Lagrangian particle tracking to analyse the efficacy of a novel
patented medical device intended to block cardioembolisms from entering the cerebrovascular
system, with a particular focus on AF, and potentially TAVR patients. The studied device
consists of a strut structure deployed at the base of the treated artery. Particles of different
sizes are used to model dislodged debris, which could potentially lead to cerebral embolisms
if transported into these arteries.
The first stage of the work evaluates a variety of strut thicknesses and inter-strut spacings,
contrasting with the device-free baseline geometry. The analysis is carried out by imposing
flowrate waveforms characteristic of both healthy and AF patients. Boundary conditions are
calibrated to reproduce physiological flowrates and pressures in a patient’s aortic arch. Re-
sults from numerical simulations indicate that the device blocks particles of sizes larger than
the inter-strut spacing. It was found that lateral strut space had the highest impact on effi-
cacy.
In the second stage, the optimal geometric design from the first stage was employed, with the
addition of lateral struts to prevent the filtration of particles and electronegatively charged
strut surfaces, studying the effect of electrical forces on the clots if they are considered charged.
Flowrate boundary conditions were used to emulate both healthy and AF conditions. When
deploying the electronegatively charged device in all three aortic arch arteries, the number of 1 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. particles entering these arteries was reduced on average by 62.6% and 51.2%, for the healthy
and diseased models respectively, matching or surpassing current oral anticoagulant efficacy.
The device demonstrated a two-fold mechanism for filtering emboli: while the smallest par-
ticles are deflected by electrostatic repulsion, avoiding microembolisms, which could lead to
cognitive impairment, the largest ones are mechanically filtered since they cannot fit in be-
tween the struts, effectively blocking the full range of particle sizes analyzed in this study. 1
Introduction Stroke is the second leading cause of death worldwide [1]. Nearly two-thirds of strokes are produced
by cardioembolisms [2]. Cardioembolic strokes are caused by blood clots that form in the heart,
due to disease (e.g., atrial fibrillation) or a cardiac intervention (e.g., transcatheter aortic valve
replacements or left atrial appendage occluders), and travel through the bloodstream into the
brain. Half of the cardioembolic strokes are caused by Atrial Fibrillation (AF), a heart condition
that causes an irregular and often abnormally fast heart rate with a significant reduction in the
cardiac output. It is proven that this dysfunction increases the propensity of emboli, which tend to
travel through the Brachiocephalic Trunk (BCT), Left Carotid Common Artery (LCCA), and Left
Subclavian Artery (LSA), to the brain, obstructing the superior arteries and triggering cerebral
strokes [3]. 1.1
Why is atrial fibrillation a critical problem? AF is the most common heart rhythm disorder, responsible for approximately one-third of hos-
pitalizations for cardiac rhythm disturbances in the US. The prevalence and incidence of AF are
increasing. It is predicted to affect 6-12 million people in the US by 2050 and 17.9 million in
Europe by 2060, significantly impacting wellbeing and healthcare costs. AF is associated with
increased morbidity and mortality, due to the risk of ischemic stroke, systemic embolism, heart
failure, and cognitive impairment, reducing the quality and quantity of life in these patients [4].
This condition is associated with a six-fold increase in stroke. Moreover, patients with previous
ischemic stroke are at an even higher risk [5]. In the case of cardioembolic strokes (two-thirds of
the total), echocardiographic and pathologic studies suggest that when a source can be identified,
approximately 90% of such strokes can be attributed to thrombus formation in the left atrial
appendage [2]. 1.2
What are the existing treatments for stroke and what are their
drawbacks? The primary approach to prevent ischaemic stroke risk associated with AF, are anticoagulation
therapies, which have a proven efficacy. At least four large clinical trials have clearly demon-
strated that anticoagulation with warfarin decreases the risk of stroke by 50-80% [6, 7, 8, 9]. In
relatively recent trials, the newer oral anticoagulants (OACs), such as dabigatran, rivaroxaban,
apixaban, and edoxaban, have proven to be similarly effective to warfarin for the prevention of
stroke and thromboembolism [10]. However, anticoagulants have significant drawbacks. Although
anticoagulants reduce 30-day mortality from ischaemic stroke, these agents increase intracranial
haemorrhage-related mortality [11]. Moreover, patients having a stroke despite being on therapy
with an oral anticoagulant are at high risk of recurrent ischaemic strokes [12]. In the ANAFIE
registry, patients at high bleeding risk had higher incidences of stroke, major bleeding, intracranial
haemorrhage, gastrointestinal bleeding, cardiovascular events, and all-cause death than patients
in the reference group, despite a high prevalence of OAC therapy (89.0%) [13]. In elderly patients,
non-adherence to OAC treatment, associated commorbidities and additional risk factors can sig-
nificantly increase the incidence and severity of cerebrovascular accidents. In this age group, a
delicate balance may exist between multiple conditions, being thrombotic disease, chronic kidney 2 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint disease, cancer, coronary artery disease, and heart failure, some of the most challenging scenarios
encountered in clinical practice [13]. In addition, OACs have multiple contraindications [14]: • prior intracranial haemorrhage or diseases predisposing to intracranial haemorrhage, • active gastrointestinal bleeding or diseases predisposing to gastrointestinal bleeding (such as
active ulcer), or inflammation of the gastrointestinal tract, • anaemia, defined as Hgb level below 8 mg dl, • thrombocytopenia, defined as PLT < 50, 000 platelets per µL, • end-stage liver disease, and • allergy Decisions to start effective AF-related stroke thromboprophylaxis following an acute ischaemic
stroke or intracerebral haemorrhage are rarely clear-cut: patients have reluctance and own prej-
udices, and relative contraindications are influenced by their individual clinicians’ perceptions of
risks and benefits [5]. It is therefore necessary to rigorously evaluate the current status of oral
anticoagulation agent use for AF-related stroke prevention. Considering the difficulties involved in
anticoagulant treatments, it is only reasonable to devise CEPDs capable of matching or surpassing
current OAC efficacy. 1.3
What is the relation between TAVR and stroke? TAVR has emerged as an alternative, rapidly evolving non-invasive procedure for patients with se-
vere aortic stenosis and medium-to-high surgical risk. By 2025, there will be an estimated 280,000
TAVR procedures performed worldwide and the total market will exceed $8 billion. Although this
highly promising treatment modality results in less morbidity, shorter time to recovery and similar
mortality rates, it is still associated with one of the most devastating and feared complications:
cerebral embolism, which in turn may cause stroke [15]. Stroke is associated with a 6-fold increase
in mortality in TAVR cohorts, a moderate to severe permanent disability in up to 40% of sur-
vivors [5], a 4.7-fold increased risk of permanent work disability [2], social isolation and significant
financial strain in 80% of stroke survivors [11], and an increased risk of readmission in patients
with stroke after cardiac catheterization [16].
The time in between the TAVR procedure and the cardioembolic event is an important factor
when choosing stroke prevention treatments. Most of them occur in the acute phase following
TAVR where cerebral embolic events are frequent [17]. Nonetheless, according to the ADVANCE
trial, half of the reported strokes occurred between day 2 and 30 after the TAVR procedure [18].
Moreover, evidence is mounting on ischaemic brain lesions being produced after day 30, causing
SBIs and long-term neurological symptoms. New ischaemic brain lesions were found in 74% to
100% of patients on diffusion-weighted magnetic resonance imaging (DW-MRI) after TAVR [19].
Although studies have shown that SBI may not be related to apparent short-term neurological
symptoms, evidence points to an association with accelerated cognitive decline and strengthening
of the risk of long-term dementia (most commonly, Alzheimer’s disease) [20]. Later events are
associated with patient specific factors [21]. SBIs have been associated with accelerated cognitive
decline and higher risk of long-term dementia. The situation is especially worrying given that
TAVRs are being implanted in increasingly younger and lower risk patients, hence potentially
increasing the prevalence of dementia. 1.4
What is the state of the art of cerebral protection devices? As mentioned, in patients undergoing TAVR, stroke remains a potentially devastating complica-
tion associated with significant morbidity and mortality. This is especially worrying, given that
TAVR is now indicated in aortic stenosis low- and intermediate-risk patients [22], and that stroke 3 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint rate 30 days post-TAVR has been reported at 3.4% in low risk-patients [23]. To prevent debris
from embolizing to the brain during the procedure and reduce the risk of stroke, cerebroembolic
protection devices (CEPDs) were developed [24]. These devices are implanted during the pro-
cedure and up to 2 days after, since the clogging of CEPDs impedes them from being used to
prevent ischaemic strokes > 2 days post-TAVR. Nonetheless, as explained above, risk of stroke
may not be limited to the procedure itself or the perioperative period. Moreover, the clinical
benefit of current CEPDs in reducing strokes, transient ischaemic attacks or death remains un-
known [25, 26, 27]. It is therefore worrying that for strokes triggered by AF or other conditions
extended in time, there are no currently approved CEPD in the market. The SENTINEL-LIR
study demonstrated that embolic debris captured by the SENTINEL-CPS (Cerebral Protection
System) during TAVR in low- to intermediate-risk patients was similar to that in previous stud-
ies conducted among higher-risk patients. These findings suggest lower-risk patients undergoing
TAVR have potentially a similar embolic risk as high-risk patients, as evidenced by embolic debris
capture [28]. Most captured debris had a size of < 500 µm, with 78% between 150 and 500 µm.
Larger size particles (≥1000 µm), which can cause significant vessel obstruction, were present in
67% of cases. Therefore, the Sentinel CEPD can functionally capture large debris that may cause
a severe stroke. In contrast, debris on the micrometer scale may pass through the gaps between
the filters and arteries, leading to stroke even in CEPD-protected territories (with less likelihood of
severe symptoms) [29]. Dedicated meta-analyses demonstrated that SBIs of small volume 3 mm3 are independent predictors of later stroke and mortality [16], further highlighting the need for a
CEPD able to filter small debris in the long term post-TAVR. Such a device would be a radical
improvement for a large population at risk. 1.5
How can this paper help improve the current situation? The main purpose of the work presented in this paper can be described with the simple motto:
Save the brain. The proposed solution consists of a stent attached to an electronegatively charged
strut structure, intended to block the passage of clots or deflect their trajectories thanks to both
fluid- and electro-dynamical forces on the clots. This can be achieved thanks to the electrostatic
repulsion acting on naturally electronegatively charged blood clots, and its geometrical design,
capable of filtering thrombi based on the distance between struts. The main objective of this
device is to reduce or eliminate the percentage of blood clots entering the BCT, LCCA, and
LSA, while maintaining at least 80% of the natural delivery of oxygenated blood to the brain.
The Section 4.3 will demonstrate that the presence of the devices in all three arteries reduces
the flowrate in a maximum of 7.5% in the case of BCT, and in less than 2% in the other two
aortic arch arteries. This paper provides the details of the work carried out using Computational
Fluid Dynamics (CFD) coupled with Lagrangian particle transport to validate the efficacy of the
deflecting medical device positioned at the base of the BCT, LCCA, and LSA. A sufficiently large
number of particles were injected into the domain in order to assure significant statistical samples
for validating the particle-deflecting capabilities of the device. The study is divided in two main
stages. The first one analyzes the effect of the thickness and shape of the strut design on the
device performance. In the first stage, the purely hydrodynamic effect of the device is analyzed
using a CFD and a particle transport model. The device is placed at the root of the LCCA and
the optimal strut thickness is identified by analyzing the trajectories of particles suspended in the
flow. The analysis showed a low efficacy for the deflection of thrombi and identified a deficiency
in the initial design which is was letting particles pass through the lateral struts. To overcome
this deficiency, extra struts are added in the second design employed in the second stage of the
work, oriented perpendicularly to the original struts. The second stage of this project, proposes
to simulate struts that are electrically charged on their surface. Considering that blood clots are
negatively charged, the strut surface would be negatively charged too in order to repel the clots.
To achieve the negative charge of the struts, a proposed solution is to coat the Nitinol struts
with a graphene oxide coating, with appropriate electrostatic properties. Due to the latter, it was
possible to deflect even the smallest particles from entering the aortic arch arteries, offering an
anticoagulant-free method to prevent SBIs. 4 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint (a) Macroscopic view.
(b) Lateral view. Figure 1: Aortic arch geometry. In the original geometry, the main vessels were segmented manually and recreated
based on a CT scan of a cadaver. (a) Left ventricle, aortic valve and coronary arteries have been replaced in the inlet
by a rigid tube, and superior mesenteric, iliac and renal arteries have been dismissed at the outlet. (b) Extrusion
extensions have been applied at the outlet of the vessels to stabilise the simulated flux. 2
Methods 2.1
Aortic arch geometry The aortic arch geometry used for computational simulations was designed based on an anatom-
ical model from a previous study [30]. In order to adapt the geometry to the requirements of
the present study, some modifications were made using ANSA v22.1.0 (Beta Simulation Solutions,
Lucerne, Switzerland). The applied modifications preserve the original structure of the aortic arch
geometry, only in the LCCA and BCT significant changes (i.e., go from an anatomical structure
to a synthetic one) have been applied. It was observed that the original geometry was not repre-
sentative of an average BCT geometry, so it was manually corrected to adapt it to a more realistic
one. Likewise, it was noticed that the LCCA presented a broader morphology at the entrance of
the vessel, which could imply obtaining wrong conclusions, thus, it had to be corrected. Finally,
let us comment that the aortic root was truncated to focus on the vessels in the aortic arch. For
the inlet, the area corresponding to the left ventricle outflow tract (LVOT), with the coronary
arteries and the aortic valve was replaced by a rigid tube to ensure the correct stabilisation of the
fluid. At the outlet, the area corresponding to superior mesenteric, illiac, and renal arteries was
dismissed. The resulting geometry used in this study can be observed in Figure 1. For the second stage of the present work, the device is implanted at the base of the three
arteries: BCT, LCCA, and LSA. Figure 2 shows the patient-specific geometry of the aortic arch
employed, with the arteries where the device will be placed. The inlet and outlet boundaries are
labelled as: • Inlet: Aortic root • Outlets: – Brachiocephalic Trunk (BCT), – Left Carotid Common Artery (LCCA), – Left Subclavian Artery (LSA), and – Descending Aorta (DAO). 2.2
Device design First stage
The emboli released from the aortic root due to TAVR-related debris tend to travel
through the BCT, LCCA and LSA, obstructing the superior arteries and producing cerebral 5 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint Figure 2: Geometry of the aortic arch and zoom-in of the deflectors deployed at all three arteries. Figure 3: Deflector device (blue) implanted at the base of the LCCA. strokes, with the aforementioned associated consequences. Therefore, it is crucial that the in-
troduced CEPD can be implanted in all three arteries of the aortic arch. The proposed solution,
detailed in the patent no. US9517148 [31], fulfils exactly this requirement. It consists of a stent
attached to a strut structure to block or deflect clot trajectories, as can be seen in Figure 4. This
can be achieved thanks to its design, capable of filtering thrombi based on the distance between
struts and eventually to modify trajectories of the smallest particles, responsible for SBI.
It is worth noting that the patent provides some flexibility in design parameters such as the
strut orientation, convexity, profile, and quantity. The main application of this device is to reduce
or eliminate the percentage of embolisms entering the BCT, LCCA and LSA in the mid- to long-
term in AF and TAVR patients, while maintaining the delivery of oxygenated blood to the brain.
The introduced device has the objective of reducing the number of particles travelling to the brain
through the branches of the aortic arch. To do so, in the first part of the study, it is implanted at
the base of LCCA. Figure 3 shows the deflector device positioned at the LCCA.
The curved struts of the device are aligned with the aortic blood flow to effectively deflect
particles from entering these vessels, without significantly reducing the fraction of ejected flow
into the vessel where it is placed. The deflector structure has been designed with FreeCAD 0.19
[32]. For this step, both the dimensions stated in the reference study [33] and in the device patent
have been considered. Five different deflector geometries have been generated considering two
geometric parameters: the distance between deflector struts or strut interval (Lsi) and the strut
thickness (Lst). Figure 5a shows the deflector design and the above mentioned parameters. The 6 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint (a) Stand-alone patented device (Fig. 2 in patent).
(b) Patented device implanted in LCCA and LSA (Fig. 3A
in patent). Figure 4: Diagrams from the patent of thrombus deflecting device, patent no. US9517148 [31]. (a) Schematic of a deflector implanted at the LCCA inlet,
Lsi and Lst denote the strut interval and thickness respec-
tively. (b) Bottom view of deflector geometries: (1) geometry 1,
(2) geometry 2, (3) geometry 3, (4) geometry 4, and (5)
geometry 5, corresponding to the dimensions stated in Table
1. Figure 5: Schematics of device from different views. resulting geometry dimensions are illustrated in Table 1. Figure 5b shows the aforementioned
deflector geometries created with FreeCAD. As shown in Figure 3, in this first stage of the study
the deflector device has been implanted at the LCCA. The choice to begin with this configuration
takes into account different studies [34, 35] which suggest that cerebrovascular diseases, such as
strokes, are significantly influenced by clots travelling through this artery.
In order to merge the deflector device geometry to an aortic arch geometry, ANSA v22.1.0
(Beta Simulation Solutions, Lucerne, Switzerland) has been used. The deflector orientation angle
with respect to the mean aortic flow was selected taking into account the results from [33]. In this
study, the centerline of the aortic arch has been taken as the direction for the deflector positioning. Lateral spacing
An important factor that has to be considered in the construction of the
geometry and the implantation of the device is the lateral free space between the first struts of
the deflector and the wall of the artery. As shown in Figure 6, the space between the lateral strut
and the device annulus at the base of the LCCA varies among geometries, corresponding to 2.17,
1.76, 2.76, 1.9 and 1.76mm for the Lsi = 0.5, 0.75, 1.0, 1.25, 1.5mm devices respectively. That 7 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint Deflector geometry
Lsi [mm]
Lst [mm] Geometry 1
0.50
0.50 Geometry 2
0.75
0.75 Geometry 3
1.00
1.00 Geometry 4
1.25
1.25 Geometry 5
1.50
1.50 Table 1: Dimensions for the design of the thrombus deflector for the different geometries generated. Figure 6: Deflector geometries of different strut thicknesses and inter-strut spacings. On top row from left to right:
no deflector, Lsi = 0.5 and 0.75mm bottom row: Lsi = 1.0, 1.25 and 1.5mm. Spacings between lateral struts and
annulus of base of LCCA are also displayed for each geometry. The Lsi = 0.75 mm is of particular interest, since
it is the most effective in blocking particles, with a lateral spacing of 1.76mm as highlighted in red. Figure 7: Particle with diameter larger than Lsi passing through the device. 8 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint Figure 8: New deflector design. Lateral struts added are circled in red. is, all geometries have a bigger lateral spacing than their corresponding Lsi. The smaller lateral
space of the Lsi = 0.75mm design with respect to designs with Lst ∈[0.5, 1.25] mm is consistent
with the highest effectiveness in blocking particles from entering the LCCA as will be shown in
Figure 15a. This is indicative of the critical impact of this geometric parameter in the design of
the medical device.
Moreover, it is observed that some particles with larger diameter than the Lsi of the device are
able to enter the treated LCCA due to the above mentioned issue. Figure 7 shows a case where
a large particle passes through the device and ends up in the LCCA. This extra lateral spacing
explains the number of particles with diameter larger than Lsi that get deposited in the LCCA
output as well as the fluid jet that is also shown through this space in Figure 13 (top-left).
To overcome this deficiency, extra struts has been added in the new design employed in the
second stage of the present work, oriented perpendicularly to the original struts. Second stage
For the second stage of the present work, lateral struts were added to prevent
particles from escaping through the side holes, the strut diameter was optimized and adapted to
each of the three arteries.
Figure 8 shows the new struts added to the device to improve the particle filtering. It can be
seen in this figure, that the strut thickness is not homogeneous, and instead thins out towards
the ends (circled with red pointed lines). Note that this is an artefact of CAD model generation
algorithm, in the creation of the 3D model, but is not a feature of the design or the patent involved.
The thinning is an artefact of CAD model generation algorithm and in future versions of the design
will be modified this morphology to maintain the section of the struts constant. Neither of the
previous studies of this device had considered an electrical charge. In the second stage, electric
charge is applied to additionally deflect small particles that could normally pass between the
struts.
The device would be covered with a negatively charged graphene oxide coating, with
surface charges of −24000.0 statC cm−2 , which corresponds to the most extreme electrical charges
found in the literature of biomedical devices, as will be described later in section 2.3.2. [36]. 2.3
Physical model 2.3.1
Blood flow model: computational fluid dynamics Incompressible flow
Blood was modeled as a Newtonian incompressible viscous fluid. The
Newtonian approximation is an acceptable assumption and not far from reality in a significant
part of the circulatory system under normal conditions. This is especially true in large blood
vessels where red blood cells are well below the characteristic sizes of the vessels [37]. The velocity
and pressure fields in the aortic arch, u and p respectively, have thus been obtained by solving 9 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint the incompressible Navier-Stokes momentum and continuity equations: ∂u + u · ∇u = −1 ρ∇p + ν∆u
(1) ∇· u = 0
(2) where ρ = 1.106
g cm3 and ν = 0.035
g cms are the blood density and kinematic viscosity respectively.
The corresponding boundary conditions are specified in Figure 2. Boundary conditions for the flow: healthy and atrial fibrillation conditions
In order
to set up the computational simulations, physical governing equations must be complemented by
boundary conditions for both the fluid and particle dynamics as well as the electric field. For the
fluid, the following boundary conditions are prescribed at the boundaries shown in Figure 2: • Inlet: velocity Dirichlet boundary condition. • Outlets: reduced-order (3-element Windkessel) model, imposed as a traction boundary condi-
tion, in which distal resistances and capacitances are modelled for a more accurate boundary
conditions. In particular, the values for the present work are a 0 Pa for imposed pressure in
the outlets, a resistive term equal to 100.0 and a scaling coefficient of 0.5. • Arterial walls and deflector: no-slip Dirichlet boundary conditions. It is noted that each one of the Windkessel boundary conditions were calibrated for the flow rates
and pressures to fit the experimental results from [38] for the healthy patient case. The flowrate
waveforms imposed at the inlet are representative of a healthy patient and an AF patient. In
Figure 9 these two inflows used in the first stage are shown. The second stage modifies the AF
inflow to adjust the physiology of a real patient in a more accurate way. Figure 9a the new inflow
for AF is compared with the healthy patient. The healthy patient has a heart rate of 70 bpm and
a cardiac output of 4.286 L min−1. In contrast, the AF patient has a higher heart rate, of 150
bpm, and a lower cardiac output, of 3.429 L min−1, a 20% decrease with respect to the healthy
patient, as reported in [39].These flowrates were used for the second phase addapted to this specific
geometry. 2.3.2
Thrombus model: particle transport Particle transport was simulated in a Lagrangian frame of reference, following each individual
particle. The main assumptions of the model were: • particles were sufficiently small and the suspension sufficiently diluted to neglect their effect
on blood-flow: i.e. one way coupling; • particles were spherical and did not interact with each other; • particle rotation was negligible; • thermophoretic forces were negligible; and • the forces considered were drag Fd , gravitational Fg, buoyancy Fb, and, for the second stage,
electric force Fe is also considered. Particles are injected assuming a circumferential distribution at a radial distance of 40% of
the aortic radius [40], as observed in Figure 10. Clot injections are conducted at three different
cardiac cycle moments assumed which correspond to a beginning of systolic (S), accelerating (A),
and peak stage of the cardiac cycle (P) as in [40], illustrated in Figure 9 .
In order to properly define the deflection/blocking capacity of the device, different particle
types, corresponding to different particle models defined by forces considered and the diameter
and density properties for each type. These different particles were used as shown in Table 2. 10 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint (a) Final flow rate waveforms imposed at the inlet for healthy (red) and
AF (blue) patients (b) Flowrate waveform showing particle injection in-
stants in a healthy patient. (c) Flowrate waveform showing particle injection in-
stants in a patient with AF. Figure 9: (a) Improved flowrate in the second stage of the work (above) and instants of particle injection, overlaid
with inflow curve (below) (b) from a healthy patient and (c) from a patient with AF. Points in time S, A and P
represent the beginning of systolic, accelerating, and peak stage of the cardiac cycle, respectively. In particular all 7 were used in the first stage and only the first 5 (from 1st to 5th of the Table
2) in the second one. Force models are applied to the particles with types 1 to 6, which have a
density of 1.08g cm−3 and different diameters, ranging from dp10 µm to 2mm. Particles of types
1 to 3 (dp ∈[10, 100]µm) are of importance for evaluating the efficacy of the device in avoiding
micro-embolisms and associated SBI. Larger particles of types 4 to 6 (dp ∈[0.5, 2]mm) represent
larger embolisms, which may produce cerebral strokes.
The trajectory of these particles can be obtained by using Newton’s Second Law, F = ma,
where the force F is the sum of the different forces exerted on the particle, m is the particle mass
and a is the particle acceleration. In this case the forces applied on the particles are: • Gravity: Fg = mpg, where mp and g corresponds to particle mass and gravitational accel-
eration. • Buoyancy: Fb = mpg ρf ρp , where ρf and ρp corresponds to the fluid and particle densities
respectively. • Drag: Fd the force acting in the opposite direction to the relative motion of an object
moving with respect to a surrounding fluid. The equation for the drag force assumed the
particle reached its terminal velocity and is given by: Fd = −8πµfdpCdRe(up −uf)
(3) 11 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint Figure 10: Zoomed view of the circumferential distribution (left) and vertical position at the inlet (right). being µ the dynamic flow viscosity, dp particle density and Re the particle Reynolds number,
calculated from its relative velocity with the fluid: Re = |up −uf|dp/ν with up and uf
corresponding to particle and fluid velocity. Cd is the drag coefficient given by Chen model
described in [41] • Electric force: Fe electrically charged deflector created an electric field which exerts a
repulsive force through the electrical charged blood clots and described mathematically in
the next section . Particle transport was modelled by considering the above described forces, and solving Newton’s
second law to obtain particle accelerations: ap = Fg + Fb + Fd + Fe mp
(4) where ap and mp correspond to particle acceleration and particle mass.
For the case of type 7 particles, their trajectory is obtained by computing the velocity integral,
meaning that these particles act like tracers, without considering any force acting on them and
they move following the flow. All particles injected in the geometry are considered spheres with
their diameters described in Table 2. Particle type
Diameter (dp)
dp ≥Lsi Type 1
10 µm
None Type 2
50 µm
None Type 3
100 µm
None Type 4
500 µm
Geometry 1 Type 5
1.00 mm
Geometries 1, 2 & 3 Type 6
2.00 mm
All Type 7
-
N/A Table 2: Particle types with their corresponding diameters (dp) and comparison with inter-strut spacing of deflector
geometries (Lsi) . It is important to remark that in vivo studies are more likely to show larger clot diameters[42]
than those specified in the table 2. However, those large clots were not considered in our analysis
as they will simply not fit between the struts. Moreover, it is important to emphasise that as
indicated in Table 2, for some deflector geometries, particles of types 4, 5 and 6 have a diameter
larger than or equal to the inter-strut spacing (geometries 1, 2 & 3) and thus cannot enter the
LCCA through this space. 12 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint Boundary conditions thrombus model
For the particles injected into the flow, the following
boundary conditions are prescribed: • inlet & outlets: absorbing boundary condition, which means particles passing through
such surfaces. • arterial walls: slip boundary condition, which allows tangential velocities and removes the
normal one and • deflector: elastic bounce boundary condition, which implies imposing normal velocity with
opposite sense removing the tangential momentum. Electrostatic field
The resulting electric field is obtained by solving the potential Poisson
equation: ∇· (ϵr∇V ) = Qd ϵ0
(5) where Qd is the total charge distributed superficially on the struts of the devices present in each
configuration, ϵ0 and ϵr are the electrical permittivity for the vacuum and relative permittivity of
blood. Finally, we use the relation between potential and electric field: E = −∇V . The electric
field generated by the superposition of the three devices deployed in the three arteries is illustrated
in Figure 11. (a) Side-view of streamlines of the electric field given by the
superposition of all three devices. (b) Top-view of streamlines of the electric field
given by the superposition of all three devices. Figure 11: Streamlines of electric field generated by superposition of the 3 electrical charged devices placed in the
3 arteries Boundary conditions electric potential
The boundary conditions of the Poisson equation
are −1.3mV in the intima or internal aortic walls and free in the inlet and outlet surfaces. 2.3.3
Electrical parameters of model Blood flow model parameters
Blood viscosity, or the thickness of blood, is generally consid-
ered to be normal when it falls within a range of 3.5 to 5.5 cP. However, it is important to note
that blood viscosity is not a constant value and can vary significantly depending on the specific
conditions in which it is being measured. For example, the viscosity of blood can change based on
the shear rate, or the speed at which the blood is flowing. At a shear rate of 0.1 s−1, the viscosity of
the same blood sample may be as high as 60 cP. However, at a shear rate of 200 s−1, the viscosity
would be much lower, at around 5 or 6 cP. This means that the viscosity of blood can differ in
different parts of the circulatory system, such as the large arteries, veins, and microcirculation, 13 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint Diameters [cm]
Charges [µF] 0.001
-11.0 0.005
-55.0 0.01
-111.0 0.05
-554.0 0.1
-1108.0 0.2
-2215.0 Table 3: Particle diameters and their respective estimated electrical charges. where shear rates can range from a few s−1 to over 1000 s−1. Blood viscosity is influenced by
several factors, including the concentration of red blood cells, the thickness of plasma, the ability
of red blood cells to deform under flow, and their tendency to clump together [43], and therefore
we considered blood visocisity value of 0.035 g cs Thrombus model parameters
The charge of the simulated blood clots was determined fol-
lowing [44], where the authors study electrophoretic mobility of electrically charged particles.
Electrophoresis involves the separation of charged clots under the influence of an electric field.
In fact, this mechanism is the result of the combination of two different processes, intrinsic elec-
trophoretic mobility and electroosmotic flow. The first one can be expressed as: µ =
q 6πνrs
(6) where µ is the electrophoretic mobility, q is the charge of the particle that we are interested in,
ν the viscosity of the medium, and rs the hydrodynamic radius. The electrophoretic mobility
was taken as µ = −0.0336 cm2 statV−1s−1 = −1.12 µm s V−1 cm−1[45]. The densities assigned
to blood clots were 1.08 g cm−3 [46]. By expressing the charge as a function of the remaining
parameters in (6), electrical charges were obtained for each diameter, and summarized in Table 3. Device model parameters
As described in Section 2.2, the surface charge on the graphene
oxide in 0.001MNaCl was determined approximately as −75 mC m−2 in good agreement with
the previous work [36] where the value was equal −80 mC m−2 , at pH 6.5 extracted from a
zeta potential measurement, albeit in NaCl.
In CGS units, these two values correspond to
−22, 484.43 statC cm−2 and −23, 983.39 statC cm−2. Based on these results, we have considered
a surface charge of the device corresponding to −24, 000 statC cm−2. This value needs to be inte-
grated for all the surfaces of the devices placed in the domain producing the total charge Qd in
(5), responsible for the electric field, which exerts the repulsive force on blood clots. 2.4
Quantities of interest The following quantities of interest are used for both particle statistics and flow dynamics: • The particle deflecting efficacy at the arterial outlets where the device is deployed, with and
without considering electric charge. This metric provides a means of computing the efficacy
of the device CEPD. • The maximum ejected flowrate in the treated artery. This value can be used to determine
the variation in flowrate ejected to the arteries with respect to the case without deflector.
It thus enables computing what will be the effect on oxygen delivery to the cerebrovascular
system. • Volume-averaged metrics have also been computed in order to evaluate the risk of thrombus
formation due to the presence of the medical device. The region of interest considered is 14 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23288032
doi: 
medRxiv preprint (a)
(b) Figure 12: Region of interest where volumetric quantities of interest are integrated. the volume encapsulated by the deflector as shown in Figure 12. The quantities of interest
evaluated are: – The average kinetic energy, associated to washout. – The average second invariant of the velocity gradient ∇u, that is, Q = 1 2
",1
"Background While neurological manifestations are core features of Fabry disease (FD), quantitative neuroimaging biomarkers allowing to measure brain involvement are lacking. We used deep learning and the brain-age paradigm to assess whether FD patients’ brains appear older than normal and to validate brain-predicted age difference (brain-PAD) as a possible disease severity biomarker. Methods MRI scans of FD patients and healthy controls (HC) from a single Institution were retrospectively studied. The Fabry stabilization index (FASTEX) was recorded as a measure of disease severity. Using minimally preprocessed 3D T1-weighted brain scans of healthy subjects from 8 publicly available sources (N=2160; mean age=33y[range 4-86]), we trained a model predicting chronological age based on a DenseNet architecture and used it to generate brain-age predictions in the internal cohort. Within a linear modeling framework, brain-PAD was tested for age/sex-adjusted associations with diagnostic group (FD vs HC), FASTEX score, and both global and voxel-level neuroimaging measures. Results We studied 52 FD patients (40.6±12.6y; 28F) and 58 HC (38.4±13.4y; 28F). The brain-age model achieved accurate out-of-sample performance (mean absolute error=4.01y, R2=0.90). FD patients had significantly higher brain-PAD than HC (estimated marginal means: 3.1vs-0.1, p=0.01). Brain-PAD was associated with FASTEX score (B=0.10, p=0.02), brain parenchymal fraction (B=-153.50, p=0.001), white matter hyperintensities load (B=0.85, p=0.01), and tissue volume reduction throughout the brain. Conclusions We demonstrated that FD patients’ brains appear older than normal. Brain-PAD correlates with FD-related multi-organ damage and is influenced by both global brain volume and white matter hyperintensities, offering a comprehensive biomarker of (neurological) disease severity. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 2 Introduction Fabry disease (FD; OMIM 301500) is a rare X-inherited lysosomal storage disorder characterized by the accumulation of metabolites in various cell types, resulting from the absent or markedly deficient activity of the enzyme α-galactosidase A (α-Gal A) and leading to damage and loss of function of especially the kidney, heart, and brain.(1) Involvement of the central nervous system is mainly characterized by vascular pathology, the severity of which may greatly vary, reflecting the complex pathophysiology of tissue damage in FD.(2) However, while the recommended follow-up of patients with FD includes brain MR imaging, an accurate evaluation of FD-related brain damage is hampered by the lack of quantitative imaging biomarkers,(3) which also limits the possibility to monitor the effectiveness of recently introduced specific treatments on cerebral manifestations.(4) In the search for objective imaging-derived markers of brain health and pathology, the brain-age paradigm has emerged as a promising approach.(5) Briefly, machine learning methods are used to model chronological age as a function of structural brain MRI scans in healthy people, and the resulting model of ‘normal’ brain aging is used for neuroimaging-based age prediction in unseen subjects.(5) The extent to which each subject deviates from healthy brain-aging trajectories, expressed as the difference between predicted and chronological age (the brain-predicted age difference, brain-PAD), has been proposed as an index of structural brain health, sensitive to pathology in a wide spectrum of neurological and psychiatric disorders.(6) As a relevant example, brain-age predictions are sensitive to white matter hyperintensities (WMH) and brain volumes, imaging features that are both manifestations of cerebral small vessel disease,(7–9) which is thought to be the main pathogenetic mechanisms through which FD impacts brain health.(2) Here, we applied the brain-age paradigm to investigate brain involvement in patients with FD. Our main aims were: i) to assess whether FD brains deviate from normal aging trajectories; ii) to validate brain-PAD as a measure of disease severity against other established clinical markers; iii) to explore the neuroimaging determinants of brain-age prediction in this condition. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 3 Materials and Methods Participants In this retrospective, cross-sectional study, part of a larger monocentric research program on FD,(10–13) patients with a genetically confirmed diagnosis were selected,(14) along with age- and sex-comparable healthy controls (HC). Participants with history of major cerebrovascular events were excluded. Additional exclusion criteria were age <18 or >65 years, and the presence of other relevant neurological or systemic conditions. Scores quantifying the involvement of nervous, renal and cardiac systems in FD patients were computed based on clinical variables recorded within 1 month from the MRI and summed to obtain a cumulative measure of multi-organ damage severity, the total raw Fabry stabilization index (FASTEX) score, ranging from 0 (normal) to 28 (maximum severity).(15) The study was conducted in compliance with ethical standards and approved by the local ethics committee “Carlo Romano”. Written informed consent was obtained from all participants according to the Declaration of Helsinki. MRI acquisition and preprocessing All MRI examinations were acquired between October 2015 and April 2019 using the same 3T scanner (Magnetom Trio, Siemens Healthineers), equipped with an 8-channel head coil. The acquisition protocol included a 3D T1-weighted (T1w)  magnetization prepared rapid acquisition gradient echo (MPRAGE) sequence (TR=1900 ms; TE=3.4 ms; TI=900 ms; flip angle=9°; voxel size=1x1x1 mm3; 160 axial slices) and, for FD patients, a 3D T2-weighted  fluid attenuated inversion recovery (FLAIR) sequence for WMH assessment (TR=6000 ms; TE=396 ms; TI=2200 ms; Flip Angle=120°; voxel size=1x1x1 mm3; 160 sagittal slices). For FD patients, WMH were automatically segmented on FLAIR images using Lesion Segmentation Tool v3.0.0 (www.statistical-modelling.de/lst.html) and used to fill lesions in T1w images. For all participants, we used the Computational Anatomy Toolbox v12.8 (http://www.neuro.uni-jena.de/cat) to segment T1w volumes into grey matter (GM), white matter (WM) and cerebrospinal fluid (CSF) and obtain MNI-normalized, modulated, smoothed (1mm full-width at half-maximum isotropic Gaussian kernel) GM and WM probability maps  Summary measures of GM, WM, CSF, and total intracranial (TIV) volumes were also generated, and brain parenchymal fraction (BPF) was computed as the ratio of brain volume to TIV. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 4 Brain-age modelling A model of healthy brain aging was trained and evaluated on a large external dataset (total N=2160; male/female=1293/867; mean age=33 years, age range=4-86) comprising 3D T1- weighted brain scans of healthy subjects from 8 publicly available sources (Supplementary Table 1). Raw T1w volumes underwent minimal preprocessing, including DICOM to NIfTI conversion, correction for intensity non-uniformity with N4 bias field correction,(16) rigid registration to the MNI space and resampling to 1.5 mm3 voxels, to ensure consistency of spatial orientation and resolution. Finally, images were intensity-normalized by subtracting the image mean and dividing by the image standard deviation. Our brain-age model was based on the DenseNet264 architecture,(17) adapted from the implementation 
available 
at 
Project 
MONAI (https://docs.monai.io/en/stable/_modules/monai/networks/nets/densenet.html) by adding a linear regression layer for the prediction of a continuous variable and a 0.2 dropout rate after each dense layer to reduce the risk of overfitting. Modeling was performed with PyTorch 1.12.0(18) using one NVIDIA Tesla V100S 32 GB graphics processing unit. The full dataset was randomly split into training (64%=1382), validation (16%=346) and test (20%=432) sets (Supplementary Figure 1). Mean absolute error (MAE) and coefficient of determination (R2) were used to quantify model performance. Lastly, age bias (i.e., underestimation of age in older subjects and vice versa) was statistically corrected as in de Lange et al.,(19) and the final model was applied to the internal cohort of FD patients and HC to generate brain-predicted ages and corresponding brain-PAD values. An outline of the different steps of the brain-age modelling procedure is displayed in Figure 1. Statistical analysis Unless otherwise specified, statistical analyses were carried out using the Statistical Package for Social Science (SPSSv25.0, IBM corp.), with a statistical significance level α=0.05 and 95% confidence intervals (CI) and p values computed using bootstrap with 1000 resamples. To assess possible between-group differences in terms of brain-PAD, we used one-way ANCOVA, controlling for the effects of age, age2 and sex and calculating estimated marginal means for the two groups. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 5 To validate brain-PAD as a measure of disease severity, we tested its association with the FASTEX score in a linear regression model including also age, age2 and sex. To investigate the neuroimaging determinants of brain-PAD in patients with FD, we used hierarchical linear regression analyses with age, age2 and sex in the first block and WMH load or BPF in the second block. Similarly, we tested age-, age2- and sex-adjusted associations between brain-PAD and TIV-scaled, preprocessed GM and WM maps, using a nonparametric approach based on 5000 permutations applied to the general linear model(20) via the Threshold Free Cluster Enhancement toolbox (http://www.neuro.uni-jena.de/tfce). The same analysis was repeated after adding the variables group and group*brain-PAD interaction in the model, with this latter term intended to test the hypothesis that different voxel-wise patterns might influence brain-age prediction in the two groups. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 6 Results A flow diagram summarizing the patient selection procedure is shown in Figure 2. A total of 52 patients with FD were identified (40.6 ± 12.6 years; M/F: 24/28), along with 58 HC (38.4 ± 13.4 years; M/F: 30/28). The median FASTEX score of FD patients was 6 (range 3 – 9), indicating mild/to/moderate involvement. Complete demographic, clinical, and MRI characteristics of the studied population are available in Table 1. The brain-age model trained on healthy subjects’ MRI scans achieved an accurate out-of-sample age prediction (test set MAE = 4.01 years, R2 = 0.90). When looking at brain-PAD values in the internal cohort, there was a significant effect of diagnostic group when controlling for age, age2, and sex (F[1, 105] = 6.46, p = 0.01, partial η2 = 0.06), with FD patients showing higher values than HC (estimated marginal means 3.1 [95% CI = 1.0 – 5.3] vs -0.1 [95% CI = -1.9 – 1.4]) (Figure 3). Brain-PAD was significantly associated with the FASTEX score (B = 0.10 [95% CI = 0.02 – 0.19]; standard error B = 0.04; p = 0.02), in a linear model including also age, age2 and sex (R2 = 0.41, p < 0.001) (Figure 3), corresponding to a 0.10 increase in the FASTEX score for each additional year of brain-predicted age. As for the neuroimaging determinants of brain-PAD in FD patients, both higher WMH load (p = 0.01) and lower BPF (p = 0.001) were associated with older-appearing brains (Table 2). Voxel- wise, we found a significant inverse correlation between brain-PAD values and tissue volumes diffusely throughout the brain, with the strongest effect sizes observed at the level of the deep and periventricular WM (Figure 4). The interaction analysis revealed no significant effect of diagnostic group on the relationship between local tissue volumes and brain-PAD. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 7 Discussion By applying the brain-age paradigm in a relatively large cohort of patients, we found that FD is associated with older-appearing brains (on average, 3 years more than normal), indicative of accelerated cerebral aging. The brain-PAD metric correlates with FD-related multi-organ damage and is influenced by both global brain volume and WMH load. Our brain-age model was built using an ‘off-the-shelf’ standard DenseNet264 configuration, to ensure reproducibility and ease of use. Interestingly, not only the achieved performance was close to that of literature benchmarks (21,22), but the model of healthy brain aging was also sensitive to FD-related cerebral pathology. Brain involvement in FD is thought to be mainly mediated by lysosomal deposition of catabolites in endothelial cells, leading to micro- and macro-vascular manifestations that partly overlap with those occurring in common small vessel disease and normal aging.(23) Uncertainty exists on the relative importance of a possible direct brain tissue damage through lysosomal deposition at the level of other cell types (i.e., neuronal or glial cells).(24) Interestingly, the accumulation of lysosomal storage bodies in microglia subsets is a physiological process that increases linearly with aging, suggesting a convergence of mechanisms operating during normal aging and in lysosomal storage disorders, with the accumulation of storage bodies potentially driving microglia dysfunction and contributing to neurodegeneration.(25) Taken together, these observations reinforce the analogy between FD and accelerated aging, with brain-PAD as a plausible neuroimaging biomarker of progressive FD-related brain damage. On the other hand, FD has been also described as a neurodevelopmental disorder,(11) and brain-age predictions are known to be influenced by early- life factors related to brain development.(26) Therefore, we cannot exclude that abnormal neurodevelopment might play a role in the observed group differences in terms of brain- predicted age. The clinical relevance of brain-PAD in FD is illustrated by a significant association with overall, multi-organ, clinical severity in patients with FD. An intricate network of mutual interdependencies exists between brain-age and other bodily (patho)physiological aging phenomena.(27,28) Brain health is shaped by other systems in the body, with cardiovascular and renal (mal)functioning known to have a potentially major impact.(7,29,30) Likewise, common, genetically-determined, mechanisms may underlie the development of FD-related damage in different organs in parallel. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 8 From a neuroimaging perspective, brain-age predictions in patients with FD were influenced by WMH and brain volume, with no anatomical specificity other than  greater effect sizes observed at the level of the deep and periventricular WM, which are known to be preferentially impacted by FD-related lesions and microstructural damage.(31) Our results are in line with previous evidence, confirming the role of a smaller brain volume, rather than the atrophy of specific regions, and a greater WMH burden as the main neuroimaging correlates of higher brain-PAD values,(7,21,22) with no FD-specific determinants emerging from the interaction analysis. Interestingly, several structural MRI studies using more traditional approaches (i.e., voxel-based morphometry and region of interest-based analyses) have failed to reveal consistent changes of brain morphometry in FD patients compared to HC,(3,10,32) at least partially because of the small sample sizes. In this light, brain-PAD might represent a more comprehensive biomarker of brain structural health in FD, potentially more sensitive than conventional methods. Overall, our findings support the role of brain-PAD as a sensitive quantitative biomarker of FD severity, with potentially relevant implications for patient stratification in clinical and research settings, especially in relation to the assessment of treatment response. Indeed, the efficacy of recently introduced specific treatments for FD on cerebral involvement has remained unclear so far, partly because of the very lack of objective neuroimaging measures of disease severity.(33,34) The main limitation of our study lies in its cross-sectional nature. As mentioned, disentangling the relative effects of neurodevelopmental factors and ongoing pathological processes on brain- PAD is challenging in a cross-sectional setting. Assuming that the impact of neurodevelopment remains constant over time, longitudinal studies are warranted, where brain-predicetd age deltas ideally depend solely on ongoing healthy/pathological aging phenomena. Also, a longitudinal design would allow to assess the prognostic value of brain-PAD towards clinical outcomes. Furthermore, crossing brain MRI-derived metrics with additional, more specific, biomarkers of disease severity (e.g., plasma Lyso-Gb3 levels)(35) would help to link brain-PAD more closely to FD pathogenesis. In conclusion, we provide evidence that the brains of patients with FD appear older than normal. The brain-PAD metric is sensitive to FD-related brain damage and is associated with systemic involvement, positioning it as a relevant quantitative imaging biomarker to assess (neurological) disease severity in clinical and research settings. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 9 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288000
doi: 
medRxiv preprint 10",1
"Objective: To determine whether graph neural network based models of electronic health
records can predict specialty consultation care needs for endocrinology and hematology
more accurately than the standard of care checklists and other conventional medical rec-
ommendation algorithms in the literature.
Methods: Demand for medical expertise far outstrips supply, with tens of millions in
the US alone with deficient access to specialty care. Rather than potentially months long
delays to initiate diagnostic workup and medical treatment with a specialist, referring
primary care supported by an automated recommender algorithm could anticipate and di-
rectly initiate patient evaluation that would otherwise be needed at subsequent a specialist
appointment. We propose a novel graph representation learning approach with a hetero-
geneous graph neural network to model structured electronic health records and formulate
recommendation/prediction of subsequent specialist orders as a link prediction problem.
Results: Models are trained and assessed in two specialty care sites: endocrinology and
hematology. Our experimental results show that our model achieves an 8% improvement
in ROC-AUC for endocrinology (ROC-AUC=0.88) and 5% improvement for hematol-
ogy (ROC-AUC=0.84) personalized procedure recommendations over prior medical rec-
ommender systems. These recommender algorithm approaches provide medical procedure
recommendations for endocrinology referrals more effectively than manual clinical checklists
(recommender: precision=0.60, recall=0.27, F1-score=0.37) vs. (checklist: precision=0.16, © 2022 S. Fouladvand∗,$ et al. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Fouladvand∗,$ et al. recall=0.28, F1-score=0.20), and similarly for hematology referrals (recommender: pre-
cision=0.44, recall=0.38, F1-score=0.41) vs. (checklist: precision=0.27, recall=0.71, F1-
score=0.39).
Conclusion: Embedding graph neural network models into clinical care can improve dig-
ital specialty consultation systems and expand the access to medical experience of prior
similar cases.
Keywords: Graph neural networks, Electronic Medical Consultation, Hematology, En-
docrinology. 1. Introduction Access to medical specialty care is often delayed due to growing limitations in clinicians’
time and resources leading to over 25 million Americans with deficient access to specialty
care (Woolhandler and Himmelstein, 2017), associated with higher mortality (Prentice and
Pizer, 2007).
Prediction of medical procedures to be ordered during initial outpatient
specialty consultation care can facilitate specialist consultations by eliminating delay and
further follow-up of diagnostic steps, in some cases completely eliminating the need for an
in-person consultation (Chiang et al., 2020; Kim-Hwang et al., 2010). Clinical checklists are
the current standard of practice to improve specialty referral healthcare delivery (Fantasia
et al., 2021; Keely et al., 2013; Siepierski, 2013; Vimalananda et al., 2015) and in critical
situations such as those resulting from the COVID-19 pandemic (Webster, 2020).
Clinical checklists are labor-intensive to manually produce while both being not easily
generalizable or personalizable to complex scenarios when they largely offer one-size-fits-all
generic guidance and checklists for considerations (Kumar et al., 2020; Tricoci et al., 2009;
Li et al., 2019; McGlynn et al., 2003). Automated AI systems could improve specialty care
systems by providing personalized recommendations based on prior subspecialist care, ad-
dressing limitations in general guidelines and checklists (Middleton et al., 2016; Berner and
La Lande, 2016; Bright et al., 2012). Leveraging artificial intelligence (AI) models trained
using large scale routinely collected electronic health records (EHR) to create automatic spe-
cialty care procedure recommendation could improve efficient use of scarce clinician time
and in turn increase access for more patients to reach appropriate care and consultation.
To this end, there have been multiple attempts to create automated and data-driven
medical order recommender systems. OrderRex (Chen et al., 2016) was created based on
association statistics and Bayesian rules to show promising results in improving clinical
order decision making process and usability (Kumar et al., 2020). Ip et al. (2022) used co-
occurrence statistics to create a recommender algorithm to predict pediatric endocrinology
patients’ initial workup needs.
Classical machine learning models (Hunter-Zinck et al., 2019), assessing coverage of manually authored order sets using optimization-based and
clustering techniques (Zhang et al., 2014), item-based collaborative filtering (Klann et al.,
2009), and artificial neural networks were used for personalized general clinical orders (Wang
et al., 2020) and endocrinology procedure recommendation (Noshad et al., 2021).
To improve upon prior methods, we consider that the heterogeneity and structured
nature of electronic health records (EHR) can be captured more effectively using graphical
models (Park et al., 2022; Choi et al., 2018, 2017). A Graph Convolutional Transformer
(GCT) (Choi et al., 2020) maps encounters into a fully connected graph and infers the
underlying structure by computing self-attentions on the graph connection. Liu et al. (2020) 2 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint GNN based clinical recommender addressed the high visibility (Li et al., 2018) of hub nodes such as demographic nodes
and showed the effectiveness of modeling EHR data into heterogeneous graphs. Further,
heterogeneous graph neural networks (GNN) have been utilized in drug pair side effect
prediction (Zitnik et al., 2018), medical diagnosis prediction (Liu et al., 2021) and medical
concept representations (Wu et al., 2021; Vretinaris et al., 2021).
Motivated by Hamilton et al. (2017), Zitnik et al. (2018), and Veliˇ
ckovi´
c et al. (2018)
we propose a novel GNN-based framework to provide personalized procedure order recom-
mendations prior to or during patients’ initial specialty care visits. Note, here we use the
terminology ‘order’ to refer to the procedures ordered by physicians (e.g., laboratory tests,
imaging studies, additional referrals and consultations).
We tested our models on Endocrinology and Hematology specialty referral care as two
of the most common specialties that are receptive to virtual consultations given their strong
basis in structured diagnostic test results (Palen et al., 2012; Keely et al., 2013; Liddy et al.,
2013). Our objective is to determine whether GNN based models of EHR data can predict
specialty consultation care needs for endocrine and hematology more accurately than the
standard of care guidelines and checklists and other conventional medical recommendation
algorithms in the literature. 2. Materials and Methods Figure 1 shows the overall schema of our proposed framework. We mapped patients’ his-
torical EHR data recorded prior to the patients’ first referrals to specialty care clinics into
a heterogeneous graph neural network. This model was trained to predict procedures or-
dered by endocrinology and hematology specialists during patients visits at the specialty
care clinics. 2.1. Endocrinology Data Our data includes all outpatients referred by XXXX primary care providers to the XXXX
Endocrinology clinic between January 2008 and December 2018. Use of this data for this
study was approved by XXXX Institutional Review Board. We only included patients’ first
visit with the respective specialist within four months of their referral dates to reflect initial
engagement with the specialist in response to to the referral consultation request. Our final
data set include 6,821 new referrals to the endocrinology clinic.
We denote the list of patient referrals as P = {p1, . . . , pn} in which n is the number of
patient referrals. Each patient referral pi constitutes a tuple (ti, Di, Oi, Li, Y i), where ti
is referral’s date and Di ∈R10, Oi ∈R60, and Li ∈R300×3 are multi-hot encoded vectors
representing diagnoses codes, procedure orders, and lab results for pi recorded prior to ti.
We used a two month look back window for lab results and procedures. Each lab result
was converted to a vector with three elements indicating (a) if pi has had the lab result,
(b) if the result was high, and (c) if the result was low. Y i is a multi-hot encoded vector
representing the procedures ordered by the specialist during patient’s special care visit.
Our final feature set includes 370 features. The target set includes 60 procedure orders. A
full list of diagnoses, procedures and lab tests are presented in Table A1, Table A2, and
Table A3 in Appendix A, respectively. 3 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint Fouladvand∗,$ et al. Figure 1: Overall schema of the proposed framework. (a) Specialists’ orders and workup
can be initiated by primary care providers at referral or consultation time. (b) Patients’
historical EHR data including diagnoses, procedures, and lab results were used to create
a heterogeneous GNN. Nodes are patients and orders. Solid line edges show orders before
referral and dashed line edges show the specialists orders after referral date. (c) The GNN
model was used to predict future specialists’ orders. This figure shows the GNN model
recommendations for one example patient referred to the hematology clinic. (d) Procedures
ordered by specialists (ground truth) for the example patient shown in (c). This list highly
overlaps with the procedures recommended by the GNN model (c). 4 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint GNN based clinical recommender Table 1: Patient characteristics among endocrinology and hematology cohorts. Numbers
are N(25th percentile, 75th percentile) for age and N(%) for other variables. Variable
Endocrinology
(n=6,802)
Hematology
(n=2007) Age
52.85 (39, 66)
58.93 (46, 72)
Female
4,104 (66%)
1,066 (53%)
Race
White
3,098 (50%)
1,131 (56%)
Other
1,083 (17%)
308 (15%)
Asian
1,420 (23%)
381 (19%)
Black
287 (5%)
122 (6%)
Unknown
245 (4%)
21 (2%)
Pacific Islander
82 (1%)
25 (1%)
Native American
23 (0.4%)
8 (0.4%)
Ethnicity
Non Hisp./Lat.
5,251 (84%)
1,738 (86%)
Hisp./Lat.
737 (12%)
234 (12%)
Unknown
250 (4%)
35 (2%) 2.2. Hematology Data Our hematology cohort includes all outpatients referred to XXXX Hematology clinic by
primary care providers as new patients from 2008 to 2021.
Our final cohort includes
2,007 patients. Data format is similar to the Endocrinology cohort described in section
2.1. Feature set includes the top-100 most commonly recorded diagnoses, top-100 most
commonly ordered labs as well as 33 medical procedures. The procedures were selected
based on the health system’s internally produced checklists for clinic referrals and elec-
tronic consultations. Lab tests were one-hot encoded based on their results and flagged as
one of the following options: abnormal, normal, low, low off-scale, low panic, high, high
off-scale, high panic, negative, and positive. Each Hematology patient referral pi constitute
(ti, Di, Oi, Li, Y i), where ti is referral’s date and Di ∈R100, Oi ∈R33, and Li ∈R298×3 are
multi-hot encoded vectors representing diagnoses codes, procedure orders, and lab results
for pi prior to ti. Table A4 in Appendix A lists the diagnoses, Table A5 shows the lab tests,
and Table A6 shows the procedures used in this study for the Hematology cohort. 2.3. Descriptive Analysis Table 1 shows the cohort demographics. In general, demographics in terms of age, sex, race,
and ethnicity were similar among endocrinology and hematology patients.
Tables 2 and 3 show the most common diagnoses, procedures and medications recorded
for endocrinology and hematology referrals before patients’ referral dates, respectively. Vari-
able frequencies for each cohort were computed against new patients to all other specialty
care clinics except our study cohorts. For our endocrinology cohort, the top-3 most fre-
quently observed diagnosis codes were thyroid nodule, hyperthyroidism and hypothyroidism,
and the top-3 most frequently ordered procedures were comprehensive metabolic panel, 5 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint Fouladvand∗,$ et al. thyroid-stimulating hormone test (TSH), and T4 free. The Top-3 most frequently prescribed
medications for this cohort were ondansetron, acetaminophen, and Normal Saline IV Bolus
(reflecting supportive treatments that accompany specialty treatments like chemotherapy). Table 2: Top-10 most observed diagnoses, procedures, and medications in endocrinology
patients records. Count shows the number of encounters and Lift shows the ratio of fre-
quency of each variable in the endocrinology cohort to its frequency in all specialty clinics
except endocrinology. Variable
Count Lift Diagnoses
Thyroid nodule
422
17.01
Hyperthyroidism
403
60.87
Hypothyroidism
336
9.04
Diabetes mellitus
334
6.43
Essential hypertension
278
2.68
Osteoporosis
255
10.53
Hyperlipidemia
198
3.39
Neoplasm of thyroid
184
18.82
Vitamin D deficiency
149
4.04
Malaise and fatigue
131
4.63 Procedures
Metabolic panel, comprehensive
1147
2.28
TSH
1121
4.27
T4, Free
744
8.06
Hemoglobin A1c
689
4.34
CBC with differential
492
1.71
Metabolic panel, basic
479
3.44
Vitamin D (25-Hydroxy)
365
2.73
ECG 12-lead
334
1.82
Lipid panel with calculated LDL
296
2.04
Magnesium
286
11.97 Medications
Metformin
66
11.80
Diphth, pertus, tetanus
63
1.40
Pantoprazole
46
5.53
Metformin
43
10.35
Docusate sodium
41
14.66
Atorvastatin
39
6.82
Hydrocortisone
37
82.13
Levothyroxine
36
12.59
Sennosides
35
95.30
Insulin glargine
35
64.97 Anemia, thrombocytopenia and essential hypertension are the top-3 most frequently ob-
served diagnosis codes recorded for hematology patients. Comprehensive metabolic panel,
CBC, prothrombin time were most frequently ordered procedures and ondansetron, ac-
etaminophen and polyethylene glycol were the most frequently prescribed medications for 6 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint GNN based clinical recommender Table 3: Top-10 most observed diagnoses, procedures, and medications in hematology pa-
tients records. Count shows the number of encounters and Lift shows the ratio of frequency
of each variable in the hematology cohort to its frequency in all specialty clinics except
hematology. Variable
Count Lift Diagnoses
Anemia
365
37.66
Thrombocytopenia
345
45.92
Essential hypertension
200
2.7
Hyperlipidemia
147
3.51
Iron deficiency anemia
139
21.47
Pulmonary embolism and infarc-
tion
128
53.25 Leukopenia
118
75.44
Deep venous thrombosis of lower
extremity
103
34.79 Diabetes mellitus
101
2.54
Shortness of breath
101
4.39 Procedures
Metabolic panel, comprehensive
1,353
2.42
CBC with differential
937
2.6
Prothrombin time
825
8.79
ECG 12-lead
722
3.89
PTT partial thromboplastin time
614
13.31
Ferritin
580
8.2
Lactate Dehydrogenase (LDH)
567
14.89
Up ad lib
551
4143.86
Magnesium
549
22.46
Sequential
compression
device
(SCD)
519
8.96 Medications
Acyclovir
120
39.46
Pantoprazole
118
12.74
Polyethylene glycol
114
36.90
Sennosides
89
157.00
Docusate sodium
78
27.62
Acetaminophen
73
152.28
Oxycodone
72
12.33
Sulfamethoxazole-trimethoprim
70
7.95
Apixaban
61
46.12
Ferrous sulfate
58
37.08 hematology patients. Table 3 shows the full list of the top-10 most frequently observed
diagnoses, procedures and medications in hematology patients. 7 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint Fouladvand∗,$ et al. 2.4. Proposed Method 2.4.1. Graph Structure We modeled patients’ EHR data into a heterogeneous graph neural network G = (V, E)
(see Figure 2(a)).
V contains two node types: patient referral nodes {gp
1, ..., gp
|P |}, and
procedure order nodes {go
1, ..., go
|O|}. Each patient node gp
i is assigned a multi-hot encoded
feature vector consisting of the concatenation of Di and Li and each procedure order node
go
i are associated with one-hot encoding of the entity IDs (Isi and Ioi, respectively).
Edge set E contains two edge types.
‘ordered-with’ edges with edge labels set to 0
that are edges between patient nodes and the procedures they have done before ti, and
‘ordered-with’ edges with edge label set to 1 that connect the patients with the procedures
that their specialist ordered during the specialty care visit after ti. Note, ‘ordered-with’
edges with edge labels equal to 1 that represent specialist’s orders after referral date were
not used during training and were only used in the prediction phase as we are aiming to
predict procedure orders after ti. We formulate this task as binary link prediction of the
existence of ‘ordered-with’ edges between a patient and an order. Further, node degree,
node clustering coefficient and centrality transformations were applied to add synthetic
features to each node feature vector. While the model can learn those features on its own,
we added them to help the model focus on learning other features. We apply a different
graph convolutional layers with independent parameters to each message type of (head,
relation, tail) and aggregate embeddings across all node types. The same graph attention
mechanism was applied to all node types. 2.4.2. Message Passing and Graph Attention Figure 2(b) shows our proposed architecture. A fully connected layer with hidden size of
128 was used to map each node feature vector to pre-embedding vectors. Distinct fully
connected layers were used for each node type. Two message passing layers were used each
consisting of a dropout layer, a PReLU activation function, and a graph convolutional layer.
A custom heterogeneous graph attention layer was used using 1-head attention mostly
following the structure of the original graph attention networks (Veliˇ
ckovi´
c et al., 2018),
with the following modifications: 1) we applied fully connected layers with batch normaliza-
tion to the node embeddings and the neighbor embeddings,and 2) we aggregated neighbor
embeddings using the attention mechanism and concatenated the aggregated embedding
to the current node’s embedding. This is then passed into a fully connected layer that re-
duces this down to a single output embedding followed by a batch normalization operation.
Equation (1) shows our message passing function. x(1)
v
= MLP(x(0)
v ) x(2)
v
= GATConv(PReLU(Dropout(x(1)
v ))) x(3)
v
= GATConv(PReLU(Dropout(x(2)
v )) + β ∗x(0)
v ) x(4)
v
= MLP(x(3)
v ) (1) and Equation (2) shows the GATConv update function 8 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint GNN based clinical recommender (a) Graph construction.
(b) Model architecture. Figure 2: Patients EHR data are formatted in a multi-hot encoding matrix. Red nodes show
patients and blue nodes show procedures. Each patient node is assigned with a multi-hot
encoded feature vector consisting of the concatenation of diagnoses and lab results features,
and each procedure order node are associated with one-hot encoding of the procedure IDs.
Solid edges show procedures ordered before referral dates and dashed edges show procedures
ordered by specialists and after referral dates (targets). aggr = Σvo∈N(v)αvo ∗MLP(x(k)
vo ) x(k+1)
v
= MLP(aggr + MLP(x(k)
v ))
(2) Where αvo is the 1 head GAT attention score calculated for vo, N(v) is neighbors of
v, and x(0)
v
represents the node features of node v. The final predictions on existence of
an ‘ordered-with’ edge eij between nodes gp
i and go
j is inferred by concatenating their node
embeddings and passing that through a fully connected two-layer perceptron, a batch nor-
malization, a ReLU activation, and a final fully connected layer that outputs 2-dimensional
logit vectors that are converted to final binary predictions using a softmax function. The
formula for the link prediction head is as follows: p = FC(ReLU(BN(FC([x(4)
gp
i ; x(4)
go
j ])))) ∈R2
(3) where BN refers to Batch Normalization and the first value corresponds to the probability
that the edge exists and the second that it doesn’t. 3. Experimental Results 3.1. Endocrinology medical procedure recommendation We used transductive disjoint training with a 1:4 positive:negative sampling implemented
using PyG (Fey and Lenssen, 2019; pyg). Randomized cross validation with a set of 30
randomly selected hyper-parameter sets were used to tune the model. The final model uses
Adam optimizer, dropout of 0.2, GAT convolutional layer, skip connection, and learning
rate, hidden size, embedding size, and pre-embedding size are 1e-3, 64, 128 and 64, respec-
tively. The model was tested on predictions made on all ‘ordered-with’ edges between a 9 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint Fouladvand∗,$ et al. Table 4: Performance of endocrinologist procedure order prediction models. Model
AUC
ROC
P@R
0.50
P@R
0.40
P@R
0.30 Diagnostic Model
0.65
0.33
0.42
0.46
AE
0.73
0.23
0.33
0.49
PMF
0.62
0.22
0.31
0.43
SVD
0.74
0.23
0.33
0.50
Aggregated NN
0.73
0.31
0.41
0.53
Ensemble Model
0.80
0.37
0.47
0.57
GNN
0.88
0.42
0.49
0.57 patient and an order placed during specialty visit and on an unseen test set consisting of 1,
321 patients.
Table 4 compares prediction results of our proposed GNN model with the baselines
presented by Noshad et al. (2021) including fully connected multi-layer neural network (Di-
agnostic Model), a collaborative filtering auto-encoder (AE), singular value decomposition
(SVD), probabilistic matrix factorization (PMF), an aggregate neural networks (Aggregated
ANN), and an ensemble model (Ensemble Model) that uses a multi-layer neural network to
combine the outputs of the diagnostic model, the collaborative filtering auto-encoder and
the specialists’ identifiers as a separate input signal.
Our proposed model can predict endocrinology specialty procedure orders for the new
patient referrals more effectively (ROC-AUC=0.88) compared to all models evaluated by
Noshad et al. (2021) (best ROC-AUC of the baselines = 0.80). Further, our model showed
significantly higher precision at recalls 0.5, 0.4 and 0.3 compared to all baseline models.
Note, we used the same data as the data that were used in Noshad et al. (2021) except we
removed features related to the specialists that patients were referred to, because although
incorporating specialists’ information in the model can lead to even higher accuracy, the
information on specific specialists in the clinic can add bias to the model.
Further, we compared our proposed GNN model with clinical checklist for endocrinol-
ogy procedure recommendation. The proposed recommender algorithm approach provides
medical procedure recommendations for endocrinology referrals more effectively than man-
ual endocrinology checklists (recommender: precision=0.60, recall=0.27, F1-score=0.37) vs.
(checklist: precision=0.16, recall=0.28, F1-score=0.20). 3.2. Hematology medical procedure recommendation We compared our proposed GNN model’s performance in hematology procedure order rec-
ommendation with classical neural network and collaborative filtering based methods de-
scribed in the previous section including a fully connected multi-layer neural network (Di-
agnostic Model), collaborative filtering auto-encoder (AE), singular value decomposition
(SVD), probabilistic matrix factorization (PMF), aggregated neural networks (ANN), and
an Ensemble Model combining the diagnostic model and the collaborative filtering auto-
encoder. Model tuning follows a randomized cross validation method similar to the model
trained for the endocrinology cohort. Our optimized GNN model has GAT convolutions 10 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint GNN based clinical recommender Table 5: Performance of the models on medical procedure order recommendation for new
referrals to hematology department. Model
AUC
ROC
P@R
0.50
P@R
0.40
P@R
0.30 Diagnostic Model
0.60
0.18
0.19
0.21
AE
0.49
0.17
0.22
0.33
PMF
0.38
0.10
0.16
0.22
SVD
0.74
0.38
0.43
0.46
Aggregated NN
0.64
0.24
0.27
0.32
Ensemble Model
0.79
0.40
0.41
0.41
CR-GNN
0.84
0.41
0.44
0.45 and the hidden size, embedding size, pre-embedding size and learning rate are 32, 128, 32,
1e-3, respectively. All models are tested using an unseen test set including 603 patients.
The comparison results are presented in Table 5. Our proposed GNN model predicts proce-
dures ordered during patients first visit with hematology clinic at least 5% more effectively
in terms of ROC-AUC (ROC-AUC=0.84) compared to all baseline models. Further, the
proposed model has higher precision at recalls 0.50 (precision = 0.41), 0.40 (precision =
0.44), and 0.30 (precision = 0.45) compared to the baselines.
Further, we compared our proposed GNN model with the clinical checklist in active use
by the healthcare system’s electronic consultation program. This checklist was produced
by clinical committees in the health system to guide primary care providers when creating
virtual consultations for XXXX hematology department. The checklist is offered for referral
diagnoses including anemia, isolated erythrocytosis, elevated ferritin, isolated leukocytosis,
isolated leukopenia, mgus, thrombocytopenia, thrombocytosis, and VTE/thrombophilia.
We compared the prediction performance of the proposed model with XXXX hematology
electronic consultation checklist.
The GNN recommender algorithm approaches provide
medical procedure recommendations for hematology referrals more effectively than manual
clinical checklists in terms of precision and F1-score (GNN recommender: precision=0.44,
recall=0.38, F1-score=0.41) vs.
(checklist: precision=0.27, recall=0.71, F1-score=0.39).
We extracted referral diagnoses for the patients in our testing set and used the clinical
checklist to predict the procedures they will need and compared these guideline suggestions
against the procedures actually ordered during patients’ first visits at the hematology clinic
as the ground truth. Utilizing the clinical checklist is not possible for many patients because
the guideline doesn’t cover all referral diagnoses in the data.
As a result, the test set
for guideline is a subset of our original test set including 315 patients, and these results
thus overestimate the performance of the guideline checklists given that they would not
perform at all in the cases without a clear matching referral diagnosis that our recommender
algorithm models are always able to adapt to.
We further explored the GNN model and hematologists behaviours for an example subset
of the patients in our testing set who were referred to the hematology clinic with an anemia
diagnosis (124 patients) as the most common referral diagnosis in our data.
Figure 3 shows the top six procedures that were most frequently ordered by hematologists which 11 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint Fouladvand∗,$ et al. Figure 3: (a) Procedures commonly ordered by hematologists for new anemia patient refer-
rals to the hematology clinic. (b) Procedures recommended by the hematology checklist for
anemia referrals (c) GNN most common recommendations for new anemia patient referrals
to the hematology clinic. Arrows connect similar procedures. Model predictions, hematol-
ogy checklist, and procedures ordered by hematologists overlap indicates the consistency
of the proposed GNN model decision making with the existing clinical checklist and the
ground truth. naturally overlaps with the six procedures provided in the hematology checklist. A list
of six procedures that were commonly recommended by our GNN model is also shown
in Figure 3. Model’s predictions overlap with both the hematology checklist suggestions
(vitamin B12, ferritin, transferrin saturation, CMP, CBC with differential, and reticulocyte
count) as well as the commonly ordered procedures by hematologists including ferritin,
transferrin saturation, metabolic panel, comprehensive, vitamin B12, reticulocyte count
automated, and CBC with diff and slide review.
Although, given the high accuracy of
the model this was expected, this provides some explanations on the proposed model’s
performance and shows that our GNN model’s recommendations are consistent with the
guideline as well as the ground truth (the procedures ordered by the specialists). We further
explored the target variables (procedures) where the model had highest true positives. The
top 5 procedures (targets) which had the highest true positives (model recommended them
correctly) include comprehensive metabolic panel, reticulocyte count, ferritin, transferrin
saturation, and vitamin B12. Excluding the most commonly predicted targets by the model,
procedures such as CBC with differential, transferrin saturation, and haptoglobin had the
highest false positive rates (model recommended but not actually ordered in subsequent
specialist visits). 4. Discussion In this study, we proposed a novel graph neural network based framework for medical
procedure recommendation for specialty referral and virtual consultations. Models were
trained and tested using new patients’ referrals to endocrinology and hematology clinics as
two of the most common specialties with frequent consultation requests. Patients’ historical
electronic health records were used to extract the predictors and the problem was models as
a link prediction task in a heterogeneous graph structure. Proposed graph neural network
based framework outperforms similar endocrinology and hematology medical procedure
recommender algorithms in the literature in terms of ROC-AUC, precision, recall and f1- 12 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.11.21.22282571
doi: 
medRxiv preprint GNN based clinical recommender score. The GNN model outperforms endocrinology clinical checklists in terms of precision,
recall and f1-score, and outperforms hematology clinical checklist in terms of precision and
f1-score.
Clinical checklists are often limited to pre-defined set of diagnoses which precludes them
from being used for a large group of patients with referral diagnoses outside of the scope
of the guidelines. Our proposed AI models are end to end models that can analyze entire
patient histories of EHR data and provide personalized recommendations.
Using auto-
mated medical recommender tools could improve access to medical consultation guidance
to patients by reducing the labor for clinicians and provide a consistent decision making
support system for endocrinologists and hematologists and help them manage the ever-
escalating complexity of electronic health records and medical guidelines. Primary care
providers could also benefit from an order recommender system that suggests orders that
sub-specialists might place, anticipating patient needs without time delays and space sep-
aration that both reduce access for vulnerable patient population. This study showed the
opportunities and a pathway toward such an automated medical procedure recommender
system.
Limitations in the study include that the models were built as an outpatient recom-
mender system, but many of the features (notably hematology related chemotherapy sup-
port orders) were based in the inpatient setting. This may have implications for generaliz-
ability in settings where in-patient records are not as easily accessible. Therefore, available
clinical checklists such as order set templates remain valuable for specialty care settings.
It is worth noting that the current models were trained to recommend procedures ordered
by specialists. However, these targets may include noise and human errors and may not
necessarily be correct and accurate. More follow-up studies needed for outpatient based out-
comes and reinforcement learning towards crowdsourced recommender items and literature
evidence-based clinical practice checklist as well as association with patients outcomes
AI performance in personalized recommendation for medical procedures for endocrinol-
ogy and hematology patients shows the potential of combining both AI and manual ap-
proaches to help primary care providers when referring patients for specialty care or re-
questing virtual consultation.",1
"Background: Risk alleles in a gene for a genetic disorder can often cause a spectrum of syndromes.  The number of copies, deleteriousness and position in the sequence could influence phenotype manifestation. Methods: Whole exome sequencing in 310 individuals from 100 families with severe mental illness revealed 851 instances of variants in the PLA2G6 gene. We assessed the population frequency and deleteriousness of the nonsynonymous variants using in-silico prediction methods.  Molecular docking analyses with antipsychotics was performed to investigate possible pharmacogenomic implications of the PLA2G6 mutations identified. Results: We found six nonsynonymous variants predicted to be deleterious by VarSome. The frequency of non-synonymous variants was found to vary across populations. The preliminary molecular docking analysis suggests that chlorpromazine and risperidone are predicted to bind at three drug-binding sites however, risperidone has a greater binding affinity to PLA2G6. The occurrence of variants close to these drug-binding sites suggests a possible mechanism for the mediation of parkinsonian side effects on drug intake in patients harboring these variants. Conclusion:  Variants in the PLA2G6, a gene previously known to be associated with Parkinson’s disease may thus contribute to the risk of psychiatric phenotypes, as observed in these 9 individuals from 6 families with severe mental illness. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288108
doi: 
medRxiv preprint 3 Missense variants in PLA2G6 contribute to a spectrum of clinical syndromes and provide pharmacogenomic correlates Meghana Janardhanan1, Padmanabhan Anbhazagan2, Biju Viswanath1, Padmanabhan Balasundaram3, Sanjeev Jain1, Meera Purushottam1 1Molecular Genetics Laboratory, Department of Psychiatry, National Institute of Mental Health and Neurosciences, Bangalore, India 2Experimental Drug Development Centre, Singapore 3Department of Biophysics, National Institute of Mental Health and Neurosciences, Bangalore, India Introduction Many risk alleles for genetic diseases seem to produce a spectrum of syndromes, depending on whether they occur as heterozygous, homozygous or compound heterozygotes. This may also be linked to differences in phenotype manifestation and the prevalence of these rare damaging variants. Variants in the PLA2G6 gene are a good example of this. The PLA2G6 protein contains 806 amino acids and harbors various domains including ankyrin repeats, a GXSXG lipase catalytic site, a nucleotide-binding domain, and two binding sites for calmodulin (https://www.uniprot.org/). The gene is highly conserved (dN/dS ratio 2.464; missense variant Z-Score 1.21 (gnomAD)), and is syntenic across many vertebrates. The disease-linked variants may contribute to clinical presentation, by modifying the structure and the function of the enzyme. Neurodegenerative syndromes related to mutations in the PLA2G6 gene (homozygous and heterozygous) have been described in humans [OMIM 603604] [Guo et al., 2018] and animals [OMIA 002105-9615, OMIA 002105-9940]. Young-onset INAD, early adult life dystonia-PD complex (Park14) and late-onset parkinsonism have all been associated with these variants, and both homozygous and heterozygous states have been implicated. Abnormalities in metabolic processes, phospholipid biosynthesis and membrane remodeling have been implicated in many models of Parkinson’s disease (PD) and schizophrenia. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288108
doi: 
medRxiv preprint 4 Experimental models that disrupt mitochondrial function (rotenone treatment), and many mutations that affect lipid pathways (MAPT/PLA2G6, synuclein etc.) are also known to predispose to PD syndromes [Mori et al., 2019, Magrinelli et al 2022]. Increased phospholipase A2 (PLA2) activity has been reported in schizophrenia, and treatment with antipsychotic drugs reduces the enzyme activity to levels similar to those in unaffected individuals [Gattaz et al., 1987]. Certain networks in the brain may be sensitive to metabolic perturbations, leading to a propensity to develop clinical symptoms. We assessed the frequency and deleteriousness of variants using in-silico prediction methods in the PLA2G6 gene in a sample of patients and controls.  Molecular docking analyses with antipsychotics was performed to investigate possible pharmacogenomic implications of the PLA2G6 mutations identified. Methods We examined the Whole Exome Sequencing (WES) data of 310 individuals with severe mental illness (SMI) (cases(n=190); and controls (familial n=60; population n=60)) [Ganesh et al., 2019]. We used in-silico prediction tools such as SIFT, LRT, MutationTaster, FATHMM, MetaSVM from VarSome (Kopanos et al., 2019), which utilise sequence homology, evolutionary conservation, physicochemical and structural properties of amino acids, to predict the effect of the variant on protein function, stability and possible pathogenicity. Network analysis of the gene was performed using GeneMANIA, a website that provides information about interacting protein partners and co-expression, pathways involved and possible function. Further, we carried out molecular docking analysis of antipsychotic drugs with the predicted AlphaFold structure of the protein. Ascertaining the binding pockets would help us to make predictions on the effect of the variants if any on drug interactions. The full-length predicted protein structure of human 85/88 kDa calcium-independent phospholipase A2 was downloaded from https://alphafold.ebi.ac.uk/entry/O60733. The structure was then imported to the Schrodinger Maestro software package (Maestro, Schrödinger, LLC) and the protein was prepared by adding hydrogen atoms and assigning proper bond orders. Prime (Jacobson et al., 2004) was used to fill in missing side chains, and . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288108
doi: 
medRxiv preprint 5 Epik (Shelley et al., 2007) was used to generate protonation states with a pH of 7.0 +/- 2.0. The protein structure was further optimized by PRCG (Polak-Ribier Conjugate Gradient) minimization (Polak and Ribiere, 1969) method with a maximum of 2500 iterations and converge threshold of 0.05. The total energy of the system after minimization was - 168465.094 KJ/mol. SAVES v6.0 server (https://saves.mbi.ucla.edu/) was used to assess the quality of the model. Results from the Ramachandran plot showed 90.1% and 9.1% of the residues to be in the most favoured regions, and additional allowed regions respectively, suggesting that the quality of the model was reliable. The possible drug-binding sites in the AlphaFold predicted human PLAG26 protein structure were identified using the SiteMap method (Halgren, 2009). Three potential binding sites (site1, site2 and site3) with high druggability scores (DScore >1) have been identified in the catalytic domain of the enzyme, including the interface region.  Prior to docking, all the hydrogen atoms were removed from the protein and only the polar hydrogens were added and Gasteiger Charges were computed. The grid box was centered in the catalytic region of the receptor region (X: -15.188 Y: 7.689 Z: 17.443). The number of grid points in XYZ directions was set to 56*40*40 with a grid spacing of 0.909 Å, such that the grid box covered the whole catalytic site as well as the interface region between the catalytic and ankyrin repeat domains. The docking calculations were performed using AutoDockTools (ADT) v1.5.6 (Morris, G., et al., 2009) and AutoDock Vina (Trott, O et al., 2010) with default settings. Results WES in 310 individuals from 100 families with SMI revealed 851 instances (720 intronic, 57 exonic, 56 5’UTR, 18 3’UTR) of variants in the PLA2G6 gene. Ten exonic variants (4 synonymous and 6 non-synonymous) were seen in 57 individuals. The six non-synonymous variants, their pathogenicity and prevalence were obtained from Varsome and gnomAD respectively (Table 1, 2 and 3). A novel heterozygous missense mutation p.D377Y (chr22:38525518 C>A) located in the ankyrin repeats region was seen in three ill siblings (P10, P11 and P12) from a family with complex neuropsychiatric symptoms. P10 was diagnosed to have schizophrenia, while P11 had Parkinsonian features with extrapyramidal symptoms and was suicidal, and P12 had . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288108
doi: 
medRxiv preprint 6 epilepsy and psychosis and also exhibited Parkinsonian symptoms towards the end of life. Variants p.H117R and p.H136Y, located in the region preceding the ankyrin repeats, were identified in individuals diagnosed with schizophrenia (P4 and P5). A p.A343T variant located within the ankyrin repeat domain was detected in four individuals diagnosed with the bipolar affective disorder (P6, P7, P8, P9); and also in  our healthy young controls (C2, C3, C4, C5).  P6, P7 and C2 were all individuals from the same family, Two more variants- p.I256V and p.P379L both located in the ankyrin repeat domain were identified in two different young healthy individuals (C1 and C6), who had no symptoms at the time of assessment. GeneMania detected a network of twenty genes involved in oxidative stress, mitochondrial dysfunction and lipid remodeling, to be interacting with PLA2G6. The preliminary molecular docking analysis suggests that while both chlorpromazine and risperidone are predicted to bind at three drug-binding sites, risperidone has a greater binding affinity (Figure 1). In addition, the positions Arg741 (near site 2) and Asp377 (near site 3) which is the interface region between the catalytic domain and the ankyrin repeat, are near these predicted binding sites. Amino acid changes at these positions as observed in our samples could impact drug binding and biological function. Since the protein probably functions as a dimer  (Malley et al, 2018), the effect of these variants may be further amplified. Previously, mutations in this gene have been associated with PD. It is thus plausible that these variants that   affected the shape and hence possible protein interactions have triggered parkinsonian symptoms in our patients. Discussion Eighteen individuals out of 310 were identified to carry deleterious non-synonymous mutations in the PLA2G6 gene. Twelve had a diagnosis of a neuropsychiatric condition, while 6 were unaffected (one of whom was related to an ill individual, while the other five were unrelated, healthy controls). Of the 8 variants reported here, p.H136Y, I256V and p.A343T are reported in the GenomeAsia (GenomeAsia100K Consortium, 2019). The p.H136Y and I256V are found only in the South Asian (SAS) population at a low frequency (0.00069 and 0.00003375 respectively) (Supplementary). The p.A343T variant seen in multiple cases and controls has a low prevalence across several populations in gnomAD. The p.D377Y is not reported on Clinvar/gnomAD. We had earlier observed the homozygous . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288108
doi: 
medRxiv preprint 7 p.R741Q variants in two individuals (P1, P2) with AREP [Sakhardande et al., 2021], and a p.R741W variant in an individual (P3) with INAD. These variants lie  in the region succeeding the calmodulin-binding domain. It has been proposed that variants in this gene cause a spectrum of phenotypes, as these variants may influence different functions of the protein (Erro et al, 2016; Magrinelli et al 2022). The Ca2+-independent phospholipase A2 (iPLA2), the enzyme encoded by PLA2G6 is known to mediate the release of free fatty acids and lysophospholipids by catalyzing the hydrolysis of the sn-2 fatty acyl bond of phospholipids. This plays an important role in phospholipid remodeling, signal transduction, cell proliferation, endoplasmic reticulum stress-mediated apoptosis, and ferroptosis (Wan et al, 2022), by its impact on membrane permeability and fluidity, and ion homeostasis. The in-silico analysis of the pathogenic/deleterious variants of the 8 non-synonymous variants identified in the PLA2G6 gene shows that these mutations may impact protein structure and function. All eight mutations involving conserved positions were predicted to be damaging and affected protein stability to varying degrees.  This would also have an indirect effect on other proteins that interact with it. The pathobiology of both Parkinson’s disease and schizophrenia is thought to be related to neurotransmitter imbalance at the synapse, but the impact of metabolic processes that interact with synapse function may also be a critical component.  Antipsychotic drugs bind membrane receptors, are invaginated, endocytosed and then recycled. Many actions are dependent on this transition through the post-synaptic cellular milieu. Metabolic processes that impact lipid membrane function, and thus endocytic and vacuole formation, would influence sensitivity to antipsychotics. The Parkinsonian side effects occur in a large proportion of patients exposed to antipsychotic drugs and may be influenced by variations in the PLA2G6 gene. The two individuals with AREP developed severe Parkinsonian side effects when treated with antipsychotics. This sensitivity to antipsychotic drugs was perhaps linked to the presence of variants in a critical region of the protein that binds to these drugs (and perhaps to the endogenous ligand, dopamine). Dopamine-mediated regulation of intracellular lipid dynamics may thus be an important factor in integrating signaling and metabolic function. Long-term dysfunction of this process, by homozygous variants, may underlie the progressive nature of the AREP/Park14 syndrome. The extreme sensitivity to antipsychotics . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288108
doi: 
medRxiv preprint 8 may be one pharmacogenomic aspect of this variation.  Other variants (less damaging but more common) may even underlie the differences in sensitivity to parkinsonian side effects in the general population. Variants in the PLA2G6 gene may thus contribute to the risk of psychiatric phenotypes, as observed in these 9 individuals from 6 families. Many of these individuals showed parkinsonian symptoms during follow-up. This seems to suggest that the continuum of phenotypes described for this gene might even extend to drug-induced side effects. We suggest that variants in the PLA2G6 gene be investigated with reference to the propensity to develop parkinsonian side effects on treatment with antipsychotics. Thus, long-term follow-up is essential to understand the effect of rare, damaging variants over a lifetime. Reports also suggest the enrichment of particular gene variants in different populations (Chu et al, 2020). This is interesting and might need an extensive compilation of mutation databases to understand the differences in clinical presentation and epidemiology of these genetic variants across the world. Modelling the links between genetic variation, protein structure, and the impact on cell biology can thus help understand the risk of disease, its progression, drug response, and side effects.",1
"Viewing laboratory test results is patients' most frequent activity when accessing patient portals, but lab results can 
be very confusing for patients. Previous research has explored various ways to present lab results, but few have 
attempted to provide tailored information support based on individual patient’s medical context. In this study, we 
collected and annotated interpretations of textual lab result in 251 health articles about laboratory tests from 
AHealthyMe.com. Then we evaluated transformer-based language models including BioBERT, ClinicalBERT, 
RoBERTa, and PubMedBERT for recognizing key terms and their types. Using BioPortal’s term search API, we 
mapped the annotated terms to concepts in major controlled terminologies. Results showed that PubMedBERT 
achieved the best F1 on both strict and lenient matching criteria. SNOMED CT had the best coverage of the terms, 
followed by LOINC and ICD-10-CM. This work lays the foundation for enhancing the presentation of lab results in 
patient portals by providing patients with contextualized interpretations of their lab results and individualized 
question prompts that they can, in turn, refer to during physician consults. Introduction With the wide adoption of EHR systems, more patients have direct access to their clinical data via patient portals, 
allowing them to view their visit summaries, lab test results, medications, allergies, diagnoses, etc. [1]. Research shows 
that giving patients access to their medical records through patient portals improves health behaviors, medication 
adherence, and self-management of chronic conditions; enhances doctor-patient communication; reduces utilization 
of high-cost healthcare services among patients with chronic conditions; improves recovery; reduces hospital 
readmissions; and facilitates timely and patient-centered care [2,3]. Viewing lab test results is the most frequent 
activity patients do when accessing patient portals but lab results can be very confusing for patients [1]. Most patient 
portals present lab results in tabular format with a universal reference range, similar to the format seen by clinicians 
[5,6]. Merely providing access to their records is insufficient for improving patient engagement in their care because many 
patients, especially those with low health literacy, cannot make sense of the results and act upon them [4]. Patients 
with limited health literacy are more likely to misinterpret or misunderstand their lab results, which in turn, may delay 
their seeking critical medical attention [1,9]. Various studies have found a significant inverse relationship between 
health literacy and numeracy and the ability to make sense of lab results [7,8]. Giardina et al. [10] conducted interviews 
with 93 patients and found that nearly two-thirds did not receive any explanation for their lab results, and 46% 
conducted online searches to understand their results better. Similarly, another study found that patients who were 
unable to assess the gravity of their test results were more likely to wait for their doctor to call or seek information on 
the Internet [8]. As such, there is a pressing need to study how to present lab test results to improve patients’ understanding and to 
support shared decision making. Various studies, including our own, have explored different strategies for presenting 
numerical data to patients, to name a few: use of reference ranges, tables, charts, color, text, numerical data with verbal 
explanations, etc. [5,11,12]. Nonetheless, to the best of our knowledge, no studies have explored how to provide 
tailored textual explanations based on the medical context of individual patients. There are abundant online health 
sources that provide lab test information. However, they are not organized in a way that is computable. With 
informatics approaches such as natural language processing and ontologies, we can build a robust knowledge base 
that systematically organizes relevant content from online sources to enable flexible querying and linkage to patients’ 
medical records. Consequently, this system will be able to facilitate the generation of tailored support. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. In this study, we demonstrate the feasibility of extracting contextualized interpretations of lab test results from credible 
health articles collected from AHealthyMe.com. We employed a multi-stage annotation procedure to annotate key 
terms, entity types, and their relationships in the textual content of collected records. To support large-scale 
information extraction, we evaluated transformer-based language models for recognizing key terms and their entity 
types. We also used BioPortal’s term search function to map the annotated terms to concepts from controlled 
vocabularies, including SNOMED CT, LOINC, RxNORM, and ICD10-CM to improve interoperability with other 
sources and systems. The contributions of this work are two-fold. First, we provide an annotation dataset of 
interpretations of lab results from over 200 records of health articles from a credible health source. Second, we 
demonstrate the feasibility of building a computable knowledge base using entity recognition, entity linking, and 
knowledge graph techniques. Such a knowledge base can be used to enhance the presentation of lab results in patient 
portals and, in turn, aid patients in understanding their lab results and help them prepare for follow-up consults with 
their doctors. Related Work Challenges for Lab Result Comprehension Previous studies have investigated the challenges associated with lab results comprehension. Zhang et al. [11] used 
both a web-based survey and semi-structured interviews to understand patient challenges and needs in comprehending 
lab test results. They found that patients need both generic information and tailored information such as treatment 
options, prognosis, and action. They also found that result normality, health literacy, and technology proficiency 
significantly impact people’s perceptions of the use of patient portals to view and interpret laboratory results. In 
another study, Zhang et al.  [13] analyzed questions related to lab tests posted on a social Q&A site and found that 
most patients need support for understanding their test results, doctor’s diagnoses, learning about lab tests, and figuring 
out the next steps. Zikmund-Fisher et al. [7] conducted an online survey to determine how numeracy level (an aspect 
of health literacy) affects individuals’ comprehension of lab results. Notably, only slightly half (51.24%) of the 
participants could correctly identify out-of-range hemoglobin readings. Compared to those with lower literacy, those 
with higher numeracy had greater sensitivity for out-of-range results and showed more initiative with contacting 
doctors. Providing support for patients with low health literacy and numeracy through the use of interpretation-based 
approaches can help improve their ability to correctly identify abnormal lab test values as well as their ability to 
participate more actively in managing their health. Improving Lab Results Comprehension Existing work have also investigated ways to improve the comprehension of lab results.  Kopanitsa [14] studied how 
patients perceive interpretations of lab results automatically generated by a clinical decision support system. They 
found that all the patients who received interpretations of the abnormal test results had a significantly higher rates of 
follow-up (71%) compared to those who received only test results without interpretations (49%). Patients appreciate 
the timeliness of automatically-generated interpretations compared to interpretations they can receive from a doctor.  
Zikmund-Fisher et al. [16] surveyed 1,618 adults in the US to assess how different visual presentations of lab results 
influence their perceived urgency. They found that a visual line display -- that included both the standard range and a 
harm anchor reference point which many doctors may not consider as particularly concerning -- reduced the perceived 
urgency of close-to-normal alanine aminotransferase and creatinine results (p-value < .001). In a similar study, Scherer 
et al. [17] tested different display formats of HbA1c result (i.e., a table, a simple line, and a number line with diagnostic 
categories indicated via colored blocks). They found that the goal range only displays achieved higher levels of 
comprehension of test results and decreased negative reaction compared with the no goal range and goal range added 
conditions. They concluded that replacing the standard range with a clinically appropriate goal range could improve 
the comprehension of lab results.  Morrow et al. [18] investigated whether providing verbally-, graphically-, and video-
enhanced context for patient portal messages 
about lab results can improve responses to the 
messages. They found that compared to a 
standardized format verbally- and video-
enhanced contexts were able to improve older 
adults’ gist but not verbatim memory. All of 
these aforementioned studies focused on 
improving lab results reporting but the Figure 1. Overall workflow of this study. Consumer- Friendly Website Annotation Guideline Development Annotation and Curation Named Entity Recognition with Transformers Entity Linking Knowledge Graph Construction . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint approaches used did not take individual patients’ health status into account. Methods Overall Workflow Figure 1 shows the overall workflow of this study. After we obtained all the health articles about laboratory tests from 
AHealthyMe.com, we extracted the sections about lab results interpretation. Following a rigorous multi-stage 
annotation and curation process, we generated an annotated dataset of over 200 records of lab result interpretations. 
Then we trained and tested transformer-based models for named entity recognition of the key terms and used 
BioPortal’s term search API to map the annotated terms to concepts in well-established controlled terminologies. 
Lastly, we stored the annotation results in Neo4j to enable graph-based queries. The detail of the workflow is described 
below. Data Source AHealthyMe is a credible health website created and maintained by Blue Cross Blue Shield of Massachusetts. It 
provides a health library with articles, decision tools, and symptom checker on a wide range of health and wellness 
topics. The health articles are reviewed and verified by medical professionals. The website provides consumer-
oriented semi-structured content about lab tests organized under eight questions: “Does this test have other names?”, 
“What is this test?”, “Why do I need this test?”, 
“What other tests might I have along with this 
test?”, “What do my test results mean?”, “How is 
this test done?”, “Does this test pose any risks?”, 
“What might affect my test results?”, and “How do 
I get ready for this test?”. We crawled the 
webpages of “Tests & Procedures” category of 
AHealthyMe.com and obtained 251 health articles 
about lab tests. Then we extracted the sections 
“Does this test have other names?” and “What do 
my test results mean?” from these articles to 
create the corpus of our study and combine these 
sections as a single record for a lab test. We 
focused on these two sections because they 
contain primary interpretations about lab results. 
Figure 2 shows an example of a lab result 
interpretation for hemoglobin. Annotation Process To identify key information from the records, we 
annotated the data using a web-based annotation 
tool called INCEpTION, a semantic annotation 
platform that provides intelligent annotation 
assistance and knowledge management. It 
supports the seamless collaboration of multiple 
annotators, curators, and managers to ensure the 
annotation quality. We followed a rigorous 
process consisting of five steps: (1) annotation 
guideline development, (2) a pilot test with 
training, inter-rater reliability assessment, and 
conflict resolution, (3) separate annotation, (4) 
curation, (5) quality assurance. In Step 1, PI ZH 
and the doctoral student AE annotated 37 samples 
randomly selected from all the records and 
identified 12 entity types (e.g., “lab name”, 
“normal range”, “indication”, “condition”) and 7 relationships between these entities. PI ZH developed the annotation 
guideline that explains each entity type and relationship with examples. The annotation guideline was further reviewed 
and verified by AE. The explanation about the entity types with examples are provided in Table 1. Note that Figure 2. The section of lab result interpretation for 
hemoglobin. Figure 3. Relationships between entities lab_name condition alt_lab_name has_alt_name normal_range abnormal_range has_range has_range has_condition has_condition gender age_group indication action cause specimen_type Has_specimen_type has_condition has_condition has_condition has_condition has_indication caused_by has_action has_indication caused_by has_action goal has_action . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint “normal_range” and “abnormal range” labels are used for both numeric value ranges and binary test results (e.g., 
positive, negative). The details about the relationships between the entities are provided in Table 2. Figure 3 shows 
the relationships between the entities. In Step 2, four undergraduate pre-med students were recruited to annotate the 
same set of 37 samples following the guideline. PI had weekly meetings with the students to go over the annotated 
examples and made clarifications. Interrater reliability was calculated between each student annotator and the PI. 
Conflicts and inconsistencies were resolved after discussion. In Step 3, four students separately annotated all the 251 
records. Each record was annotated by two annotators. In Step 4, PI and AE performed the curation and consolidated 
the annotation. In Step 5, we performed quality assurance of the annotations with aggregate analysis of both entity 
types and relationships and made corrections to the annotations. Figure 4 shows the number of relationships between 
the entities in the annotated dataset. Table 1. Entity types and example annotations. Entity Type (#) 
Explanation 
Example Annotation (bold texts were annotated) indication (N=730) 
indication of a normal or abnormal 
value range or result A1C from 5.7% to 6.4%. You may have prediabetes. This 
means you have a higher risk for diabetes in the future. A1C 
of 6.5% or above on 2 separate tests. You may have diabetes. alt_lab_name 
(N=620) alternative name of a lab test 
Creatinine (Serum creatinine; blood creatinine): 0.9 to 1.3 
mg/dL for adult males, 0.6 to 1.1 mg/dL for adult females, 0.5 
to 1.0 mg/dL for children ages 3 to 18 years. 0.3 to 0.7 mg/dL 
for children younger than age 3. abnormal_range 
(N=406) a value range or binary test result that 
is abnormal. For value range, it may 
include the value(s), preposition(s), 
comparison operator(s). For test 
result, it may be positive or negative. A1C from 5.7% to 6.4%. You may have prediabetes. This 
means you have a higher risk for diabetes in the future. A1C of 
6.5% or above on 2 separate tests. You may have diabetes. lab_name (N= 353) 
the name of the lab test (e.g., 
“Creatinine”, “Hemoglobin”) Creatinine (Serum creatinine; blood creatinine): 0.9 to 1.3 
mg/dL for adult males, 0.6 to 1.1 mg/dL for adult females, 0.5 
to 1.0 mg/dL for children ages 3 to 18 years. 0.3 to 0.7 mg/dL 
for children younger than age 3. normal_range 
(N=350) normal value range or binary result of 
a lab test Creatinine (Serum creatinine; blood creatinine): 0.9 to 1.3 
mg/dL for adult males, 0.6 to 1.1 mg/dL for adult females, 0.5 
to 1.0 mg/dL for children ages 3 to 18 years. 0.3 to 0.7 mg/dL 
for children younger than age 3. specimen_type 
(N=118) the specimen type of a lab test (e.g., 
urine, blood) Albumin (Urine) (Urine albumin, 24-hour urine test for 
albumin) condition (N=95) 
a certain condition on which a 
certain result is dependent Creatinine (Serum creatinine; blood creatinine): 0.9 to 1.3 
mg/dL for adult males, 0.6 to 1.1 mg/dL for adult females. cause (N=91) 
cause of a certain abnormal 
range/result A lower-than-normal level of protein C may be caused by: 
Blood-thinning medicines, such as warfarin, Kidney 
problems, Deficiency in vitamin K, inherited protein C 
deficiency, Condition that causes the blood to clot too much 
(consumptive coagulopathy) age_group (N=76) 
age group can be an age range (e.g., 
“younger than 50”), a certain 
population group defined by age 
(e.g., adults, newborn, children) Creatinine (Serum creatinine; blood creatinine): 0.9 to 1.3 
mg/dL for adult males, 0.6 to 1.1 mg/dL for adult females, 0.5 
to 1.0 mg/dL for children ages 3 to 18 years. 0.3 to 0.7 mg/dL 
for children younger than age 3. action (N=71) 
action to take to achieve a goal or 
given a certain result To lower the calcium level in your urine, your healthcare 
provider might suggest that you eat more vegetables and 
fruits and less animal products, like red meat and eggs. gender (N=54) 
male or female  
Creatinine (Serum creatinine; blood creatinine): 0.9 to 1.3 
mg/dL for adult males, 0.6 to 1.1 mg/dL for adult females. goal (N=2) 
a goal to achieve 
To lower the calcium level in your urine, your healthcare 
provider might suggest that you eat more vegetables and fruits 
and less animal products, like red meat and eggs. Named Entity Recognition (NER) with Transformers Transformer models have achieved superior performance in many natural language processing tasks such as named 
entity recognition and sentiment analysis. These models pretrained with large corpus can be finetuned with annotated 
datasets for NER. To explore the feasibility of automatic extraction of key terms from lab result interpretations, we . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint used our annotated dataset to finetune 4 widely used SOTA transformer-based models including BioBERT, 
ClinicalBERT, RoBERTa, and PubMedBERT for named entity recognition. These models are based on BERT model, 
a multilayer bidirectional transformer-based encoder model pretrained with BooksCorpus and English Wikipedia 
using masked language modeling and optimized by next sentence prediction [19]. BioBERT was generated by further 
pre-training BERT with PubMed abstracts (4.5 billion words) and PMC full-text articles (13.5 billion words) [20].  
ClinicalBERT was generated by pretraining BERT with clinical notes in MIMIC III (0.5 billion words) [21]. 
RoBERTa has the same architecture as BERT and was pretrained with longer web content using dynamic masked 
language modeling which randomly selects spans of text to mask at each training epoch and different training strategies 
[22]. We further pre-trained RoBERTa model with clinical trial eligibility criteria corpus of ClinicalTrials.gov [23]. 
PubMedBERT was trained from scratch using PubMed abstracts [24]. We experimented with training the models with 
the entire paragraphs of lab result explanations and individual sentences. We converted the annotated corpus into BIO 
format (e.g., “B_lab_name”, “I_lab_name”, and “O” labels for the beginning, inside, and outside of a lab name term, 
respectively) using SpaCy and split the data into 70% training, 10% validation, and 20% testing (see Table 4). As 
such, the NER task is to predict the label for individual tokens. We evaluated model performance using precision, 
recall, and F1 based on both the strict matching criterion (i.e., exact match of both the entity type and entity with the 
annotated entity) and lenient matching criterion (i.e., requires the predicted entity overlaps with the annotated entity). Table 2. Relationship types and example annotations. Relationship Type 
Usage 
Example has_indication 
(N=728) the relation between 
“abnormal_range”/”normal_range” and 
“indication”. “Hemoglobin A1c” 
 
“from 5.7% to 6.4%” has_indication “You may have 
prediabetes. This means you have a higher risk for 
diabetes in the future” has_range (N=714) 
The relation between “lab_name” and 
“normal_range”/”abnormal_range” “Hemoglobin A1c” has_range “below 5.7%” has_alt_name 
(N=616) the relation between “lab_name” and 
“alt_lab_name”. “Hemoglobin A1c” has_alt_name  “HbA1c” has_condition 
(N=225) the relation between 
“normal_range”/”abnormal range” and 
“condition” /“age_group”/“gender”. “Creatinine”: 
“0.9 to 1.3 mg/dL” has_condition “adult males” has_specimen_type 
(N=112) The relation between “lab_name” and 
“specimen_type”. “Albumin” has_speciment_type “Urine” caused_by (N=94) 
the relation between “abnormal_range” and 
“cause”. “lower-than-normal level of protein C” caused_by 
“Blood-thinning medicines, such as warfarin” has_action (N=67) 
the relation between “normal result/abnormal 
result/indication” or “goal” and “action”. “Calcium” 
“To lower the calcium level in your urine”  
has_action “eat more vegetables and fruits and less 
animal products, like red meat and eggs.” Entity Linking Modern EHR systems use controlled terminologies such as SNOMED CT, LOINC, ICD-9/ICD-10, RxNORM to 
encode medical information for patients. To support the construction of the knowledge base that allows linkage to 
EHR data, it is important to use concepts in well-established controlled vocabularies to encode the annotated entities. 
For entity linking, we employed the term search function of BioPortal, which allows us to identify terms from well-
established controlled vocabularies that match the annotated entities from the dataset. The reason we could reliably 
use BioPortal is because we were able to limit the search of matching terms from controlled vocabularies that are 
relevant to a particular entity type. Table 3 lists the entity type and the controlled vocabularies that were considered 
for term search. Results Basic Characteristics of the Annotated Dataset Of all the 251 lab test comprehension records, our systematic annotation discovered 2,964 annotated entities of 11 
entity types and 2,556 relationships of 7 relation types. Among the 11 entity types, Indication entities appear the most 
frequently (N=730) followed by alt_lab_name entities (N=620). Entities of gender appear the least frequently in the 
records, with a handful of 54 annotated entities. Table 4 shows the number of instances for each entity type. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint Performance of Named Entity Recognition Models As shown in Table 5, the best model for strict matching in terms of F1 is PubMedBERT (0.60), followed by 
RoBERTa_trial (0.59), ClinicalBERT (0.56), BioBERT (0.56). Best F1 score of lenient matching is PubMedBERT 
(0.82), followed by RoBERTa (0.81), ClinicalBERT (0.81), BioBERT (0.81). NER models with sentences as input 
had a slightly worse performance with the best models being BioBERT (0.57) for strict matching and RoBERTa for 
lenient matching (0.89). For brevity, the detailed results are not provided in Table 5. Table 3. Entity types and relevant controlled 
vocabularies Entity Type 
Controlled Vocabularies lab_name 
LOINC, SNOMED CT alt_lab_name 
LOINC, SNOMED CT indication 
ICD-10, SNOMED CT specimen_type 
SNOMED CT cause 
RxNORM, ICD-10, SNOMED CT condition 
SNOMED CT age_group 
SNOMED CT, LOINC action 
SNOMED CT, LOINC gender 
SNOMED CT, LOINC Table 4. Basic characteristics of the paragraphs, sentences and annotated terms. Dataset and Data Split 
Train 
Validation 
Test 
Total Paragraphs 
175 
25 
51 
251 Sentences 
1,286 
143 
331 
1,760 Annotated terms 
2,066 
277 
621 
2,964 indication 
542 
51 
137 
730 alt_lab_name 
400 
60 
160 
620 abnormal_range 
301 
33 
72 
406 lab_name 
247 
29 
77 
353 normal_range 
250 
34 
66 
350 specimen_type 
75 
10 
33 
118 condition 
70 
8 
17 
95 cause 
46 
27 
18 
91 age_group 
47 
13 
16 
76 action 
51 
4 
16 
71 gender 
37 
8 
9 
54 Table 5. Performance of transformer-based NER models Model 
Strict Matching 
Lenient Matching Precision 
Recall 
F1 
Precision 
Recall 
F1 NER using paragraphs as input  
BioBERT 
0.5262 
0.5974 
0.5596 
0.7844 
0.8406 
0.8115 ClinicalBERT 
0.5281 
0.6055 
0.5641 
0.7879 
0.8374 
0.8119 RoBERTa 
0.5461 
0.6393 
0.5890 
0.7799 
0.8519 
0.8143 PubMedBERT 
0.5670 
0.6409 
0.6017 
0.7949 
0.8551 
0.8239 Figure 4. Number of annotated instances of 
relationships between entities. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint As PubMedBERT has the overall best performance, we further analyzed its performance by entity types. As shown in 
Table 6, NER for entity types with more instances (i.e., alt_lab_name, speciment_type, age_group, gender, lab_name, 
normal_range, indication, abormal_range) achieved F1 over 0.7 using the lenient matching criterion. NER for entity 
types with fewer terms in our dataset (i.e., condition, cause, action) had F1 lower than 0.5. Table 6. Performance of PubMedBERT NER models by entity types Entity Type 
Strict Matching 
Lenient Matching Precision 
Recall 
F1 
Precision 
Recall 
F1 alt_lab_name 
0.9320 
0.8562 
0.8925 
0.9932 
0.9375 
0.9645 specimen_type 
0.8750 
0.8485 
0.8615 
0.9062 
0.8788 
0.8923 age_group 
0.7778 
0.8750 
0.8235 
0.8889 
0.9375 
0.9125 gender 
0.7273 
0.8889 
0.8000 
0.8182 
1.0000 
0.9000 lab_name 
0.6706 
0.7403 
0.7037 
0.7765 
0.8312 
0.8029 normal_range 
0.5897 
0.6970 
0.6389 
0.8333 
0.9394 
0.8832 indication 
0.3672 
0.4745 
0.4140 
0.7458 
0.8394 
0.7898 abnormal_range 
0.3500 
0.4861 
0.4070 
0.6600 
0.8889 
0.7575 condition 
0.3333 
0.2353 
0.2759 
0.5000 
0.3529 
0.4138 cause 
0.1429 
0.1667 
0.1538 
0.6190 
0.4444 
0.5174 action 
0.0476 
0.0625 
0.0541 
0.4762 
0.5625 
0.5158 Overall 
0.5670 
0.6409 
0.6017 
0.7949 
0.8551 
0.8239 Entity Linking Results Table 7 shows the entity linking results by identifying the terms in major medical controlled terminologies that match 
the annotated entities in the dataset with BioPortal’s term search function. We present both the exact match and partial 
match results for each of the relevant terminologies and each entity type. Exact match means that the term in the 
controlled terminology is exactly the same as the annotated entity. Partial match means that the term in the controlled 
terminology is part of the annotated entity. Note that we only performed partial match for those terms that could not 
find a matching concept using exact match. SNOMED CT can cover 80.3% - 100% annotated entities when both exact 
match and partial match are counted. Table 7. Coverage of the annotated entities by controlled vocabularies Entity Type 
LOINC  
SNOMED CT 
RxNORM 
ICD-10-CM 
Total Exact 
Partial 
Exact 
Partial 
Exact 
Partial 
Exact 
Partial 
Exact 
Partial indication 
--  
21.6% (158/730) 65.6% (479/730) --  
5.6% (41/730) 34.4% (251/730) 21.6% (158/730) 65.6% (479/730) alt_lab_name 
11.3% (70/620) 66.6% (413/620) 18.7% (116/620) 70% (434/620) --  
--  
23.7% (147/620) 72.3% (448/620) lab_name 
39.7% (140/353) 36.5% (129/353) 55.2% (195/353) 38.8% (137/353) --  
--  
60.1% (212/353) 38.8% (137/353) specimen_type 
--  
90.7% (107/118) 5.9% (7/118) --  
--  
90.7% (107/118) 5.9% (7/118) condition 
--  
16.8% (16/95) 66.3% (63/95) --  
0% (0/95) 25.3% (24/95) 16.8% (16/95) 66.3% (63/95) cause 
--  
34.1% (31/91) 59.3% (54/91) 3.3% (3/91) 9.9% (9/91) 7.7% (7/91) 34.1% (31/91) 34.1% (31/91) 59.3% (54/91) age_group 
14.5% (11/76) 77.6% (59/76) 13.2% (10/76) 84.2% (64/76) --  
--  
14.5% (11/76) 84.2% (64/76) action 
1.4% (1/71) 98.6% (70/71) 0% (0/71) 80.3% (57/71) --  
--  
1.4% (1/71) 98.6% (70/71) gender 
5.6% (3/54) 85.2% (46/54) 14.8% (8/54) 85.2% (46/54) --  
--  
14.8% (8/54) 85.2% (46/54) Knowledgebase Graph in Neo4j We stored the annotated data in Neo4j, a graph database management system designed to store, manage, and query 
graph data. It is a powerful database system that is designed to handle complex, highly connected data. The nodes 
represent entities, and the relationships represent the connections between those entities. Each node and relationship . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint can have properties, which are key-value pairs that store additional information about the node or relationship. Neo4j 
also supports graph queries, which allow you to query the graph data using the Cypher query language. Figure 5 shows 
a graph in Neo4j representing the annotation information for prostate-specific antigen test result. Discussion and Conclusions In this study, we collected and annotated result interpretation records from 251 consumer-friendly articles about 
laboratory tests from AHealthyMe.com. Then we evaluated transformer-based language models for named entity 
recognition of the key terms and their entity types. We also mapped the annotated terms to concepts in existing 
controlled terminologies. This work lays the foundation of enhancing patient portals to provide tailored information 
support for lab result interpretation based on a knowledge graph with information extracted from credible health 
sources. There are a number of ways in which such a knowledge graph can enhance lab result reporting in patient 
portals. First, it could provide tailored information support to patients based on their demographics and medical 
context. For example, prostate-specific antigen (PSA) is a test that measures the protein produced by normal, as well 
as malignant, cells of the prostate gland. The blood level of PSA is often elevated in people with prostate cancer [25]. 
As shown in Figure 5, the reference range of PSA varies by age and enhanced patient portals can report age-specific 
reference range and provide possible indications for abnormal results (e.g., prostate cancer) with a follow-up suggested 
action (i.e., need a biopsy of the prostate). Patient-centered 
care 
highlights 
the importance of empowering patients to 
become more proactive in their healthcare and 
to make more informed decisions.  Doctor-
patient communication is a key element of 
patient-centered care and is particularly 
important for facilitating shared decision-
making and for establishing a therapeutic 
alliance between patients and their health care 
providers [26]. It influences the quality of 
patient care and health outcomes, as well as 
patient motivation, satisfaction, and treatment 
adherence. Communication skills such as 
asking/answering 
questions, 
listening attentively, 
sharing 
critical 
health information, and providing tailored guidance 
are all important. The quality and extent of 
information 
exchanged 
during 
patient encounters, however, can vary based on 
different factors such as patients’ ages, 
communication skills, education, health 
literacy, and personality traits, among others 
[27]. Previous studies have found that patients 
with low health literacy have difficulty in 
formulating questions during physician consult, it is important to prepare patients with a question prompt list. Based on the previous studies about lab result comprehension [28], annotated comprehension of lab results in this 
study, and the discussion with MD co-authors, we have identified three types of questions about the reasons for 
abnormal results: a) procedure-related questions (e.g., “Is my abnormal result due to a recent surgery?”), b) medical 
condition-related questions (e.g., “Is my abnormal result of creatinine related to my liver disease?”), c) medication-
related questions (e.g., “Is my abnormal coagulation test result due to use of heparin?”). As patients with limited 
health literacy may find it hard to construct a contextualized question about lab results, we can suggest possible 
questions they can ask during physician consults. Specifically, the questions can be generated based on the user’s EHR 
data. For example, given the lab test result “albumin: 7.3 g/dL”, we will determine that it is higher than the normal 
range (3.4 to 5.4 g/dL) based on the Neo4j subgraph of albumin (Figure 6). Based on the graph, high albumin level 
may be caused by “acute infection, burns, and stress from surgery”, we can search for such information in the patient’s 
EHR. In case the patient had surgery, we can generate questions such as “Is my low albumin due to stress from surgery?” 
– questions that they can discuss with their doctor. Figure 5. Neo4j subgraph for prostate-specific antigen. HAS_RANGE HAS_RANGE HAS_RANGE
HAS_RANGE HAS_RANGE HAS_RANGE HAS_ALT_NAME HAS_ALT_NAME HAS_CONDITION HAS_CONDITION HAS_CONDITION HAS_INDICA
TION HAS_ACTION Prostate-Specific 0-3.5 ng/mL rising PSA 0-2.5 ng/mL 0-6.5 ng/mL 0-4.5 ng/mL below 4.0 ng/mL
PSA PSA Ages 40 to 49 Ages 50 to 59 Ages 70 to 79 cancer may need a biopsy of the prostate to
confirm the diagnosis . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2022.12.19.22283692
doi: 
medRxiv preprint Limitations A few limitations need to be noted 
in this work. First, we used a single 
corpus 
collected 
from AHealthyMe.com 
for 
both annotation and evaluation of NER 
models. Ideally, an external dataset 
should be used to evaluate the 
external validity of the models. 
Second, certain annotations can be 
refined or broken down into 
multiple terms (e.g., normal range 
can be broken down to value range 
and units). Future Work In future work, we will evaluate 
models for relationship extraction 
and develop algorithms and a tool 
to generate tailored information 
support and question prompt lists 
for patients to prepare for their 
doctor visits. This tool can be 
linked to patients’ EHR data to 
provide 
tailored 
support 
and consequently 
enhance 
patient understanding and ability to engage 
in shared decision making with 
their doctors. Acknowledgements We would like to thank four undergraduate students Jessica Valyou, David Garner, Shawtah Thomas, and Madelyn 
Dupuis for annotating the data. This work was supported by the Planning Grant of Florida State University Institute 
for Successful Longevity, National Library of Medicine grant R21LM013911, and in part by the University of Florida 
– Florida State University Clinical and Translational Science Award UL1TR001427 supported by National Center for 
Advancing Translational Sciences.",1
"Background/Aims: Clinical trial funders in the United States have the opportunity to promote transparency, reduce research waste, and prevent publication bias by adopting policies that require grantees to appropriately preregister trials and report their results, as well as monitor trialists’ registration and reporting compliance. This paper has three aims: a) to assess to what extent the clinical trial policies and monitoring systems of the 14 largest public and philanthropic medical research funders in the United States meet global best practice benchmarks as stipulated by the WHO Joint Statement;[1] b) to assess whether public or philanthropic funders have adopted more WHO Joint Statement elements on average; and c) to assess whether and how funders’ policies refer to CONSORT standards for clinical trial outcome reporting in academic journals. Methods: The funders were assessed using an 11-item scoring tool based on WHO Joint Statement benchmarks. These 11 items fell into four categories: trial registration, academic publication, monitoring, and sanctions. An additional item captured whether and how funders referred to CONSORT within their trial policies. Each funder was independently assessed by 2-3 researchers. Funders were contacted to flag possible errors and omissions. Ambiguous or difficult to score items were settled by an independent adjudicator. Findings: Our cross-sectional study of the 14 largest public and philanthropic funders in the US finds that on average, funders have only implemented 4.1/11 (37%) of World Health Organization best practices in clinical trial transparency. The most frequently adopted requirement was open access publishing (14/14 funders), and the least frequently adopted were (1) requiring trial ID to appear in all publications (2/14 funders, 14%) and (2) making compliance reports public (2/14 funders, 14%). Public funders, on average, adopted more policy elements (5.3/11 items, 48%) than philanthropic funders (2.8/11, 25%). Only one funder’s policy documents mentioned the CONSORT statement. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Conclusions: There is significant variation between the number of best practice policy items adopted by medical research funders in the United States. Many funders fell significantly short of WHO Joint Statement benchmarks. Each funder could benefit from policy revision and strengthening. Keywords: Clinical trials, transparency, registration, reporting, publication bias, United States, funders, NIH . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Adoption of the World Health Organization’s best practices in clinical trial registration and reporting among top public and philanthropic funders of medical research in the United States Background Prospective clinical trial registration and timely, public disclosure of trial results are of utmost scientific, ethical, and financial importance.[1] Prescribers, patients, and public health officials rely on complete and accurate trial information for the treatment of disease. Failure to register clinical trials and report results can lead to needless duplication and research waste.[2] One estimate suggests that up to 85% of the money spent on medical research globally is wasted, half of which is due to non-reporting of results.[3] Most of this waste is avoidable. Improvements in trial design, strengthened regulatory requirements, and increased oversight can help curb waste. Because strengthening regulatory power at a national level is difficult and legislative change happens slowly, individual funders of medical research are in a unique position to improve research policy at the ground level. By requiring their grantees to meet specific criteria as a condition of funding, as well as by monitoring grantee compliance, funders can reduce waste in clinical research even if national regulators fail. In addition to waste, poor clinical trial practices can lead to publication bias. Failure to publish both positive and negative outcomes of trials affects the availability of evidence for prescribers and the public.[4] Negative trial results go unreported more frequently than positive results,[5] leading to a distortion of evidence that overstates the efficacy of new drugs, medical devices, and technologies, while downplaying their harms.[6] Appropriate trial registration and reporting is also an ethical imperative: publication bias undermines regulatory decision-making, inhibits the development of clinical guidelines, and interferes with health . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint technology assessment.[6] For this reason, at the 2013 UN General Assembly, the World Medical Association expanded the Declaration of Helsinki (a set of ethical principles guiding human subjects research) to include a new imperative: reporting negative and inconclusive research findings as well as positive ones.[7] Expanding on the Declaration of Helsinki, the World Health Organization (WHO) released a statement in 2017 outlining global best practices for clinical trial registration and reporting. The “Joint statement on public disclosure of results from clinical trials” (hereafter “WHO Joint Statement”) has been signed by 23 major medical research funders, each pledging to reduce waste, curb publication bias, and advance scientific progress through strengthened clinical trial policies.[1] Just one of the 14 funders in this cohort (the Bill and Melinda Gates Foundation) is a signatory. The WHO Joint Statement encourages funders of clinical trials to ensure their grantees preregister their trials and post results on the same registry within 12 months of trial completion. It also asks funders to monitor registration and reporting compliance and to make monitoring reports public. The 2022 World Health Assembly adopted a global resolution to bolster clinical trial quality and transparency, referencing the WHO Joint Statement standards within the resolution.[8] The WHO Joint Statement thus provides a global benchmark for registration and results reporting in clinical research. Clinical research standards vary significantly between countries - the US’ National Institute of Allergy and Infectious Disease runs a helpful website that compares regulations across 20+ countries[9] - but regardless of location, all human subject research should be held to the same high standards. The WHO Joint Statement has specific policy elements that can be universally applied and enforced. In the United States, registration of a limited subset of clinical trials has been a legal requirement since Congress’ passage of the 1997 Food and Drug Administration (FDA) Modernization Act.[10] This resulted . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint in the creation of ClinicalTrials.gov, the largest database of clinical studies in the world, maintained by the National Library of Medicine, a subsidiary of the National Institutes of Health (NIH).[11] Clinical trial submission requirements were expanded in 2007 with the passage of the FDA Amendments Act (FDAAA), requiring both registration and results for applicable trials to be submitted to the ClinicalTrials.gov database. Under FDAAA, trials must be registered within 21 days of initiation and results must be posted within 12 months of study completion or termination. The FDAAA also introduced financial penalties for results submission noncompliance, up to a maximum of US$13,000 per day after receiving a Notice of Noncompliance.[12] To date, the FDA has only ever threatened four noncompliers with a fine,[13] but could have imposed penalties totaling over US$34 billion since the FDAAA became enforceable in January 2017.[14] All applicable clinical trials (ACT) in the US, regardless of funding, must abide by the FDAAA regulations. Even so, the ACT criteria covers only a small minority of interventional trials.[15] Gaps in the legal framework and regulatory enforcement provide a strong rationale for funders to insist that their grantees register and report all interventional trials. As former NIH Director Francis Collins put it, ""It’s hard to herd cats, but you can... take their food away.”[16] Federally funded studies are subject to additional requirements for data collection and dissemination.[9] Grantees funded by any agency of the Department of Health and Human Services, including the NIH, FDA, and Agency for Healthcare Research and Quality (AHRQ), must register and submit ACT results as a condition for continued and future funding.[17] Complementary to this, the NIH issued a dissemination policy that covers all NIH-funded trials, not just ACTs.[18] However, as found in this and several other studies,[19–21] legal requirements, ethical considerations, and reality do not always coincide. Notably, federally funded studies were found to be significantly less likely to adhere to FDAAA mandates than . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint industry sponsored trials.[22] NIH-funded studies are reported within 12 months of study completion just 8% of the time, while other federally funded studies have a 12-month results reporting rate of 5.7%.[23] Rationale This study builds on prior research that reveals significant gaps between WHO Joint Statement benchmarks and the policies of major medical research funders in the US and Europe.[19–21, 24] This study differs from prior studies of US funders by specifically assessing policy elements contained in the WHO Joint Statement, as well as including funders not previously assessed.[20] Objectives The primary objective of this study is to assess the extent to which the clinical trial policies and monitoring systems of the 14 largest public and philanthropic medical research funders in the United States meet global best practice benchmarks as outlined in the WHO Joint Statement. The secondary objectives are to assess a) whether, on average, public or philanthropic funders in the US have adopted more policy items and b) whether and how funders’ policies refer to the Consolidated Standards of Reporting Trials (CONSORT) standards for clinical trial reporting in journals.[25] Though CONSORT is not mentioned in the WHO Joint Statement, it has been endorsed by nearly 600 journals and organizations worldwide.[26] Methods We closely followed the Bruckner et al. (2022) protocol (which assessed the clinical trial policies of European funders) and retained the original assessment tool and rating guide. The assessment tool is an . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint 11-item based on WHO Joint Statement benchmarks. The 11 items fell into four categories: trial registration, academic publication, monitoring, and sanctions. An additional item captured whether and how funders referred to CONSORT within their trial policies, but this item was not scored as it is not contained in the WHO Joint Statement. Two funders (Bill and Melinda Gates Foundation and the Department of Veterans Affairs – Office of Research and Development) were assessed during the pilot phase. No changes to the assessment tool and rating guide were required after the pilot's completion. The pilot phase data was later integrated into the results. This study was prospectively registered with Open Science Framework (DOI 10.17605/OSF.IO/S8PTB); the protocol, including all assessment tools, guides, and funder correspondence are available on GitHub.[27] Cohort selection We compiled a list of large (>US$50 million annual spend) US medical research funders using data that was published in a peer-reviewed journal in 2016[11] and has been used for cohort selection in at least three separate studies of clinical trial policy transparency.[19–21] The list includes only noncommercial funders, categorized as either public or philanthropic. Funders partially or wholly geographically located outside the United States as well as multilateral organizations such as WHO were excluded. Five public funders were excluded as they conducted human subject research but not clinical trials (National Aeronautics and Space Administration, Environmental Protection Agency); provided support for or regulation of clinical trials but did not fund them (National Science Foundation, Centers for Medicare and Medicaid Services); or engaged in pre-clinical testing but not clinical research (Department of Energy). Sixteen funders (7 public and 9 philanthropic) were initially identified as meeting the above inclusion criteria (>US$50 million annual spend, located in the US, and funding clinical trials). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Two additional medical research funder lists (one public and one philanthropic) were searched to identify any omitted funders. Forbes publishes an annual list of the largest US charities, ranked by donations received. The 2020 edition[28] was searched to identify omitted philanthropic funders of medical research. We filtered the Forbes list by category (Health), but no new funders were identified in the list’s entirety. For public funders, the 2018 “U.S. Investments in Medical and Health Research and Development” report was searched.[29] Again, no new funders were identified. As the financial data from the initial list of 16 funders was a decade old (2013), we chose to manually update each funder’s expenditure estimate. Public funder expenditure amounts were brought current by searching the 2021 and 2022 congressional budgets, narrowing to “research” or “grants” where reported. For philanthropic funders, the most recent tax return Form 990 was used, which allowed grant spending to be isolated from other categories such as staff salaries and fundraising. Thus, some philanthropic funders’ actual research spending dropped below the US$50 million spending threshold, but they were retained in the cohort. After cohort selection and during the assessment phase, the American Kidney Fund was found to primarily provide financial assistance for renal patients but does not sponsor clinical trials. They were excluded from the list. Additionally, after the assessment phase, the American Cancer Society were found to have no relevant clinical trial policies. Though they are listed under grant support for several trials[30, 31], their press office confirmed that they do not conduct clinical trials and do not serve as clinical trial sponsor. Thus, the resultant data from the American Cancer Society’s assessment was removed and they were excluded from the study, bringing the total number of funders to 14 (see Table 1). The STROBE checklist for cross-sectional studies was used for study design and reporting to ensure all necessary elements were included.[32] No ethics approval was required by The London School of Economics as only publicly available institutional data were used. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Rating Two researchers per funding body independently searched funder websites and policy documents in June 2022. They each filled out the assessment tool using the rating guide, capturing relevant policy statements and hyperlinks. Scoring was binary (yes/no), and funders received no points for non-binding policies or those that did not cover all trials. Non-binding and partial coverage scores were noted in a separate scoresheet and appear in Table 2. Divergent ratings due to a rater’s oversight of a relevant policy element were reviewed by the team leader who determined the final score. Inter-rater reliability was not assessed as the aim was to capture all relevant policy statements. Rater disagreements that were based on the same source text were referred to the adjudicator. Where applicable, we used precedents set in the Bruckner et al. 2022 adjudication document to settle ambiguous or difficult to score policy items. This ensured that our cohort is scored using the same criteria as were applied to the European funder cohort. All such decisions were recorded and added to the existing adjudication document. The final adjudication document, as well as all rounds of rating are available in GitHub.[27] One frequently contended item was the timeframe for clinical trial registration. Several funders’ policies (BMGF, NIH, FDA) require registration within 21 days of trial initiation, which does not fulfill the “prospective” element in the WHO Joint Statement. However, the wording for this policy item in its entirety states the entry must be made “before the first subject receives the first medical intervention in the trial (or as soon as possible afterwards)”. Additionally, the FDAAA allows for trials to be registered up to 21 days after initiation. For this reason, all non-prospective trial registration policies that specified a 21-day window were still awarded the full point. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Funder name 
Acronym 
Source 
Expenditure 
(USD) 1 
National Institutes 
of Health NIH 
2021 budget: 
https://officeofbudget.od.nih.gov/pdfs/FY22/cy/NIH%20O
perating%20Plan%20-%20FY22%20Web%20Version.pdf $41.4 billion 2 
Department of 
Defense (via 
Congressionally 
Directed Medical 
Research Programs) DoD 
(CDMRP) 2021 funding: 
https://cdmrp.army.mil/about/fundinghistory $1.8 billion 3 
Bill and Melinda 
Gates Foundation BMGF 
2020 annual report: 
https://docs.gatesfoundation.org/documents/2020_Annua
l_Report.pdf $1.79 billion 4 
US Department of 
Veterans Affairs 
(Office of Research 
and Development) VA (ORD) 
2021 budget: https://www.va.gov/budget/products.asp 
volume II, pg. 569 $795 million 5 
Centers for Disease 
Control and 
Prevention CDC 
2021 grant spending: 
https://taggs.hhs.gov/ReportsGrants/GrantsByActivityType $654 million 6 
Agency for 
Healthcare 
Research and 
Quality AHRQ 
2022 budget: 
https://www.ahrq.gov/sites/default/files/wysiwyg/cpi/abo
ut/mission/budget/2022/FY2022_CJ.pdf $488 million 7 
Patient-Centered 
Outcomes Research 
Institute PCORI 
2020 annual report: 
https://www.pcori.org/sites/default/files/PCORI-Annual-
Report-2020.pdf $290 million 8 
US Food and Drug 
Administration FDA 
2021 grant spending: 
https://taggs.hhs.gov/ReportsGrants/GrantsByActivityType $173 million 9 
American Heart 
Association AHA 
2020 Form 990: https://www.heart.org/-
/media/Files/Finance/20202021-IRS-Form-990-PDF.pdf $165 million 10 
Leukemia and 
Lymphoma Society LLS 
2020 Form 990: 
https://www.lls.org/sites/default/files/2022-
02/FY21_LLS_990.pdf $153 million 11 
Michael J. Fox 
Foundation for 
Parkinson’s 
Research MJFF 
2019 Form 990: 
https://www.michaeljfox.org/sites/default/files/media/doc
ument/2019-
12_MJFF_FED_990_PUBLIC_DISCLOSURE_COPY_FOR_WE
BSITE.pdf $97 million 12 
Alzheimer's 
Association AA 
2020 Form 990: 
https://www.alz.org/media/Documents/form-990-fy-
2021.pdf $66 million 13 
Juvenile Diabetes 
Research 
Foundation 
International JDRF 
2020 Form 990: https://1x5o5mujiug388ttap1p8s17-
wpengine.netdna-ssl.com/wp-
content/uploads/2022/05/JDRF-990-
FY21.pdf?_ga=2.50468649.1271124352.1657798787-
187016285.1657020822 $28 million . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Table 1: Funder list with updated expenditure Shading denotes a public funder Funder outreach The press departments of all 14 funders were contacted by email with a copy of their completed score sheet, rating guide, and protocol. They were requested to flag possible errors and omissions. Each funder was contacted at least twice, two weeks apart. For the 6 funders who provided a response, the team leader corrected any errors or omissions where applicable. A third rater independently assessed the policies of all 8 non-responsive funders. Additionally, the assessments from the three largest non- responders (NIH, DoD, and BMGF) were compared against the raw data from the 2018 DeVito et al. study, whose evaluation also included these three funders.[19] This was a protocol deviation due to the low response rate amongst these large funders. Scores were updated for two items based on the NIH’s response[33] to the DeVito et al. team. Ratings for BMGF and DoD were consistent with the data from DeVito et al. study and original scores were retained. Responses were received from 6/14 funders (43%) and ratings were adjusted for 18/66 items (6 funders x 11 items = 66 items total). In their responses, these six funders provided additional documents (award letters, grant terms and additional links to webpages) that contained policy items not found in public- facing documents. These were used to update the ratings for 18 items. The full scoresheet including changes made after funder response appears in the appendix. All responses and changes are also archived on GitHub.[27] 14 
Susan G. Komen 
Breast Cancer 
Foundation SGK 
2020 Form 990: https://www.komen.org/wp-
content/uploads/fy20-form-990-group.pdf $8 million Total = $47 
billion . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Results Our cross-sectional study demonstrated a large degree of variation among US funders’ adoption of WHO best practices. The most frequently adopted policy element was open access publication of research results, 14/14 (100%), followed by prospective trial registration, 9/14 (64%). Only two funders (14%) made public reports of grantee’s results reporting compliance, and only two funders (14%) required the trial ID to be included in all publications. Fig 1: Number of policy elements adopted per funder . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Table 2: Full assessment results (includes CONSORT) Fig 2: Public versus philanthropic performance by category . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Fig 3: WHO best practice adoption, by number of funders adopting each practice On average, the 14 largest US funders of medical research have adopted 4.1/11 (37%) of WHO best practices. Public funders adopted an average of 5.3/11 policy items (48%), while philanthropic funders averaged 2.8/11 (25%). The NIH had the greatest number of WHO best practices appearing in their policy documents, receiving a point for 10/11 items. Four funders (CDC, LLS, BMGF, AHRQ) had just one of the 11 WHO best practices: all required grantees to publish open access. Compared with European and other global funders assessed in the 2023 O’Riordan et al. study[24], which found that 5/11 (45%) of WHO best practices were adopted on average amongst funders outside of the US, American institutions are faring slightly worse. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Just one funder referred to the Consolidated Standards of Reporting Trials (CONSORT) standards in their clinical trial results reporting policies.[25] Two additional funders refer to CONSORT in the publishing guidelines of their journals but did not require that their grantees publish in line with CONSORT. Discussion Strong clinical trial registration and reporting policies, including the monitoring and public disclosure of these activities, can reduce medical research waste.[3] There is significant room for improvement among US funders of medical research, as on average, funders’ policies contain just 37% (4.1/11) of WHO best practices. The most frequently adopted policy across US funders is open access publication (14/14). In 2013, the White House issued a memorandum directing federal agencies to develop open access policies for all federally funded research.[34] As a result, many federal agencies now have dissemination policies in place – all 7 federal funders had adopted with this particular WHO best practice. However, these policies are only as good as the funders’ expectation that grantees post and publish all results. Just 6/14 funders (43%) require that results are posted to ClinicalTrials.gov within 12 months of study completion, while only 5/14 (36%) funders require journal publication of research findings. One funder, Patient-Centered Outcomes Research Institute (PCORI) references CONSORT within its publication policies and had developed an exemplary guide to help grantees meet scientific integrity standards in their research reports.[35] Public funders accounted for a significant portion of the medical research spending in this cohort: 95% of the US$47 billion, largely because of the NIH. Philanthropic funders, though accounting for half of funders assessed, spend far less research than public agencies. Thus, public funder policies hold greater weight, which this study did not adjust for. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Public funders’ policies were lacking in several areas. Very little information was contained within the FDA’s orphan products Request for Applications, and the CDC met only one WHO benchmark. The Department of Defense’s Congressionally Directed Medical Research Program (CDMRP) lacked several items, but it is possible that national security concerns stemming from military research might prevent full transparency. The NIH has a more developed and user-friendly grants section of their website, including a helpful compliance checklist and ACT decision tool. Though 10/11 items were identifiable in NIH policy documents, the section on clinical trial registration and reporting frequently linked to the entire 20-page FDAAA document[36] rather than itemizing each relevant policy. If full compliance is the goal, policies must be clear, concise, and contained directly on the NIH website. Additionally, though on paper the NIH is the best performer in the cohort, in practice the NIH falls significantly short. In August 2022, the Office of Inspector General (OIG) released an audit of NIH-funded clinical trials’ compliance with federal reporting requirements.[37] The OIG audit of 72 studies found that just 15 extramural (21%) and 20 intramural (28%) trials had results submitted on time between 2019- 2020. Though this is an improvement from the 8% on-time reporting rate in the 2015 Anderson et al. study, it is far from perfect. Despite comprehensive policies to the contrary, the report found: “NIH did not have adequate procedures for ensuring that responsible parties submitted the results of clinical trials, took limited enforcement action when there was noncompliance, and continued to fund new research of responsible parties that had not submitted the results of their completed clinical trials”. In response, NIH have pledged to improve procedures that will allow them to work with grantees on ClinicalTrials.gov registration and results submission compliance, as well as to reinforce their capacity to take corrective action. Several funders signaled their intent to strengthen their policy during our outreach, and one funder, the Alzheimer’s Association, immediately changed their policy wording to better reflect WHO best practices. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Per the decisions in the adjudication document, these updated changes are not incorporated into the ratings, but all planned changes are noted in the supplementary material. Clinical trial funders in the US have an opportunity to promote transparency, reduce research waste, and prevent publication bias through strong policies. Unfortunately, both public and philanthropic funders still fall short of WHO benchmarks. Limitations We had a low funder response rate, 6/14 (43%). As many scores were adjusted after receiving responses and it is likely that some non-responding funders have more policy items than this team was able to locate publicly, our final ratings may not include all items. However, this underscores the value of having accessible, public-facing policies. We received no response from 4 of the 5 largest funders. The NIH accounts for 86% of the US$47 billion in our cohort, and a response would provide validation for this significant funder. However, data from a previously published study[20] supported our findings for the 3 largest funders (NIH, CDMRP, and the Bill and Melinda Gates Foundation). Though the NIH was our top-performing funder, their failures to enforce their own policies are well documented, illustrating that there may be gaps between funder policies and funder practices, and highlighting the value of funders making audit reports public. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint Funding This research received no specific grant from any funding agency in the public, commercial, or not-for- profit sectors. Competing interests Sarai Keestra and Alan Silva both belong to the Universities Allied for Essential Medicines (UAEM) and the People's Health Movement on a voluntary basis. Till Bruckner is the founder of TranspariMED. No other disclosures were reported. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 18, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288059
doi: 
medRxiv preprint",1
"Objective. Major histocompatibility complex strongly contributes to susceptibility to systemic lupus erythematosus (SLE). In the European populations, HLA-DRB1*03:01 and DRB1*15:01 are susceptibility alleles, but C4 locus was reported to account for the association of DRB1*03:01. With respect to DRB1*15:01, strong linkage disequilibrium (LD) with a variant rs2105898T in the XL9 region, located between DRB1 and DQA1 and regulates HLA-class II expression levels, was reported; however, the causative allele remains to be determined. Leveraging the genetic background of the Japanese population, where DRB1*15:01 and DRB1*15:02 are commonly present and only DRB1*15:01 is associated with SLE, this study aimed to distinguish the genetic contribution of DRB1*15:01 and XL9 variants. Methods. Among the XL9 variants, two (rs2105898 and rs9271593) previously associated variants in the European populations and two (rs9271375 and rs9271378) which showed a trend towards association in a Japanese genome-wide association study were selected. Associations of the XL9 variants and HLA-DRB1 were examined in 442 Japanese SLE patients and 779 controls. Genotyping of the XL9 variants were performed by TaqMan SNP Genotyping Assay and direct sequencing. HLA-DRB1 alleles were determined by polymerase chain reaction-reverse sequence-specific oligonucleotide probes. Results. Among the XL9 variants, associations of rs2105898T and rs9271593C were replicated in the Japanese population. However, these associations became no longer significant when conditioned on DRB1*15:01. In contrast, the association of DRB1*15:01 remained significant after conditioning on the XL9 variants. Conclusion. In the Japanese population, HLA-DRB1*15:01 was found to be primarily associated with SLE, and to account for the apparent association of XL9 region. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 5 INTRODUCTION Systemic lupus erythematosus (SLE) is a systemic autoimmune disease caused by a combination of multiple genetic and environmental factors. Genetic studies including genome-wide association studies (GWAS) have identified approximately 180 susceptibility loci for SLE.[1-3] In the European and the African populations, the most significant association was detected in the major histocompatibility complex (MHC) region,[2] where complement C4 and human leukocyte antigen (HLA) loci are located. In the Asian populations, the GTF2I-NCF1 region was most strongly associated with SLE,[3-5], where NCF1 p.Arg90His (rs201802880) has been reported to be the causative variant.[5-7] Nevertheless, MHC remains one of the top susceptibility regions also in the Asian populations.[3,4] Among the HLA alleles, HLA-DRB1*03:01 and DRB1*15 have been established as susceptibility alleles to SLE.[8,9] Although DRB1*03:01 is associated with SLE in the European populations, DRB1*03:01 is rare in the East Asian populations, and significant association of DRB1*03:01 is not always detected.[10,11] In contrast, the association of DRB1*15:01 is shared between the European and the East Asian populations.[8-11] Further, DRB1*15:03 and DRB1*15:02 were associated with SLE in the African and the Southeast Asian populations, respectively.[12-14] Identification of causative allele(s) in the MHC region has been challenging, mainly because of the presence of numerous potentially functional variants of HLA and non- HLA genes, and linkage disequilibrium (LD) that extends from the MHC class I to class II regions. Thus, each DRB1 allele is a part of a haplotype that carries multiple potentially functional variants.[8] Most importantly, C4 genes, C4A and C4B, reside in the MHC class III region, and have a copy number variation. A lower copy number of C4, especially C4A, has been shown to confer risk to SLE.[15,16] In the European populations, due to the strong LD between C4A deficient allele and DRB1*03:01, it . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 6 was impossible to distinguish the genetic contribution of these two loci in the development of SLE. However, leveraging the genetic background of the African- American population, where the LD between C4A deficiency and DRB1*03:01 is low, Kamitaki et al. successfully demonstrated that C4A deficiency has the primary role, while the association of DRB1*03:01 was attributable to the LD with C4.[15] With respect to the other risk haplotype carrying DRB1*15:01, they also detected an association of a single nucleotide variant (SNV), rs2105898T, in the XL9 region between HLA-DRB1 and DQA1 loci, both in the European and the African American populations, which was independent of C4.[15] The XL9 region risk variants were associated with expression levels of HLA class II molecules. Raj et al. also demonstrated the association of a SNV in the XL9 region, rs9271593,[17] which is in LD with rs2105898. The XL9 risk variants were in strong LD with DRB1*15:01 in the European population; thus, the causative allele among them has not been inferred [15,17]. In the European and the African populations, HLA-DRB1*15:01 and DRB1*15:03 accounts for the majority of HLA-DRB1*15, respectively. In contrast, both DRB1*15:01 and DRB1*15:02 are commonly present in the Japanese population, of which only DRB1*15:01 is associated with susceptibility to SLE.[11] By leveraging genetic background of the Japanese population, this study was carried out to test whether the association signal of HLA-DRB1*15:01 and the XL9 region variant can be distinguished in the Japanese population. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 7 SUBJECTS AND METHODS Patients and controls A total of 1,221 Japanese individuals were analyzed, including 442 patients with SLE and 779 healthy controls. Characteristics of the patients and controls are summarized in Table 1. Patients’ genomic DNA samples were recruited at University of Tsukuba, the University of Tokyo, Juntendo University, Nara Medical University, and National Defense Medical College. The patients were diagnosed with SLE according to the 1997 American College of Rheumatology revised criteria for the classification of SLE.[18] In addition, 515 genomic DNA samples were obtained from healthy volunteers at University of Tsukuba, the University of Tokyo, and Juntendo University. Aside from that, 264 genomic DNAs were purchased from the National Institute of Biomedical Innovation (Osaka, Japan). Genotyping Genotyping of rs2105898 and rs9271593 was performed by Custom TaqMan SNP Genotyping Assays (Thermo Fisher Scientific, Waltham, MA, USA) using the QuantStudio 5 Real-Time PCR System (Thermo Fisher Scientific). Genotypes of rs9271375 and rs9271378 were determined by Sanger sequencing. A genomic region surrounding rs9271375 and rs9271378 was amplified and cycle sequencing reaction was conducted using a SupreDye v3.1 Cycle Sequencing Kit (Edge BioSystems, San Jose, CA, USA). Subsequently, sequencing was performed using 3500 Genetic Analyzer (Thermo Fisher Scientific). Primers and probes used in these assays were listed in Supplementary Table S1(Online Supplementary Materials). HLA-DRB1 alleles were determined at four-digit resolution by polymerase chain reaction-reverse sequence-specific oligonucleotide probes using a WAKFlow HLA typing kit (Wakunaga Pharmaceutical Co., Ltd., Osaka, Japan). The genotyping results of each subject are available in Supplementary Data (Online Supplementary Materials). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 8 GWAS data on SLE in a Japanese population GWAS data on 317 Japanese patients with SLE and 175,937 Japanese controls previously reported by Sakaue et al.[19] was retrieved from the NBDC Human Database (https://humandbs.biosciencedbc.jp/en/, Dataset ID: hum0197.v3.gwas.v1, accessed 29 Nov 2022). Data on variants in a region between HLA-DRB1 and HLA- DQA1 encompassing XL9 was extracted. Statistical methods Statistical analyses were carried out by logistic regression analysis with an additive model using the R software version 3.5.2. Association was tested for SLE and its subsets. Correction for multiple testing (four SNVs) was performed by the false discovery rate (FDR) method based on the Benjamini-Hochberg procedure. Significance level was set at FDR Q<0.05. The association of haplotypes formed by HLA-DRB1 and the XL9 variants with SLE was tested by permutation test (the number of permutations was 10,000,000) using Haploview 4.2 software (Broad Institute, Cambridge, MA). In LD analysis, r2 values were calculated by the Haploview software. Power calculation was done using Quanto ver 1.2.4. On the basis of the sample size (442 cases and 779 controls) and previously reported effect size in each study,[15,17,19] this study is assumed to have the power of 73% (rs2105898), 99% (rs9271593), 82% (rs9271375) and 68% (rs9271378) to detect the association with Bonferroni corrected statistical significance, α =0.0125. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 9 RESULTS Association of XL9 variants with SLE Four XL9 variants were selected for the association analysis between the XL9 region and SLE in the Japanese population. Two of them, rs2105898 and rs9271593, were previously reported to be associated with SLE in the European populations,[15,17] and were chosen to examine whether they are also associated in the Japanese population. In addition, in view of differences in the genomic configuration of MHC region between the European and the Asian populations, we employed the GWAS data on Japanese SLE reported by Sakaue et al.[19] available at the NBDC Human Database (NBDC Dataset ID: hum0197.v3.gwas.v1 https://humandbs.biosciencedbc.jp/en/). In this GWAS, although no XL9 variants reached genome-wide significance, two SNVs, rs9271375 and rs9271378, showed a trend towards association with SLE (Supplementary Figure S1 and Supplementary Table S2, Online Supplementary Materials). Thus, the association of rs9271375 and rs9271378 was also tested in this study. When LD status among the four XL9 variants were tested, LD was observed between rs2105898 
and 
rs9271593 
(r2=0.676) 
(Supplementary 
Figure 
S2, 
Online Supplementary Materials). As shown in Table 2, rs2105898T and rs9271593C were significantly associated with risk of SLE after correction for multiple comparisons (FDR Q=0.0067 and 0.037, respectively). Thus, these SNVs, previously associated in the European populations,[15,17] were found to be associated also in the Japanese. With respect to rs9271375 and rs9271378, the same trend towards the association as observed in the previous Japanese GWAS[19] was also detected in our subjects. However, the difference did not reach statistical significance in our sample set (FDR Q=0.051 for both SNVs) (Table 2). We next tested whether the XL9 variants were associated with the age of onset and . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 10 presence or absence of renal disorder, anti-dsDNA antibodies and anti-Sm antibodies by a case-case study. As shown in Supplementary Table S3 (Online Supplementary Materials), no significant association was detected with the SLE subsets. Linkage disequilibrium between HLA-DRB1 and XL9 variants HLA-DRB1*15 is an established susceptibility allele group to SLE. DRB1*15:01 and DRB1*15:03 are predominant in the European and the African populations, respectively, while both DRB1*15:01 and DRB1*15:02 alleles are common in the East Asian populations. The XL9 region is located between the HLA-DRB1 and DQA1 loci, and LD is observed between DRB1*15 and XL9 variants. We retrieved genotype data on HLA-DRB1, rs2105898 and rs9271593 in the Utah residents (CEPH) with Northern and Western European ancestry (CEU), Japanese in Tokyo, Japan (JPT), Han Chinese in Beijing (CHB), and Yoruba in Ibadan, Nigeria (YRI) from the International Genome Sample Resource (2018 data, https://www.internationalgenome.org/category/hla/, accessed 
28 
Jun 
2022) 
and 
the 
Ensembl 
database 
(Release 
106, https://asia.ensembl.org/index.html, accessed 28 Aug 2022).[20] and calculated r2 values between each DRB1*15 allele and XL9 variant (Supplementary Table S4, Online Supplementary Materials). DRB1*15:01 and DRB1*15:03 are associated with SLE in the European- and the African-ancestry populations, respectively, [8,9,12,15] and rs2105898T is associated with SLE in both populations;[15] however, due to strong LD, association signals from DRB1*15 and XL9 variants have not been clearly discriminated in these populations. In contrast, in the Japanese population, moderate LD was observed between DRB1*15 (*15:01 and *15:02) and XL9 variants (rs2105898 and rs9271593) (Table 3, Supplementary Tables S4, S5 and Figure S2, Online Supplementary Materials). Although both DRB1*15:01 and DRB1*15:02 are commonly present in the Japanese population and in LD with the XL9 variants, DRB1*15:01, but not DRB1*15:02, was . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 11 associated with SLE (Table 3), as previously reported.[11] Therefore, we thought that genetic dissection of DRB1*15:01 and XL9 variants might be possible by analyzing the Japanese subjects. The primary role for HLA-DRB1*15:01 To address this issue, we conducted a conditional logistic regression analysis to examine whether DRB1*15:01 and the XL9 variants are independently associated with SLE. Table 4 shows that the association of DRB1*15:01 remained significant after conditioning on rs2105898 (Pconditional=7.6x10-6) or rs9271593 (Pconditional=8.7x10-7), while that of rs2105898 and rs9271593 became no longer significant when conditioned on DRB1*15:01 (rs2105898, Pconditional= 0.83, rs9271593, Pconditional= 0.93). Haplotypes formed by HLA-DRB1*15:01 and the XL9 variants were also tested for their association with SLE (Table 5). DRB1*15:01-rs2105898T-rs9271593C haplotype was significantly increased in SLE compared with healthy controls (permutation P = 1.0 x 10-7); however, the haplotype with rs2105898T and rs9271593C risk alleles, but without DRB1*15:01, was not associated with SLE. Taken together, these results suggested that the association of DRB1*15:01 has the primary role in the susceptibility to SLE, and the association of the XL9 variants is secondarily caused by LD with DRB1*15:01. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 12 DISCUSSION Identification of the causative variant(s) among the MHC region remains challenging, due to the presence of multiple potentially functional variants and extensive LD. With respect to DRB1*03:01 haplotype in SLE, Kamitaki et al. reported a primary role for reduced copy number of C4A based on the results from the African-American population.[15] However, their study did not determine the causative allele on the DRB1*15 haplotype, because of the tight LD with the XL9 region SNV which might regulate expression levels of HLA class II alleles, in the European population. In this study, leveraging the genetic background of the Japanese population, we were able to present evidence for the primary role of the DRB1*15:01 allele, rather than the XL9 region SNVs, for the susceptibility to SLE. The XL9 region shows high histone acetylation levels and interacts with the promoters of HLA-class II genes, including HLA-DRB1 and DQA1. Thus the XL9 region was predicted to impact on transcriptional regulation of nearby HLA genes.[21,22] In fact, the risk alleles for SLE, rs2105898T and rs9271593C, were shown to be associated with expression of HLA-class II genes.[15,17] Whole blood eQTL data from the Genotype-Tissue 
Expression 
(GTEx) 
Portal 
V8 
(https://gtexportal.org/home/, accessed 13 Mar 2023) are shown in Supplementary Table S6 (Online Supplementary Materials). Kamitaki et al. also reported that a binding site for a transcription factor ZNF143 was disrupted by substituting rs2105898G with T.[15] These data support the functional significance of XL9 variants. Nevertheless, concerning the susceptibility to SLE, our conditional logistic regression analysis showed that the association of XL9 variants was no longer significant when conditioned on DRB1*15:01, whereas the association of DRB1*15:01 remained significant after conditioning on XL9 variants, suggesting that DRB1*15:01, rather than XL9 variants, may be the causative susceptibility allele, at least in the Japanese . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 13 population. Consistent with this, although XL9 variants were in LD with both DRB1*15:01 and DRB1*15:02, only DRB1*15:01 was significantly associated with SLE susceptibility (Table 3). There is only a single amino acid difference at position 86 between DRB1*15:01 (86Val) and DRB1*15:02 (86Gly). This amino acid sequence difference affects 
the 
peptide 
binding 
motif 
(MHC 
Motif 
Viewer (https://services.healthtech.dtu.dk/services/MHCMotifViewer/Home.html)[23] and T cell receptor repertoire selection.[24] Furthermore, DRB1*15:01 was recently reported to be associated with hypomethylation and increased expression of HLA-DRB1 in monocytes.[25] Taken together, it appears more plausible that DRB1*15:01 plays a primary role in the susceptibility to SLE through its effect on antigenic peptide specificity and/or T cell repertoire selection. On the other hand, the possible role for XL9 variants might not be entirely excluded. As shown in Table 5, a haplotype carrying DRB1*15:01 contains rs2105898T and rs9271593C, and thus it might be possible that by having regulatory XL9 variants together with DRB1*15:01 on the same chromosome, the effect of DRB1*15:01 could be enhanced as compared with having the non-risk variants of XL9. Recently, Wang et al. examined the genetic correlation for SLE between the European and the Chinese populations.[26] The transancestral genetic-effect correlation (rge) was increased from 0.64 to 0.78 when the variants in the HLA region were removed from the analysis, suggesting a genetic difference in the HLA region between the European and the Chinese populations. The possibility that the role of the XL9 region may also be different among European, African and Asian populations cannot be excluded. In view of population-specific differences in the genomic configuration of the MHC . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 14 region, the possibility that XL9 variants other than rs2105898T and rs9271593C play a causal role cannot be excluded. Therefore, to comprehensively investigate an association of XL9 variants with SLE, we accessed the previous GWAS data on Japanese SLE,[19] and included two SNVs with a tendency towards association, rs9271375 and rs9271378, into our analysis. In our case-control set, although a trend towards association of rs9271378 was observed, the statistical significance did not stand correction for multiple testing. These SNVs were not in LD with DRB1 alleles associated with SLE in the Japanese population, including HLA-DRB1*15 alleles (Supplementary Table S5 and Figure S2, Online Supplementary Materials); therefore, the possibility that future large-scale studies might lead to identification of XL9 variant(s) with an independent effect from DRB1*15:01 cannot be excluded. Limitations of this study include a relatively small sample size. Although our sample size is assumed to have 68% or larger power to detect statistical significance based on previously reported effect size for each SNV,[15,17,19] studies in larger sample sizes may be necessary to comprehensively evaluate the independent role of XL9 region. In addition, this study does not provide any explanation on the significant association of DRB1*15:02 in Southeast Asian populations.[13,14] Future fine mapping studies on these populations are required. In summary, we replicated the association of the XL9 variants with SLE in a Japanese population and confirmed the observation reported in the European and the African populations. Taking advantage of the genetic background of the Japanese population, the association of the XL9 variants was suggested to be attributable to LD with HLA- DRB1*15:01, which may play a causative role. These observations further emphasize the transethnic studies’ importance in finely mapping causative variants. Funding . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 15 This study was partly supported by the collaborative research fund from H.U. Group Research Institute G.K., and by award grants given to Dr. Tsuchiya from Japan College of Rheumatology and Japan Rheumatism Foundation. The funders had no role in the design, analysis, interpretation and paper writing of this study. Competing Interest Dr. Kawasaki has received research grants from Ichiro Kanehara Foundation, Takeda Science Foundation, and Japan College of Rheumatology, and honoraria for lectures from Chugai Pharmaceutical Co. Ltd. Dr. Kondo has received a research grant from GlaxoSmithKline Japan, and honoraria for lectures from GlaxoSmithKline Japan, AstraZeneca and Asahi Kasei Pharma. Dr. Amano has received consulting fee from Nippon Shinyaku, honoraria for lectures and support for attending meetings and/or travel from Janssen Pharmaceutical, Eli Lilly Japan, Taisho Pharmaceutical, Nippon Boehringer Ingelheim, Eisai, Mitsubishi Tanabe Pharma, 
Nippon 
Shinyaku, 
Chugai 
Pharmaceutical, 
Glaxo 
Smith 
Kline Pharmaceuticals, Ono Pharmaceutical, Asahi Kasei Pharma, Astra Zeneca, AbbVie and Ayumi Pharmaceutical. Dr. Tamura has received grants from Asahi Kasei Pharma, Asahi Kasei Medical, Ayumi, AbbVie, Eisai, Nippon Boehringer Ingelheim, Taisho, Tanabe Mitsubishi, and Chugai, and honoraria for lectures from Asahi Kasei Pharma, AstraZeneca, AbbVie, Eli Lilly Japan, GlaxoSmithKline, Chugai, Novartis, Bristol-Myers Squibb, and Janssen. Dr. Itoh and Dr. Kusanagi have received grants from Asahi Kasei Pharma, Eizai, Teijin Pharma, and Chugai Pharmaceutical. Dr. Itoh has received honoraria for lectures from Asahi Kasei Pharma and AbbVie. Dr. Tsuchiya has received grants from Bristol-Myers Squibb K.K., the Naito Foundation, the Uehara Memorial Foundation, and collaborative research fund from H.U. Group Research Institute G.K. Dr. Tsuchiya has received award grants from Japan College of Rheumatology and Japan Rheumatism Foundation, and honoraria for lectures from . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 16 Teijin Ltd. Other authors have no competing interest to disclose. Ethics: This study was reviewed and approved by the of University of Tsukuba Institute of Medicine Ethics Committee (approval ID: 122(1), 123, 268). In addition, this study was also approved by the Ethics Committees of the following institutes. which participated in the collaboration and/or recruitment of the subjects: the University of Tokyo, Nara Medical University, Juntendo University and National Defense Medical College. This study was conducted in accordance with the principles of the Declaration of Helsinki and Ethical Guidelines for Human Genome/Gene Analysis Research implemented by the Ministry of Education, Culture, Sports, Science and Technology, Ministry of Health, Labour and Welfare, and Ministry of Economy, Trade and Industry, of Japan. Written informed consent was obtained from each participant. Data availability: All data relevant to the study are included in the article or uploaded as Online Supplementary Materials. Contributors Dr. Kawasaki, Dr. Kusumawati and Dr. Tsuchiya designed the study, interpreted the data, and wrote the manuscript. Dr. Kawasaki, Dr. Kusumawati and Ms. Kawamura performed genotyping and statistical analyses. Dr. Kondo, Dr. Kusaoi, Dr. Amano, Dr. Kusanagi, Dr. Itoh, Dr. Fujimoto, Dr.Tamura, Dr. Hashimoto, Dr. Matsumoto and Dr. Sumida recruited the participants and collected clinical data.  All authors read and approved the final version of the manuscript. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted May 11, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288103
doi: 
medRxiv preprint 17",1
"Background: Systemic racial and ethnic inequities continue to be perpetuated through scientific methodology and communication norms despite efforts by medical institutions. Purpose: To characterize methodological practices regarding race and ethnicity in U.S. research published in leading medical journals. Data source: Articles published in Annals of Internal Medicine, BMJ, JAMA, The Lancet, and NEJM from 1995-2018 were sampled via PubMed. Study Selection: All original, human subjects research conducted in the U.S. Data Extraction: Information on definition, measurement, coding, use in analyses, and justifications was collected. Data Synthesis: The proportion of U.S. medical research studies including race and/or ethnicity data increased between 1995 and 2018. No studies defined race or ethnicity. and most did not state how race and/or ethnicity was measured. Common coding schemes included: “Black, other, White,” “Hispanic, Non-Hispanic,” and “Black, Hispanic, other, White.” Race and/or ethnicity was most often used as a control variable, descriptive covariate, or matching criteria. Under 30% of studies included justification for their methodological choices regarding race and/or ethnicity. Conclusions: Despite regular efforts by medical journals to implement new policies around race and ethnicity in medical research, pertinent methodological information was systematically absent from the majority of reviewed literature. This stymies critical disciplinary reflection and progress towards equitable practice. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 3 INTRODUCTION Following global protests for racial equity, an increasing number of health researchers are studying racism as a fundamental cause of morbidity and mortality. Such investment is long overdue. However, racism-focused work must be coupled with sound methodological practices regarding the social constructs of race and ethnicity. Effectively using these constructs is integral to documenting and understanding how systems of racism and ethnocentrism affect health. Unfortunately, practices surrounding race and ethnicity in medical research are often deficient regarding definitions, measurement, coding, analysis, and interpretation of findings. Perpetuating problematic methodological practices maintains an ethnocentric status quo and may contribute to challenges in understanding how racism affects health, ultimately hindering effective and equitable healthcare and policy-making. Debates over appropriate methodological approaches to race and ethnicity in health are longstanding. In the 1990s, researchers challenged many methodological decisions, including the necessity of racial and/or ethnic data, construct definitions, measurement choices, appropriateness of coding schemes, and role of variables in analyses (1-8). At the time, Thomas LaVeist (1996) argued that racial and ethnic data retained high utility for health research. He challenged health researchers to “do a better job” of conceptualizing race, understanding nuances of racial and ethnic measurements, and interpreting findings with care in order to help reduce health disparities in the U.S. (9). Recent work in surgery and oncology has identified infrequent reporting of race and ethnicity data (10-12), however, no comprehensive systematic review of the state of these methodological practices in medicine over time currently exists. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 4 The present study seeks to fill this gap by systematically reviewing trends in methodological practices regarding the conceptualization, operationalization, and utilization of race and ethnicity in U.S. medical literature. By examining publications in influential medical journals over the past quarter-century, we document the state of medicine’s methodological norms and identify patterns of disciplinary practices that may reify misconceptions about race and ethnicity, with implications for scientific quality, reproducibility, and equity. In total, we investigated five core questions using a sample of U.S. medical publications: 1) What proportion of studies incorporate data on race and ethnicity? 2) What proportion provides conceptualization of race and ethnicity? 3) How is race and ethnicity data operationalized? 4) How is race and ethnicity data utilized in analyses? And 5) Do the authors justify their methodological decisions regarding race and ethnicity in publication? All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 5 METHODS This study is a methodological systematic review under Munn et al.’s taxonomy (13), as the foundational methodological treatment (i.e., definitions, measurement, coding, analytical use, and scientific justifications) of two key variables - race and ethnicity - is the focus of this investigation. We define race as a social and political construct whereby social meanings (e.g., beliefs about ability, health, worth, etc.) are assigned to arbitrary phenotypes and which capture differential access to power, opportunities, and resources in a race-conscious society (14, 15). Similarly, we define ethnicity as a social construct, stemming from a sense of belonging over shared cultural elements (e.g., language, religion, traditions, values) and/or of place (e.g., national origin) (14, 16). Both race and ethnicity are contextually, temporally, and geographically specific; neither race or ethnicity are determined by biology (17-19). For the purpose of this review, “Hispanic” and “Latino/a/x/e” are defined as a pan-ethnic identities, not as racial identities. Furthermore, “African American” is defined as an ethnic identity and is not synonymous with “Black.” See Appendix 1 for additional background and rationale. Capitalization practices were not collected from sampled articles; however, we follow the AMA capitalization style guidelines and capitalize all racial and/or ethnic terms in this article (20). Data sources and searches The target articles under study include all U.S.-based, original, human subjects medical research published in Annals of Internal Medicine, BMJ, Journal of the American Medical Association (JAMA), The Lancet, and the New England Journal of Medicine (NEJM) between Jan 1, 1995 and Dec 31, 2018 (Figure 1). Journals were All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 6 selected based on impact factor and reputation, consistent with other methodological systematic reviews (21-23). Studies were identified by searching PubMed for empirical work published between Jan 1, 1995 and Dec 31, 2018. To reduce ineligible articles the following search terms were used: (English[Language]) NOT (Letter[Publication Type]) NOT (Comment[Publication Type]) NOT (Editorial[Publication Type]) NOT (Review[Publication Type]) NOT (News[Publication Type]) NOT (Case Reports[Publication Type]) AND ((""United States""[MeSH]) OR (""United States""[tw]) OR America[tw] OR ""U.S.""[tw] OR ""US""[tw]). Given the number of articles returned by the original search (35,194; Figure 1) and the richness of the data we aimed to collect, we took a stratified random sample of 210 articles from five, five-year periods (1995-1999; 2000-2004; 2005-2009; 2010-2014; 2015-2019; 1050 articles total). Data collection occurred between July 2019 and November 2021. Study selection All human-subjects research conducted exclusively in the U.S. was included. Non-U.S.-based research or multi-national research was excluded because of the unique social and geopolitical structures through which race and ethnicity function. We encourage researchers in other countries to conduct similar reviews using language and racial and/or ethnic categories that are important and specific to their context. Letters to the editor, commentaries, meta-analyses, and simulation studies were excluded. No restrictions were made on study outcome, exposure, or study design. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 7 Data extraction and quality assessment Full details on the protocol have been reported elsewhere (23). In brief, all included articles were independently reviewed in-full by two reviewers; data were abstracted into a standardized REDCap form (24, 25). Abstraction was conducted using an existing protocol and all reviewers were primed using practice articles. Any abstraction discrepancies were discussed between the pair of reviewers, and if consensus could not be reached, were reviewed collectively by the author team. A third data quality check was conducted by the primary author. See Appendix 2 for details. Software Articles were sampled with Python 3.5.2 (26) using Biopython (27) and NumPy (28) libraries. Analyses were performed in R, version 4.0.2 (29) with packages tableone (30), tidytext (31), and tidyverse (32). Funding Financial support was provided in part by training grants from the Eunice Kennedy Shriver National Institute of Child Health and Human Development [T32- HD091058], National Cancer Institute [T32-CA057711], and the National Institute of Allergy and Infectious Disease [T32-AI007001] with general support from the Carolina Population Center [P2C-HD050924, P30-AG066615]. Additional pilot funding was provided by the Department of Sociology, University of North Carolina at Chapel Hill. Funding sources had no role in data collection, analysis, interpretation, or any other aspect pertinent to the study. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 8 RESULTS Of 1050 screened articles, 242 were included (Figure 1). The majority of excluded articles were either international studies or commentaries (Figure 1). Across time periods, the majority of studies were either cohort studies (range 56- 73%) or randomized control trials (range 18-41%; Table 1). Most studies examined a physical or mental health outcome (range 70-80%). “Other” outcomes were the second most prevalent (16-23%) and included studies on topics such as medical training, medical errors and the prevention of adverse events, or physician decision making. Question 1: Inclusion of racial and ethnic data. The proportion of reviewed studies that included data on participants’ race increased over time (range 44-74%, Table 1). Studies that did not include participants’ racial data do not substantially differ from the overall sample with respect to study design, study outcome, or sample size (Appendix Table 2). Over the same period, the proportion of reviewed studies that included participants’ ethnicity data has similarly increased (range 20-58%, Table 1). Racial and ethnic data were almost always included together in the same study. Across all 149 studies which included participants’ race and/or ethnicity data, only a single study included data on participants’ ethnicity without also including data on participants’ race. When ethnicity data was included in the study, it was frequently combined with race into a single ethnoracial construct (range 81-100%, Table 1). Only 11 studies across all strata included both race and ethnicity data and kept them as separate entities. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 9 Question 2: Conceptualization of race and ethnicity. Across all 149 studies which included data on participants’ race and/or ethnicity, none provided a definition of either construct. Question 3: Operationalization. In 59-90% of articles across strata, the measurement of race was “not stated or unclear” (Table 2). In articles that indicated using “self-reported” race, it was frequently ambiguous if the measure was open-ended (i.e., free response) or close-ended (i.e., selection from preset options). Ambiguity between “open” and “closed” measures was more common in later strata (2005-09, 2010-14, 2015-18, Table 2). Use of other measures (e.g., observed, reflected, or phenotype) was infrequent or absent (Table 2). Results for ethnicity are similar; across all strata, articles commonly lacked any information on measurement of ethnicity (range 52-89%, Table 2). Ambiguity between open and closed measures was more common in later strata (2005-09, 2010-14, 2015- 18, Table 2), and other measures (e.g., country of origin) were rare. Coding schemes were collapsed across sampling strata and examined by use of a strictly racial, ethnic, or a collapsed ethnoracial construct. Racial and ethnoracial coding schemes were more heterogeneous, while ethnic coding schemes were more similar (Table 3). Although “non-White, White” and “nonWhite, White” are functionally the same, we made no attempt to collapse coding schemes based on similarity due to concern about the subjectivity of those decisions. The most common racial coding schemes reflected predominantly binary racial framing centering “Whiteness,” while ethnic coding schemes primarily centered on “Hispanic” or “Latino” binary coding. In the most common ethnoracial coding schemes “Hispanic” - an ethnic group - is compared to All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 10 the racial categories of “White” and “Black.” Ethnic, racial, and ethnoracial codings all included “ns (not stated),” where no information was provided in the article about how participants’ racial and/or ethnicity data was re-coded for the study. Appendix Tables 3 and 4 contain the complete list of racial and ethnoracial coding schemes, respectively. Question 4: Use in analyses. Race and ethnicity were predominantly classified as “not of interest” in analyses (i.e., used as a descriptive covariate, confounder, or matching criteria; range 64-84%; Appendix Table 5). Only four studies across stratum used race and/or ethnicity as an exclusion criterion, two of which restricted analysis to solely White participants. In 10-25% of studies across stratum, race and/or ethnicity were “of interest” (e.g., specific group comparisons, effect measure modification, or predictive variable). Question 5: Justification. Approximately 30% of the 149 studies across strata which included participants’ racial and/or ethnic data provided a justification for at least one of their decisions surrounding race and/or ethnicity (e.g., the relevance of race and/or ethnicity to the study question, choice of measure, generation of coding scheme, and why an analytical approach or use of the variable was appropriate; data not shown). No studies provided justifications for the selection of a particular measure (e.g., selection of close-ended, self-report question over an open-ended, self-report question). Three studies referenced National Institutes of Health or other institutional guidelines with respect to decisions making on measurement and coding. As in Castro et al. (2014), authors explained “race was assessed by participant self-report, using National Institutes of Health race/ethnicity reporting standards and categories” (p.2085-2086) (33). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 11 DISCUSSION We systematically review methodological practices regarding the conceptualization, operationalization, and use of race and ethnicity in U.S. medical research published in prominent journals between 1995-2018. We found that information specific to race and ethnicity was routinely, if not systematically, absent from articles. While inclusion of racial and ethnic data has increased since 1995, no studies defined either construct and most did not describe how race and/or ethnicity was measured. Occasionally, the coding schemes of racial and ethnic variables were even omitted. Most studies across time periods did not provide scientific justification for their choices with respect to race and/or ethnicity. Scientific rigor relies on replication and validation, which is rendered impossible if core methodological decisions are not clearly communicated. Core methodology includes information on definitions, measurement, and coding of variables, as well as scientific rationale. Absence of such information may impact interpretation of findings or their translation into interventions, especially when it is unclear who is under study and why. Lack of basic information on methodology threatens our ability to conduct responsible and rigorous science. Scientific and cultural racism Journal word limits provide a potential structural explanation for lack of clarity regarding race and ethnicity. Descriptions of methodological choices regarding race and ethnicity may compete with information on foundational literature, study design, exposure, outcome, results, or interpretations for inclusion. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 12 The absence of information could also reflect a misguided belief in the presumed universality of race: that what race is and is not, the number of racial groups, boundaries between racial groups, and the “scientific relevance” of race to medical research are invariably understood. If race and ethnicity are universally understood across temporal, socio-cultural, and geopolitical contexts, then “race” does not need explanation or justification. Race, however, is not universal. Rather, what “race” is, the number of and boundaries between “racial groups,” and mechanisms by which the multilevel system of racism operates are deeply contextual. A large body of literature has theorized on how the social construction of racial and ethnic categories is historically situated and changes over time and place (14, 34-38). The U.S., for example, is a nation explicitly designed to prioritize the life chances of a single group of people. As a settler-colonial state which achieved global financial power through slave labor and imperialism, the structures which continue to support the political, financial, judiciary, and educational systems maintain a hierarchical status quo based on established racial groups (39). Racism may be globally pervasive, but the structure of the system and the experience of living within it is different in the U.S. than it is in Mexico, Brazil, South Africa, India, or any other country. Combatting scientific racism in medicine, in part, requires naming the methodological assumptions behind the treatment of race and/or ethnicity in medical research. For over 150 years, medicine as a discipline actively reified the biological essentialist definition of race - that perceived behavioral and health differences between “racial groups” were true, immutable, and inherent to an individual's genetic makeup (40). By routinely justifying biological essentialism with pseudoscientific evidence, race All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 13 became “common sense” and perceived as part of the natural world (40). This idea has so deeply infiltrated scientific institutions and thought, that it remains present today despite the scientific process demonstrating the falsity of these claims. Within these structures, medical research in the U.S. has historically adhered to practices which harm subordinated groups as research subjects while the knowledge produced by these research practices most benefits those of the dominant group (41, 42). This practice contributed to the current state, in which racial and ethnic minorities are often systematically excluded in medicine, as both research participants and researchers (43- 47). Thus, medical knowledge is predicated on only some bodies, cultures, and experiences (48, 49). The lack of diverse perspectives contributes to the perpetuation of unconscious bias and racist practices in medicine. Institutions and structure Over the years, journals and other institutions have developed communication guidelines around race and ethnicity. The International Committee of Medical Journal Editors (ICMJE) developed two such recommendations in 2004, namely that 1) the inclusion of racial and ethnic data is explicitly motivated and 2) the measurement of race and ethnicity is clearly explained (50, 51). All of the journals sampled in our study aim to follow the standards set forth by ICMJE (52). However, for U.S.-based human subjects research published in these journals, adherence appears limited. After 2004, most studies still did not include information on how race and/or ethnicity was measured. Even considering the possibility of a lag between the release of new standards and the publishing of articles All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 14 following those standards, adherence is low. Furthermore, few articles included justifications for decisions surrounding race and ethnicity. Editors and specific medical journals have further echoed and elaborated upon the ICMJE recommendations. Following the 2004 update, former JAMA Deputy Editor Dr. Margaret Winker introduced expanded ICMJE guidance specifically for network journals, calling for authors to provide details on (1) who assessed an individual's race, (2) whether self-designation options were “open” or “closed,” (3) what the closed self- designation categories were, (4) if and how closed self-designation categories were combined, and (5) the rationale or relevance of race and ethnicity to a particular study (53). In supplemental analyses, there is minimal evidence of adherence to these additional higher standards among sampled JAMA articles (Appendix 3). Recently, the AMA has released more explicit policies (54, 55). Actions for improvement Previous work in medicine and adjacent disciplines has provided suggestions for improvement (23, 56-59). We build on this work by calling for clear communication of these improved practices, including definitions, measurement, coding, use, and justifications. This is not a radical position. We simply argue that race and ethnicity should be given the same interrogation and justification as other variables, and that this be clearly communicated in publication. We urge health researchers to follow existing guidelines and implore medical journals and editors to implement mechanisms for accountability to these standards. For example, authors could be prompted to certify at submission that they have adhered to ICMJE or AMA guidelines. At the peer-review level, additional training could be All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 15 implemented to ensure that reviewers are confident in recognizing whether a manuscript meets criteria. We further encourage out of the box thinking to overcome structures; for example, pertinent details on race and ethnicity could be without word count, similar to human subjects statements or acknowledgements. Conducting annual reviews of policy adherence across medical journals could ensure that baseline benchmarks are being met. Responsibility for meeting disciplinary standards of research falls on both medical journals and authors, as both are ultimately in service of patients and study participants. As “key players in the production of knowledge” (p.1288) and gatekeepers of research dissemination, editors and medical journals are in a unique position to ensure adherence to stringent scientific communication norms (60). In particular, prominent medical journals, by setting and requiring adherence to guidelines on clear communication, may influence disciplinary-wide standards. For authors, meeting these standards may require critical thought and conscious decoupling from earlier norms of conducting and reporting race and ethnicity in medical research. Limitations The abstraction from sampled articles is imperfect. The data retain a degree of subjectivity, despite protocols to standardize data entry and data quality checks. This is perhaps most true for the data on scientific justifications. Data abstractors were instructed to be as broad as possible when collecting information on justifications, thus data may be an overestimate of articles which included at least one justification. Second, it is possible that recent attention to addressing racism and ethnocentrism broadly has resulted in a renewed effort to “do a better job.” Subsequently, All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 16 methodological practices and the communication thereof may have substantially shifted between Jan 1, 2019 and today. Finally, we did not review supplementary materials. If information on definitions, measurement, coding, or scientific justifications was included in supplements, they were missed. Conclusion Interventions aimed at addressing racism as a fundamental cause of disease in the U.S. must be based on unassailable research achieved through strict methodological rigor. Quality science enables knowledge democracy and health equity by providing a strong evidence base for changes in medical practice and policy. Dismantling systematic oppression in medicine requires clear, critical, and honest communication around the use of race and ethnicity data in medicine. Collectively, the health research community needs to hold each other accountable to continue improving how race and ethnicity are conceptualized, operationalized, and utilized in medical research. This should be one element in a holistic, multipronged approach to addressing racism and health inequity which also centers additional systems reforms. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 17 CONTRIBUTORS RAMM conceived of the study and directed its implementation, including quality assurance and control. All authors contributed to study design and data acquisition. Authors RAMM, NRS, and PNZ conducted analyses. RAMM and REW wrote the primary draft; all other authors (NA, ANG, NRS, PNZ) contributed to further drafts and edits. All authors had full access to and verified the data. DECLARATION OF INTERESTS We declare no competing interests. DATA SHARING REDCap data entry form and full list of articles will be made available upon request with publication. Please contact the corresponding author for data inquiries. ACKNOWLEDGEMENTS We are thankful to Dr. Allison E. Aiello and Dr. Robert A. Hummer for their guidance and support. We are indebted to Denise Mitchell for their assistance in data collection. NRS contributed to this work while at the University of North Carolina and is now a postdoctoral fellow in the Harvard TH Chan School of Public Health Department of Social and Behavioral Sciences. FUNDING Funding was provided through training grants from the Eunice Kennedy Shriver National Institute of Child Health and Human Development [T32 HD091058] and the Department of All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 18 Sociology, UNC Chapel Hill. Carolina Population Center provided general support [P2C HD050924, P30 AG066615]. NRS received additional support from the National Cancer Institute [T32 CA057711]; PNZ received additional support from the National Institute of Allergy and Infectious Diseases [T32-AI007001]. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2022.03.07.22271661
doi: 
medRxiv preprint 19",1
"Frailty is a complex trait. Twin studies and a high-powered Genome Wide Association Study (GWAS) conducted in the UK Biobank have demonstrated a strong genetic basis of frailty. The present study utilized summary statistics from this GWAS to create and test the predictive power of frailty polygenic risk scores (PRS) in two independent samples – the Lothian Birth Cohort 1936 (LBC1936) and the English Longitudinal Study of Ageing (ELSA) aged 67-84 years. Multiple regression models were built to test the predictive power of frailty PRS at five time points. Frailty PRS significantly predicted frailty at all-time points in LBC1936 and ELSA, explaining 2.1% (β = 0.15, 95%CI, 0.085-0.21) and 1.6% (β = 0.14, 95%CI, 0.10-0.17) of the variance, respectively, at age ~68/~70 years (p < 0.001). This work demonstrates that frailty PRS can predict frailty in two independent cohorts, particularly at early ages (~68/~70). PRS have the potential to be valuable instruments for identifying those at risk for frailty and could be important for controlling for genetic confounders in epidemiological studies. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Frailty is a clinical state commonly associated with ageing and weakening in physiology and risk to stressors [1]. This deterioration often leads to poorer health outcomes in later life, including falls, long-term hospital stays, disability, and mortality [2]. Worldwide, the prevalence of frailty is ~16% in adults above 60 years old [3]; within the United Kingdom, the prevalence of frailty in over 60-year- olds was estimated to be ~6.5%, rising substantially for adults over 80 years old and estimated at ~65% for adults over 90 years old [4]. Given that between 2015 and 2030 the number of people aged 60 across the world is expected to grow from 901 million to 1.4 billion, frailty is now a recognised global health issue – as the population ages, the prevalence of frailty is predicted to rise [5]. Population ageing and the rise in age-related conditions, such as frailty, bring a necessity to use omics and data science to understand the aetiology and mechanisms influencing the development of frailty. Despite growing evidence that frailty is a public health issue, a universal definition or measurement is yet to be established for frailty [6]. Some studies view frailty as a physical condition that should be considered a medical condition/clinical state - measured, for example, with Fried’s frailty phenotype [7]. Others take a wider definition where frailty is characterised by a reduction in strength, endurance, cognitive and physiological function - measured, for example, with the Frailty Index [8] - all of which contribute to a decline in independent living and an increased risk of death [6]. The most recognised predictors of frailty are age and sex [9, 10]. Further predictors include: cognitive, physical, biological, lifestyle and environmental factors, social, sociodemographic and psychological factors [11, 12]. Such factors accumulate across the lifespan and during early life. Despite a multitude of risk factors being identified for frailty, the underlying mechanisms behind such risk factors and the development of frailty have yet to be fully understood, making prediction challenging/imprecise at an individual level [13, 14].  To refine the prediction of traits like frailty the genetic propensity for the manifestation of frailty must be explored. Like many human traits, Frailty Index is partly inherited; twin studies have found that genes explain 30-45% of trait variance [1]. Developments in molecular genetics allow for more refined prediction of complex traits, such as frailty. For example, Genome-Wide Association Studies (GWAS) explore genetic markers across the genomes of many different people to uncover genetic variations associated with diseases and traits [15] and explore how these are associated with other biological mechanisms. GWAS reveal that much of the genetic basis for many complex traits come from multiple small effects of thousands of variants [15, 16]. Such genetic associations can allow researchers and clinicians to develop methods to . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint detect, delay and prevent diseases and certain traits. Furthermore, given that frailty has been shown to be reversible [17, 18], polygenic risk scores (PRS) may not only reveal associations with  frailty status, but may also assist in identifying groups of higher risk individuals who could benefit from early intervention [19]. GWAS have only recently reached large enough sample sizes to uncover reliable and reproducible associations between genetic variants and complex polygenic traits. There have not been many high- powered GWAS studies examining frailty [15, 16]. The largest is a GWAS on 164,510 UK Biobank participants, aged between 60 and 70 years, in which 14 loci were found to be associated with the Frailty Index [15]. 13 of these loci were previously associated with diseases and traits such as depression, smoking, Body Mass Index (BMI), cardiovascular diseases, neuroticism, and Human Leukocyte Antigen Proteins (HLA). The Single Nucleotide Polymorphism (SNP)-based heritability for frailty, measured via the Frailty Index, was 11%, lower than twin-based estimates of 30-45%. However, this is unsurprising given that the SNP based estimate does not include rare genetic variation and structural genetic variation that is captured by twin and family modelling – SNPs index common genetic variation. This high-powered GWAS of frailty points towards genetic determinants linked to cardiovascular health, mental health, and brain functioning [15]. Thus, frailty is a highly polygenic trait, and GWAS are important to understand the underlying biology with potential to robustly predict frailty. Given the highly polygenic nature of frailty, one method that has become increasingly used to investigate the genetic propensity of diseases and traits is polygenic scoring [20]. This method utilises summary statistics from GWAS, which have examined the associations of millions of SNPs with phenotypes of interest, including in this case, frailty. Weightings (regression coefficients) are then taken for each SNP from GWAS data to create a polygenic risk score for genotyped individuals in an independent sample (participants who are not in the targeted GWAS). This PRS indicates the small cumulative effects contributing to a genetic risk or probability of a higher level of a particular disease or trait [20, 21]. There have been multiple updates in polygenic risk scoring methods to achieve efficient and generalizable results [21, 22]. PRS can identify individuals at high risk to a certain disease or trait, such as cardiovascular disease [20]. Thus, many researchers have advocated the potential for PRS to be instrumental biomarkers in identifying, predicting, and informing treatment in individuals [20, 21, and 22]. Considering the ageing population, as personalised genomics expands and robust data and methods are applied to . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint understand complex diseases and traits, PRS may be an objective tool in understanding and identifying complex traits like frailty. Furthermore, PRS are used to discriminate environmental from genetic sources of variability, allowing outcomes such as frailty to be better understood. It is clear that frailty is a multifaceted trait, influenced by many genetic determinants linked to various biological, physical, social, psychological, and environmental traits [15, 16, and 23]. Despite recent findings, there remains a dearth of genetic frailty research. Thus far, no studies have utilised the summary statistics from the most recent frailty GWAS [15] from the UK Biobank to compute polygenic risk scores in independent samples. This study addresses this gap in the literature by applying the Biobank GWAS summary statistics to two independent cohorts, LBC1936 and ELSA, at five different age bands within the range of 67 to 84. The scores will then be used in multiple linear regression models to predict frailty at five different time points. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Methods Prediction Samples 
 
LBC1936 The target data contains the genotypes (genome-wide SNPs) and phenotypic data from 1005 older adults in LBC1936. LBC1936 is an ongoing longitudinal study of older adults living in the community in Edinburgh and surrounding Lothian areas of Scotland, United Kingdom [24]. Individuals were initially recruited based on having been part of the Scottish Mental Survey (1947) and have thus far taken part in 5 waves of testing. The mean age at the first wave was 69.58 (SD = 0.83, n = 1091, 548 males), at the second wave was 72.54 (SD = 0.71, n=866, 448 male), at the third wave was 76.30 (SD = 0.68, n = 697, 360 male), at the fourth wave was 79.38 (SD 0.62, n= 550, 275 male) and at the fifth wave was 82.06 (SD = 0.53, n = 431, 209 male). Ethical permission was approved from the Multi- Centre Research Ethics Committee for Scotland (Wave 1: MREC/01/0/56), the Lothian Research Ethics Committee (Wave 1: LREC/2003/2/29), and the Scotland A Research Ethics Committee (Waves 2, 3, 4 and 5: 07/MRE00/58). Written consent was obtained from participants at each of the waves. The genotypes, collected via blood samples from the majority of participants at wave 1 were processed using stringent quality control measures [25]. ELSA For ELSA, the target data contained polygenic risk scores and phenotypic data from 5448 adults aged between 67 and 84 (from 9 waves/data collection points in ELSA) living in the community in England [26]. To mirror the format of the LBC1936 waves, the ELSA data was split into five groups based on the same age bands. The mean age in the first group (mirror of LBC1936 Wave 1) was 68.44 (SD = 1.10, n = 3983, 1851 male), in the second group (mirror of LBC1936 Wave 2) was 72.4 (SD = 1.09, n = 3491, 1605 male), in the third group (mirror of LBC1936 Wave 3) was 76.37 (SD = 1.09, n = 2727, 1247 male) , in the fourth group (mirror of LBC1936 Wave 4) was 79.95 (SD = 0.76, n = 2020, 874 male), and in the fifth group (mirror of LBC1936 Wave 5) was 82.88 (SD = 0.76, n = 1495, 619 male) .  Unlike LBC1936, samples in ELSA are refreshed with new participants; therefore, the age groups created for ELSA contained longitudinal participants (with more than one frailty measure across groups) and participants with just one measure in one group. Within the ELSA age groups (1-5), which were created to mirror the format of LBC1936 and validate the PRS prediction, there are individuals who have been tested at different times (from ELSA waves 1-9, which spans from 2002/3 to 2018/19). Supplementary tables S1-S5 show the number of participants, and frailty index means and standard . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint deviations in each age group (1-5) by ELSA wave (1-9), supplementary table S6 shows how many participants have longitudinal measures across the five groups. Ethics for ELSA have been approved via the South Central – Berkshire Research Ethics Committee (21/SC/0030, 22nd March 2021). Polygenic risk score data was acquired through request to the ELSA genetics team and the phenotypic data was curated at the Advanced Care Research Centre from ELSA data available on the UK data service. Predictors 
Discovery sample GWAS summary statistics for the largest GWAS to date on frailty were sourced from the European Bioinformatics Institute (EBI) GWAS catalogue [27]. The GWAS included 164,610 individuals (48.5% male) from the UK Biobank aged between 60 and 70 years old. The researchers used the Frailty Index based on 49 self-reported items on a range of physical, social, cognitive, and biological characteristics alongside disabilities and diagnoses. Quality control was carried out on the GWAS summary statistics. Initial steps included ensuring that the effect and non-effect allele were known to ensure the direction of effect is in the correct direction. Any duplicate and ambiguous SNPs were removed. Outcomes  
Phenotypic data LBC1936 Frailty is measured by the Frailty Index, previously constructed in the LBC dataset (28). The Frailty Index in the LBC1936 constitutes 30 deficits, including physical, biological, social, psychological, and cognitive deficits, consistent with the Frailty Index in previous research [8]. Deficits were either dichotomised as either 0 (absent) or 1 (present); in some cases, 0.5 was used to represent a partially present deficit or were on a continuous scale (e.g. walking time) on a scale ranging from 0 to 1. For each individual, the number of deficits present was summed and divided by the total number of deficits (30). Scores ranged from 0 to 1 – with higher scores indicating higher frailty. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint ELSA The Frailty Index in ELSA was derived from the ELSA dataset following previous work [16]. The index contained 62 deficits and a participant would ascertain a frailty score if data were available for 30 (similar, if not the same deficits as LBC1936) out of the 62 deficits. Despite there being more deficits in the Frailty Index used in ELSA than LBC1936, guidelines indicate that as long as a minimum of 30 deficits are used to cover the relevant domains (disability, disease, cognitive functioning) then differences between number of deficits should not be an issue[8].  Due to skewness in the data, the frailty index variable was transformed using a square root transformation. Further, the index was calculated in the same way as the LBC1936 index; both LBC1936 and ELSA followed the same guidelines when creating the index [8]. For LBC1936 and ELSA frailty index scores were standardised to allow comparisons when interpreting the results. Covariates 
Variation in frailty caused by age and sex (the strongest frailty predictors) were controlled in analysis. Four ancestry principal components for LBC1936 and 10 ancestry principal components for ELSA were also included as covariates to account for population stratification, that is, systematic genetic differences due to ancestry differences – both LBC1936 and ELSA only included participants with European ancestry in genotyping. Polygenic risk scores 
 
LBC1936 
Using the summary statistics from the frailty GWAS [15] (the base data) and the LBC1936 raw genotype and phenotype data (the target data), PRS were created for 1005 individuals at multiple p- value thresholds (PT). Quality control processing was done using the R package QCGWAS and PRS were derived using PRSice (version 2) polygenic software [29, 30]. ELSA PRS for 7223 individuals were provided by the ELSA genetics team and were calculated on genotyped data at multiple PT to match the thresholds selected in LBC1936. PRS calculation methods and quality control methods in ELSA can be found in the documentation report [31]. When joined with the phenotypic data 5448 ELSA individuals remained for downstream analysis. Statistical analysis . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Data preparation details can be found in supplementary materials. Prediction analyses were run using the PRSice 2 software and R studio in R version 2022.7.1.554 [32]. Polygenic risk scores for individuals were calculated with P value thresholds optimised to select the best fitting PRS – the p-value thresholds were between 0.01, 0.05, 0.1, 0.3 and 1. That is, multiple PRS were calculated that included SNPs with association p-values less than the specified threshold and the most predictive of these was retained (93579 SNPs under p-value threshold 1 in LBC1936 and 1349585 SNPs under p-value threshold 1 in ELSA).  Clumping was performed to thin SNPs according to the linkage disequilibrium (how correlated SNPs close together are) and p-value. The SNP with the smallest p-value in every 250kb frame was retained, and all SNPs having r2 > 0.1 were removed from further analysis. To explore genetic correlation, multiple linear regression models were built on the PRS against the frailty phenotype and adjusted for age, sex and ancestry principal components. As the PRS were constructed in PRSice for LBC1936, the regression models were also run by the program but built separately in R for the ELSA analysis. A multiple linear regression was first performed between the covariates (sex, age, and ancestry principal components) and the phenotype (Frailty Index), and constitutes the null model. The PRS is then added as a predictor in the model and re-run (i.e., the full model).  The variance explained by the PRS (PRS R2) was calculated by subtracting the R2 of the null model from that of the full model. ELSA groups were mixed across data collection time points; for example, anybody with a Frailty Index aged 67-70 in group 1 could come from any of the 9 waves of ELSA data collection which can span 16 years. Due to this sampling structure, sensitivity analyses were performed to control for potential cohort effects. This consisted of creating dummy variables for each ELSA wave and adding them as covariates to the regression models. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Results The mean scores for frailty and age, at each time point, in the LBC1936 and ELSA are shown in Tables 1 and 2. As expected, mean frailty scores increase with age across the waves. The pairwise correlations of frailty between the waves confirmed that frailty is relatively stable over time (see supplementary tables S7 and S8). Correlations were at a minimum moderate, > 0.5 correlation, and most were strong, Pearson’s r > 0.7. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Table 1. Descriptive statistics for age and raw Frailty Index at each wave in LBC1936 N 
Mean 
SD 
Range Wave 1 Frailty Index Age Wave 2 Frailty Index Age Wave 3 Frailty Index Age Wave 4 Frailty Index Age Wave 5 Frailty Index Age 1005 1005 866 866 697 697 550 550 431 431 .16 69.58 0.18 72.54 .20 76.30 0.21 79.38 0.22 82.06 0.09 0.83 0.09 0.71 0.09 0.68 0.09 0.62 0.09 0.54 0 – 0.49 67.66 – 71.35 0 – 0.55 71.96 – 74.21 0.02 – 0.65 74.64 – 77.75 0.03 – 0.63 78.01 – 80.93 0.03 – 0.58 80.98 – 83.19 Table 2. Descriptive statistics for age and raw Frailty Index at each wave in ELSA N 
Mean 
SD 
Range Group 1 Frailty Index Age Group 2 Frailty Index Age Group 3 Frailty Index Age Group 4 Frailty Index Age Group 5 Frailty Index Age 3983 3983 3491 3491 2727 2727 2020 2020 1495 1495 .14 68.44 0.15 72.4 .18 76.37 0.20 79.95 0.22 82.88 0.10 1.10 0.10 1.09 0.11 1.09 0.12 0.76 0.12 0.76 0 – 0.68 67.00 – 70.00 0 .007 – 0.66 71.00 – 74.00 0.007 – 0.69 75.00– 78.00 0.01 – 0.79 79.00 – 81.00 0.01 – 0.74 82.00 – 84.00 Figure 1 compares the predictive power of the differing PT in LBC1936, with the bar explaining the most variance representing the best fit. Figure 2 similarly demonstrates the significant relationship at various PT in ELSA. The multiple regression model outputs for each time point can be found in Tables 3 and 4. LBC1936 At Wave 1, the optimal PT was 0.3, demonstrating the best prediction between frailty PRS and frailty (p < .001). As Table 3 shows, the PRS explains 2.1% of variation in frailty within LBC1936 at aged ~70 and is based on 49325 SNPs. At Wave 2, the optimal PT was 0.1 (p < .001) explaining 1.9% of variation in frailty at ~73 years and is based on 24171 SNPs.  At Wave 3, the optimal PT was 0.3 (p < .01), explaining 1.4% of variation in frailty at ~76 years.  At Wave 4, the optimal PT was 0.3 (p < .05), with the PRS explaining 1.3% of variation in frailty at ~79 years. At Wave 5, the optimal was 0.1, p < 0.1, and the PRS at age ~82 explains 1.7% of variation in frailty. ELSA For Group 1, the optimal PT was 1, demonstrating the best prediction between frailty PRS and frailty (p < .001) explaining 1.6% of variation in frailty at aged ~68 and based on 1349,585 SNPs. For Group 2, the optimal PT was also 1, with the best prediction between frailty PRS and frailty (p < .001) explaining 1.6% of variation in frailty at ~73 years. For Group 3, the optimal PT was again 1 (p < .01), explaining 1.2% of variation in frailty at ~76 years. For Group 4 the optimal PT was 0.3 (p < .05), with the PRS at this wave explaining 1.1% of variation in frailty at ~79 years and is based on 486,667 SNPs – thus far following similar trends to the variance explained in LBC1936. Finally for Group 5, the optimal PT was 0.1, p < 0.1. The PRS explains 0.2% of variation in frailty and is based on 204,805 SNPs. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint The findings between LBC1936 and ELSA, apart from at the final time point at Wave/Group 5, were consistent. Figure 3 shows that the standardised coefficients overlap across the waves/groups at each time point across the cohorts, even when the variance drops at the last time point in ELSA – Wave 1 LBC1936 (β = 0.15, 95%CI, 0.085-0.21) and Group 1 ELSA (β = 0.14, 95%CI, 0.10-0.17); Wave 5 LBC1936 (β = 0.13, 95%CI, 0.034-0.23) and Group 5 ELSA (β = 0.06, 95%CI, 0.016-0.10). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Figure 1. Multiple bar plots, from the multiple regression models, showing the optimal p-value thresholds when predicting frailty using PRS at five time points the LBC1936. The x axis displays the varying different p-value threshold levels. The y axis displays the variance explained by the PGS. The values above the bar are the p-values from the regression output.  The darker and taller the bar the stronger the prediction of the frailty PGS. Predicting frailty using PRS with LBC1936 age ~76 
 Predicting frailty using PRS with LBC1936 age ~79 Predicting frailty using PRS with LBC1936 age ~82 Predicting frailty using PRS with LBC1936 age ~70 
 Predicting frailty using PRS with LBC1936 age ~73 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Figure 2. Multiple bar plots, from the multiple regression models, showing the optimal p-value thresholds when predicting frailty using PGS at five time points the ELSA. The x axis displays the varying different p-value threshold levels. The y axis displays the variance explained by the PGS. The values above the bar are the p-values from the regression output.  The darker and taller the bar the stronger the prediction of the frailty PRS. Predicting frailty using PRS with ELSA age ~68 
 Predicting frailty using PRS with ELSA age ~72 Predicting frailty using PRS with ELSA age ~76 
 Predicting frailty using PRS with ELSA age ~80 Predicting frailty using PRS with ELSA age ~83 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Table 3. Results of multiple linear regression analyses showing associations between the optimal frailty PRS and the Frailty Index in the LBC1936. Multiple Linear Regression  
β 
SE 
p 
PRS R2 Frailty PRS at  ~70 Frailty PRS at  ~73 Frailty PRS at  ~76 Frailty PRS at  ~79 Frailty PRS at  ~82 .15 .14 .11 .11 .13 0.03 0.04 0.04 0.04 0.04 <.001 <.001 <.01 <.01 <.01 .021 .018 .014 .013 .017 All analyses controlled for sex and age and population stratification. Table 4. Results of multiple linear regression analyses showing associations between the optimal frailty PRS and the Frailty Index in the English Longitudinal Study of Ageing Multiple Linear Regression  
β 
SE 
p 
PRS R2 Frailty PRS at ~68 Frailty PRS at ~72 Frailty PRS at ~76 Frailty PRS at ~80 Frailty PRS at ~83 .14 .14 .12 .10 .06 0.01 0.01 0.01 0.02 0.02 <.001 <.001 <.001 <.001 <.05 .018 .018 .013 .013 .004 All analyses controlled for sex and age, population stratification and ELSA wave. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Figure 3: A bar plot comparing the standardized coefficients from the most predictive model at each time point in LBC1936 and ELSA. Error bars represent 95% confidence intervals. The darker the bar the stronger the effect size. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint Discussion This present study used summary statistics from the most recent and highly powered GWAS on frailty to compute polygenic risk scores (PRS) in an independent sample of community-dwelling older Scottish adults and a nationally representative sample of older adults living in England.  Frailty PRS significantly predicted frailty at all five time-points, accounting for the most variance at Wave 1/Age Group 1 (when LBC1936 participants were ~70 years old and ELSA participants were ~68 years old); the proportion of variance explained decreased, albeit minimally, across the waves. At the final Wave/Age Group (~age 82 in LBC 1936 and ~age 83 in ELSA) the variance explained by frailty PRS had a greater discrepancy between the two cohorts when compared to other ages. Although frailty PRS predicted frailty at all ages, it was most predictive when adults were younger at ~68/70 years old. This study adds further evidence to the role of genetics in the development of frailty and demonstrates that PRS for frailty can significantly predict frailty outcomes. Regarding the specific findings, it may have been expected that frailty PRS would explain more variance in adults in the later waves who are approaching and beyond 80 years old given the prevalence of frailty is higher at these ages [4]. But previous research shows that genetic effects on cognitive aging decrease in later life [33], so it is possible that whereas frailty phenotypic variance increases with age its genetic variance decreases. Our finding that the frailty PRS explained the most variance at the earliest age, when participants were ~68/70 years, supports such an interpretation. An alternative interpretation of this finding relates to characteristics of the base data (GWAS on frailty) which was performed on adults aged 60 to 70 in the UK Biobank [15]. The age characteristics of the base data were most aligned with the first time point in LBC1936 and ELSA where the mean was ~68/70 years old. In terms of Frailty Indexing – frailty in the GWAS and at Wave/Age Group 1 in the LBC1936 and ELSA was at a similar mean level. Frailty measured in the GWAS had not yet progressed . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint to higher levels and may not be the most accurate GWAS to use when predicting frailty in older adults, who are at most risk of being frail [2, 4]. Unless there is presence of a chronic condition, frailty would not be screened for at or before age 70 [6]; therefore, measuring frailty between 60-70 years old, as the GWAS did, may limit the utility of the findings. Nonetheless, it still offered significant prediction in our samples across the range of 67 to 84 years. To understand the findings further, it is useful to understand the frailty score in the GWAS and how that may impact the findings. The Frailty Index mean in the UK Biobank GWAS was 0.12, which most closely resembles the mean frailty levels of .16 and .14 in the respective LBC1936 Wave 1 and ELSA Age Group 1 cohorts. It is possible that frailty measured in a younger group (such as the base data in the UK Biobank) may not necessarily resemble frailty measured at older ages. The example of BMI/weight loss is a good way to illustrate that frailty can mean different things at different ages. In middle age, a relatively lower weight is usually indicative of good health, however in older age a much lower relative weight is indicative of frailty/sarcopenia/poor health [34, 35] – these age- moderated nuances may not be picked up well in the current GWAS-PRS data. The current study would benefit from research comparing the Frailty Index at an item level with younger and older ages. This would allow researchers to explore the different genetic correlations in frailty at younger ages and at older ages and investigate whether frailty at older ages is tapping the same measure as frailty at younger ages. Alongside exploring the influence of frailty PRS at increasing ages, the current study has several strengths. The analysis was firstly conducted in LBC1936 and then validated in ELSA. LBC1936 is a unique cohort and is not as representative of the British population when compared to ELSA, thus replicating the analysis with the ELSA cohort increased the validity and generalisability of the findings. Further, the study used p-value thresholding to find the most predictive threshold when creating the polygenic risk scores. Thus, we ensured that the most predictive PRS was used when . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint modelling frailty. The frailty measure in the GWAS and the Frailty Index in both LBC1936 and ELSA used similar indices to create the Frailty Index. If the base and target measures are too dissimilar this can be an issue as it weakens the maximal variance in the target measure that can be explained by the PRS [19, 20]. Despite such strengths, the results should be contextualised within various confines. Firstly, the base data, as previously mentioned, represented frailty at a very early/mild stage [36]. Thus, due to the narrow age examined, the polygenic prediction of frailty at later ages will be biased. Furthermore, participants within the UK Biobank, in which the base data consisted of, are less likely to be obese or inactive, smoke, have lower educational attainment and have fewer health conditions when compared to the general population – the 'healthy volunteer' effect [37]. LBC1936 and ELSA are also volunteer studies and are vulnerable to the healthy volunteer effect.  The Frailty Index in the base data, LBC1936 and ELSA at ~70 represented individuals who would be classified as fit or pre-frail – consistent with the idea of healthy participation selection bias [38, 39].  Despite similarities between ELSA and the LBC1936, we imposed group categories in ELSA to represent waves (time points) to mirror the LBC1936 ones. This is a limitation as LBC1936 and ELSA differ in recruitment methods – unlike LBC1936, ELSA recruits and introduces new participants at new time points – this addition of new cohort members (and sampling variation) could explain the lack of variance explained at the last time point in ELSA. Another issue is the reduced sample size from Wave/Age Group 1 to Wave/Age Group 5, and that individuals who remained in the subsequent waves/older ages were healthier on average than those who did not remain in the cohorts [38, 39]. Thus, opportunities were likely missed to study individuals with the highest levels of frailty - this may have been another reason why predictive power decreased at the later waves.  In following the GWAS analysis, we deliberately fitted only three covariates (sex, age, and ancestry principal components) in the model given that the frailty PRS is known to genetically correlate with a range of variables which may represent the constituents of frailty; by including them as confounders we would reduce PRS prediction. Future . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint studies could include such covariates alongside their associated PRS to understand if any unique variance in frailty PRS remains following multiple adjustment. Future studies should continue polygenic prediction of frailty but with more power and refinement to address the challenge of population ageing and support those in and approaching later life. To address the issue of the base data having a narrow age range of 60-70 years, GWAS samples from population representative adults 80+ is needed. This would maximise PRS prediction and allow for more refined identification of those at risk to frailty. This will be challenging as such a GWAS would need to be highly powered with many participants which can prove difficult when attempting to circumvent healthy selection to collect data on older adults, and likely necessitates a meta-analysis approach. A GWAS study conducted on a different measure of frailty - the frailty phenotype – is due to be published [40]. The frailty phenotype takes a physiological approach to measuring frailty, measuring five physical systems weight loss, exhaustion, weakness, slowness when walking and low levels of physical activity [7]. Unlike the Frailty Index it does not feature cognitive elements of frailty and there is a moderate correlation (R = 0.65) between the two [6]. Nonetheless, a future study could test the utility of this PRS in both LBC1936 and ELSA who have measures of the frailty phenotype. Lastly, the models built in this study contained single polygenic predictors. Despite us showing that single polygenic prediction can be informative when predicting traits like frailty, due to the complexity of frailty, it would add even more value to use a more novel approach such as a multi- polygenic method (MPS) [41]. MPS would increase predictive power via exploiting the combined power of multiple PRS. For example, summary statistics of high-powered GWAS on traits associated with frailty, such as cardiovascular diseases, BMI, and diabetes, could be used to create multiple polygenic risk scores and examine how they perform together in a prediction model to explore the complex nature of frailty. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint The present findings have several implications. The summary statistics from the largest GWAS on frailty to date can significantly predict frailty in independent cohorts at multiple time points. This is a starting step in research with complex traits, such as frailty, in utilising genetic data to refine prediction. Polygenic risk scores have the potential to be efficient instruments in detecting genetic liability to a disease or trait and identify these individuals at a point before frailty progresses – a point at which interventions would be effective (12,17). Furthermore, this study contributes to the small body of research exploring genetic predictors of frailty. These findings support the notion that genetics play an important role in the development of ageing conditions such as frailty and the magnitude of the effect sizes found here are similar to those reported for environmental predictors’ frailty and cognitive ageing [42, 43]. As we tested the associations at five different time points from ages ~67 to ~84, this study also supports the importance of an accurate measurement of the Frailty Index – one in which frailty has had the opportunity to progress in both the base and target samples, as this will allow for more refined prediction.  Future work could include a high-powered GWAS on advanced frailty, high-powered independent target samples to test frailty PRS in and applying novel methods to utilise multiple PRS in the prediction of frailty. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288064
doi: 
medRxiv preprint",1
"Under plantar loading regimes,
it is accepted that both pressure and shear strain
biomechanically contribute to formation and deterioration of diabetic foot ulceration (DFU).
Plantar foot strain characteristics in the at-risk diabetic foot are little researched due to lack
of measurement devices. Plantar pressure comparatively, is widely quantified and used in
the characterisation of diabetic foot ulceration risk, with a range of clinically implemented
pressure measurement devices on the market. With the development of novel strain quantification
methods in its infancy, feasibility testing and validation of these measurement devices for use
is required. Initial studies centre on normal walking speed, reflecting common activities of
daily living, but evaluating response to differing gait loading regimes is needed to support the
use of such technologies for potential clinical translation. This study evaluates the effects of
speed and inclination on stance time, strain location and strain response using a low-cost novel
strain measurement insole. The STrain Analysis and Mapping of the Plantar Aspect (STAMPS)
insole has been developed, and feasibility tested under self-selected normal walking speeds to
characterise plantar foot strain, with testing beyond this limited regime required. A treadmill was
implemented to standardise speed and inclination for a range of daily plantar loading conditions.
A small cohort, comprising of five non-diabetic participants, were examined at slow (0.75 m/s),
normal (1.25 m/s) and brisk (2 m/s) walking speeds and normal speed at inclination (10%
gradient). Plantar strain active regions were seen to increase with increasing speed across all
participants. With inclination, it was seen that strain active regions reduce in the hindfoot and
show a tendency to forefoot with discretionary changes to strain seen. Stance time decreases
with increasing speed, as expected, with reduced stance time with inclination. Comparison of
the strain response and stance time should be considered when evaluating foot biomechanics in
diabetic populations to assess strain time interval effects. This study supports the evaluation of
the STAMPS insole to successfully track strain changes under differing plantar loading conditions
and warrants further investigation of healthy and diabetic cohorts to assess the implications for
use as a risk assessment tool for DFU. 1 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Crossland et al. Keywords: diabetes, shear, strain, plantar, digital image correlation 1
INTRODUCTION The global diabetic population has increased significantly in recent decades with growth predicted to
continue (International Diabetes Federation, 2019). With this comes a rise in the associated development of
diabetic foot disease. From this population it is expected up to 25% will develop diabetic foot ulceration
(DFU) within their lifetime (Armstrong et al., 2017). The associated healing times and treatment pathway
requirements for DFU lead to a labour and cost intensive process with over £900 million spent annually in
the UK market alone (Kerr et al., 2019), which is neither beneficial to the patient or healthcare provider.
Prophylactic intervention is fundamental to reducing DFU rates, but is often unsupported in clinical practice
due in part to poor evidence base and cost to implement across the at-risk diabetic population (Heuch
and Streak Gomersall, 2016; Kerr et al., 2019; Bus et al., 2020). The current evidence base for orthotic
intervention is focused on pressure as a predictor of ulceration risk to inform offloading (Bus et al., 2020).
This has centered the development of diabetic foot risk assessment tools to solely focus on pressure. While
elevated and sustained plantar pressures in DFU are well researched, there is often discrepancy between
ulcer location and the peak plantar pressure site (Lavery et al., 2003). Shear stress on the foot is thought in
part to contribute to this deviation in expected location (Jones et al., 2022), but remains little understood
and is not measured in risk assessment of the diabetic foot due to the poor availability of measurement
tools. The complexities seen in the feet of people with diabetes leads to a requirement of bespoke treatment
approaches. This in turn drives the development of objective risk assessment tools that allow quantifiable
metrics of the at-risk diabetic foot and allow for earlier prophylactic interventions to reduce DFU formation
risk and work towards preventing long term escalation of treatment costs (Bus et al., 2020). Current
approaches to quantify shear at the plantar surface utilise a wide range of technologies including capacitive
sensors and strain gauges (Rajala and Lekkala, 2014), but have not established a clinically viable tool
(Yavuz et al., 2007; Jones et al., 2022). With a gulf in technology addressing both the pressure and shear
components of plantar load. Whilst pressure time integral is considered alongside peak plantar pressure and average pressure in
assessing DFU risk (Keijsers et al., 2010; Waaijman and Bus, 2012; Bus and Waaijman, 2013), the
contribution of shear strain time integral remains unclear, due to the limited systems available to measure
strain in lieu of shear forces and no current clinically utilised techniques for data collection. Yavuz et al.
(2008) employed a custom built sensor platform to measure normal and tangential forces simultaneously
of the unshod foot during stance phase to derive pressure and shear time integrals for a diabetic and
non-diabetic cohort. This showed by an increase in both time integrals for the diabetic population and led
to calls for further investigation of temporal strain responses. The current pressure data capture techniques are divided into two distinct focuses of shod or unshod
measures. Whilst unshod measures can give an understanding of intrinsic pressures due to anatomical
variances and gait deviations, they do not reflect the activities of daily living where footwear is worn.
However, in clinic these pressure devices, including pressure plates (Abdul Razak et al., 2012), offer a
convenient method of data capture with which to inform orthoses design. Shod pressure data allows data to
be collected during these activities of daily living to provide a representative understanding of the pressure
events acting upon the diabetic foot. Technologies including as pedar® [Novel GmbH, Munchen Germany]
pressure measurement insoles are currently used in clinical and research settings to achieve shod pressure Frontiers
2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. data collection. Recent trends include the emerging market of pressure reporting insoles offering real-time
feedback to inform user behaviour and minimise DFU risk(Chatwin et al., 2021). For both of these methods,
the cost, initial set-up, calibration requirements and training are prohibitive factors to their implementation
in a clinical environment. The shod environment also presents influential factors which may instigate the formation of ulceration
due to pressure and shear events leading to mechanical tissue stress (Lord and Hosein, 2000). The interfaces
between the foot, sock and shoe must be considered in this instance, alongside the pressure changes
brought about by the footwear design and the influence on tissue stress (van Netten et al., 2018). To begin
to understand the effect on differing loading regimes to the plantar aspect of the foot within the shod
environment, controlled speed and inclination trials have been employed (Segal et al., 2004; Kernozek
et al., 1996; Warren et al., 2004; Ho et al., 2010) using the pedar® pressure measurement insole. This
method allows for a benchmark to be provided, allowing reporting of patterns in pressure deviation with
changing speeds that reflect activities of daily living. Current clinical pressure measurements systems, such as pedar® [Novel GmbH, Munchen Germany],
provide the functionality to monitor pressure response changes under differing loading regimes in the
feet of people with diabetes. Recognition of the need to assess the plantar aspect during functional gait
is seen with use of technologies such as pedar® and should form a basis for future monitoring method
requirements. This clinical need for a loading regime responsive assessment method drove the methodology
to analyse the STAMPS insole response (Crossland et al., 2023). The development of the STrain Analysis and Mapping of the Plantar Surface (STAMPS) insole by
Crossland et al. (2023) bridges these gaps in the literature by allowing for strain assessment as a surrogate
for the components of plantar load during gait. Digital image correlation (DIC), computer vision tracking
of changes to an applied stochastic speckle pattern (Michael A. et al., 2009), is used here to quantify the
cumulative effects of plantar loading in the form of strain imparted on a plastically deformable insole during
gait. Currently STAMPS has been optimised for functionality and feasibility tested at self-selected normal
walking speeds and without a gradient. This paper uses the STAMPS insole technique as a responsive tool
to evaluate changes in strain characteristics aligned to changes in walking speed and inclination, including
stance time, strain location and strain response. 2
MATERIALS AND METHODS 2.1
Study Protocol To provide a consistent achieved walking speed and inclination across all studies a Nordictrack C200
Treadmill was implemented for use. Trials were selected to be conducted at 0.75 m/s, 1.25 m/s and 2 m/s
speeds to reflect a slow, lower bound normal and brisk walking pace. These values align with conducted
treadmill trials to monitor pressure variance with speed during gait using pedar ® (Segal et al., 2004) and
also reflect the range of speeds that might be adopted in typical activities of daily living. Inclination was
set to a gradient of 10% reflecting a mid value condition selected by Ho et al. (2010). It was decided that
for the purpose of this study, the inclination trial would deviate from Ho et al. (2010) and be conducted at
the ’normal’ 1.25 m/s speed, to reflect the expected general gait reported in a slower population (Segal
et al., 2004), such as may be expected in the ageing diabetic population . Due to safety limitations, the
treadmill belt restricts starting at the target speed and instead provides an acceleration to reach this speed.
The treadmill acceleration profiles were collated using image analysis, recorded using a Nikon D5300 with Frontiers
3 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Figure 1. STAMPS insole layer view schematic showing standardised footwear utilised within the
participant study. AF-S Nikkor Lens (Nikon) , to track belt speed changes under the three speed conditions (Padulo et al.,
2014). 2.1.1
Insole Manufacture The STAMPS plastically deformable insoles were prepared following the protocol described previously
(Crossland et al., 2023). A commercial clay roller (CT-500, North Star Polaris) was used to provide a
targeted 5 mm thickness plasticine slab from which the insoles were cut to size requirements. Cross
patterned Nylon mesh was used to reinforce the base of the insole and provide a posterior tab for ease of
removal following use (Fig. 1). The optimised computer generated stochastic speckle (Correlated Solutions
Speckle Generator, v1.0.5), consisting of a 0.8 mm speckle with a 65% pattern density and 75% pattern
variation, was applied via a thin film, 180 µm, temporary tattoo (Silhoutte, USA) for the purpose of DIC.
The insoles were allowed to rest for a period of 24 hours minimum prior to use after moulding to allow
for any temporal hardening effects (Chijiiwa et al., 1981). The insoles were then stored at a controlled 15
°temperature prior to use, in line with Crossland et al. (2023) findings on storage and use optimisation for
ten step gait studies. 2.1.2
Participant Study To verify the proposed study protocol, a participant cohort was recruited. The aim of which was to assess
the ability of the STAMPS insole to effectively detect strain changes under differing loading regimes
through controlled speed and inclination trials. A five participant non-diabetic cohort was recruited and
provided consent, see Table 1. The University of Leeds Engineering and Physical Sciences joint Faculty
Research Ethics Committee granted ethics approval (LTMECH-005) for the study design. The study
assessed right foot stance phase loading solely, with each participant provided with a STAMPS insole
for the right footwear with a contra-lateral sham insole in the left footwear to reduce inconsistency in leg
length. Standardised neoprene footwear (Ninewells Boot, Chaneco LTD) were used for consistency across Frontiers
4 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Table 1. Participant characterisation data collated for speed and inclination treadmill study.
Participant
Gender
Height (m)
Weight (kg)
Age Range (Years)
Shoe Size (UK) 1
F
1.75
64.5
26-30
7
2
M
1.94
83.2
31-35
12
3
M
1.85
85.0
26-30
11
4
M
1.90
77.6
26-30
12
5
M
1.82
83.1
26-30
11 all participants. Participants were asked to walk for ten steps on the right foot during each trial, inline with
insole usability limits procured from insole optimisation (Crossland et al., 2023). Three repeats were taken
at each trialled speed and at inclination. Images were recorded of the STAMPS insole before and after
undertaking each trial and participants were recorded using an camera recording at 50 fps (Nikon D5300,
Nikon) to capture stance phase contact time. 2.2
Plantar Strain Analysis Commercially available DIC software (GOM Correlate 2019) was used for first stage post image DIC
analysis to allow for the generation as insole strain maps. Strains were determined relative to the reference
photo of the insole taken prior to each trial. For exportation of the data for post processing to derive
positional strain values, an equidistant spread of points at 6.5 mm intervals was applied to each insole. Post processing was conducted in MATLAB (R2021b) for implementation of custom scripts to improve
visualisation and allow for anatomical regional analysis of strain data. Pedar® [Novel GmbH, Munchen
Germany] used as a tool for risk assessing the diabetic foot, employs an Automask feature to divide the foot
by regions of anatomically significance for segmented analysis in areas of DFU prevalence (Pit’hov´
a et al.,
2007). Replicative masking across key anatomical landmarks was applied to the post-processed strain maps
and aligned anatomical by a qualified orthotist (SRC). A reductive masking approach was then used to
combine localised regions which would be difficult to distinguish clearly through assessment of the insole
imaging. A resulting eight region mask was applied to determine strain outputs (Fig. 2), covering: hallux,
second to fifth toes, first metatarsal head, second and third metatarsal head, fourth and fifth metatarsal
heads, lateral midfoot, medial midfoot and calcaneus. Average and peak strains across each segment were
determined. 3
RESULTS All trials were successfully completed for ten stance phases on the right foot by each participant. Fig.3
provides representative strain visualisation outputs from a single participant, showing the three repeated
trials under each loading regime. The figure shows regions identified as being strain active increase with
increasing speed, strain within the active regions also increases in line with the increasing speed. Fig. 3 also
highlights the variance between the two trials conducted at 1.25 m/s at 0% and 10% inclinations. Strain
active regions are maintained in the forefoot with a reduction in activity seen in the hindfoot with increased
inclination. These patterns are seen generally across all participants, with supplementary corresponding
figures supplied for each of the remaining participants. Tables 2 and 3 show the averaged trial strains, standard deviations and percentage strain changes seen
between speed changes (Table 2) and due to inclination change (Table 3). The trend between increasing Frontiers
5 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Figure 2. Anatomically defined regional mask which is used to catergorise strain output data for each
participant trial. speed and increasing average and peak strain can be seen for all participants in the majority of anatomical
regions. There is some variance in the reported strain changes for inclination across differing anatomical
regions and participants. All participants show a reduction in strain with increasing inclination at the
rearfoot, in line with the reduction in strain active regions as seen in Fig. 3. Average stance time, across all ten stance phases and over three repeated trials per loading regime (Fig4).
decreases with increasing speed for all participants. A marginal decrease in average stance time is seen for
all participants comparative between 1.25 m/s 0% to 1.25 m/s 10% inclination. Frontiers
6 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Figure 3. A representative example of strain profiles for repeated trialled speeds and inclinations for one
participant (P03). Frontiers
7 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Table 2. Anatomical regional average and peak strains to 2 d.p. and standard deviations (SD) averaged across all three repeat trials, with
comparative percentage strain changes for all participants (P01-P05) at walking speeds of 0.75, 1.25 and 2.00 m/s Average Strain
Average Strain - Strain Change (%)
Peak Strain
Peak Strain - Strain Change (%) 0.75 m/s
SD
1.25 m/s
SD
2.00 m/s
SD
[Slow to Normal]
[Normal to Brisk]
[Slow to Brisk]
0.75 m/s
1.25 m/s
2.00 m/s
[Slow to Normal]
[Normal to Brisk]
[Slow to Brisk] P01 Hallux
6.13
3.65
8.79
5.17
15.46
8.60
43.35
75.78
151.99
11.57
19.73
32.11
70.57
62.72
177.55 2nd-5th Toes
2.54
2.68
3.14
2.80
5.18
4.38
23.31
65.29
103.82
12.19
13.53
21.23
10.94
56.91
74.08 1st Met Head
1.61
1.90
2.21
1.83
3.43
3.74
37.58
55.31
113.68
6.44
8.00
16.52
24.21
106.65
156.67 2nd-3rd Met Heads
1.23
0.63
1.37
0.71
1.60
1.11
11.28
17.53
30.79
2.69
3.36
5.31
24.93
57.80
97.14 4th-5th Met Heads
1.54
0.91
1.38
0.89
2.39
3.26
-10.35
73.82
55.84
4.01
3.89
14.23
-3.21
266.11
254.36 Lateral Midfoot
0.77
0.67
0.68
1.00
0.90
0.75
-11.42
32.04
16.96
3.79
6.55
3.33
72.77
-49.12
-12.09 Medial Midfoot
0.74
0.87
0.80
0.38
0.81
0.86
9.11
0.87
10.06
3.82
2.33
4.30
-39.09
84.78
12.55 Heel
2.35
1.73
3.01
2.65
4.29
3.13
28.20
42.31
82.45
8.84
15.00
16.31
69.72
8.68
84.46 P02 Hallux
10.83
8.47
8.63
9.14
13.62
13.81
-20.32
57.81
25.74
35.11
51.32
51.01
18.92
46.20
73.86 2nd-5th Toes
5.49
5.37
3.94
3.93
5.41
5.15
-28.28
37.41
-1.45
17.59
21.89
21.02
-40.85
24.48
-26.37 1st Met Head
2.77
2.74
5.48
6.77
8.39
9.63
97.84
53.14
202.96
24.05
34.43
43.18
97.17
43.18
182.30 2nd-3rd Met Heads
2.48
1.42
4.41
3.73
5.19
4.75
77.94
17.78
109.58
16.11
24.78
21.16
129.63
53.88
253.34 4th-5th Met Heads
4.20
5.16
2.66
2.91
3.17
4.68
-36.58
19.06
-24.49
14.52
22.77
9.76
-37.54
56.83
-2.05 Lateral Midfoot
1.86
1.83
1.23
1.22
0.85
1.00
-33.77
-30.76
-54.14
6.50
6.26
7.69
-19.75
-3.62
-22.66 Medial Midfoot
0.57
0.61
0.69
0.96
0.59
0.44
21.31
-15.38
2.66
6.36
1.84
3.97
80.25
-71.09
-47.88 Heel
3.28
2.96
4.17
4.09
5.40
4.63
27.15
29.62
64.81
24.16
24.53
14.92
63.78
1.56
66.34 P03 Hallux
12.10
11.15
14.17
10.79
24.92
18.94
17.12
75.86
105.97
38.23
31.73
63.35
-17.02
99.68
65.70 2nd-5th Toes
2.83
3.15
4.04
4.49
8.07
9.82
42.68
99.65
184.87
15.29
25.08
42.45
64.04
69.30
177.73 1st Met Head
6.36
5.52
8.23
6.36
10.11
6.98
29.47
22.79
58.97
25.71
30.04
32.76
16.83
9.06
27.41 2nd-3rd Met Heads
2.63
1.78
6.47
5.38
6.17
4.88
146.27
-4.67
134.77
8.46
23.71
21.34
180.18
-10.00
152.18 4th-5th Met Heads
1.86
1.26
3.45
3.03
5.53
6.57
85.29
60.51
197.40
5.66
15.46
29.21
173.11
88.95
416.05 Lateral Midfoot
1.31
1.99
1.09
1.07
1.38
2.18
-16.82
27.13
5.74
15.88
4.60
12.73
-71.01
176.52
-19.84 Medial Midfoot
1.38
1.70
1.34
1.70
1.55
1.73
-2.97
15.72
12.28
8.18
8.70
8.05
6.24
-7.37
-1.60 Heel
5.47
4.92
7.16
5.65
10.11
7.21
30.78
41.26
84.744
27.12
28.60
37.99
5.46
32.82
40.08 P04 Hallux
12.12
17.53
23.78
22.03
19.11
22.38
96.26
-19.63
57.73
55.80
68.34
85.32
22.49
24.85
52.92 2nd-5th Toes
7.68
11.74
9.03
10.82
14.39
13.55
17.61
59.31
87.35
59.28
51.71
62.91
-12.77
21.66
6.13 1st Met Head
1.14
0.89
3.73
4.12
2.94
3.07
225.13
-21.17
156.28
4.32
17.10
11.94
295.99
-30.20
176.39 2nd-3rd Met Heads
1.79
1.12
4.52
4.15
3.76
2.87
152.52
-16.77
110.16
4.64
17.55
13.83
278.20
-21.17
198.14 4th-5th Met Heads
1.41
1.01
3.22
4.29
5.96
8.19
128.41
85.07
322.71
4.98
19.17
32.51
284.76
69.56
552.41 Lateral Midfoot
0.41
0.27
0.52
0.78
0.41
0.68
25.41
-20.58
-0.40
1.53
4.80
4.91
214.87
2.15
221.65 Medial Midfoot
0.76
0.92
0.47
0.56
0.41
0.28
-37.30
-13.76
-45.93
5.92
3.14
1.44
-46.89
-54.15
-75.65 Heel
2.40
1.75
3.30
2.44
4.89
4.42
37.13
48.42
103.53
9.32
15.02
29.41
61.27
95.77
215.73 P05 Hallux
10.12
6.72
14.26
7.76
18.48
13.69
40.90
29.61
82.62
24.66
29.59
60.15
20.03
103.24
143.94 2nd-5th Toes
2.98
3.68
3.70
4.20
11.06
11.75
24.24
198.66
271.07
20.77
19.79
58.84
-4.68
197.26
183.34 1st Met Head
3.06
1.97
6.65
5.08
15.08
12.87
117.38
126.76
392.93
8.88
20.17
46.59
127.15
130.97
424.66 2nd-3rd Met Heads
2.96
2.97
7.40
6.85
9.25
6.95
149.69
24.92
211.91
15.97
36.08
29.62
125.90
-17.88
85.50 4th-5th Met Heads
4.16
6.70
7.31
9.43
5.78
8.11
75.93
-20.93
39.11
28.60
33.27
35.39
16.36
6.36
23.76 Lateral Midfoot
1.86
1.30
1.82
1.30
1.83
1.46
-2.34
0.84
-1.52
5.31
4.91
6.44
-7.50
31.07
21.24 Medial Midfoot
0.98
1.12
0.77
0.88
0.95
1.40
-21.25
23.22
-2.96
6.00
4.80
5.68
-20.07
18.38
-5.38 Heel
3.67
2.99
3.93
3.34
7.73
5.91
6.83
96.87
110.33
20.18
21.31
35.45
5.61
66.37
75.70 Frontiers
8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Table 3. Anatomical regional average and peak strains to 2 d.p. and standard deviations (SD) averaged across all three repeat trials, with
comparative percentage strain changes for all participants (P01-P05) at 1.25 m/s speed at 0 % and 10% inclination. Average Strain
Average Strain - Strain Change (%)
Peak Strain
Peak Strain - Strain Change (%) 0%
SD
10%
SD
[0% to 10%]
[0% to 10%]
Regional Average
Region
0%
10%
[0% to 10%]
[0% to 10%]
Regional Average
Region P01 Hallux
8.79
5.17
7.89
4.63
-10.26 -6.12
Forefoot 19.73
17.12
-13.24 17.44
Forefoot 2nd-5th Toes
3.14
2.80
2.73
2.19
-12.84
13.53
10.09
-25.39 1st Met Head
2.21
1.83
1.63
1.66
-26.09
8.00
9.05
13.22 2nd-3rd Met Heads
1.37
0.71
1.39
0.88
2.12
3.36
4.49
33.43 4th-5th Met Heads
1.38
0.89
1.60
1.36
16.48
3.89
6.96
79.17 Lateral Midfoot
0.68
1.00
0.54
0.34
-21.25
-6.95
Midfoot
6.55
1.62
-75.20
24.99
Midfoot
Medial Midfoot
0.80
0.38
0.86
0.87
7.35
2.33
5.25
125.19 Heel
3.01
2.65
2.57
2.04
-14.74
-14.74
Rearfoot
15.00
10.44
-30.38
-30.38
Rearfoot P02 Hallux
8.63
9.14
14.30
14.00
65.64 27.32
Forefoot 35.11
51.01
45.32 28.61
Forefoot 2nd-5th Toes
3.94
3.93
3.97
3.66
0.88
17.59
21.02
19.55 1st Met Head
5.48
6.77
8.14
10.61
48.60
24.05
43.18
79.55 2nd-3rd Met Heads
4.41
3.73
5.68
4.31
28.80
16.11
21.16
31.38 4th-5th Met Heads
2.66
2.91
2.47
1.92
-7.33
14.52
9.76
-32.77 Lateral Midfoot
1.23
1.22
1.26
1.43
2.59
-0.52
Midfoot
6.50
7.69
18.32
-9.61
Midfoot
Medial Midfoot
0.69
0.96
0.67
0.61
-3.63
6.36
3.97
-37.54 Heel
4.17
4.09
3.48
3.00
-16.42
-16.42
Rearfoot
24.16
14.92
-38.24
-38.24
Rearfoot P03 Hallux
14.17
10.79
16.15
14.00
13.95 18.68
Forefoot 31.73
44.14
39.14 9.75
Forefoot 2nd-5th Toes
4.04
4.49
4.88
6.21
20.76
25.08
23.94
-4.53 1st Met Head
8.23
6.36
10.83
7.73
31.60
30.04
32.25
7.36 2nd-3rd Met Heads
6.47
5.38
6.83
4.99
5.53
23.71
20.98
-11.54 4th-5th Met Heads
3.45
3.03
4.19
3.99
21.54
15.46
18.29
18.30 Lateral Midfoot
1.09
1.07
1.46
2.44
34.42
86.05
Midfoot
4.60
14.40
212.70
236.90
Midfoot
Medial Midfoot
1.34
1.70
3.19
6.57
137.67
8.70
31.40
261.10 Heel
7.16
5.65
4.52
4.07
-36.81
-36.81
Rearfoot
28.60
20.16
-29.50
-29.50
Rearfoot P04 Hallux
23.78
22.03
12.08
16.56
-49.21 -10.05
Forefoot 68.34
60.90
-10.89 -19.83
Forefoot 2nd-5th Toes
9.03
10.82
10.35
11.00
14.59
51.71
60.21
16.44 1st Met Head
3.73
4.12
2.61
2.30
-29.99
17.10
9.25
-45.91 2nd-3rd Met Heads
4.52
4.15
4.79
3.46
5.88
17.55
14.09
-19.72 4th-5th Met Heads
3.22
4.29
3.49
2.65
8.46
19.17
11.69
-39.05 Lateral Midfoot
0.52
0.78
0.57
0.72
10.57
25.18
Midfoot
4.80
5.45
13.36
35.54
Midfoot
Medial Midfoot
0.47
0.56
0.66
0.90
39.78
3.14
4.96
57.72 Heel
3.30
2.44
2.48
1.91
-24.80
-24.80
Rearfoot
15.02
10.13
-32.58
-32.58
Rearfoot P05 Hallux
14.26
7.76
12.12
6.89
-14.99 -5.80
Forefoot 29.59
26.44
-10.67 2.86
Forefoot 2nd-5th Toes
3.70
4.20
4.71
4.85
27.24
19.79
25.31
27.86 1st Met Head
6.65
5.08
8.76
7.81
31.73
20.17
29.18
44.64 2nd-3rd Met Heads
7.40
6.85
4.05
3.13
-45.23
36.08
14.28
-60.42 4th-5th Met Heads
7.31
9.43
5.29
8.36
-27.73
33.27
37.57
12.91 Lateral Midfoot
1.82
1.30
1.17
1.45
-35.83
-13.52
Midfoot
4.91
9.28
88.87
58.05
Midfoot
Medial Midfoot
0.77
0.88
0.84
0.96
8.79
4.80
6.10
27.24 Heel
3.93
3.34
2.92
2.65
-25.70
-25.70
Rearfoot
21.31
19.08
-10.48
-10.48
Rearfoot Frontiers
9 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. Figure 4. Average stance time per participant across all plantar loading regimes with associated standard
deviations. 4
DISCUSSION The aim of the study was to utilise the STAMPS novel measurement insole to evaluate strain characteristic
changes (Crossland et al., 2023), including stance time, strain location and strain change, instigated through
changes in walking speed and inclination. The strain responses captured using the STAMPS insole were
compared to the capabilities of current pressure measurement systems used in DFU assessment, namely
the pedar® [Novel GmbH, Munchen Germany] pressure capture insole. Studies by Segal et al. (2004) and
Ho et al. (2010) using the pedar® insole showed increased pressure with increasing speed. Strain captured
by the STAMPS insole is related to plantar loading comprised of pressure and shear strain contributions, it
is therefore expected that with an increasing speed and associated pressure, an increase in strain would be
observed. Strain outputs for all participants, Table 2, confirm this expectation by showing increased strain
consistently across all trials with increasing speed (Segal et al., 2004). Average stance time, Fig. 4,
reduces with increasing speed which is concurrent with expectations for normal gait (Kirtley et al., 1985;
Roth et al., 1997; Demur and Demura, 2010; Olney et al., 1994). Inclination strain change reductions across
participants for both average and peak strains, Table 3, align with the reduction in rearfoot strain active
locations seen, example shown in Fig. 3. Though there is variation in participant strain changes to the mid
and forefoot in both average and peak strain, the peak strain changes in these regions tend to show a general
increase for participants at 10% gradient. Ho et al. (2010) reported the effect of inclination on pressure
showed reduction in peak pressures at the rearfoot at inclinations of 5%, 10% and 15% gradient, with
changes to the location of strain actives regions present from the 0% gradient. This is congruent with the
strain changes reported from this study. The gradient chosen for this study does not necessarily reflect the
daily gait activities of a person at-risk of diabetic foot complications. A 10% gradient can be considered a
moderate-large incline, and with high-risk diabetics skewing towards and older and less mobile population,
their daily activity locations may be centred around lower gradient terrains such as the home. Further
studies at a reduced gradient and during other activities such as stair ascending and descending would be
beneficial to understand strain response variations. Frontiers
10 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. The regional strain percentage decreases reported for some participants across the trialled conditions
for speed and gradient do not necessarily reflect the strain changes across the foot as a whole. While a
tendency for the whole foot to show increasing strain with increasing speed is seen, the decreasing regional
values may reflect the loading changes required of the foot to offset gait deviations undertaken in response
to increasing speed as seen in Kernozek et al. (1996) shod pressure study. Likewise the same can be seen in
the response of the foot with inclination reported by Ho et al. (2010). Peak plantar pressure has been considered in DFU analysis alongside pressure time integral as metric
used to determine DFU risk (Bus and Waaijman, 2013; Waaijman and Bus, 2012; Keijsers et al., 2010).
Whilst the STAMPS insole does not allow for the recording of strain changes during stance phase for
direct calculation of strain time integral, and instead provides a reflection of cumulative strain, analysis
of regional average strain in relation to longevity of stance can be considered in lieu of this metric. The
small cohort in this study does not allow for statistical analysis and reporting of significance, but can be
considered a benchmark study to assess this metric in a larger healthy cohort. A limitation of this study is the use of a treadmill to standardise the walking speeds achieved. Whilst
it achieves that aim, it can result in altered gait biomechanics compared to non-treadmill walking to
compensate for controlled speed and belt movement (Lee and Hidler, 2008). Therefore the strain profiles
may be altered in comparison to strain results recorded due to natural speed changes. A study analysing the
strain response of self-selected slow and faster walking speeds should be run to address this. The acceleration profile of the belt was dependent on the target outcome speed, with lower speeds having
a lower initial rate of acceleration comparative to the higher target speed. The acceleration profiles were
also non-liner in presentation. Speeds in all three targeted trials were reached prior to half of the steps being
completed in all cases, with the plastically deformable STAMPS insole recording peak strains occurring at
the target speed. The acceleration profile of the treadmill, rather than immediate target speed reached, does
enable the full ten steps undertaken in the trial to be conducted at the target speed. The increased time it
takes to reach higher speeds, due to the same initial starting speed, means that a differing number of steps
are completed at the target velocity in relation to which speed the trial was conducted at. The result this has
on the strain outcomes should be minimal due the plastic deformation of the insole, however this cannot
be measured in the remit of this study. Acceleration profiles can be present when achieving self-selected
walking speeds within brisk activities of daily living. These acceleration profiles are often not seen with
slower self-selected walking speeds, which can be achieved instantaneously from initiation of gait. These
natural deviations from a single continuous walking speed, whilst not directly represented in the study due
to the controlled acceleration profile, should be considered when assessing the feasibility of strain data
capture methods to respond to change in speed during gait events. The participant study was conducted with a small cohort of healthy participants to assess both if there
was a difference seen in strain response and stance time, and if this was measurable using the STAMPS
insole technique. Due to this, no statistical significance can be attributed to the strain differences reported,
a larger cohort study is required with appropriate power to further this work. To work towards translation
of the data to reflect a range of activities of daily living in clinical decision making process, a range of
inclinations should be trialled over a larger population. A singular inclination value is studied at a relatively steep incline of 10%, to report how inclination
affects strain response. Beyond this the cohort demographic only covers a young adult, non-diabetic
population, meaning that it is not generalizable to other cohorts. However the opportunity to measure strain
and potentially reduce the incidence of DFU requires further studies in this population. Development of 3D Frontiers
11 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. DIC image capture is also required to enhance the analysis of the insole deformation profiles, ensuring the
recorded strains reflect a true representation of regionalised strain response. This is particularly important
in relation to potential future clinical translation to allow for strain data to support DFU risk assessment
and treatment pathways. Prior use of the insole has been limited to normal self-selected walking speeds to reflect the average
patients gait speed undergoing activities of daily living, but expanding this to reflect the altered activities
of daily living experienced by diabetic cohorts due to foot structure, deformities and gait deviations is
important. Assessment of the characteristics of these daily activities has been increasing in recent years
and emphasises the importance of characterising gait beyond a research setting (Rozema et al., 1996).
Understanding strain response to speed and inclination offers the opportunity to provide informed treatment
approaches, such as footwear design, to optimise plantar loading for reduced DFU risk. With the increase
in biomechanical assessments of activities of daily living, the clinical translation potential of the STAMPS
insole could also be optimised to explore pathology and disease progression through plantar loading. ACKNOWLEDGMENTS I would like to extend my thanks to Leeds Teaching Hospitals NHS Trust for allowing me to work in their
Diabetic Limb Salvage Service on an honorary basis which has influenced my drivers and motivation for
this study. Thanks should also be extended to my supervision team for their continual support and guidance
and to EPSRC for funding this research through the CDT Centre Grant EP/L01629X/1. AUTHOR CONTRIBUTIONS SRC: Writing – original draft, project administration, methodology, investigation, formal analysis, data
curation and conceptualisation. HJS: Writing – review and editing, conceptualisation. CLB: Writing
– review and editing, supervision, methodology, conceptualisation PC: Writing – review and editing,
supervision, methodology, conceptualisation. CONFLICT OF INTEREST STATEMENT The authors declare that the research was conducted in the absence of any commercial or financial
relationships that could be construed as a potential conflict of interest. CONTRIBUTION TO THE FIELD STATEMENT The research reported in this study adds to a growing body of literature addressing and enhancing the
risk assessment methodologies of the vulnerable diabetic foot. The focus of current methods is centred
on pressure assessment whilst neglecting the contribution of shear in the formation of diabetic foot
ulcers, which are a leading cause of non-traumatic amputations. This research assesses a novel strain
measurement insole, detecting the combined contributions of pressure and shear, and looks to assess the
strain response changes brought about by changes in speed and inclination. Current risk assessment is often
currently conducted within the clinical environment, with an emerging focus on data collection during
more representative activities of daily living taking focus. Understanding strain response, as detailed in this
study, during these daily activities, such as when changing speed, is important to be able to work towards
characterising at-risk plantar sites within the diabetic population. Frontiers
12 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288138
doi: 
medRxiv preprint Crossland et al. ETHICS Ethics reference: LTMECH-005. University of Leeds Engineering and Physical Sciences joint Faculty
Research Ethics Committee provided ethics approval. Prior consent was obtained from all participants
within the study. FUNDING This study was funded via EPSRC funded CDT Centre Grant EP/L01629X/1. SUPPLEMENTAL DATA Supplementary figures are provided as an additional document showing the remaining participant strain
profiles for repeated trialled speeds and inclinations. DATA AVAILABILITY STATEMENT (Currently in process of getting DOI from Leeds Data Repository)",1
"The COVID-19 pandemic has highlighted the importance of monitoring mobility patterns and their impact on disease
spread. This paper presents a methodology for developing effective pandemic surveillance systems by extracting scal-
able graph features from mobility networks. We utilized Meta’s ”Travel Patterns” dataset to capture the daily number
of individuals traveling between countries from March 2020 to April 2022. We have used an optimized node2vec
algorithm to extract scalable features from the mobility networks. Our analysis revealed that movement embeddings
accurately represented the movement patterns of countries, with geographically proximate countries exhibiting similar
movement patterns. The temporal association dynamics between Global mobility and COVID-19 cases highlighted
the significance of high-page rank centrality countries in mobility networks as a key intervention target in control-
ling infection spread. Our proposed methodology provides a useful approach for tracking the trajectory of infectious
diseases and developing evidence-based interventions. Introduction Pandemic outbreaks pose devastating consequences on public health, economies, and social structures [1]. Early de-
tection and rapid response are crucial for mitigating their impact, which requires the timely identification of infected
individuals, effective contact tracing, and isolation measures. However, traditional surveillance methods, such as
laboratory testing and manual contact tracing, can be slow, labor-intensive, and limited in their scope [2]. With the
advancement of technology, data-driven approaches have become increasingly crucial for disease surveillance. In par-
ticular, mobility networks, which capture human movement patterns, can provide valuable information for pandemic
surveillance [3]. Mobility networks can be constructed from various data sources, such as mobile phone location data,
transportation logs, and social media check-ins. Mobility networks are essential for pandemic surveillance. They can
provide valuable information about how diseases are spreading geographically and help public health officials make
informed decisions about allocating resources and implementing mitigation measures. By analyzing mobility net-
works, researchers can gain insights into the spread of infectious diseases, identify high-risk areas, and inform public
health interventions. However, analyzing and extracting useful information from mobility networks can be challenging
due to their complex structure. Mobility networks can be represented as graphs, where nodes represent locations or
individuals, and edges represent movement or connections between them. Graphs can have millions or even billions
of nodes and edges, making traditional analysis methods impractical. There has been significant research on using mobility networks for pandemic surveillance [4] [5]. One of the earliest
examples of this approach [6], used mobile phone data to construct a network of human mobility and simulated the
spread of influenza in the United States. They found that mobility networks can provide valuable insights into the
spread of infectious diseases and that targeted interventions based on mobility patterns can be more effective than
random interventions. Since then, numerous studies have used mobility networks for pandemic surveillance, focusing
on different aspects of the problem. For example, researchers used mobile phone data to construct a mobility network
that can predict the spread of malaria [7] [3]. However, analyzing and extracting useful information from mobility
networks can be challenging due to their large scale and complexity. To overcome this challenge, researchers have
proposed using scalable graph features to summarize the information contained in mobility networks. In this research,
we propose to learn scalable graph features from mobility networks to construct robust pandemic surveillance. The
objective is to develop an efficient approach that can extract relevant information from mobility networks in a scalable
and effective manner. By doing so, we hope to contribute to developing effective pandemic surveillance systems that
can aid in the early detection and control of infectious disease outbreaks. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Method This paper proposes a node embedding methodology (Figure 1) to extract scalable features from mobility networks
using COVID-19 as a use case. We also presented various methods to evaluate the effectiveness of the extracted
features by analyzing their correlation with attributes relevant to modeling COVID-19 pandemic policies. Our ap-
proach is suitable for tracking the trajectory of infection in other infectious diseases and provides a strong basis for
evidence-based interventions. Node Embedding Node embeddings are vector representations of nodes in a network or graph. Mathematically, we defined node em-
bedding for a graph G on N nodes with vertex set V (G) = {v1, v2, . . . , vN} as d-dimensional (generally d < N)
representations of the nodes [8] [9]. These embeddings capture the topology of graphs and are frequently utilized
as features of machine learning algorithms to model outcomes. Node embedding algorithms are a mapping function
f : V (G) →Rd. To learn f, a neighborhood sampling strategy S is used, which generates a network neighborhood
of node u defined as NS(u) ⊂V for every source node u ∈V . The objective function mentioned below is opti-
mized by the algorithm, which maximizes the log-probability of observing a network neighborhood NS(u) for a node
conditioned on its feature representation, max
f X u∈V
log Pr(NS(u)|f(u))
(1) The embedding algorithms make the following two assumptions:
Conditional independence: This states that the observation of any other node does not influence the chance of
observing a node in a neighborhood.
Symmetry in feature space: This states that there is symmetry between the source node and the neighborhood node,
meaning that they have equal and opposite effects on each other. These assumptions simplify the above equation to: max
f X u∈V  −log Zu +
X ni∈NS(u)
f(ni) · f(u)  
(2) The per-node partition function is and [10] [11] Zu =
X v∈V
exp(f(u) · f(v))
(3) Classical Search Strategy The optimization of Eqaution 2 involves the selection of neighborhood nodes surrounding a source node through
neighborhood search strategies. The algorithms defined below are popular algorithms that suggest a solution to limit
the neighborhood size and generate multiple sample neighborhood sets for a given node, u. Breadth First Search (BFS): This approach confines the neighborhood to the source’s immediate neighbors.
Depth First Search (DFS): This approach includes nodes at progressively greater distances from the source node. Node2Vec node2vec [12] is a node embedding algorithm that learns low-dimensional representations of nodes in a network by
optimizing an objective function that captures the structural properties of the graph. node2vec builds upon the Breadth- . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint First Search (BFS) and Depth-First Search (DFS) algorithms. Specifically, it uses a biased random walk strategy to
generate node sequences that capture the graph’s local and global structural information. During the random walk,
the node2vec algorithm chooses the next node to visit based on a probability distribution that balances BFS-like
behaviour and DFS-like behaviour. This allows node2vec to capture both the local connectivity patterns of nodes (BFS-
like behaviour) and the long-range structural dependencies between nodes (DFS-like behaviour). After generating
the node sequences using the random walk strategy, node2vec applies a skip-gram model to learn low-dimensional
representations of nodes optimized to predict each node’s context (i.e. the nodes that appear in its neighborhood). Policy Inferences from Mobility Networks Utilizing Node Emdedding Understanding mobility networks is challenging due to their complex patterns of movement and connectivity between
nodes, which can be influenced by multiple factors such as travel preferences, socioeconomic status, and infrastructure.
These networks consist of nodes representing geographic locations and edges representing movement links. node2vec
can address this challenge by learning low-dimensional representations of nodes in a network. Below is a list of
possible policy inferences that can be effectively derived from mobility network embedding. Figure 1: Pipeline depicts our methodology for analyzing mobility network data; we used the COVID-19 country-
level mobility pandemic as an example. We used the node2vec method to extract the distinctive features of each
country. By integrating these features with country-specific COVID-19 indicators, we identified able to determine
policy implications, such as the need for travel restrictions. 1. Target interventions to high-risk areas Policymakers can use data on mobility patterns to identify high-risk
areas and develop targeted interventions to reduce the spread of the virus. This can include increased cleaning
and sanitation measures or restrictions on the number of people allowed in certain areas. 2. Resource allocation policies Mobility networks and node embedding may be used to create more efficient
resource allocation strategies. This strategy may aid in the identification of high-traffic locations, the response
to dynamic situations, and the assistance of vulnerable groups. 3. Lockdown and Travel Restriction policies Node embedding and mobility network analysis may help formulate
appropriate pandemic lockdown and travel restriction policies. It can identify high-risk areas, evaluate the
impact of restrictions on mobility, and inform targeted interventions. 4. Effective vaccination policy Policymakers can maximise vaccination distribution and outreach by assessing
movement patterns and high-risk locations. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint Results The results section of this paper presents the use of the node2vec method on mobility networks in the context of
COVID-19. The experiments began with optimizing node2vec model hyperparameters, followed by analyzing the
spatiotemporal patterns and community learning from the learned node embedding features within the context of
COVID-19. Data Our study utilized the ”Travel Patterns” dataset provided by Facebook (now Meta). This dataset records the daily
number of Facebook users who travel between countries, and it was collected from individuals who voluntarily shared
their location data with the Facebook app. The dataset includes those who have travelled over long distances, such as
by air or train. The Travel Patterns dataset is especially useful for epidemiological study, as it can aid in understanding
the impact of international travel on disease spread as well as provide insights into the economic consequences of
reduced travel during public health emergencies and other events. We analyzed data from March 2020 to April 2022
to cover all the major waves of COVID-19, including alpha, beta, gamma, delta, and omicron. Also, we have used
country-wise daily COVID-19 data, which we have extracted from a publicly available data repository World Health
Organization (WHO). 1 Data Preprocessing The country-level longitudinal data downloaded from the Meta portal and WHO website were originally tabular, and
we performed the following preprocessing steps to prepare the data for analysis. Mobility Data Considering the commonly held theory that COVID-19-infected individuals usually experience symp-
toms, such as mild respiratory symptoms and fever, within 1–14 days [13], we analyzed movement between countries
on a biweekly basis. We summarized the country-to-country movement biweekly by taking the mean of the daily
movement. Covid-19 data We have also summarized the COVID-19 daily new cases on a biweekly basis. Hyperparameter-optimized node2vec learning Optimizing the node2vec algorithm involves adjusting several hyperparameters, among which the embedding dimen-
sion is one of the most significant. The embedding dimension determines the number of features in the node vector
representation. Optimizing it can enhance the quality of embeddings and the overall performance of subsequent tasks
[14] [15]. This study aimed to determine the optimal embedding dimension for the node2vec algorithm in the context
of a bi-weekly mobility network and COVID-19 case prediction. To achieve this, we embedded the mobility network
into dimensions ranging from 5 to 50, with a 5-unit increment, where each node represented a country. The maximum
value of 50 was selected based on the size of the smallest network in our mobility network, which had 54 nodes. Using
the embedded features, we then integrated the next 15 days’ COVID-19 cases and employed a regression approach to
train machine learning models (random forest and xgboost) [16] [17]. We divided the dataset into training and testing
sets at an 80:20 ratio and then trained the model using five-fold cross-validation on the training set. We evaluated the
model performance score (R2) on the testing set. 95% confidence interval of R2 socre was calculated using Equation
4. Our findings revealed that for both random forest and xgboost models, the R2 increased with the embedding di-
mension and then started decreasing (Figure 2A, Figure 2B). We identified the highest R2 at embedding dimensions
25 for random forest and xgboost. Therefore, we selected the optimal embedding dimension of 25 to preserve the
maximum information. Additionally, we calculated the top 5 features (Figure 2C, Figure 2D) from the random forest
and xgboost models with the optimum embedding dimension of 25. We found that both models had 80% of their top
features in common, indicating consistency in the learned machine-learning models. 1https://covid19.who.int/data . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint SE(R2) = s 4R2(1 −R2)2 (n −k −1)2(n2 −1)(n + 3)
where 95% CI is R2. ± 2SE(R2)
(4) Figure 2: Node2Vec Learning. (A) R2 score with 95% CI for the prediction of the next 15 days Cases of COVID-19
from the Random Forest model with an embedding dimension of 5 to 50 (B) R2 score for predicting the next 15 days
of COVID-19 cases from the XGBoost model with an embedding size of 5 to 50 C) The top five features of a Random
Forest model with the optimal embedding dimension D) The top five features of n XGBoost model with the optimal
embedding dimension Discovering Spatiotemporal Patterns in Embedded Features: A Way to Enhance Travel Restrictions and Re-
source Allocation Policies Learning the effective implementation of policies like travel restrictions, resource allocation, and social distancing
during a pandemic is a challenging task due to the unpredictable nature of the pandemic, the emergence of new
variants, and the complex interactions between socioeconomic and geopolitical factors related to the pandemic. But
analyzing spatiotemporal mobility patterns could help policymakers make better decisions. However, the asymmetric
proximity scores (mobility from country A to country B is not the same as mobility from country B to country A)
make it hard to analyze the spatiotemporal patterns of the original mobility network. We used the embedded features
to calculate the Pearson correlation between countries and built correlation networks to solve this problem. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint Figure 3: Spatio-temporal representation of the mobility network pattern extracted from the embedding fea-
tures. A) Alluvial diagram representing the cluster of countries based on movement patterns and their flow from
March 2020 to April 2022. B) A biweekly jaccard score between the top 20 countries that were central in the move-
ment network and countries with the 20 highest COVID-19 cases The temporal dynamics of the correlation networks were learned using an alluvial diagram (Figure 3A) and biweekly
Jaccard Index calculations between the top 20 countries in the movement network and those with the highest COVID-
19 cases (Figure 3B). Results showed that the movement embeddings were a meaningful representation of the move-
ment patterns of countries, with countries nearby having similar movement patterns. As COVID-19 threats declined
and lockdown policies became lenient, more clusters were formed, indicating increased movement patterns. The Jac-
card score also revealed that countries with high page rank centrality were key nodes in the movement embedding
correlation network and had higher COVID-19 cases during peaks in the pandemic suggesting that movement patterns
play a significant role in determining how the pandemic spreads from one region to another. Also, after a sudden
drop in Jaccard Score, there is a gradual increase in its value as soon as government policies change indicating how
knowing these trends might help the policymakers in planning for such pandemics effectively. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint Enhancing and Tracking Multilevel Implementation of Interventions through Community Learning in Move-
ment Graphs Next, We conducted community detection in the mobility network to enhance and track the multilevel implementation
of interventions. This can aid in understanding the hierarchical spread of infection and inform strategies to mitigate its
impact. [18] [19]. We used a country-wise correlation network learned from an embedded feature and summarized it
to create an overall network. To identify communities, we performed a link community algorithm; link communities
can unveil the complex structure of networks that is both nested and overlapping, and identify the nodes that play a
critical role in connecting different communities.[20] [21]. To perform the analysis, we used R based linkcomm [22]
package and utilized the McQuitty algorithm [23], which first calculates pairwise distances between all pairs of links
in the network (Equation 5). The distance between two links is typically defined as the inverse of their similarity. S(eik, ejk) = |n+(i) ∩n+(j)| |n+(i) ∪n+(j)|
(5) where eik and ejk are link that share a node and n+(i) refers to the first-order node neighbourhood of node i, Figure 4: Community learning from the correlation network of embedded features.A) summarized movement
network with detected community B) hierarchical clustering in the summarized network to detect communities Com-
munity membership of countries with communities representing countries D) Page rank centrality of countries in the
overall summarized network . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287969
doi: 
medRxiv preprint We found 15 unique communities in the entire movement network using the, with the largest community having 34
nodes. Our findings demonstrate the importance of geographic proximity in COVID-19 mobility patterns by showing
that countries that are close by were more likely to belong to the same community. (Figure 4A, Figure 4B, Figure
4C). We captured the relationships between overlapping groups and produced a more accurate representation of the
COVID-19 movement network by characterizing communities as groupings of links. Using this method, we were able
to pinpoint groups of countries that are more connected to one another than to countries in other groups, revealing
information about the relationships between countries based on their COVID-19 movement patterns. For instance,
European nations, including Germany, the United Kingdom, Sweden, Belgium, and Poland, were seen to be part
of the same community. This is also the same for South American nations such as Brazil, Bolivia, Argentina, and
Paraguay being in the same community. (Figure 4A) Also, we determined the page rank centrality of the entire
movement network (Figure 4D), and the results show that Mexico, The Bahamas, and the United States have the
highest page rank centralities, followed by the Philippines, Greece, and the United Kingdom. This information can
help us comprehend the countries that have contributed significantly to the COVID-19 pandemic spread. This shows
essential insights into the COVID-19 movement network’s dynamics and helped identify critical countries for targeted
interventions to control the disease’s spread. Overall, this result offers insightful information about the movement and
propagation patterns of COVID-19 throughout the pandemic, which can be utilized to guide public health policies and
interventions to restrict the disease’s spread. Conclusion Graph-based methods have the potential to facilitate infectious disease surveillance more efficiently by strongly com-
prehending the complexity of data. However, these methods have not been extensively explored or implemented in
traditional disease surveillance systems, which rely more on individual-level data, such as symptom reporting or lab-
oratory results. This limitation could be due to the challenges of collecting and processing large-scale network data,
a lack of expertise in network science, or limited resources for implementing such approaches in real-time disease
surveillance. In this study, we have provided a graph-based algorithm that can understand mobility more extensively
and extract scalable features that can be further used for downstream tasks like building lockdown policies, travel re-
striction policies, forecasting infectious disease trajectories, and vaccination policies. Also, a case study of COVID-19
mobility at the country level showed that the embedded features could predict COVID-19 cases with a good R2 value,
which suggests another way to predict how infectious diseases spread. Also, in other results, we’ve detected the groups
of countries that show a similar movement. Countries that are close together were found to be in the same group, which
shows again how stable embedded features are. From the temporal trend of the Jaccard index, we found that in the
time range where the COVID waves were at a peak, the overlap between countries with high movement centrality and
countries with high COVID-19 cases is low compared to when COVID-19 cases were not at a peak, which represents
that embedded features have nicely captured the travel restriction policies. We also acknowledge that our analysis has
some limitations. For example, we used Facebook user data, but Facebook is banned in multiple countries like China,
so there might be some missing points in the spatiotemporal analysis. Overall, as per our knowledge, this study is the
first to explore the node embedding algorithm for mobility networks. We strongly believe this algorithm can offer a
valuable opportunity for enhancing data-driven infectious disease surveillance. Acknowledgments We acknowledge support from Facebook (Meta), Delhi Cluster-Delhi Research Implementation and Innovation (DRIIV)
Project supported by the Principal Scientific Advisor Office, Prn.SA/Delhi/Hub/2018(C) and the Center of Excellence
in Healthcare supported by Delhi Knowledge Development Foundation (DKDF), Prn. SRP206 at IIIT-Delhi. We also
acknowledge Abdal Lalit, Onkar Mahapatra, and Urvi Midha for helping us in data collection and data preprocessing.",1
"Objective The aim of this study was to investigate the clinical trials of Perampanel (PER) at the First Affiliated Hospital of Xiamen University (FAHXU). Recommendations have been developed for the use of Perampanel in clinical practice. Design A retrospective single-center validation study. Setting A tertiary general hospital in Xiamen, a city located on the southeastern coast of China. Participants A total of 831 prescriptions were included for 188 patients were treated with Perampanel. Outcome measures Record the patient's medication, including the diagnosis of the patient's disease, the dose, course of medication, and the combination of medication, analyze the characteristics of the drug population and the medication unreasonableness by comparing the drug instructions and referring to relevant literature and guidelines. Results PER was mainly used in Neurology. Epilepsy is the most common conditions prescribed. There were 119 cases of inconsistent drug indications, 56 inappropriate dosing frequency, 3 inconsistent dose, and 6 inconsistent dose adjustment intervals. Conclusion There is an improper use of PER within the FAHXU. Greater attention should be paid to changes in patients’ condition and observation of discomfort following medication administration. Monitoring of PER use should be further enhanced. A rational drug evaluation system should be established to promote the rational use of PER. Article Summary All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 Our study was one of the few studies on the clinical use of Perampanel, especially in Southeast China. As a third-generation antiepileptic drug, data on the clinical rational drug use of Perampanel is limited in China. There were several limitations worth noting. Firstly, the data in our study were collected at only one institution, thus not covering a broader population from other regions. Clinical outcomes, including medication efficacy and adverse events, were not tracked in patients with irrational medication use. Strengths and limitations Our study was one of the few studies on the clinical use of Perampanel, especially in Southeast China.  As a third-generation antiepileptic drug, data on the clinical rational drug use of Perampanel is limited in China. There were several limitations worth noting. Firstly, the data in our study were collected at only two institutions, thus not covering a broader population from other regions. Clinical outcomes, including medication efficacy and adverse events, were not tracked in patients with irrational medication use. Abbreviations: AED = antiepileptic drug；AL= alprazolam; Botox A = Botulinum toxin A; CBZ: Carbamazepine; CS = Corticosteroids；CZP = Clonazepam; DP = diazepam; FA= folic acid; LEV = Levetiracetam; LEXAPRO = Lexapro (Escitalopram); LTG = lamotrigine; OMP = Omeprazole; OXC = oxcarbazepine; PB = phenobarbital; PHB = Phenobarbital; PTA = Prednisone acetate; PPC = Polyene phosphatidylcholine; POT CL = potassium chloride; SV = sodium valproate; TPM = topiramate; ZBT = zolpidem; Introduction Epilepsy is a chronic, noncommunicable disorder of the brain characterized by transient central nervous system dysfunction1. Seizure are defined as abnormal neuronal discharges in the brain2. It has paroxysmal, transitory, repetitive, and stereotypic features3, 4. The symptoms of epilepsy are usually body convulsions and tremors, with more severe episodes of momentary loss of consciousness or abnormal sensations lasting seconds to minutes5. The global incidence of epilepsy is approximately 50 per 10,000 people, with at least 50~70 million people are afflicted, indicating Epilepsy is one of the most common brain disorders 6, 7. With 80% of epilepsy patients residing in developing countries8. The recurrence of  seizures has a All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 3 serious impact on the patient’s physical and mental health and impose a significant burden on the family and society9. The management of epilepsy is therefore an important and pressing clinical issue. In general, there are many methods to treat illness and relieve symptoms, including diet therapies, medication, surgery, psychotherapy and immunotherapy10. The main tool to control seizures is drugs11. Antiepileptic drugs (AEDs) can be roughly divided into three generations based on listing time. Valproic Acid, Ethyl Succinate, and Phenytoin are some of the first- generation AEDs. Oxcarbazepine, Levetiracetam and Topiramate are the second- generation AEDs. Lacamide, Isodoxorubicin Acetate and Perampanel represent the third generation of antiepileptic medications12, 13. Perampanel (PER) (Fycompa®) is developed by the Japanese pharmaceutical company Eisai. As an new anti-epileptic drug, PER is a highly selective and non-competitive antagonist of alpha-amino-3- hydroxy-5-methyl-4-isoxazole-propionic acid (AMPA) glutamate receptors14, 15. PER, by targeting inhibition of postsynaptic AMPA receptors, induces an increase intracellular calcium ion, thus reducing neuronal excitability16. The mechanism of PER is distinct from that of other antiepileptic drugs, opening a new avenue for the treatment of epilepsy. PER was launched as a new antiepileptic drug in China in July 2021. As of January 2022, it is used at FAHXU. There are nearly 10 million patients with epilepsy in China, with approximately 400,000 new cases each year17.  Relatively little evidence exists to guide PER use in China. The purpose of this article is to present an analysis of the unreasonable situation of PER in FAHXU in order to provide a reference for clinically irrational drug use, with the goal of promoting rational drug use, so that this drug can exert its best effect in the case of rational drug taking. METHODS Study design, data source and patient population We used the Hospital Information System (HIS) at FAHXU, to collect PER prescription data from January 2022 to January 2023. FAHXU is a famous tertiary hospital in Southeast China, with 4,500 staffs, 2500 beds, and 5 million ambulatory care visits annually. It consists of 65 departments and treats patients of all ages. There were 100,000 visits to the neurology department each year. These data include patient department, basic physiologic information, drug use and dosage, medication duration, and concomitant medications. And compare the drug instructions, refer to relevant literature and guidelines, and analyze the rationality of the characteristics of the drug All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 4 users and the phenomenon of irrational drug use. In order to analyze the rationality of the PER use situation in FAHXU, we also compared the PER package insert. RESULTS From January 2022 to January 2023, PER was prescribed to a total of 188 patients in FAHXU, with a total of 831 prescriptions. Table 1 shows the basic characteristics of patients treated with PER, including gender, age and Body Mass Index (BMI). Table 1 Demographic and clinical characteristics of epilepsy patients Patients’ basic 
situation 
Copies 
Proportion 
Patients’ basic 
situation 
Copies 
Proportion Male 
111 
59.04% 
< 12 years 
6 
3.19% Female 
77 40.96% 12 - 17 years 
24 
12.77% BMI < 18 
13 
6.91% 
18 - 39 years 
100 
53.19% 18 ≤ BMI < 24 
113 
60.11% 
40 - 59 years 
50 
26.60% BMI ≥ 24 
62 
32.98% 
> 60 years 
8 
4.26% Table 2 shows the distribution of services, and PER is primarily concentrated in the Neurology Outpatient Department, with a total of 724 prescriptions, accounting for 87.12% of cases. Table 2 Distribution of Department and Diagnosis for the use of PER Department 
Copies Proportion Neurology clinic 
724 
87.12% Epilepsy clinic 
102 
12.27% Thoracic surgery outpatient I 
2 
0.24% Pulmonary nodule specialist  
1 
0.12% Neurosurgery clinic 
1 
0.12% Pediatric Neurology Clinic 
1 
0.12% Total 
831 
100% Table 3 shows the diagnosis for the prescriptions, and it can be seen that the PER is primarily used for epilepsy, symptomatic epilepsy [secondary epilepsy], epileptic seizures, and other related disorders. More information about diagnosing PER can also be found in Table 3. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 5 Table 3 Diagnosis of prescription diseases of PER tablets Disease diagnosis 
Copies Proportion Epilepsy 
555 
66.79% Symptomatic epilepsy [Secondary epilepsy] 
99 
11.91% Symptomatic epilepsy 
19 
2.29% Pain 
15 
1.81% Intracranial space-occupying lesion (post glioma surgery) 
11 
1.32% Post-traumatic syndrome, epilepsy 
13 
1.56% Juvenile absence epilepsy 
10 
1.20% Dizziness, headache syndrome, other specified 
13 
1.56% Burn 
9 
1.08% AVM embolization postoperative 
8 
0.96% Encephalitis (Autoimmune encephalitis) 
8 
0.96% Epilepsy (Psychomotor seizure) 
6 
0.72% Seizures, intracranial parasitic infection (after surgery) 
6 
0.72% Brain abscess (Postoperative) 
6 
0.72% Sjögren 's Syndrome\[Sjogren' s] 
6 
0.72% Limbic encephalitis (?) 
5 
0.60% Observation of Suspected Diseases and Conditions 
6 
0.72% Diffuse astroglioma WHO Grade II postoperative 
3 
0.36% Lumbar disc herniation 
3 
0.36% Epilepsy, mesial temporal lobe epilepsy with hippocampal 
sclerosis 2 
0.24% Antisynthetase syndrome 
1 
0.12% Meningioma 
1 
0.12% Partial seizures 
1 
0.12% Frontal lobe epilepsy 
1 
0.12% Anxiety state 
1 
0.12% Immune-mediated encephalitis 
1 
0.12% Hydrocephalus 
1 
0.12% Postoperative chemotherapy for malignant tumor 
1 
0.12% Sleep disorder 
1 
0.12% Other (hepatitis B, sleep disorder, diabetes mellitus, 
bradycardia, chronic gastritis, methemoglobinemia) 19 
2.29% Total 
831 
100.00% Table 4 details the usage of PER within FAHXU (compared to the drug instructions). Overall, 119 prescriptions, 14.32%, were inconsistent with the indications for the PER; 56 prescriptions with inappropriate dosing frequency, representing 6.74%; three prescriptions with inconsistent dosage, 0.36% repentance, and six prescriptions had inappropriate dosage interval adjustments, 0.72%. More information on the use of the PER within FAHXU could be found in Table 4 as well. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 6 Table 4 Administration of PER Tablets (as compared to the package insert) Type of use 
Relevant manual content 
Drug administration 
Copies Indications Epilepsy in adults and children 12 years of age and older. Add-on therapy for partial seizures Pain, Burn, Hepatitis B, Sleep disorder, Diabetes, Bradycardia, Chronic gastritis, Hemiglobin 
119 Medication Route 
Oral 
None 
0 Dose Frequency 
Once daily 
Twice daily 
56 Administered Dosage Adults and adolescents: (adjust dose range 2-12 mg according to disease) 
1 mg 
3 Adjust Dose Interval Dose increases separated by at least 1 or 2 weeks 
1 days, 3 days, 5 days 
6 Target Population 
Elderly: Use with caution Use in Elderly 60 Years of Age 
8 Children < 12 years 
Children < 12 years 
6 Renal impairment: Not recommended in patients with moderate or severe renal impairment or on hemodialysis Moderate to severe renal impairment 
6 Hepatic injury: Patients with mild and moderate liver injury, the dose should not exceed 8 mg/day; No recommended in patients with severe liver dysfunction 
None 
0 Not recommended during pregnancy 
None 
0 Not recommended during lactation and stopping Breast-feeding first if taking medicine is necessary 
None 
0 Drug interactions 
Oral Contraceptives 
None 
0 Other Antiepileptic Drugs OXC, PB, PER, SV, CBZ, LTG, TPM, LEV 
557 Cytochrome P450 Inducers 
CS, PB, CBZ, OXC 
179 Cytochrome P450 Inhibitor 
None 
0 Other Swallow whole, do not chew, crush or split. 
1 mg, 3 mg, 5 mg 
8 Note: OXC: oxcarbazepine; PB: phenobarbital; SV: sodium valproate; All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 7 CBZ: carbamazepine; LTG: lamotrigine; TPM: topiramate; LEV: levetiracetam; CS: Corticosteroids Table 5 shows the combination use of PER in FAHXU hospital, with 311 prescriptions were combination with one antiepileptic drug, 231 prescriptions were in combination with two antiepileptic drugs, and 15 prescriptions were combination with three antiepileptic drugs. Table 5 Concomitant medication of PER Tablets Concomitant medication 
Drug Name Combined Antiepileptic Drug Varieties Copies Novel antiepileptic drug LEV (378), SV (296), 
OXC (81), TPM (67), 
LTG (61) Concomitant use of an antiepileptic drug 
311 Traditional antiepileptic drugs 
CBZ (32), PHB (25) 
Combined with two antiepileptic drugs 
231 Sedative hypnotic drugs CZP (66), AL (6),  
ZBT (3), DP (1) Combined with three antiepileptic drugs 
15 Vitamins & 
Electrolytes Vitamin B1 (125),  
POT CL (1), FA (1) Antidepressants 
Duloxetine (12),  
Lexapro (9) Hormones 
PTA (10) Gastrointestinal system medication 
OMP (9) Hepatoprotective drugs PPC (6) Antipsychotics 
Olanzapine (6) Antimigraine drugs 
Flunarizine (2) Central stimulant 
Piracetam (4) Others 
Botox A (5) Note: AL: alprazolam; CBZ: Carbamazepine; PHB: Phenobarbital; DP: diazepam; ZBT: zolpidem; CZP: Clonazepam; LEV: Levetiracetam; LTG: lamotrigine; FA: folic acid; POT CL: potassium chloride; Lexapro (Escitalopram); PTA: Prednisone acetate; OMP: Omeprazole; OXC: oxcarbazepine; PPC: Polyene phosphatidylcholine; SV: sodium valproate; TPM: topiramate; Botox A: Botulinum toxin A; All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 8 Discussion Analysis of the Basic Characteristics of Patients Using PER As shown in the table 1, 111 male patients, accounting for 59.04%, and 77 female patients, accounting for 40.96%, were treated with PER. This is consistent with fact that there are more males and fewer females with epilepsy in China. Patients under the age of 12 accounted for 3.19%, and according to the medication instructions, ""the safety and efficacy of PER in children under 12 years old has not yet been established."" The efficacy and safety of PER in 96 Chinese pediatric patients aged 2-14 years were reported by Qu and Chen, et al18. Treatment effect was good, safety was high and adherence was good.  PER has been on the commercially available since 2008 and has been approved by the U.S. FDA in 2012 for the treatment of focal seizures in patients 12 years and older19, 20, with progressively expanding indications in a variety of countries.  PER is a new indication that was approved in China in July 2021, and can be used to treat patients with partial-onset seizures (with or without secondary generalized seizures) in adults and children aged 4 years and older21.  Pending the author’s investigation and analysis, the instructions have not been updated in time. Table 1 shows that, 32.98% of patients had a BMI ≥ 24. Studies have shown that the use of antiepileptic drugs can cause weight gain. It is possible that the obese population in this study is related to PER use. Due to the short duration of clinical use of PER in China, there is a need to study the side effects leading to obesity. Analysis of medication of PER in FAHXU hospital Indications for PER According to Table 3, PER is primarily used for the treatment of epilepsy, symptomatic epilepsy [secondary epilepsy], epileptiform convulsions and other diseases. Its indication is in accordance with the package insert.  The indications include alternative diagnoses such as traumatic brain injuries and encephalitis, which are not included in the medication labeling, but these conditions may cause seizures. Traumatic brain injury (TBI) is a common cause of epilepsy, epidemiologic studies have demonstrated a clear relationship between the severity of injury and the likelihood of developing epilepsy 22, 23. In addition, encephalitis, an inflammation and swelling of the brain, is a leading cause of acute symptomatic seizures and subsequent epilepsy, with approximately one-third of patients presenting with seizures24, 25. Encephalitis may be of either infective or autoimmune in origin26. We also included patients with intracranial All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 9 space-occupying lesions, seizure symptoms have been reported in 30-50% of patients with brain tumors, especially benign slow-growing brain tumors27-29. Even if the pain diagnosis does not match the routine use of antiepileptics, after the onset of disease, patients with epilepsy are subject to pain or headache30-32.  Studies have shown that during epileptic seizures, nerves in the brain are over discharged, leading to local tissue disruption, abnormal blood supply, and ultimately physical deterioration and death of some cells, resulting in the development of pain symptoms33. The above analysis suggests that there is inappropriate prescriptions use of PER in FAHXU hospital. Consulting the cases reveals that most of them were seizures caused by other conditions, because physicians did not fully diagnose epilepsy, the indications were not suitable. Frequency of dosing with PER Table 4 shows the most common inappropriate dosing frequency is twice daily (56 cases, 6.74%). The pharmacokinetic studies of PER have shown that peak concentration is reached approximately one hour after oral administration, with an average half-life of 105 hours. The elimination half-lives after single and multiple doses are 52-129 hours and 66-90 hours, respectively. Steady-state blood concentrations are reached after 14 days of multiple doses. Pharmacokinetic characteristics are consistent with a single- compartment model of first-order elimination. As a result, a once-daily dose is appropriate. PER is primarily used to treat epilepsy by suppressing abnormal nerve discharge in the brain. Seizures are most likely to occur while sleeping, when the inhibitory effect of the cerebral cortex is reduced, facilitating epileptic foci that have lost inhibition to produce abnormal discharges. Furthermore, neurons are also more likely to exhibit synchronized discharges, which can trigger epileptic seizures, during light sleep. Taking drugs at night can effectively control abnormal synchronized discharges of neurons, in addition to improving seizure status, alleviating the condition, and promoting early recovery from the disease. Therefore, it is recommended to take PER at night. Concomitant use of PER. Pharmacotherapy is the mainstay of treatment for epilepsy. Relevant guidelines suggest beginning with low-dose monotherapy. If monotherapy fails to control seizures or side effects occur, another type of monotherapy may be considered.  PER has a different mechanism than other antiepileptic drugs. It is often treated as an adjunctive therapy to All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 10 other antiepileptics. This survey shows that it is usually used in combination with other medications, with a total of 331 prescriptions for combination with one antiepileptic drug, 231 prescriptions for combination with two antiepileptic drugs, and 15 prescriptions for combination with three antiepileptic drugs. However, when used in combination with other antiepileptic drugs, attention should be given to drug-drug interactions. Antiepileptic drugs such as carbamazepine, phenytoin, and oxcarbazepine, which concomitantly take enzymatic inducers in the liver, have been shown to be effective, may enhance the rate of clearance of this product, leading to a reduction in the plasma concentration of PER14. Consequently, when combined with such antiepileptic drugs, attention must be paid to titrating the drug dosage to achieve efficacy. Relatively high proportions of vitamin B1 is also prescribed, in addition to antiepileptic drugs.  Vitamin B1 belong to the group of nutritional neurotrophic vitamins and have a certain effect of improvement in patients with epilepsy. They can stabilize the nerves and have a sedative effect. The combination of PER and VB1 can have a positive effect on the severity and frequency of seizures and on the side effects of the drugs on the body. It has been demonstrated by studies: Patients with epilepsy are often associated with the development of mental illness. 34-36. Depression is the most common mental illness in these patients, with an incidence as high as 30%, which is 5 ~ 20 times that of the general population. Patients with epilepsy have a prognosis that is influenced by the comorbidity of mental illness, resulting in poor quality of life and early death36-39. Botulinum toxin type A, Botulinum Neurotoxin (BoNT) is also a drug combination. It inhibits acetylcholine release at axon terminals in surrounding neurons, paralyzes flaccid muscle, and reduces glandular secretions. BoNT has been empirically used in a wide variety of ophthalmological, gastrointestinal, urologic, orthopedic, dermatologic, secretory and pain disorders40, 41. Research results show BoNT can inhibit neuronal excitability, reducing acute seizures as well as chronic spontaneous epileptic seizures. Thereby reducing neuronal damage and abnormal brain firing, providing a novel means of treating epilepsy42-44. Characteristics of PER During this investigation, it was discovered that PER tablet had been ruptured and the drug dosage was 1 mg, 3 mg and 5 mg. The PER used in FAHXU is a scarless tablet made by Eisai Europe limited and is an unmarked tablet. It is clear from the instructions All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 11 that it should be swallowed whole and should not be chewed, crushed, or divided.  PER tablet scission can lead to inaccurate dosing and debris creation. Although the loss of some debris may have little effect on the therapeutic effect in short term, antiepileptic drugs often require long-term use. Thus, drugs should be used in strict accordance with medication instructions. Dose Adjustment Interval for PER The study showed that some patients adjust the medication dosage within a range of 1 day to 16 weeks. The instruction of PER indicated that the dose should be adjusted at intervals of 2 weeks or at least one week. Studies have shown that the development of drug tolerance can be delayed by a reasonable interval between doses45.  Tolerance to antiepileptic drugs is an important cause of clinical treatment failure46. Therefore, medication dosage adjustments should be made according to the drug instructions in a regular and reasonable manner, in order to slow down the development of drug tolerance. Conclusions In conclusion, epilepsy is a chronic neurologic disorder that severely affects patients' daily lives. Not only a medical problem, but also an important public health and social issue.  Pharmacological is the most common method of treating epilepsy, and the clinical demand for antiepileptic drugs is increasing as a result of the long course of epilepsy and the prolonged duration treatment. The novel antiepileptic drug Perampanel offers a new option for the treatment of epilepsy. Irrational use of PER still occurs in FAHXU hospital.  Changes in patients’ illnesses and adverse event after medication should receive more attention. It is a need for an appropriate medication use assessment system to strengthen supervision of PER clinical use and promote rational medication use. Acknowledgements The authors would like to acknowledge the assistance of staff at the Department of Information Technology at the hospital for data extraction. Author Contributions LL and YH contributed to data collection, data analysis and writing of the article. ZS, LM gave advice on the design of the study. LL and YH helped collect data. All authors revised the manuscript and eventually approved it for publication. LL was responsible for the overall content as guarantor who accepted full responsibility for the finished work and the conduct of the study, had access to the data, and controlled the decision to publish. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288128
doi: 
medRxiv preprint 12 Funding The authors have not declared a specific grant for this research from any funding agency in the public, commercial or not-for-profit sectors. Competing interests None declared. Patient and public involvement Patients and/or the public were not involved in the design, or conduct, or reporting, or dissemination plans of this research. Patient consent for publication Not applicable. Ethics approval This study involves human participants and was approved by the Ethics Committee of the First Affiliated Hospital of Xiamen University. Provenance and peer review Not commissioned; externally peer reviewed. Data availability statement Data are available upon reasonable request. Open access This is an open access article distributed in accordance with the Creative Commons Attribution Non-Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.",1
"Large language models (LLMs) have shown promise for task-oriented dialogue across a range of domains.
The use of LLMs in health and ﬁtness coaching is under-explored. Behavior science frameworks such as
COM-B, which conceptualizes behavior change in terms of capability (C), Opportunity (O) and Motivation
(M), can be used to architect coaching interventions in a way that promotes sustained change. Here we
aim to incorporate behavior science principles into an LLM using two knowledge infusion techniques: coach
message priming (where exemplar coach responses are provided as context to the LLM), and dialogue re-
ranking (where the COM-B category of the LLM output is matched to the inferred user need). Simulated
conversations were conducted between the primed or unprimed LLM and a member of the research team,
and then evaluated by 8 human raters. Ratings for the primed conversations were signiﬁcantly higher in
terms of empathy and actionability. The same raters also compared a single response generated by the
unprimed, primed and re-ranked models, ﬁnding a signiﬁcant uplift in actionability from the re-ranking
technique. This is a proof of concept of how behavior science frameworks can be infused into automated
conversational agents for a more principled coaching experience. Institutional Review Board (IRB)
The study does not involve human subjects beyond the volunteer
annotators. IRB approval was not sought for this research. 1. Introduction It is estimated that 81% of adolescents and 27% of adults do not achieve the levels of physical activity
recommended by the World Health Organization (WHO) (1). A sedentary lifestyle is associated with long
term adverse health outcomes, ranging from cardiovascular disease and diabetes to mental health problems
and cognitive decline (2).
A 2022 report found that progress toward these goals has been slower than
expected and highlighted digital health tools as a particular opportunity area (3).
Numerous smartphone nudging tools have been designed to promote physical activity (4; 5).
These
interventions are low-cost and highly scalable relative to human ﬁtness coaches, with promising early evidence
(6; 7; 8; 9). One randomized controlled trial of a digital walking coach found short-term improvements in
physical activity (10). However, in an era of notiﬁcation overload, there is also a risk of desensitization and
alert fatigue if the nudge strategy is not well designed.
Automated conversational agents oﬀer an opportunity to create interactive dialogue, with widespread
applications in e-commerce, home automation and healthcare (11; 12).
Health and Fitness coaching is
emerging as a promising use case for these conversational agents (13; 14; 15; 16). However, most traditional
systems are limited in their degree of personalization and persuasiveness because they depend on rule-based c
⃝M. Vardhan*, N. Hegde*, D. Nathani, E. Rosenzweig, A. Karthikesalingam & M. Seneviratne. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. INTERNAL - Knowledge infusion for fitness LLMs nudge engines with static message content rather than adaptive conversational agents that can mimic realistic
dialogue from a human coach (17).
Large language models (LLMs), such as GPT-3 (18), PaLM (19), Gopher (20) and LaMDA (21), excel in
natural language generation with greater expressivity and versatility compared to rule-based chatbots. To
date, use of LLMs in the health and ﬁtness space has been limited, however interest is growing rapidly fol-
lowing the release of LLMs tailored to biomedical tasks (22). A major challenge in using LLMs in health care
is how to ensure the model is personalized and adaptive while still remaining consistent with evidence-based
practice and within safety guardrails (23). Activity coaching relies on complex interpersonal dynamics where
the coach builds rapport with the trainee, provides motivation, helps to overcome pre-existing patterns of
behavior, etc.- which are not explicitly optimized in LLMs (24). Knowledge infusion refers to the integration
of established knowledge or practice into a model. In principle this is often achieved via ﬁnetuning on a
task-speciﬁc dataset (25). The disadvantage of ﬁnetuning in the coaching domain is that it requires coaching
transcripts, which are diﬃcult to obtain. Finetuning has also been shown to diminish the few-shot perfor-
mance of a pretrained LLM with in-context prompts - i.e. over-specialization of the model (26). Knowledge
infusion is an active area of research and many other methods exist including customizing training objectives
(27), reinforcement learning with human feedback (28; 29), in-context learning via prompt engineering or
priming (30; 31) and many associated prompt design variants (32; 33; 34; 35). There have also been numer-
ous strategies to ensemble knowledge infusion techniques, including post-hoc re-ranking or summarization
of model outputs to further align the model with the task of interest (36; 37). Customizing knowledge infu-
sion strategies for the health care domain remains an area of active research. Here we propose two simple
in-context learning methods to infuse behavior science principles into LLMs without the requirement for
ﬁnetuning or reinforcement learning.
Coaching in the context of physical activity ranges from delivering tailored products that serve elite
athletes, to creating motivational tools that support inactive users to become ﬁtter through a progressive
and personalised programs. Our LLM is designed to target latter use case to help users lead more active
lifestyle using behavioral nudges and resolving barriers through conversations.
Behavioral science oﬀers theoretical frameworks to help understand the factors inﬂuencing human behavior
and design eﬀective behavior change interventions for a given context. COM-B is a well-known framework
which conceptualizes behavior change along three axes: Capability (the psychological and physical skills to
act); Opportunity (the physical and social conditions to act); and Motivation (the reﬂective and automatic
mental processes that drive action) (38). Behavioral science can be useful to guide the design of automated
nudging systems for habit formation (39).
We extend the PACE (16) work on designing automated physical activity coaching engine based on an
analogous behavior science framework called Fogg’s Behavior Model. A rule-based automated nudging agent
based on this model had comparable outcomes to human coaches in terms of user step count and engagement.
In this study, we extend ﬁndings of the PACE study by connecting the strengths of a behavioral science
rule-based model with the conversational versatility of an LLM. The goal is to address the broader question
of how behavior science principles might guide or constrain conversational LLMs. Speciﬁcally, we make use
of priming and dialogue re-ranking. These are both lightweight techniques that do not require additional
model retraining or ﬁnetuning. Overall, the key contributions contributions of this study are as follows: 1. Deﬁning evaluation metrics for LLM conversations in the activity coaching domain 2. Introducing two diﬀerent approaches to behavioral science knowledge infusion: coach phrase priming
and dialogue re-ranking 3. Evaluating the beneﬁt of knowledge infusion relative to an unprimed LLM using quantitative and
qualitative approaches 2. Methods The following sections outline the datasets, language modeling techniques and evaluation methods used. 2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs 2.1. Data The previous PACE study dataset was re-purposed for this analysis (16). Speciﬁcally, this dataset was used
to construct the example coaching phrases used in the behavior science priming, create training data for
ﬁnetuning BERT user and coach statement classiﬁers and to select the user queries (initial user responses) in
simulated conversations for evaluation. This dataset consists of dialogue transcripts between ﬁtness coaches
and subjects, generated from real coaching interactions across various activity habit formation related issues.
In this Wizard-of-Oz study design, consented subjects were randomized to coaches or coaches using a FBM
assistant that suggested example responses based on behavior science using a rule-based engine. The dataset
included 520+ conversations from 33 participants over 21 days. A total of 6 independent annotators labeled
these conversations as one of Motivation, Capability and Opportunity. Both user and coach statements
where separately annotated with presence or absence of each of these three themes. Data collection and
annotation protocol is described in detail in (16). 2.2. Language models The Language Models for Dialog Applications (LaMDA) pretrained LLM was used as the primary archi-
tecture (21), with no further ﬁnetuning. LaMDA is a decoder-only transformer architecture with 64 layers,
used here in its 137 billion parameter conﬁguration. We used the following LaMDA hyperparameters: tem-
perature 0.9; maximum token length 1024, top k (controls sampling diversity) 40. LaMDA has an option to
provide context alongside the LLM prompt - this was how the coach phrase priming was conducted. LaMDA
also provides top-k outputs, which were used in the re-ranking (see below). 2.3. Coach phrase priming Coach phrase priming was performed by inputting 30 example coach nudges as context to the LLM prior to
the prompt. The 30 nudges were selected from the data in the PACE study - speciﬁcally the 10 most common
coach responses in each of the three behavior science categories of interest: C/O/M. Details regarding coach
phrase selection and priming method are described in section 1 of supplementary paper.
For example,
the Capability category included activity planning and barrier conversations; and Opportunity included
social engagement conversations and activity planning; and Motivation included congratulations and positive
aﬃrmation; [ref]. The order of the 30 nudges was randomized. The priming prompts are shown in Table 1. 2.4. Simulated dialogue The following LLM conﬁgurations were compared via simulated conversations with a single member of the
research team: 1. Unprimed (trigger prompt only) 2. Coach-primed (30 example nudges provided as LLM context) All conversations begin with the trigger prompt: Hey John, It’s time for your morning walk. The sub-
sequent user responses were sampled from a set of 9 user statements, with 3 each designed to evoke a low
Motivation, low Capability and low Opportunity (user queries are included in the Supplementary Materials
table 2). An example user statement with low opportunity was: I am super busy with work today. I have
chores to do in the morning and work meetings after that..
This culminated in a total of 18 transcripts: 9 each for the unprimed and primed LLMs. The conversations
were continued with dialogue between the LLM and the human interlocutor (researcher). The conversations
were terminated at a natural breakpoint at the discretion of the researcher. Any follow up questions to the
LLM response were added appropriately to continue the conversation on the original topic until a logical
end was reached. Additional example transcripts are contained in the Supplementary Materials. 3 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs Table 1: LLM prompts used in coach phrase priming. Behavior Science Priming The following is a conversation with an AI Health Coach.
The coach tries to motivate the users when the user lacks motivation, can resolve barriers.
Here are some examples of how a coach can help users: ”I know you probably have a busy schedule. I still think
you can manage and hit your goal of daily step count.”
”Looks like you are having a busy day. I would recommend
setting up gentle reminders daily of your goal to have them
as part of each day. Hope that can help you be all set for having an exercise routine!” ”You know, building a new habit is really really hard. But it doesn’t have to be that way :)
Starting with a little stroll outside for some fresh air cannot be bad idea as long as the
weather is right. So why not head out today for a few minutes, and come in. What do you think? :) ” ”You must keep that ﬁre burning, your excitement and conﬁdence for maintaining
a healthy lifestyle will take you far with healthy habit formation.
I believe a daily stroll with be no problem for you at all:)” ”So do you reckon you’ll manage your walk today?” ”It is nice and bright outside today. What is your plan for the day, why not start walking today?” ”The question you can ask yourself is that do you feel walking can help you?” ”You knew starting a healthy habit can be hard, but it’s a life changing
experience of rebuilding your identity as someone who exercises :)
If you’re not feeling up for a long walk today, perhaps we can aim for a shorter one? :)” ”You know walking can be especially enjoyable as it allows you to put
on your favourite playlist and podcast. So, what do feel like listening to today?” ... Coach prompt: Hey John, It’s time for your morning walk. 2.5. Constraining LLM responses using a COM-B classiﬁer In order to further constrain or guide the LLM to provide nudges based on COM-B principles, we trained
two classiﬁers to assess C/O/M levels: 1. User statement classiﬁer: Given a user statement sentence, the user-query classiﬁer assigns a high
vs low value for each of the capability, opportunity and motivation(COM) dimensions (multi-label
classiﬁcation). 2. Coach statement classiﬁer: Given a shortlist of 15 top LLM outputs, the coach response classiﬁer maps
each response to either C, O or M (multi-class classiﬁcation). The classiﬁers were designed as follows. The input string (could be multiple sentences) was embedded
using a BERT-base model with the ﬁnal layer ﬁnetuned over either a multi-label head (user statement
classiﬁer) or 3 separate C/O/M heads (coach statement classiﬁer). Models were optimised with a cross-
entropy loss. Separate user and coach classiﬁers were trained using samples of 432 user statements and 531
coach statements from the PACE study, manually annotated with C/O/M status. These datasets were split
70:10:20 across train, validation and test splits. Weights were not shared between the user and coach models. 4 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs Figure 1: Comparison of example conversations with unprimed, coach-primed and primed+reranked LLMs. 2.6. Simulated dialogue with re-ranking The simulated conversation experiment was repeated with the primed LaMDA model, using the above
classiﬁers to align the coach response to the inferred user need. For the 9 coach-primed LaMDA transcripts
above, a single user statement was manually selected as the most representative of the user’s behavioral
need.
The selected text was input into the user statement classiﬁer to identify the C/O/M need. The same user
text was input into the coach-primed LaMDA model to generate the top 15 candidate responses. These
15 responses were then separately run through the coach statement classiﬁer to generate a likelihood score
across each C/O/M category. The coach action was aligned based on the user’s inferred C/O/M need based
on the rules in Table 2 (i.e. the statement with the highest score in the desired coach action was chosen).
In addition, we conducted an ’oracle’ experiment where the user response was manually categorized into
C/O/M need and the corresponding coach-primed output was chosen.
Two manual review exercises were then conducted: 1. Comparing the coach-primed output to the classiﬁer re-ranked output; and 2. Comparing coach-primed with the oracle re-ranked output. Note that in both these review exercise,
only a single coach response was being adjudicated rather than an entire conversation as previous. 2.7. Evaluation attributes An evaluation framework was deﬁned based on four key attributes of an LLM-based ﬁtness coach: actionabil-
ity, realism, motivation and empathy. Coupled with a global assessment of coaching quality, these attributes 5 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs Table 2: COM-B policy to select nudge theme based on C/O/M values derived from the user statement
classiﬁer. Capability
Opportunity
Motivation
COM-B Action Low
High/Low
High/Low
Boost Capability
High
Low
High/Low
Boost Opportunity
High
High
High/Low
Boost Motivation Table 3: Evaluation attributes cross-referenced with established attributes of coaches and LLMs. Evaluation attributes
Coach attributes
LLM attributes Actionability
Professional competence
Informativeness
Realism
Context sensitivity
Sensibleness & safety
Motivation
BS interventions
Interestingness
Empathy
Social-emotional competences
Groundedness informed the design of the quantitative and qualitative review detailed below. Table 3) shows how these
attributes align with published evaluation frameworks for coaches (40) and for LLMs (21; 41). 2.8. Quantitative review For each architecture, the unprimed versus coach-primed transcripts generated from the same starting prompt
were compared in a pairwise manner. The conversations were evaluated based on the following quantitative
attributes: average length of reply, number of conversational turns, user sentiment at conversation end, pres-
ence of questions in the coach dialogue and use of coaching-speciﬁc words (’goal’, ’health’, ’routine’, ’recover,
’challenge’, ’workout’, ’training’, ’rest’). The results for unprimed versus primed LLMs were compared using
a two sided t-test. 2.9. Qualitative review 8 independent reviewers were selected to adjudicate the transcripts. Reviewers were blinded to the manner
of LLM priming (naive vs BS) and Re-Ranked LLM variations (naive vs primed vs re-ranked LLM). Raters
completed a structured survey with Likert scale responses for the same pairwise comparisons of naive-primed
and coach-primed transcripts as above. Questions evaluated the following attributes of the conversation:
actionability, realism, empathy, motivation, overall quality. The questions are included in Supplementary
Table 2. Table 4: Quantitative assessment of unprimed versus coach-primed LLM conversations. Metric
Unprimed
Coach-primed
p value Average length of LLM reply (# words ± S.D.)
25.7 ± 6.5
23.7 ± 7.1
0.3 Turns of conversation by user/LLM (# turns ± S.D.)
3.1 ± 0.3
3.7 ± 0.7
0.2 Conversations ending with positive user sentiment (%)
30
60
0.02 Conversations containing a question asked by LLM (%)
0
30
0.03 Conversations containing coaching-speciﬁc words used by LLM (%)
40
80
0.08 6 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs Table 5: Qualitative assessment of unprimed versus coach-primed LLM conversations based on the reviews
of 8 adjudicators. Survey question (1, strong disagree 5, strong agree)
Unprimed
Coach-primed
p value Which conversation provides a better overall coaching experience (%, remainder unsure)
21
72
- The quality of the coaching experience is high
3.42 ± 0.88
3.97 ± 1.0
<0.001 The coach provides concrete ﬁtness strategies that are actionable to the user
3.61 ± 1.1
4.25 ± 0.84
<0.001 The coach provides motivation or encouragement to the user
3.68 ± 1.1
3.97 ± 0.97
0.095 The coach is empathetic toward the user’s needs and challenges
3.51 ± 1.0
3.83 ± 1.1
0.123 The language used by the coach is realistic and appropriate for the setting
3.71 ± 1.1
4.10 ± 1.0
0.02 Table 6: Class balance and model performance on C/O/M classiﬁcation for user statements. Class balance (high:low)
Classiﬁer performance
(ROC-AUC)
Train
Test Motivation
220:40
68:16
0.86
Capability
158:66
51:40
0.77
Opportunity
112:34
52:13
0.83 3. Results Quantitative analysis (Table 4) showed that the number of turns of dialogue was higher in coach-primed
versus unprimed. Across both architectures, priming was associated with a signiﬁcant boost in the rate of
conversations ending in a positive user sentiment, the rate of question-asking by the coach LLM, and the
use of coaching-related vocabulary.
To determine whether ratings for the primed and unprimed models diﬀered from each other, we ran a series
of linear mixed model analyses. These included a ﬁxed eﬀect for primed vs unprimed, and random eﬀects
for rater and prompt to account for non-independence of the observations.
Regarding message content,
the ratings of blinded reviewers were overall more favorable for the coach-primed LLMs. The (Table 5).
Speciﬁcally, the coach-primed model was rated as signiﬁcantly higher in terms of quality, providing actionable
suggestions, and using realistic language. The ratings for the classiﬁer re-ranked versus unprimed were less
conclusive, but this may be because those ratings were based on a single statement response from the model
rather than a full back-and-forth dialogue.
Tables 6 and 7 show the performance of the user and coach statement classiﬁers, including the size and label
distribution in the train and test sets. The BERT-base model had 81% multi-class accuracy in accurately
classifying the coach message as motivation, capability or opportunity.
To quantitatively evaluate the re-ranked response compared to the default response, 8 independent re-
viewers rated both the responses across several dimensions of activity coaching (Table 8). Based on Likert
scale responses, the re-ranked answers were rated as more actionable [3.66±0.89 vs 2.88±0.85]; however the
other attributes did not reach statistical signiﬁcance. Table 7: Model performance on C/O/M classiﬁcation for coach statements. Category
Train
Test
Precision
Recall
F1 Score
Multi-class accuracy Motivation
256
121
0.87
0.86
0.86
0.81
Capability
139
66
0.88
0.72
0.79
Opportunity
212
74
0.83
0.71
0.77 7 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs Table 8: Qualitative review of coach-primed versus classiﬁer re-ranked and oracle re-ranked dialogues. Survey question (1, strong disagree →5, strong agree)
Unprimed
Coach-primed
Classiﬁer Re-ranked
Oracle Re-ranked
p value
(Classiﬁer re-ranked
vs Unprimed The coach response provided concrete ﬁtness strategies that are actionable
2.88 ± 0.85
3.22 ± 0.95
3.66 ± 0.89
4.02 ± 0.68
0.02
The coach response to user questions was in a realistic manner
3.02 ± 0.98
3.23 ± 0.97
3.59 ± 0.99
3.75 ± 0.80
0.18
The coach response provided motivation or encouragement to the user
3.05 ± 1.0
3.19 ± 0.85
3.75 ± 0.92
3.45 ± 1.05
0.36
The coach is empathetic toward the user’s needs and challenges
2.94 ± 0.99
3.05 ± 0.94
3.56 ± 0.87
3.77 ± 0.83
0.47
The language used by the coach is realistic and appropriate for the setting
3.33 ± 0.87
3.48 ± 0.84
3.69 ± 0.87
3.78 ± 0.79
0.29
Average total score
3.04 ± 0.95
3.24 ± 1.36
3.65 ± 1.32
3.75 ± 0.84 4. Discussion This proof-of-concept study introduces two methods to infuse behavior science into LLM dialogue.
We
demonstrate that behavior science-based priming is a simple but eﬀective strategy to tailor LLMs for activity
coaching, with speciﬁc beneﬁts in terms of actionability and the provision of concrete coaching advice.
Additionally, post-hoc re-ranking of LLM responses based on behavior science principles can further enhance
attributes such as perceived empathy.
BS priming yielded some signiﬁcant boosts in various proxies for coaching quality. This trend was evident
across both quantitative and qualitative metrics.
Notably, coach phrase priming was associated with a
higher number of conversational turns, a greater rate of question-asking, and more frequent use of coaching
vocabulary. Manual review also judged coach phrase priming as providing signiﬁcantly greater motivation
and concrete coaching strategies versus the unprimed LLM. This suggests that BS priming may be an eﬀective
and accessible strategy for customising LLMs for various coaching scenarios.
A unique aspect of this work is the combination of priming with post-hoc re-ranking to enable knowledge
infusion at multiple touchpoints. Interestingly, re-ranking resulted in signiﬁcant incremental improvements
in actionability, with upward trends in empathy, motivation and realism that did not meet statistical sig-
niﬁcance. We demonstrate this uplift both for a classiﬁer-based re-ranking, which introduces error from
mis-classiﬁcation; and for oracle-based re-ranking, which showed a further marginal advantage over the for-
mer. Together, these results demonstrate the ability to stitch together multiple simple constraints as part of
a hybrid knowledge infusion strategy. As LLMs become more pervasive in the coaching domain, this will be
increasingly important.
Since Capability has marginally lower user statement classiﬁer accuracy, it was wrongly identiﬁed as
motivation in few cases of classiﬁer based BeSci dialogue alignment LLM. This resulted in higher motivational
character to classiﬁer based LLM over Oracle LLM at the expense of lower empathy and actionability scores.
This study has a number of limitations.
First, the evaluation was predominantly based on simulated
conversations with a single human interacting with the LLMs, which invariably introduces bias even in the
presence of blinding. Future work could trial a similar evaluation with larger groups of users engaging in
dialogue, as per [ref]. The rudimentary priming method used here could be extended, e.g. by more explicit
instruction prompting or chain of thought prompting. The re-ranking method was limited in only focusing
on a single user query and coach response. In reality, it is important to consistently align the coach responses
to user need throughout a conversation and adapt as the dialogue unfolds. Methods such as reinforcement
learning with human feedback can help to oﬀer this adaptability (29). Finally, the behaviour model used
was a simplistic one that conceptualizes user behaviour only along three axes - future studies could consider
using more sophisticated behavior science frameworks, which may help to better target coach actions. 5. Conclusion Knowledge infusion methods based on behavior science principles can be used to improve the quality of LLM-
generated physical activity related conversations. The combination of coach phrase priming with re-ranking
of LLM outputs oﬀers optimal results in terms of manually-adjudicated actionability, empathy and overall
coaching experience. These methods can help to constrain and guide LLMs in various coaching scenarios. 8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287995
doi: 
medRxiv preprint INTERNAL - Knowledge infusion for fitness LLMs",1
"Background During the pandemic period, healthcare systems were substantially reorganized for managing 
COVID-19 cases. The corresponding changes on the standard care of persons with chronic diseases and the 
potential consequences on their outcomes remain insufficiently documented. This observational study investigates 
the direct and indirect impact of the pandemic period on the survival of kidney transplant recipients (KTR), in 
particular in those not hospitalized for COVID-19. Methods We conducted a cohort study using the French national health data system which contains all healthcare 
consumptions in France. Incident persons with end stage kidney disease between January 1, 2015 and December 
31, 2020 who received a kidney transplant were included and followed-up from their transplantation date to 
December 31, 2021. The survival of KTR during the pre-pandemic and pandemic periods was investigated using 
Cox models with time-dependent covariates, including vaccination and hospitalization events. Findings There were 10,637 KTR included in the study, with 324 and 430 deaths observed during the pre-
pandemic (15,115 person-years of follow-up) and pandemic periods (14,657 person-years of follow-up), including 
127 deaths observed among the 659 persons with a COVID-19-related hospitalization. In multivariable analyses, 
the risk of death during the pandemic period was similar to that observed during the pre-pandemic period (hazard 
ratio (HR) [95% confidence interval]: 0·92 [0·77–1·11]), while COVID-19-related hospitalization was associated 
with an increased risk of death (HR: 10·62 [8·46–13·33]). In addition, pre-emptive kidney transplantation was 
associated with a lower risk of death (HR: 0·71 [0·56–0·89]), as well as a third vaccine dose (HR: 0·42 [0·30–
0·57]), while age, diabetes and cardiovascular diseases were associated with higher risks of death. Interpretation Considering persons living with a kidney transplant with no severe COVID-19-related 
hospitalization, the pandemic period was not associated with a higher risk of death. Funding Initiative Économie de la Santé de Sorbonne Université (Idex Sorbonne Université, programmes 
Investissements d’Avenir); Ministère de la Solidarité et de la Santé (PREPS 20-0163). Keywords COVID-19; Epidemiology; Kidney; Mortality; Proportional Hazard Models; Transplants. . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 3 Introduction Kidney transplant recipients (KTR) are at increased susceptibility to many viral infections because of their 
immunosuppressive treatments, and this led to justifiable anxiety about the effect of COVID-19 on them.1,2 As 
mentioned by Vart et al,3 KTR were quickly identified as a vulnerable population, prompting tremendous efforts 
from the nephrology community to gather data informing clinical practice in record time. Reviews examined 
princeps studies which collected and reported such data on KTR,4–7 and indicate that documenting the direct impact 
of COVID-19 in KTR raised many concerns. The issues addressed evolved with time and may be roughly 
categorized according to whether these studies were conducted before or after the availability of vaccines against 
COVID-19. Before vaccine availability, a major concern was the mortality of KTR with COVID-19, and a review 
based on 74 studies totalizing 5,559 KTR estimated this risk at 23% [95% confidence interval: 21%–27%].5 A 
study based on the 87,809 hospital admissions with COVID-19 that occurred in France until June 15, 2020, 
reported that compared to the general population, KTR had a 4·6-fold higher risk of being hospitalized with 
COVID-19, and a 7·1-fold higher risk of in-hospital mortality.8 After vaccination roll-out, a major concern was 
assessing the protection provided by the vaccines,7 including a personalization of the vaccination schedule in KTR 
in order to take into account their decreased immune response.9,10 A French national study conducted in the earliest 
period of the vaccination period, when two doses constituted a complete vaccination schedule, reported that 
compared to the persons vaccinated in the general population, vaccinated KTR had a 5·9-fold higher risk of 
hospitalization for COVID-19 and a 6·3-fold higher risk of in-hospital mortality.11 In contrast with the great interest brought to documenting the direct impact of COVID-19 on KTR, the indirect 
impact of the pandemic on the health of KTR is a more complex topic to investigate. More precisely, although 
most KTR did not experience a severe episode of COVID-19, to our knowledge no study has yet addressed the 
following important issue: were the pandemic-related changes in healthcare organization simultaneously 
associated with a higher mortality in KTR who did not experience a severe episode of COVID-19? Indeed, in 
many countries, healthcare systems were reorganized during the COVID-19 pandemic in order to manage 
symptomatic cases of COVID-19 and such a reorganization may have indirectly impacted the health outcomes of 
those who did not experience a severe COVID-19 episode but who suffered from other diseases, particularly 
persons with chronic diseases including KTR. For example, in France, intensive care capacity was prioritized and 
extended during the pandemic, with a maximum of 10,000 beds available while the national capacity of such beds 
was 5,080 before the pandemic.12 Conversely, the activity of kidney transplantation was totally suspended between 
March 10 and May 15, 2020.13 In order to investigate the indirect impact of the pandemic on KTR, we undertook a national observational study 
in France comparing the survival (all-cause mortality) in this vulnerable population before and during the 
pandemic. Our main goal was to investigate if there was an excess mortality in KTR not hospitalized with COVID-
19 during the pandemic. The longitudinal analyses performed took into account the potential role of relevant 
covariates such as comorbidities, age, time between the initiation of dialysis and the transplantation date, and time 
since transplantation date. Assessing the indirect impact of the pandemic on the survival of KTR also required 
estimating the direct effect of COVID-19. Therefore, this study also details the dynamics of COVID-19 
vaccination in KTR in France, and investigates how vaccination may have impacted COVID-19-related 
hospitalizations and survival in KTR. Methods This observational study was conducted according to STROBE guidelines.14 This study was based on data from the Système National des Données de Santé (SNDS) Database, a French 
National Health Data System which covers nearly 99% of the French population.15 Any health care consumption 
subjected to a reimbursement by the national medical insurance system is recorded and linked to the individual 
who received the corresponding healthcare through a unique and anonymous identifier. Therefore, data include 
prescription-based medication deliveries with corresponding Anatomical Therapeutic Chemical (ATC) codes and 
dates of delivery, medical consultations, hospitalizations with corresponding International Classification of 
Diseases tenth Revision ICD10 codes for primary and related diagnoses, and dates of admission and discharge. 
The SNDS also contains demographic data, e.g., sex and dates of birth and death, and the date of each dose of 
COVID-19 vaccine received by a given individual. With the growing interest of the scientific medical community . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 4 in real-world data from large administrative healthcare databases, the SNDS is increasingly used for 
pharmacoepidemiologic studies,16,17 including studies on COVID-19.8,11,18,19 The study took advantage of the G9 mapping, a useful tool provided within the SNDS which allows an automatic 
selection of individuals labelled with a given chronic disease in a given year.20 Several pathologies have been 
considered in the G9 mapping including persons with end stage terminal kidney disease (ESKD) and we used it 
for selecting the persons included in the study. This selection is therefore reproducible and based on an algorithm 
developed by experts of the SNDS and which was extended to all persons in the database from 2015 and onwards. Patients included in the study First, prevalent persons with ESKD during any year between 2015 and 2020 were selected in the SNDS using G9 
mapping. These included any individual who had spent more than 45 days on haemodialysis or had a peritoneal 
dialysis or had benefited from a kidney transplant or was followed-up after transplantation. Then, among this 
population of prevalent persons with ESKD between 2015 and 2020, we only selected those who developed ESKD 
during this period (incident cases), benefited from a kidney transplantation before December 31, 2021, and were 
aged between 18 and 85 years at the date of transplantation. This restriction to incident persons with a 
transplantation is a critical characteristic of the study, and was adopted in order to guarantee that the history of the 
disease was duly documented for each individual included in the study, especially the delay between ESKD onset 
and transplantation date. Pre-pandemic and pandemic periods of the study We set the beginning of the pandemic period on March 1, 2020: the number of nationally reported cases rose from 
100 to 1000 between February 29 and March 8, with a first lockdown starting on March 17. Pre-pandemic period 
was therefore defined between January 1, 2015 and February 29, 2020. The pandemic period was censored on 
December 31, 2021 considered as the end date of the study. In France, the roll-out of COVID-19 vaccination began 
on December 27, 2020. Statistical analyses The date of transplantation was considered as the initial time (t0) of the corresponding individual follow-up. The 
investigations on KTR survival during the pre-pandemic and pandemic periods also considered the following 
additional covariates that we deemed important to simultaneously study: COVID-19-related hospitalization 
event(s) (COVID-19 as the principal or related code of an hospitalization was considered as a proxy for a severe 
COVID-19 episode), vaccination against COVID-19, diabetes and cardiovascular comorbidities (including stroke 
sequelae, chronic heart failure, coronary diseases, peripheral arterial obliterative disease), time spent on dialysis 
before transplantation date, mode of entry into transplantation (pre-emptive, i.e., without prior dialysis, or not), 
age and sex. We also explored the effect of the vaccine for preventing COVID-19-related hospitalization. The 
value of some covariates might have varied along the longitudinal follow-up of the individuals, and accordingly, 
the following variables were appropriately handled as time-dependent variables in the analyses: period (pandemic 
versus pre-pandemic), diabetes and cardiovascular comorbidities (baseline presence or incidence versus absence), 
age, history of COVID-19-related hospitalization, and COVID-19 vaccination. The age variable was transformed 
into an ordinal variable handled according to the following breakdown: 18–44 (reference), 45–54, 55–64, 65–74 
and 75–84 years. COVID-19-related hospitalization was also considered as an ordinal variable with 3 modalities: 
0, 1, and ≥2 hospitalizations. Analyses assumed that no information was missing in the database. Analyses of KTR survival during the pre-pandemic and pandemic periods were conducted using a Cox modelling 
approach with time-dependent covariates. Therefore, depending on the timing of event and censoring features, 
some KTR had their period status (either pre-pandemic or pandemic) changed along their follow-up, while other 
did not (Figure 1). In addition, the study design required censoring the follow-up duration of individuals at five 
years because no counterfactual observation could be observed for longer durations of follow-up. Several statistical 
models were fitted: a univariable model (with no covariate), a multivariable model adjusted only for COVID-19-
related hospitalizations, and finally a model adjusted for all covariates except vaccination. Additional analyses 
detailed the schedule of vaccination doses received by the individuals included in this study, and also investigated 
the relationship between this vaccination and two events: hospitalization for COVID-19 and death. Analyses were performed using R version 4·1·2. A P value < 0·05 was considered statistically significant. . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 5 A, B, and C: whole follow-up of the person in either the pre-pandemic (person A) or the pandemic (person B or C) period; 
D,E,F: whenever a patient had a transplantation date during the pre-pandemic period and survived until the beginning of the 
pandemic period, this person left the pre-pandemic follow-up group and entered the pandemic group when the pandemic period 
began; F and G: follow-up of any person included in the study has been censored at five years since there would not be any 
counterfactual observation of a person with a follow-up duration greater than five years and two months. Figure 1. Graphic examples of some individual follow-up in the pre-pandemic and pandemic periods 
according to the timing of event or censoring features Results Figure 2 details the study profile: considering the whole population of adult persons living with ESKD in France 
each year, from 2015 to 2020 (prevalent cases), the selection of incident ESKD cases occurring each year resulted 
in a total of 62,827 persons, with eventually 10,637 of them (nearly 17%) who benefited from a first kidney 
transplant at an age < 85 years old between 2015 to 2021. The inclusion of these 10,637 persons in the study 
resulted in a total follow-up of 29,772 person-years analysed in this study. 2015
2016
2017
2018
2019
2020
2021 March 1, 2020 Pre-pandemic period
Pandemic period X Censoring in the pre-pandemic period due to the pandemic outbreak Kidney transplantation event (time 0 of follow-up) Censoring in the pandemic period due to the end of the study Death event Censoring at 5 years of follow-up Labels A to G illustrate typical cases of the follow-up of individuals 
according to the timing of event or censoring features: A B X
E X
D C G X
F Year . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 6 Figure 2. Study flow chart Table 1 presents the characteristics of the KTR included in the study at the time of transplantation (baseline). The 
median age at kidney transplantation was 54 years old (interquartile range (IQR): 43–66), 54 years old (IQR: 42–
65) and 56 years old (IQR: 45–67) for the KTR whose transplantation date was within the pre-pandemic period 
and the pandemic period, respectively; 64% were males and 44% presented with a comorbidity (diabetes, a chronic 
cardiovascular disease, or both). The median duration of pre-transplant dialysis was 1·21 year (IQR: 0·18–2·33), 
0·89 year (IQR: 0·00–1·87) and 2·24 years (IQR: 1·15–3·62) for the KTR whose transplantation date was within 
the pre-pandemic and pandemic period, respectively. All transplantations 
(n=10,637) Pre-pandemic period 
transplantations (n=7,772) Pandemic period 
transplantations (n=2,865) Age group 18-44 
2,982 (28·03) 
2,278 (29·31) 
   704 (24·57) 45-54 
2,368 (22·26) 
1,756 (22·59) 
   612 (21·36) 55-64 
2,343 (22·03) 
1,689 (21·73) 
   654 (22·83) 65-74 
2,102 (19·76) 
1,458 (18·76) 
   644 (22·48) 75-84 
   842 (7·92) 
    591 (7·60) 
   251 (8·76) Male sex 
6,830 (64·21) 
4,992 (64·23) 
1,838 (64·15) Diabetes 
2,885 (27·12) 
2,126 (27·35) 
   759 (26·49) Chronic cardiovascular disease 
3,227 (30·34) 
2,249 (28·94) 
   978 (34·13) All data reported as n (%) Table 1. Characteristics of the kidney transplant recipients included in the study Transition flows along the follow-up of the 10,637 KTR of the study (Figure 3) indicate that during the 15,115 
person-years followed up in the pre-pandemic period, 324 death events were observed, while during the 14,657 
person-years followed-up in the pandemic period, 430 death events were observed, including 127 deaths observed 
among the 659 KTR hospitalized for COVID-19. The upper panel of Figure 4 shows how the cumulative 
probability of death during the pre-pandemic and pandemic periods evolved according to time since transplantation 
date. The crude HR [95% confidence interval] of death during pandemic period versus pre-pandemic period was 
1·59 [1·37–1·86] globally, 1·78 [1·42–2·23] when restricted to the first year after transplantation, and 1·45 [1·18–
1·78] when considering only the following years (Table 2, model M1). However, the excess mortality was 
concentrated in KTR with COVID-19-related hospitalization (red curve in the bottom panel of Figure 4). 
Conversely, during the pandemic period, survival of KTR without COVID-19-related hospitalization (mauve 
curve in the bottom panel of Figure 4) was similar to that of the KTR during the pre-pandemic period (green curve 
in the bottom panel of Figure 4). ESKD prevalent persons ESKD incident persons 10,646 10,637 persons included in the analysis, resulting in a total follow-up of 29,772 person-years, 15,115 person-years during the pre-pandemic period,
and 14,657 person-years during the pandemic period. no kidney transplantation: 52,221 persons excluded age > 85: 9 persons excluded ESKD incident persons with a (first) kidney transplantation a given year 88,761 Year 2015 10,414 560 in 2015
500 in 2016
466 in 2017
338 in 2018
269 in 2019
132 in 2020
116 in 2021 Total: 2381 91,606 Year 2016 10,303 579 in 2016
528 in 2017
456 in 2018
361 in 2019
160 in 2020
179 in 2021 Total: 2263 94,612 Year 2017 10,740 673 in 2017
489 in 2018
505 in 2019
255 in 2020
243 in 2021 Total: 2165 96,630 Year 2018 10,600 623 in 2018
448 in 2019
306 in 2020
305 in 2021 Total: 1682 99,285 Year 2019 11,070 610 in 2019
339 in 2020
395 in 2021 Total: 1344 100,921 Year 2020 9,740 454 in 2020
357 in 2021 Total: 811 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 7 Figure 3. Detailed follow-up of KTR in the pre-pandemic and pandemic periods This finding is supported by the multivariable analysis when considering whether individuals experienced COVID-
19-related hospitalization (Table 2, model M2): when adjusting for this factor, the risk of death during the 
pandemic period was not different from that during the pre-pandemic period (HR [95% CI] = 1·15 [0·97–1·35]), 
while in contrast, experiencing COVID-19-related hospitalization was associated with a dramatic higher risk of 
death (HR = 13·59 [11·15–16·57]). Therefore, the analysis of model M2 suggests that the excess mortality during 
the pandemic period in the univariable model M1 was primarily driven by the KTR with COVID-19-related 
hospitalizations. Nevertheless, in the adjusted model M2, an excess risk of death remains observed in KTR 
experiencing the pandemic during the first year after their transplantation date (HR = 1·28 [1·00–1·97]), while the 
very high risk of death in individuals with COVID-19-related hospitalization was stable whether considering the 
first year after the transplantation date or considering the following years (Table 2, model M2). Table 3 presents the HR issued from a third model (model 3) which extends the adjustment made in model 2 to 
additional relevant covariates: as observed with model 2, the risk of death during the pandemic period was similar 
to that observed during the pre-pandemic period, and experiencing COVID-19-related hospitalization dramatically 
increased the risk of death. Model 3 further indicates that the risk of death was higher in the presence of chronic 
diseases, and this risk also increased with age. The risk of death was not associated with sex or dialysis duration, 
whereas pre-emptive transplantation was associated with a higher post-transplantation survival (HR 0·71 [0·56–
0·89]). Additional analyses presenting the dynamics of COVID-19 vaccination and investigating the impact of 
vaccination are shown in the Supplementary Appendix: landmark analyses detail how the course of vaccination 
status in KTR was associated with the probability of hospitalization for COVID-19. The Supplementary Appendix 
also shows that adjusting for the vaccination status rather than for a history of hospitalization in the Cox model 
did not modify the patterns of the HRs estimated in model 3, except for dialysis duration before transplantation 
that became associated with the risk of death. While the first and second doses of vaccine were not associated with 
the risk of death, receiving three doses or more was associated with a 58% decrease in the risk of death (HR 0·42 
[0·30–0·58]). 324 deaths
during the
pre-pandemic
period 659 KTR with a 
COVID-19-related 
hospitalization 127 deaths 430 deaths during the pandemic period 8,383 KTR survived Dec. 31,  2021 57 KTR censored 
during the  pre-
pandemic period 
because of a follow-up 
duration > 5 yr 1,443 KTR censored during the  pandemic period because of a follow-up duration > 5 yr 560 KTR transplanted in 2015 1,077 KTR transplanted in 2016 1,665 KTR transplanted in 2017 1,905 KTR transplanted in 2018 2,190 KTR transplanted in 2019 1,645 KTR transplanted in 2020 1,595 KTR transplanted in 2021 303 deaths 10,637 KTR included in the study, 7,772 transplanted during the pre-pandemic period,
2,865 transplanted during the pandemic period 9,597 KTR without any COVID-19- related hospitalization Pre-pandemic period
Pandemic period . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 8 Kidney transplant recipients followed-up during the pandemic period are shown altogether in the upper panel (pale gold curve), 
and split in the bottom panel according to whether they experienced a COVID-19-related hospitalization (red curve) or not 
(mauve curve). The pre-pandemic curve (green curve) is shown in both panels. Figure 4. Probability of death according to time since transplantation date . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 9 Table 2. Associations of pandemic period with mortality in kidney transplant recipients, univariable model and model adjusted for history of COVID-19-related 
hospitalization Model 
Variable 
Variable value 
Time range From transplantation to study end 
First year after transplantation 
From the beginning of the second year after 
transplantation until study end Deaths 
observed (n) / 
person-
years(n) HR [95% CI] 
P 
Deaths 
observed (n) / 
person-years 
(n) HR [95%CI] 
P 
Deaths 
observed (n) / 
person-years 
(n) HR [95% CI] 
P M1: unadjusted 
model Period 
Pre-pandemic 
(reference) 324/15,115 
  1 (reference) 
 
172/6,598 
  1 (reference) 
 
152/8,517 
1 (reference) Pandemic  
430/14,657 
1·59 [1·37–1·86] 
<0·01 
139/3,110 
1·78 [1·42–2·23] 
<0·01 
391/11,547 
1·45 [1·18–1·78] 
<0·01 M2: adjusted for 
history of 
COVID-19-
related 
hospitalization Period 
Pre-pandemic 
(reference) 324/15,115 
  1 (reference) 
 
172/6,598 
  1 (reference) 
 
152/8,517 
1 (reference) Pandemic  
430/14,657 
1·15 [0·97–1·35] 
0·09 
139/3,110 
1·28 [1·00–1·65] 
<0·05 
391/11,547 
1·05 [0·84–1·30] 
0·09 COVID-19-
related 
hospitalization None (reference) 
303/14,208 
  1 (reference) 
 
97/3,010 
  1 (reference) 
 
206/11,198 
1 (reference) ≥1 hospitalization 
127/449 
13·59 [11·15–16·57] 
<0·01 
42/100 
13·66 [9·51– 19·63] 
<0·01 
85/349 
13·56 [10·71–17·18] 
<0·01 10 Table 3. Multivariable analysis of factors associated with mortality in kidney transplant recipients (model 3) Variable 
Variable value 
Time range From transplantation to study end 
First year after transplantation  
From the beginning of the second year after 
transplantation until study end Deaths 
observed (n) / 
person-years 
(n) HR [95% CI] 
P 
Deaths 
observed (n) / 
person-years 
(n) HR [95%CI] 
P 
Deaths 
observed (n) / 
person-years 
(n) HR (95% CI) 
P Period 
Pre-pandemic 
(reference) 324/15,115 
  1 (reference) 
 
172/6,598 
  1 (reference) 
 
152/8,517 
1(reference) Pandemic  
430/14,657 
0·94 [0·78–1·12] 
 0·46 
139/3,110 
1·09 [0·83–1·42] 
0·54 
391/11,547 
0·83 [0·66–1·05] 
0·12 Hospitalization for COVID–19 
None (reference) 
303/14,208 
  1 (reference) 
 
97/3,010 
  1 (reference) 
 
152/8,517 
  1 (reference) 1 hospitalization 
100/373 
10·66 [9·61–13·38] 
<0·01 
29/80 
9·75 [6·4314·82] 
<0·01 
71/293 
11·26 [8·5814·78] 
<0·01 ≥2 hospitalizations 
27/76 
14·31 [9·61–21·31] 
<0·01 
13/20 
19·24 [10·63–34·82] 
<0·01 
14/56 
11·35 [6·57–19·61] 
<0·01 Age group 
18–44 
48/9,129 
  1 (reference) 
 
17/2,809 
  1 (reference) 
 
31/6,320 
  1 (reference) 45–54 
79/6,972 
1·94 [1·35–2·77] 
<0·01 
37/2,201 
2·53 [1·42–4·49] 
<0·01 
42/4,771 
1·59 [1·00–2·53] 
<0·05 55–64 
163/6,458 
3·62 [2·61–5·01] 
<0·01 
60/2,137 
3·58 [2·07–6·17] 
<0·01 
103/4,321 
3·60 [2·39–5·41] 
<0·01 65–74 
277/5,265 
6·87 [5·02–9·42] 
<0·01 
121/1,837 
7·84 [4·67–13·19] 
<0·01 
156/3,428 
6·25 [4·20–9·30] 
<0·01 75–84 
187/1,948 
11·90 [8·60–16·47] 
<0·01 
76/724 
11·30 [6·60–19·34] 
<0·01 
111/1,224 
12·46 [8·28–18·76] 
<0·01 Sex 
Female 
243/10,683 
1·09 [0·93–1·27] 
0·30 
103/3,476 
1·10 [0·87–1·40] 
0·43 
140/7,276 
1·06 [0·86–1·29] 
0·59 Diabetes 
Yes 
409/9,656 
1·36 [1·17–1·58] 
<0·01 
151/2,880 
1·29 [1·02–1·62] 
0·03 
258/6,776 
1·43 [1·18–1·75] 
<0·01 Chronic cardiovascular disease 
Yes 
471/9,672 
1·90 [1·62–2·23] 
<0·01 
183/3,047 
1·85 [1·45–2·35] 
<0·01 
288/6,625 
1·94 [1·57–2·39] 
<0·01 Dialysis duration (year) 
 
754/29,772 
1·04 [0·97–1·11] 
0·30 
211/9,708 
1·02 [0·93–1·12] 
0·67 
543/20,064 
1·05 [0·86–1·17] 
0·41 Pre–emptive kidney transplant 
Yes 
120/8,510 
0·71 [0·56–0·89] 
<0·01 
39/2,324 
0·73 [0·49–1·07] 
0·10 
81/6,186 
0·69 [0·51–0·92] 
0·01 11 Discussion The study reported here addresses four main issues considering the whole French subpopulation of persons who 
benefited from a kidney transplant since year 2015: the indirect impact of the pandemic period on the survival of 
persons with no severe COVID-19-related event, the impact of experiencing COVID-19-related hospitalizations, 
the course and impact of vaccination, and the global relationship between various relevant factors and survival. First, to our knowledge, this study is the first to investigate the indirect impact of the pandemic on the health status 
of KTR. Indeed, several features specific to the pandemic period such as the prioritization of hospital beds for 
patients with severe COVID-1912,21 and modifications of immunosuppressive regimen22 have contributed to 
modify the standard care management of KTR. Such modifications might result in decreasing the long-term kidney 
function and overall survival of KTR, a feature not yet detectable during the limited follow-up in our study. 
However, the present study demonstrates that no excess mortality was yet observed during the pandemic period 
(bottom panel of Figure 4, model 3 in Table 3) in the whole national French sub-population of persons having 
benefited from a kidney transplant since 2015 and with no history of COVID-19-related hospitalization. Such an 
important result indicates that despite the disruptions of the French healthcare system related to the pandemic, the 
survival of the persons living with a kidney transplant has not been worsened. Nevertheless, this result contrasts with the dramatic increase in mortality observed when considering the remaining 
individuals studied, i.e., the KTR who experienced COVID-19-related hospitalizations. Unfavourable outcomes 
in KTR hospitalized for COVID-19 when compared to persons without solid organ transplant have been previously 
reported.23,24 However, these results differ with those reported in other studies based on propensity-matched cohort 
analyses.25–27 Our large-scale study further quantifies the excess mortality specifically related to such 
hospitalizations in KTR when compared to KTR not hospitalized for COVID-19 at the national level (bottom panel 
of Figure 4, model 2 in Table 2, and model 3 in Table 3). Our analyses indicate that the development of a severe 
COVID-19 in KTR dramatically worsened their prognosis, as compared to those who did not experience such 
COVID-19-related hospitalizations. Third, the present study details the dynamics of the COVID-19 vaccination in KTR at the national level, and 
investigates how vaccination has impacted COVID-19-related hospitalizations and survival (see Supplementary 
Appendix). In our study, a second and a third vaccine dose were associated with 35% and 67% decreases in the 
risk of COVID-19-related hospitalization, respectively. A Danish study reported that a second dose of vaccine was 
not associated with a lower risk of hospitalization.28 A recent French study showed a strong association between 
immunosuppressant consumption and being hospitalized with COVID-19 after a full vaccination.11 We found that 
the first and second doses of vaccine were not associated with the risk of death, while a third dose or more was 
associated with a 58% decrease in the risk of death. Since most of the KTR who were vaccinated received two 
first doses of BNT162b2 (see Supplementary Appendix), our results are in agreement with those of a national 
study on solid organ transplant recipients in England which reported a similar absence of association between two 
vaccine doses of BNT162b2 and the risk of death (HR 0·97 [0·71–1·31]).29 Moreover, our study further indicates 
that additional dose(s) of BTN162b2 were associated with lower risks of COVID-19-related hospitalization and 
death in the vulnerable population of KTR, and advocates for an enhanced vaccination process (schedule and 
doses) in KTR whose immunosuppressant consumption increases risks of hospitalization and death. Fourth, the investigations globally reported here do not only concern the pandemic but estimate the whole 
relationship of relevant covariates with the survival of KTR. The information on the association of other covariates 
(age, sex, diabetes, cardiovascular comorbidities) with mortality should be considered as additional valuable side 
results. Importantly, pre-emptive transplantation was associated with a 30% lower risk of death and dialysis 
duration before transplantation in the remaining KTR was not associated with the risk of death (see model 3 in 
Table 3), in contrast with other studies reporting that dialysis duration before the transplantation was associated 
with a higher risk of death.30,31 Two features have likely contributed to our study results. First, adopting a design 
based on the G9 mapping inherently resulted in selecting a target sub-population of relatively recent incident KTR 
(median follow-up: 3.4 years) with a time spent on dialysis < 5 years. Second, there is an overwhelming effect of 
hospitalizations for COVID-19 on mortality. Such an hypothesis is supported by the analysis shown in the 
Supplementary Appendix: when hospitalization for COVID-19 was removed from the model, dialysis duration 
was associated with a higher risk of mortality (9% per year). Our study has several limitations. The first one is inherent to the SNDS data: the study design implied that only 
incident ESKD patients between January 1, 2015 and December 31, 2020 nationwide and who received a transplant . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint
this version posted April 13, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288113
doi: 
medRxiv preprint 12 between January 1, 2015 and December 31, 2021 were included, and therefore KTR who had spent more than 5 
years on dialysis were not considered in this study. Moreover, the design inherently implied longer durations of 
dialysis in KTR transplanted at recent dates, with potential resultant confounding when studying the association 
of the pandemic period with mortality. However, the study design allowed us to consider factors such as the mode 
of entry into the transplant, the duration of dialysis and retrieve comorbidities with G9 mapping, including the 
incidence of comorbidities which are updated every year. Another limitation of the study is due to its observational 
nature which raises usual critical concerns. We tried to mitigate as much as possible the inherent flaws of this 
observational study by taking into account time-dependence of covariates, adjusting the estimates with features 
that we deemed relevant, and we applied STROBE recommendations for reporting.14 Nevertheless, as underlined 
by others,3,4 exposure to the different variants of the virus, mitigation measures, health care system reorganization, 
vaccination schedules, and unknown confounding factors likely varied from one setting to another and with time, 
while additional methodological issues relating to the data collected and reported (e.g., various biases, relatively 
limited sample sizes) were not rare, globally yielding most caution about the generalizability of studies' findings. 
In regards to generalizability issues, the main lines of the methodological framework proposed in the present study 
could be replicated by others for investigating the impact of the pandemic on other diseases and in other countries. 
One may consider that using COVID-19-related hospitalization as a proxy of severe COVID-19 episodes is another 
limit of the study. Direct COVID-19-related deaths that potentially occurred at home with no corresponding 
COVID-19-related hospitalization might have resulted in underestimating the direct COVID-19-related deaths in 
the analyses. Nevertheless, whatever the death cause and death place, all death events that occurred during the pre-
pandemic and pandemic periods were included and appropriately handled in the analyses comparing the survival 
of KTR during the pre-pandemic and pandemic periods. This study has several strengths. It was conducted at the national level of a European country with more than 66 
million inhabitants, and more than 10,000 KTR totalizing nearly 30,000 person-years were followed-up. In 
addition, basing analyses on recent data allowed considering the main waves of the pandemic and including 
investigations on the impact of a third vaccine dose. In conclusion, this national observational study showed a high excess mortality during the pandemic period in 
KTR with COVID-19-related hospitalization. However, in contrast with this dramatic direct impact of the disease, 
no indirect impact of the pandemic period on the survival of KTR was detected during study follow-up. The study 
results further indicate that a third dose or more of vaccine was associated with a reduced risk of death in the 
vulnerable population of KTR.",1
"Immunohistochemistry of post-mortem lung tissue from patients with SARS-CoV2 infection showed marked decline in intensity and distribution of N-acetylgalactosamine-4- sulfatase (Arylsulfatase B; ARSB), increase of total chondroitin sulfate by immunohistochemistry, and increase of vascular-associated carbohydrate sulfotransferase (CHST)15 [1]. The mechanisms leading to these observations were not explained by signaling pathways known to be activated by exposure to coronaviruses. This report addresses the underlying reactions leading to these observations in a cell-based model, using normal, human, primary small airway epithelial cells, treated with the SARS-CoV-2 spike protein receptor binding domain protein. for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 Introduction Chondroitin sulfates are vital components of cells and the extracellular matrix (ECM) in all human tissues and throughout living organisms. Biosynthesis of CSE by CHST15 proceeds by transfer of the sulfate residue from 3′-phosphoadenosine-5′-phosphosulfate (PAPS) to the 6-OH of GalNAc4S of C4S [2]. Mechanisms for enhanced CHST15 or CHST11 expression or for decline in ARSB following viral infection have not been reported previously, although the role of sulfated glycosaminoglycans, particularly heparin and heparan sulfate, in SARS-CoV-2 viral uptake and in inhibition of viral infection of human cells has been investigated [3-5]. CHST15 
[N-acetylgalactosamine 
4-sulfate 
6-O-Sulfotransferase; 
B-cell 
RAG (Recombination Activating Gene)-associated protein; GALNAc4S-6ST] is required for the synthesis of chondroitin sulfate E (CSE) which is composed of alternating β-1,4 and β-1,3 linked D-glucuronate and D-N-acetylgalactosamine-4,6-sulfate residues. Increases in CHST15 or in CSE, have been associated with increased fibrosis in cardiac, lung, and other tissues and with infectivity of dengue virus [6-11], as well as in several malignant cells and tissues, including pancreas, ovary, colon, lung, and brain [12-16]. CHST11 is a carbohydrate sulfotransferase that adds 4-sulfate groups to D-N-acetylgalactosamine residues which are linked to D-glucuronate by β-1,3 glycosidic bonds in chondroitin 4-sulfate or to D-iduronate in dermatan sulfate. Increases in chondroitin 4-sulfate (C4S) and dermatan sulfate follow decline in arylsulfatase B (ARSB, N- acetylgalactosamine-4-sulfatase), since ARSB is the enzyme that removes 4-sulfate groups from N-acetylgalactosamine 4-sulfate residues and is required for the degradation of C4S and dermatan sulfate, as evident by the accumulation of these sulfated glycosaminoglycans in the congenital disease Mucopolysaccharidosis VI (Maroteaux-Lamy Syndrome) caused by mutations of ARSB [17,18]. Accumulation of chondroitin sulfate is recognized as a significant factor in lung fibrosis of different etiologies [19-22], and treatment with recombinant ARSB ameliorated cardiac fibrosis in an animal model [23]. for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 3 Many reports have considered how the pathophysiology of Covid-19 relates to interference with the normal balance between effects of Angiotensin (Ang) II and Ang1-7, due to binding of the SARS-CoV-2 spike protein with ACE2 (angiotensin converting enzyme receptor 2). SARS-CoV-2 spike protein binding with ACE2 can inhibit the ACE2-mediated production of Ang1- 7 and lead to imbalance between AngII and Ang1-7/Mas receptor effects. AngII causes vasoconstriction initiated by interaction with the AT1 receptor, and decline in opposing vasodilation, due to reduced production of Ang1-7 when ACE2 is bound by the SARS-CoV-2 spike protein, may predispose to unopposed signaling events and significant pathophysiology [24- 27]. The impact of inhibition of the renal angiotensin system (RAS) by angiotensin-converting enzyme (ACE) inhibitors or by angiotensin receptor blockers (ARBs) has been considered in relation to treatment and outcome of COVID-19 infection, and studies are ongoing [28-30]. Prior work showed marked increases in expression of CHST15 and CHST11 in rat vascular endothelial cells following exposure to Angiotensin (Ang) II [1]. These increases were largely, but incompletely, inhibited by treatment with an angiotensin receptor blocker, and consistent with possible activation by ACE2. The studies in this report support a sequence of activation of cellular transcriptional events which proceed from direct effects of the interaction between the spike protein receptor binding domain (SPRBD) and the ACE2 receptor in human airway epithelial cells (AEC). Recognition of the usurpation of normal signaling mechanisms by the SARS-CoV-2 spike protein-ACE2 interaction suggests the potential benefit of some targeted pharmacological interventions based on these pathways. Focus on the specific impact of the spike protein receptor binding domain (SPRBD) interaction with ACE2 on distinct signaling, mechanisms which increase CHST15 and CHST11 expression and inhibit ARSB expression has enabled identification of specific interventions which may have clinical benefit. In the experiments which follow, normal, primary human small airway cells were treated with SPRBD with and without Interferon-β (IFNβ), which augments the expression of ACE2 [31,32], and, thereby, amplifies the impact of the interaction between the SPRBD and ACE2. Elucidation of the roles of phospho- for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 4 p38MAPK, phospho-Smad3, and Rb phosphorylation provides novel insights into how viral usurpation of endogenous signaling pathways can produce sustained pathophysiological consequences in human cells. for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 5 Results Chondroitin sulfate, carbohydrate sulfotransferase (CHST)15, and N-acetylgalactosamine- 4-sulfatase (Arylsulfatase B; ARSB) in post-mortem Covid-19 lung tissue and airway epithelial cells In post-mortem lung tissue of patients with COVID-19, immunostaining showed decline in intensity and distribution of ARSB, in contrast to increases in total chondroitin sulfate in alveolar epithelial cells and in CHST15 in vascular-associated cells [1]. Previously published representative images from Covid-19 lungs and control tissue of total chondroitin sulfate (Fig.1a,1b), ARSB (Fig.1c,1d), and CHST15 (Fig.1e,1f) are reproduced [1]. Increased CHST15 was prominent in vascular smooth muscle cells of the Covid-19 lung tissue, compared to control tissue (Fig.1g,1h). Normal, primary human, small airway epithelial cells (AEC) were treated with the SARS- CoV-19 spike protein receptor binding domain (SPRBD; 2.5 µg/ml x 2h), following exposure to Interferon-β (IFNβ; 10 ng/ml x 24h), which was used to increase the expression of angiotensin converting enzyme 2 (ACE2) receptor, and thereby amplify the effect of SPRBD and better simulate the impact of viral infection. The expression of ACE2 increased to more than four times the baseline following IFNβ or the combination of IFNβ and SPRBD, and SPRBD alone did not have any significant impact on ACE2 (Fig.1i). Consistent with the observed findings from immunohistochemistry, total chondroitin sulfate and total sulfated glycosaminoglycans (GAGs) increased significantly following exposure to SPRBD and to the combination of SPRBD and IFNβ. Almost two thirds of the increase in the sulfated GAGs was due to the increase by over 5 ug/mg protein of chondroitin sulfate (Fig.1j). Sulfotransferase activity increased by over 200% following exposure to the SPRBD and IFNβ (Fig.1k). ARSB activity (Fig.1l) and mRNA expression (Fig.1m) declined following exposure to the SPRBD. The mRNA expression of carbohydrate sulfotransferases CHST15 and CHST11 increased to 3.8 and 2.7 times the control values following exposure to SPRBD and IFNβ (p<0.001, p<0.001; n=6) (Fig.1n). for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 6 Inhibition of phospho-Thr180/Tyr182-p38-MAPK or of phospho-S423/S425-SMAD3 blocks increases in CHST15 and CHST11 The cultured AEC were treated with inhibitors of cell signaling, including SB203580, an inhibitor of phospho-(Thr180/Tyr182)-p38 MAPK, and NSC23766, a Rho/Rac-1 GTPase inhibitor (Fig.2a). Marked declines in expression of CHST15 and CHST11 followed exposure to the phospho-(Thr180/Tyr182)-p38 inhibitor, but NSC23766 had no effect. Increases in CHST15 and CHST11 expression were blocked by SIS3 (specific inhibitor of SMAD3) (Fig.2b). Effectiveness of ACE2 silencing was confirmed by QPCR (Fig.2c), and the impact of silencing ACE2 tested to confirm that ACE2 was required for the observed effect of the SPRBD. ACE2 siRNA blocked the SPRBD-induced increase in phospho-(Thr180/Tyr182)-p38 (Fig.2d), demonstrating dependence on ACE2 for the observed effects. As expected, the increase in phospho-(S423/S425)-SMAD3 following exposure to SPRBD was inhibited by SIS3 (Fig.2e), and was also partially inhibited by SB203580 (Fig.2e), indicating participation by phospho-p38 in the SPRBD-induced increase in phospho-SMAD3. Phospho-(Thr180/Tyr182)-p38 MAPK was significantly increased following exposure of the cells to SPRBD (Fig.2f), but SIS3 had no impact on the increase. Increases in promoter activation of CHST15 (Fig.2g) and CHST11 (Fig.2h) were abrogated by treatment with either SIS3 or SB203580 following exposure to SPRBD. Phospho-p38 MAPK mediates decline in ARSB expression through effects on Rb phosphorylation and E2F1 In contrast to the observed increases in CHST11 and CHST15, the mRNA expression and activity of ARSB declined significantly following exposure to the SPRBD (Fig.1l,1m). ARSB expression increased following treatment by SB203580 (Fig.3a). Neither NSC23766 (Fig.3a) nor SIS3 (Fig.3b) reversed the effect of SPRBD. ARSB promoter activation was reduced by SPRBD exposure, and treatment with SB203580 reversed the decline in ARSB (Fig.3c). Since phospho-p38-MAPK was reported to activate N-terminal Rb (retinoblastoma protein) phosphorylation [33,34] and thereby inhibit E2F1-DNA binding and the ARSB promoter for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 7 has multiple E2F1 binding sites [35], the impact of the p38-MAPK inhibitor SB203580 on Rb phosphorylation following exposure to SPRBD was addressed in the AEC. SB203580 reversed the SPRBD-induced decline in C-terminal phospho-(Ser807/811)-Rb (Fig.3d). In contrast, SB203580 reduced the SPRBD-induced increase in N-terminal Rb phosphorylation (phospho - Ser249/Thr252), as shown by Western blot (Fig.3e) and corresponding values of phospho- (S249/Thr252)-Rb to total Rb (Fig.3f). The phospho-(S249/T252)-Rb to total Rb ratio increased by exposure to SPRBD and declined following SB203580, reflecting the dependence of N- terminal Rb phosphorylation on phospho-p38. Since N-terminal Rb-phosphorylation, in contrast to C-terminal Rb phosphorylation, activates Rb binding with E2F1 [33,34] and reduces availability of E2F for DNA binding, the impact of SPRBD on E2F-DNA binding was assessed. E2F transcription factor-DNA binding assay showed decline following SPRBD-IFNβ exposure and recovery following SB203580 (Fig.3g). Chromatin immunoprecipitation (ChIP) assay was performed to assess specific E2F1-binding to the ARSB promoter. Percent DNA input declined following SPRBD-IFNβ exposure, but increased following SB203580 (Fig.3h). Agarose gel shows that E2F1-DNA binding to the ARSB promoter was reduced by SPRBD-IFNβ exposure, but was reversed by SB203580 (Fig.3i), as confirmed by densitometry (Fig.3j). These findings implicate a complex signaling mechanism whereby phospho-p38 MAPK phosphorylates N-terminal Rb, thereby enhancing Rb-E2F1 binding and negatively regulating the ARSB promoter and suppressing ARSB expression. Effects of Desloratadine, Monensin, and Dexamethasone on expression of CHST15, CHST11, and ARSB Multiple treatments have been investigated in relation to impact on mortality and morbidity from Covid-19 infection. Both H1 and H2 antihistamine receptor antagonists have been considered, including the H1 antihistamine loratadine and its metabolite desloratadine [36-38]. Treatment of SPRBD-exposed AEC by desloratadine reduced the SPRBD-induced increase of phospho-Thr180/Tyr182-p38-MAPK (Fig.4a) and the mRNA expression of CHST15 and CHST11 for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 8 (Fig.4b). Desloratadine also countered the SPRBD-induced decline in ARSB expression (Fig.4c) and activity (Fig.4d). The polyether antibiotic monensin, which has been shown to have effects on sulfated glycosaminoglycan biosynthesis [39-41], reduced the SPRBD-induced increases in mRNA expression of CHST15 and CHST11 (Fig.4e), but had no effect on the SPRBD-induced decline in ARSB (Fig.4f). Dexamethasone, which has been widely used clinically, did not reverse effects of SPRBD on CHST15, CHST11, or ARSB expression. for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 9 Discussion In this report of in vitro effects following exposure of cultured normal, human small airway epithelial cells to the SARS-CoV-2 spike protein binding domain, data demonstrate increased expression of chondroitin sulfotransferases CHST15 and CHST11, increased sulfotransferase activity and increased content of chondroitin sulfate, and decline in ARSB expression and activity. Reduced expression of ARSB leads to the accumulation of chondroitin sulfate, since hydrolysis of the 4-sulfate group is required for chondroitin 4-sulfate degradation [42-45]. Both the increases in CHST15 and CHST11 and the decline in ARSB are mediated by phospho-p38 MAPK, and SMAD3 is required for CHST15 and CHST11 expression, whereas the expression of ARSB is regulated by N-terminus Rb phosphorylation and E2F1-Rb interaction, as presented in Fig.5. Detailed studies of N-terminal Rb phosphorylation [33,34] present a complex mechanism by which Rb-E2F binding is increased, rather than decreased by C-terminal Rb-phosphorylation. Application of this mechanism of Rb activation by N-terminal phosphorylation, which overrides Rb inhibition by C-terminal phosphorylations, provides a novel, coherent approach to how ARSB expression may be down-regulated following stimulation of phospho-p38 by SPRBD-ACE2 in the AEC. Study results indicate that pathways leading to decline in ARSB and increases in expression of CHST15 and CHST11 following exposure to the SPRBD require activation of phospho-p38 MAPK. P38-MAPK has been implicated in critical interactions between exogenous exposures, intracellular signaling, and transcriptional events in a wide range of experiments [46- 49]. Crosstalk between the p38 and Smad pathways is involved in fibrogenic signaling following AngII and downstream of TGF-β [50-52]. The current experiments provide novel insight into how phospho-p38 participates in the regulation of the biosynthesis and maintenance of 4-sulfated chondroitins. Other work has shown that when ARSB is inhibited and 4-sulfation sustained, SHP2 binding to chondroitin 4-sulfate is increased, leading to reduced SHP2 activity and sustained for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 10 phosphorylation of ERK1,2, JNK, and p38 [14,53-56]. The impact of phospho-38-MAPK on N- terminal Rb phosphorylation, Rb-E2F1 binding, and suppression of ARSB promoter activation, may be part of a loop, in which p38 phosphorylation is sustained, due to enhanced chondroitin 4- sulfate-SHP2 binding which follows decline in ARSB and increased chondroitin 4-sulfation. Increased understanding of how chondroitin sulfate is involved in the pathogenesis of Covid-19 may help in the development of preventive and therapeutic strategies. The potential benefit of the H1 histamine receptor blocker Desloratadine is suggested by the measured decline in phospho-p38 and by inhibition of the SPRBD effects on expression of CHST15, CHST11 and ARSB, as well as by pharmacological studies that identified desloratadine/loratadine as a mechanism-based target [36-38]. Several clinical studies of H2 blockers on the response in Covid- 19 have been reported and additional studies of antihistamines are ongoing [57-59]. Analysis of intensity and distribution of total chondroitin sulfate, CHST15 and ARSB immunohistochemistry in sections from SARS-CoV-2-infected lungs demonstrated marked increase in chondroitin sulfate and prominence of vascular-associated CHST15, in contrast to decline in ARSB [1]. These findings were similar to those observed in diffuse alveolar damage from other causes and suggest that accumulation of chondroitin sulfate might be a significant component in refractory lung disease and pulmonary fibrosis. Previously, decline in ARSB was implicated in the response to hypoxia in human bronchial epithelial cells [60] and in failure of patients with moderate COPD to respond to oxygen therapy [61]. These findings provide additional evidence that decline in ARSB contributes to refractory clinical response in SARS-CoV- 2 infection. By use of the specific spike protein receptor binding domain which binds with ACE2, the experiments in this report have focused on how disruption of normal ACE2 function by the SARS- CoV-2-ACE2 interaction affects signaling in normal human airway cells. In this model, viral uptake does not occur, and other spike protein mediated interactions, such as with TMPRSS2, are not anticipated.  A specific p38 isoform, such as p38α, may predominate in the reactions presented for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 11 in this report, and future work will help to clarify which p38 isoform(s) is most involved. Also, the precise mechanism by which the SPRBD-ACE2 interaction leads to activation of phospho-p38- MAPK is not yet clarified. Interception of the activation of p38-MAPK emerges as a therapeutic goal, and H1-receptor blockers may directly inhibit phospho-p38 activation and, thereby, prevent the effects of increased expression of CHST15 and CHST11, reduced expression of ARSB, and accumulation of excess chondroitin sulfate following SARS-CoV-2 infection. Also, atypical antibiotics, such an monensin, which directly affect chondroitin 4-sulfate and dermatan sulfate synthesis, may provide new targets to disrupt virus-initiated signaling. Increased attention to the impact of chondroitin sulfates and ARSB in Covid-19 pathobiology and the role of phospho-p38 activation in the mechanisms of their expression may yield new tools and new insights which will reduce the morbidity and mortality of Covid-19. Author Contributions: JKT and SB designed the experiments, SB performed the experiments, and wrote the manuscript. Conflict of Interest:  The authors have no conflicts of interest with the content of this article. Supplementary information accompanies the manuscript on the Signal Transduction and  
 
Targeted Therapy website http://www.nature.com/sigtrans for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 12 Figure Legends Fig. 1. Chondroitin sulfate, Arylsulfatase B, and CHST15 in Covid-19 and normal control lung tissue and in human small airway epithelial cells. a,b. In lung tissue obtained post-mortem from patients who died due to SARS-CoV-2 infection, immunohistochemistry showed increased total chondroitin sulfate compared to normal control lung tissue. c,d. In contrast, intensity and distribution of ARSB were markedly reduced in the Covid-19 lung tissue. Marked decline is evident in the membrane immunostaining, compared to the normal control. e,f. Carbohydrate sulfotransferase (CHST) 15 which is the sulfotransferase that adds a 6-sulfate group to chondroitin 4-sulfate to form chondroitin-4,6-sulfate (chondroitin sulfate E), has regions of marked intensity of immunostaining in both the Covid-19 lung and the normal lung. g,h. In vascular smooth muscle tissue of the Covid-19 lung, the CHST15 immunostaining is much more intense and less diffuse than in the normal lung vascular tissue. i. Treatment of the cultured cells with Interferon (IFN)β amplifies the impact of SPRBD by increasing the expression of ACE2 more than four times the control level (p<0.1x10-5, n=6). j. Corresponding to the findings in the infected lung tissue, total chondroitin sulfate (CS) increased by ~2 ug/g protein (p=0.001, n=6) and total sulfated glycosaminoglycans increased by over 3 ug/g protein (sGAG) (p=0.0005, n=6) in the AEC following exposure to the SPRBD. Increases are more following combined treatment with SPRBD and IFNβ (>5 µg/g protein for CS, n=6; 7.7 µg/g protein for sGAG, n=6). k. Consistent with the observed increases in chondroitin sulfate in Covid-19 lung tissue and in the airway cells, sulfotransferase activity increased by 63% following SPRBD and by over 200% following the combination of SPRBD and IFNβ (n=6, n=6) l. Arylsulfatase B (ARSB) activity declined in the AEC following exposure to the SPRBD (p=0.005, n=6), and declined over 50% by the combined exposure to SPRBD and IFNβ. m. The mRNA expression of ARSB also declined (p=2.6x10-5; n=6) following SPRBD, and declined further following exposure to the combination of SPRBD and IFNβ. n. Expression of both CHST15 and of CHST11, which transfers for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 13 4-sulfate groups to N-acetylgalactosamine residues to create chondroitin 4-sulfate, is significantly upregulated following exposure to the SPRBD and the combination of SPRBD and IFNβ. All of the p-values were determined using unpaired t-test, two-tailed, with unequal variance, and error bars represent one standard deviation. [ACE2=angiotensin converting enzyme 2 receptor; AEC=normal, human small airway epithelial cells; ARSB=arylsulfatase B=N-acetylgalactosamine-4-sulfatase; SPRBD=SARS CoV-2 spike protein receptor binding domain] Fig.2. SMAD3 and p38-MAPK inhibitors abrogate the effects of SPRBD. a. The SPRBD- and SPRBD+IFNβ- induced increases in the mRNA expression of CHST15 and CHST11 were significantly reduced following exposure to SB20850, the p38-MAPK inhibitor. In contrast, NSC23766, a Rho/Rac-1 GTPase inhibitor, had no impact on their expression. b. SIS3, an inhibitor of phospho-Smad3, inhibited the SPRBD- and SPRBD+IFNβ- induced increases in expression of CHST15 and CHST11. c. Expression of ACE2 was inhibited by ACE2 specific siRNA following exposure to SPRBD alone or with IFNβ. d. Following silencing of ACE2 by siRNA, the SPRBD- and SPRBD+IFNβ- induced increases in phospho-T180/T182-p38 MAPK were inhibited. e. The SPRBD- and SPRBD+IFNβ- induced increases in phospho-S423/S425 SMAD3 were completely inhibited by SIS3 and, to a lesser extent, by SB20850, the p38 MAPK inhibitor. f. The SPRBD and SPRBD+IFNβ- induced increases in phospho-Thr180/Tyr182-p38 were unaffected by exposure to SIS3 and were inhibited by SB20850. g,h. Promoter activity of CHST15 and CHST11 was enhanced by exposure to SPRBD and to a greater extent by the combination of IFNβ and SPRBD. Both SIS3 and SB20850 inhibited promoter activation. [SIS3=specific inhibitor of SMAD3]. All p-values were determined by unpaired t-test, two-tailed, with unequal variance, and with at least three independent experiments. Error bars represent one standard deviation. Fig.3. Exposure to spike protein receptor binding domain inhibits ARSB expression by activation of phospho-p38 MAPK and phospho-(S249/T252)-RB-E2F1 interaction. for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 14 a. ARSB mRNA expression was unaffected by NSC23766, but following exposure to the p38- MAPK inhibitor SB20850, mRNA expression was restored to baseline control values. b. In contrast, SIS3 had no impact on the SPRBD- or SPRBD+IFNβ- induced decline in ARSB expression. c. ARSB promoter activation was reduced by SPRBD and by SPRBD+IFNβ. These declines were reversed by exposure to SB20850, but not by SIS3. d. Following treatment with SPRBD or SPRBD+IFNβ, C-terminal phospho-(Ser807/811)-Rb declined as shown by ELISA. These declines were reversed by SB203580, but not by SIS3. e. In contrast to the decline in C- terminal Rb phosphorylation, exposure to SPRBD+IFNβ increased N-terminus phospho-(S249)- Rb, as detected by Western blot. This increase was inhibited by SB20850, and total Rb was unchanged. f. Densitometry confirms the impression of Western blot and shows the ratio of phospho-S249-Rb to total Rb following SPRBD+IFNβ has increased to 3.89 times the baseline. g. Following exposure to SPRBD and SPRBD+IFNβ, E2F-DNA binding declined significantly, and SB20850 reversed the declines. h. %DNA input declined following SPRBD+IFNβ and increased following inhibition of p38-MAPK by SB20850. i. Chromatin immunoprecipitation (ChIP) shows decline in E2F1 binding to the ARSB promoter following exposure to SPRBD+IFNβ; decline was reversed by SB203580. Agarose gel demonstrates no effect of the IgG negative control on E2F1 binding to the ARSB promoter at baseline, IFNβ control, reduced binding following SPRBD+IFNβ, and reversal of the decline following treatment with SB20850. j. Measurements of optical density confirm the impression from the gel. P-values were determined by unpaired t-tests, two-tailed with unequal variance, with n of at least 3 independent experiments. Error bars show one standard deviation. Fig. 4. Effects of selected agents on expression of CHST15, CHST11, and ARSB. A. The antihistamine desloratadine reduced by 62% the SPRBD-induced increase in T180/T182 phospho-p38-MAPK in the AEC. B. Consistent with the decline in phospho-p38, desloratadine reduced the SPRBD-induced and SPRBD with IFNβ-induced increases in CHST15 (p=0.045, p=0.0041 with IFNβ) and CHST11 expression (p=0.008, p=0.0017 with IFNβ, unpaired t-tests, for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 15 two-tailed, unequal variance, n=3). C, D. Consistent with the observed decline in ARSB, desloratadine partially reversed the SPRBD-induced decline in ARSB activity (from 64.7 to 73.4 nmol/mg protein/h and from 41.0 to 61.8 nmol/mg protein/h with IFNβ; n=3) and expression (0.71 to 0.87 and 0.46 to 0.74 with IFNβ, fold-change compared to control; n=3). E. The polyether antibiotic monensin, which modifies dermatan sulfate biosynthesis and processing [39-41], reduced the SPRBD-induced increases in CHST15 mRNA (p=0.003, p=0.006 with IFNβ) and CHST11 (p=0.003, p=0.002 with IFNβ; fold-change compared to control, unpaired t-tests, two- tailed, unequal variance). F. Monensin had no significant impact on the SPRBD-induced decline in ARSB. Fig. 5. SARS-CoV-2 spike protein binding with ACE2 initiates transcriptional events which modify chondroitin sulfation through activation of phospho-p38 MAPK. Schematic of SARS-CoV-2 spike protein binding domain (SPRBD) peptide interaction and initiation of transcriptional events which modify chondroitin sulfation through activation of phospho-p38 MAPK. Both p38 MAPK and phospho-Smad3 lead to increased expression of CHST15 and CHST11. AngII interaction with AT1 is also expected to increase phospho-p38. Phospho-p38 leads to N-terminal phosphorylation and activation of Rb with enhanced binding of E2F1, which reduces ARSB promoter activation due to reduced binding of E2F1. [ACE2=angiotensin converting enzyme 2; AngII=angiotensin II; ARSB=arylsulfatase B; AT1R=angiotensin II receptor type 1; AT2R=angiotensin II receptor type 2; CHST=carbohydrate sulfotransferase; 
pRb=retinoblastoma 
protein; 
SMAD=Suppressor 
of 
Mothers 
against Decapentaplegic; SPRBD=SARS-CoV-2 spike protein receptor binding domain] for use under a CC0 license. 
This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted June 14, 2023. 
; 
https://doi.org/10.1101/2023.01.24.23284890
doi: 
medRxiv preprint 16",1
"The prevalence of autism spectrum disorder (ASD) has increased steadily in most high-
income countries over the last decade. Estimates of ASD prevalence among 8-year-old 
children in the U.S. have increased from 1.47% in 2010 to 2.3% in 2018, and up to 3.5% 
among 3-17-year-old children in 2020. In Israel, estimates have been lower, with the 
prevalence of ASD in 8-year-old children increasing from 0.3% in 2009 to 0.64% in 2018. 
Here, we examined data from the entire population of Israel (~3 million 1-17-year-old 
children) and quantified changes in their ASD prevalence between 2017 and 2021. We 
analyzed consecutive annual reports acquired from the Israeli National Insurance Institute 
(NII), which monitors all children with ASD in Israel who receive welfare services, and Clalit 
Health Services (CHS), the largest Health Maintenance Organization (HMO) in Israel that 
services ~50% of the population. Both data sources revealed a nearly twofold increase in the 
ASD prevalence of 1–17-year-old children during this five-year period. Rates differed across 
age groups with 2-3-year-old (day-care) children exhibiting a ~4.4-fold increase in 
prevalence from 0.27% to 1.19% and 4-6-year-old (pre-school) children exhibiting a ~2.3-
fold increase from 0.80% to 1.83%. These results demonstrate that ASD prevalence in Israel 
is increasing at unprecedented rates and shifting towards diagnosis at earlier ages. These 
findings highlight the challenge facing health and education service providers in meeting the 
needs of a rapidly growing ASD population that is being diagnosed at earlier ages. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Introduction Worldwide ASD prevalence in children under the age of 18 was recently estimated at 0.6-
1%, on average (Salari et al., 2022; Zeidan et al., 2022). However, ASD prevalence estimates 
differ across age groups and vary widely across countries, regions, and ethnic populations. 
For example, in 2015 the ASD prevalence in children 7-9 years-old from Finland, Denmark, 
and Iceland was 0.77%, 1.26%, and 3.1%, respectively (Delobel-Ayoub et al., 2020a), while in 
2016 the ASD prevalence in children 8 years-old from Colorado and New Jersey in the U.S. 
was 1.3% and 3.1%, respectively (Maenner et al., 2020). A variety of factors are likely to 
contribute to variable prevalence estimates across geographic locations including 
differences in social stigma, public awareness, availability and quality of health services, and 
accuracy of population monitoring techniques (Chiarotti & Venerosi, 2020; Leonard et al., 
2010a). Additional variability is likely due to differences in population genetics and/or 
exposure to environmental factors (Gaugler et al., 2014; Rzhetsky et al., 2014). While ASD prevalence may vary across countries for multiple reasons, a recent study has estimated that global prevalence has increased from ~0.6% to ~1% in the last decade 
(Zeidan et al., 2022). In the U.S. ASD prevalence has increased from 1.47% in 2010 to 2.3% in 
2018 for 8-year-old children (Maenner et al., 2021) with an additional increase to 3.5% in 
2020 for 3-17-year-old children (Li et al., 2022). In Canada prevalence has increased from 
1.5% in 2015 to 2.2% in 2019 for 5–17-year-old children (Palmeter, O’Donnell, Lagace, 
Gheorghe, & Krupovich, 2019). Accurately quantifying ASD prevalence and its change over 
time is critical for planning policy and services. The importance of this is apparent in the 
establishment of longitudinal monitoring projects such as the Autism and Developmental 
Disabilities Monitoring (ADDM) Network by the Center for Disease Control (CDC) in the U.S. 
(Maenner et al., 2021). In Israel, previous reports based on NII records have estimated that the national ASD prevalence among 8-year-old children was 0.12% in 2005, 0.3% in 2008, and 0.65% in 2015 
(Gal, Abiri, Reichenberg, Gabis, & Gross, 2012; Segev, Weisskopf, Levine, Pinto, & Raz, 
2019). This suggests a continuous increase in ASD prevalence with an overall 5.5-fold 
increase over ten years. An additional study using Maccabi Healthcare records, the second 
largest HMO in Israel (servicing ~25% of the population), has reported higher rates of ASD 
prevalence among 8-year-old children with an almost three-fold increase from 0.46% in 
2007 to 1.3% in 2018 (Davidovitch, Slobodin, Weisskopf, & Rotem, 2020). The difference 
between Maccabi and NII estimates may be due to different ethnic and socio-economic 
composition of the sub-populations insured by Maccabi HMO relative to the entire 
population of Israel. For example, several studies have reported that higher socio-economic 
abilities are associated with higher ASD prevalence in Israel (Davidovitch, Hemo, Manning- . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Courtney, & Fombonne, 2013; Raz, Weisskopf, Davidovitch, Pinto, & Levine, 2015; Segev et 
al., 2019). The purpose of the current study was to quantify changes in ASD prevalence during the last 4 years and determine whether rates are continuing to increase or have stabilized as 
suggested by one previous study (Segev et al., 2019). Moreover, we wanted to determine 
whether prevalence changes differed across age groups and to specifically quantify 
prevalence increases in 2-3-year-old and 4-6-year-old children who are eligible for day-care 
and pre-school services, respectively. The Israeli healthcare and education systems offer 
comprehensive early intervention services to ASD children placed in either special or 
mainstream education settings (Ilan et al., 2021) and accurate estimates of ASD prevalence 
at these age groups are particularly important for planning the development of these 
services, given their importance for improving later outcomes (Gabbay-Dizdar et al., 2022; 
Zwaigenbaum et al., 2015). Accurate estimates are also important for properly budgeting 
rapid early diagnosis services and a wide range of additional ASD services given the large 
costs of ASD care (Buescher, Cidav, Knapp, & Mandell, 2014), with the average annual family 
expenditure in Israel estimated at over $8,000 (USD) in 2013 (Raz, Lerner-Geva, Leon, 
Chodick, & Gabis, 2013). Methods Study design and data sources ASD prevalence was estimated based on data acquired from two independent sources: the 
National Insurance Institute (NII) of Israel and Clalit Healthcare Services (CHS) HMO. Israel 
has a universal, nationally funded healthcare system where the Ministry of Health is 
responsible for regulating health services that are supplied by four HMOs. Every Israeli 
resident is required to be registered with one of the four HMOs. CHS is the largest HMO, 
providing services to over half (52.4%) of the Israeli population (~4.9 million residents in 
2021). The NII is the national social security organization that implements the welfare policy 
of the Israeli government. Since 1981, the NII has provided financial support to families of 
children with ASD from the age of diagnosis and until the age of 18. The NII collects data on 
all Israeli residents (~9.36 million residents in 2021), enabling the assessment of national 
ASD prevalence. The data analyzed in the current study comes from five annual reports 
generated by CHS and NII from their respective computerized records for the years 2017 to 
2021. Case ascertainment by NII To receive financial support from the NII, parents of children with ASD are required to 
submit a claim after their child receives the ASD diagnosis. Israeli regulations require that . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint the ASD diagnosis be given by both a licensed psychologist and a physician (child psychiatrist 
or pediatric neurologist) according to DSM-5 criteria. The diagnosis must include physical, 
neurological, and developmental assessments with a detailed report of the child’s 
symptoms and functional, developmental, and cognitive status. All claims are reviewed by a 
professional committee at the NII to determine eligibility. As of January 2022, benefits 
included a monthly payment of NIS 3000 (US$965 in 2022) from the date of diagnosis until 
the age of 18. In addition, NII approval allows parents to receive tax deductions and access 
to early educational services (e.g., daycare special education). It is, therefore, assumed that 
the vast majority of ASD children are registered at the NII. Nevertheless, ASD related 
healthcare services at the HMOs may be accessed by individuals who have received the 
diagnosis but have not registered at the NII. Case ascertainment by CHS To receive ASD related health services from CHS, a child must receive a formal diagnosis of 
ASD as described in the previous section. Diagnosed children are eligible for speech, 
occupational, behavioral, and psychological therapy as well as physiotherapy at HMO 
facilities or associated service providers. It is assumed that the vast majority of families with 
ASD children who are insured by CHS will report the ASD diagnosis to CHS in order to receive 
ASD related health services. However, ASD diagnoses can also be completed in private 
clinical settings without reporting the results to CHS. In such cases the families may apply for 
and receive NII support without any record of the ASD diagnosis at their HMO. Study Population The study population included all 1–17-year-old children who were residents of Israel in 
each of the study years (2017-2021). Total resident numbers for each year were obtained 
from the Israeli Central Bureau of Statistics (CBS) which calculates population estimates 
based on census surveys (https://www.cbs.gov.il/en/Pages/search/yearly.aspx). The data 
includes population estimates by age, thereby allowing us to compute ASD prevalence for 
children at different ages. The total number of 1-17-year-old children in Israel (in thousands) 
was 3014.5 in 2017, 3071.5 in 2018, 3125.7 in 2019, 3239.3 in 2020, and 3360.2 in 2021. Data Analysis We computed the annual prevalence of ASD from 2017 to 2021 in NII and CHS data 
separately. Prevalence estimates using NII data were calculated by dividing the number of 
1–17-year-old children with an ASD diagnosis by the number of 1–17-year-old residents in 
Israel, separately in each calendar year. Prevalence estimates using CHS data were 
calculated by dividing the number of 1–17-year-old children with an ASD diagnosis in CHS 
records by the number of 1–17-year-old residents who were insured by CHS, separately in 
each calendar year. The number of Israeli residents insured by CHS was extracted from NII 
public data (https://www.btl.gov.il/Mediniyut/Situation/haveruth1/Pages/default.aspx). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Equivalent analyses were performed for sub-groups of children binned by age or age groups 
corresponding to day-care (1-3 years), preschool (4-6 years), primary school (7-12 years), or 
middle/high school (13-17 years) educational settings. When assessing changes over time 
(e.g., annual increase rates) we excluded children under one year of age given the negligible 
number of ASD cases in this age group. Statistics All statistical analyses were performed with the R statistical software (R Core Team, 2021). 
We performed a linear regression analysis with year and data source as predictors along 
with an additional interaction predictor for ‘year*source’ to assess whether the slope of 
prevalence changes differed across NII and CHS data. We performed a one-way ANOVA to 
test for differences in ASD prevalence across age groups with age, year of data, and source 
of data as main factors in the analysis. P-values <0.05 were considered significant. We also 
performed Z-tests of proportions to assess whether there were differences in the increase 
of ASD prevalence between NII and CHS from 2017 to 2021 for each of the 16 age groups. 
Results of these Z-tests were Bonferroni-corrected for 16 comparisons such that only P-
values <0.003 were considered statistically significant. Results According to NII data, the number of individuals with a formal diagnosis of ASD, 1-17-years-
old, who were eligible for services increased from 14,914 in 2017 to 32,222 in 2021. This 
corresponds to nearly a 2-fold increase in ASD prevalence from 0.49% to 0.96% over the 
five-year period (Figure 1). Data from CHS suggested a slightly lower increase in prevalence 
from 0.48% in 2017 to 0.89% in 2021. A linear regression analysis demonstrated a highly 
significant increase in prevalence across years (β=0.0012; p<0.0001), as well as a significant 
difference in prevalence between NII and CHS (β=0.3629; p=0.0304) and a significant 
interaction between data source (NII and CHS data) and time/years (p = 0.0302), suggesting 
a steeper increase of ASD prevalence rates in the NII data compared to CHS data. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Figure 1. ASD prevalence rates in 1–17-year-old children as computed using NII (black) and 
CHS (gray) data between 2017 and 2021. Error bars represent the standard error. Next, we compared ASD prevalence rates across age groups (Figure 2). This demonstrated 
that ASD prevalence was consistently highest in preschool children (4-6-years-old) 
regardless of the year (from 2017 to 2021). A one-way ANOVA revealed statistically 
significant differences in ASD prevalence (Figure 3, F(3,4)= 65.4; p= 0.0007) across four age 
groups: children in daycare (2-3-year-old), preschool (4-6-year-old), primary school (7-12 
year old), and middle/high school (13-17 year old). Tukey’s test for multiple comparisons 
demonstrated that ASD prevalence was significantly higher in the preschool group relative 
to the daycare (p = 0.0101, 95% C.I. = 0.0021, 0.0048), primary school (p = 0.0009, 95% C.I. = 
0.0013, 0.0057), and middle/high school (p = 0.0477, 95% C.I. = 0.0032, 0.0044) groups. ASD 
prevalence was also significantly higher in the primary school group relative to day care (p = 
0.0007, 95% C.I. = 0.0048, 0.0091) and middle/high school (p = 0.0015, 95% C.I. = 0.0035, 
0.0079) groups. There was also a significant effect for data source (F(1,4) = 12.22; p=0.0396), 
with NII data yielding higher estimates of ASD prevalence than CHS data. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Figure 2. Prevalence of ASD by age as computed from NII (left, black) and CHS (right, gray) 
data for 2017 (solid line), 2018 (diamonds), 2019 (triangles), 2020 (squares) and 2021 
(circles). Figure 3. Mean ASD prevalence in daycare (2-3-year-old), preschool (4-6-year-old), primary 
school (7-12-year-old), and middle/high school (13-17) children between 2017 and 2021. 
Error bars: standard error of the mean. Black: NII data. Gray: CHS data. ASD prevalence increased annually in all age groups from 2017 to 2021, but not at the same rate (Figure 4). The largest increases were apparent in children 3 to 7 years old, 
with mean annual increases of 0.16-0.23% and 0.15-0.18% reported in NII and CHS data, 
respectively. In contrast, 10-year-old children exhibited annual increases that were 
approximately half as large and annual increases were even smaller for older children.  
While annual increases in prevalence were overall larger according to NII data (M[SD] = 
0.13%[0.0009] vs. 0.09%[0.0103]), there were no significant differences across NII and CHS 
data by age after Bonferroni correction for multiple comparisons (p>0.003). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Figure 4. Mean annual increase in ASD prevalence per age from 2017 to 2021 as deduced 
from NII (black) and CHS (gray) data. Error bars: standard error of the mean. We also calculated the total fold increase in ASD prevalence from 2017 to 2021 per age (Figure 5). The greatest fold increase was found at age 2 with a 12 and 9.4-fold increase 
in NII and CHS data, respectively. This was followed by 3-year-old children who exhibited a 
3.5 and 3-fold increase in NII and CHS data, respectively. In contrast, the total fold increase 
from 2017 to 2021 remained relatively stable in older children ages 4 to 17 with an average 
of 1.96 (IQR = 0.04) in NII data and 1.70 (IQR = 0.06) in CHS data. This demonstrates the shift 
in ASD diagnosis to earlier ages (i.e., 2–3-year-olds). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Figure 5. Total fold increase in ASD prevalence from 2017 to 2021 per age group in NII 
(black) and CHS (gray) data. In a final analysis we re-examined the changes in ASD prevalence separately for daycare, preschool, primary school, and middle/high school age groups (Figure 6).  We 
performed equivalent linear regression analyses for each of the age groups, which revealed 
the following. For daycare children there was a significant increase in the prevalence across 
years (β= 0.0022; p= 0.0002) with no significant differences in prevalence between NII and 
CHS (β= 1.0696; p= 0.2044) and no interaction between data source (NII and CHS data) and 
time/years (p = 0.2042). For preschool children there was a significant increase in the 
prevalence across years (β= 0.0025; p<0.0001) along with a significant difference in 
prevalence between NII and CHS (β= 1.150; p= 0.0005) and a significant interaction between 
data source (NII and CHS data) and time/years (p= 0.0005), such that prevalence rates 
increased significantly more in NII relative to CHS data. For primary school children there 
was a significant increase in prevalence across years (β= 0.0017; p= 0.0001) with no 
significant differences in prevalence between NII and CHS (β= 0.9160; p= 0.1538) and no 
interaction between data source (NII and CHS data) and time/years (p= 0.1535). Finally, for 
middle/high school children there was a significant increase in prevalence across years (β= 
0.0009; p<0.0001) along with a significant difference in prevalence between NII and CHS (β= 
0.6505; p= 0.0052) and a significant interaction between data source (NII and CHS data) and 
time/years (p= 0.0051), indicating that prevalence rates increased significantly more in NII 
data. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint Figure 6. Changes in ASD prevalence from 2017 to 2021 in each of four age groups: daycare 
(2-3 years old), preschool (4-6 years old), primary school (7-12 years old), and middle/high 
school (13-17 years old).  NII (black) and CHS (gray) data are presented in separate lines. Discussion Our results demonstrate that ASD prevalence continued to grow rapidly in Israel between 
2017 and 2021. While overall ASD prevalence in children 1-17-years-old increased by a 
factor of two (Figure 1), ASD prevalence in children 2-3-years-old increased by a factor of 
4.4. This demonstrates a rapid shift towards early ASD diagnosis in Israel such that in 2021 
the ASD prevalence was 1% among 2-3-year-old children and almost 2% among 4-5-year-old 
children (Figure 5). The high percentage of diagnosed children at such young ages creates an 
opportunity for early intervention that is important for improving later outcomes (Gabbay-
Dizdar et al., 2022; Zwaigenbaum et al., 2015). However, capitalizing on this opportunity 
requires a corresponding rapid increase in the availability of early intervention services that 
are managed by the Israel healthcare and education systems. Since children rarely loose an ASD diagnosis (Wiggins et al., 2012), these findings suggest 
that ASD prevalence in older children and adolescents in the Israeli population will reach at 
least 2% within a decade as the ASD children described in the current study grow older and 
are joined by those diagnosed at older ages. These results, therefore, highlight the need to 
expand the healthcare, educational, and social services necessary to support this population 
as it ages. ASD prevalence in Israel relative to other countries Previous estimates of ASD prevalence in Israel have been considerably lower than those 
reported in most comparable high-income countries around the world. While ASD 
prevalence among 8-year-old children in the U.S. was already 1.47% in 2010 (Maenner et al., 
2021), ASD prevalence for this age group in Israel was only 0.65% in 2015 (Segev et al., 
2019). Our results demonstrate that it has increased to 1.5% in 2021 (Figure 2). This 
suggests that ASD had been previously under-diagnosed in Israel and that the Israeli 
healthcare system is closing the gap towards ASD prevalence rates described in other 
countries. Since ASD prevalence is affected by a complex combination of social, political, ethnic, 
financial, genetic, and environmental factors (Leonard et al., 2010b), it is difficult to predict 
whether ASD prevalence in Israel will continue to increase at a similar rapid rate over the 
next 4-5 years or start to stabilize. Note that in some countries such as Iceland (Delobel-
Ayoub et al., 2020b) and U.S. states such as California and New Jersey (Maenner et al., 
2021), ASD prevalence among 8-year-old children has already exceeded 3%. Hence, ASD . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint prevalence is likely to continue and grow in Israel if one assumes that there is a similar 
percentage of children with ASD across different geographic locations and that population 
genetics and environmental differences have only minor impact on ASD prevalence. Generalizability of ASD prevalence estimates across data sources Our results demonstrate the importance of using complete national data for accurately 
estimating ASD prevalence and its change over time. The rate of increase in ASD prevalence 
differed significantly across estimates from NII and CHS data (Figure 1). While the general 
trend was equivalent, ASD prevalence increased more modestly in CHS data relative to NII 
data. This suggests two possible interpretations, which are not mutually exclusive. The first 
is that more families with newly diagnosed children were registering with NII than with CHS 
systems, perhaps due to a larger urgency of receiving benefits and access to early education 
services (NII jurisdiction) rather than ASD-related healthcare services (CHS jurisdiction). The 
second is that the national increase in ASD prevalence may be driven more strongly by 
diagnosis rates of individuals insured by other HMOs rather than those of CHS. Indeed, a 
recent study with Maccabi HMO data (the second largest HMO in Israel, servicing ~25% of 
the population), reported an ASD prevalence rate of 1.3% among 8-year-old children already 
in 2018 (Davidovitch et al., 2020). This ASD prevalence was ~0.5% higher than that reported by 
CHS for the equivalent year. We speculate that these differences may be due to the ethnic 
and socio-economic characteristics of those insured by the different HMOs, given that CHS 
insures more of the marginalized ethnic populations in the geographic periphery of Israel (Davidovitch et al., 2013; Raz et al., 2015; Segev et al., 2019). Growth of health, education, and social services in corresponding years There is limited publicly available data about the corresponding growth of ASD related 
services in Israel, making it difficult to quantify the response of public systems to the growth 
in ASD prevalence. Nevertheless, according to NII data, the number of families receiving 
supplemental income due to ASD diagnoses in Israel has doubled between 2017 and 2021 
(Figure 1). Moreover, according to Israeli law, children with ASD (unlike children with ID or 
other developmental disorders) are eligible for intensive early intervention programs at the 
ages of 1-7-years-old. According to health ministry data (personal communication, 
unpublished), the utilized budget of these programs has doubled from 255 million NIS in 
2017 to 510 million NIS in 2021 (287 in 2018, 400 in 2019, and 447 in 2020, in millions NIS). 
While ASD prevalence at these ages has more than doubled during this period (Figure 4), it 
is remarkable that the availability of early intervention programs is growing at such a rapid 
pace in Israel. This is due to the intervention programs that are run by the Ministry of Health 
via non-government organization such as ALUT and the association for children at risk. 
According to data from the ministry of education (personal communication, unpublished), 
the number of special education kindergartens for ASD children 3-6-years-old has grown 
from 625 in 2020 to 723 in 2021, indicating an annual increase of ~15.6%, which is also . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23287784
doi: 
medRxiv preprint remarkable, but still lower than the annual rate of prevalence increase (~21%, Figure 3) for 
this age group. Limitations The current study had several limitations. First, we did not have information regarding the 
sex or geographic composition of ASD cases, which limited our ability to identify prevalence 
differences across males and females or distinct geographic areas of Israel. Second, we did 
not have retrospective data from before 2017, which limited our ability to calculate 
cumulative incidence rates by birth cohort as reported by previous studies in Israel (Raz et 
al., 2015; Segev et al., 2019). However, given that ASD diagnoses are rarely lost (Wiggins et 
al., 2012), ASD prevalence rates are likely to be equivalent to cumulative incidence rates 
such that, for example, ASD prevalence of 4-year-olds in 2017 will be equivalent to the ASD 
cumulative incidence rate of those born in 2013. Third, we did not have equivalent data 
from Maccabi, Meuhedet, and Leumit HMOs that service ~50% of Israeli residents. This 
limited our ability to assess HMO specific differences and ASD prevalence changes. Conclusions ASD prevalence in 1-17-year-old children has doubled in Israel between 2017 and 2021 and 
quadrupled in 2-3-year-old children. This rapid growth has considerable healthcare, 
educational, and social ramifications, which are currently met only partially. These findings 
highlight the necessity of a coordinated effort to further expand support services for this 
population at multiple ages. Improving public availability of NII, HMO, Health Ministry, and 
Education Ministry data will greatly facilitate the assessment of ASD prevalence in Israel and 
its growth over time to better plan future services for this population.",1
"Background: White matter hyperintensities are an important marker of cerebral small vessel disease. This 
disease burden is commonly described as hyperintense areas in the cerebral white matter, as seen on T2-
weighted fluid attenuated inversion recovery magnetic resonance imaging data. Studies have demonstrated 
associations with various cognitive impairments, neurological diseases, and neuropathologies, as well as 
clinical and risk factors, such as age, sex, and hypertension. Due to their heterogeneous appearance in 
location and size, studies have started to investigate spatial distributions and patterns, beyond summarizing 
this cerebrovascular disease burden in a single metric - its volume. Here, we review the evidence of 
association of white matter hyperintensity spatial patterns with its risk factors and clinical diagnoses. 
 
Design/Methods: We performed a systematic review in accordance with the Preferred Reporting Items 
for Systematic Reviews and Meta-Analysis (PRISMA) Statement. We used the standards for reporting 
vascular changes on neuroimaging criteria to construct a search string for literature search on PubMed. 
Studies written in English from the earliest records available until January 31st, 2023, were eligible for 
inclusion if they reported on spatial patterns of white matter hyperintensities of presumed vascular origin.  
 
Results: A total of 380 studies were identified by the initial literature search, of which 41 studies satisfied 
the inclusion criteria. These studies included cohorts based on mild cognitive impairment (15/41), 
Alzheimer's Disease (14/41),  Dementia (5/41), Parkinson's Disease (3/41), and subjective cognitive decline 
(2/41). Additionally, 6 of 41 studies investigated cognitively normal, older cohorts, two of which were 
population-based, or other clinical findings such as acute ischemic stroke or reduced cardiac output. Cohorts 
ranged from 32 to 882 patients/participants (median cohort size 191.5 and 51.6 % female (range: 17.9 - 
81.3 %)). The studies included in this review have identified spatial heterogeneity of WMHs with various 
impairments, diseases, and pathologies as well as with sex and (cerebro)vascular risk factors. Conclusions: The results show that studying white matter hyperintensities on a more granular level might 
give a deeper understanding of the underlying neuropathology and their effects. This motivates further 
studies examining the spatial patterns of white matter hyperintensities. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 3 Introduction White Matter Hyperintensities (WMHs) of presumed vascular origin are a widely studied marker of cerebral 
small vessel disease (SVD).1 This disease burden appears hyperintense on T2-weighted Magnetic 
Resonance Imaging (MRI), and is often characterized on FLuid Attenuated Inversion Recovery (FLAIR) 
imaging.1 Studies have demonstrated that their prevalence and severity increase with age.2,3 Moreover, it 
has been demonstrated that this cerebrovascular disease burden is associated with various impairments, 
diseases, and pathologies, such as motor4 and mood disorders,5,6 cognitive impairment (CI),7–9 dementia 
(DEM),10,11 and stroke.12,13 Additionally, the presentation of WMHs in the brain are associated with sex14 
and clinical factors including (cerebro)vascular risk factors like hypertension (HTN)15 and diabetes type 2 
(DM2).16 Due to its high prevalence, multiple studies have reviewed the evidence on its prevalence and 
modifying factors over the years.8,17–21 
 
WMH burden is heterogeneous in location and size and appears as punctate, focal, and/or confluent 
lesions.22  It is commonly characterized using semi-quantitative visual rating scales, such as Fazekas,22 
Manolio,23 or Scheltens,24 or by using fully-quantitative volumetric measurements based on either manual, 
semi-automated, or fully-automated approaches. To date, however, there is no universally established 
methodology for quantification, as their utility depends on availability, time costs, and quality of the 
imaging data.17 However, with the increased prevalence of fully automated and/or deep learning-enabled 
methodology, volumetric evaluations in increasingly larger cohorts are becoming more prevalent.14,17,25,26 
 
While most investigations tend to summarize WMH burden as a single volumetric measure, researchers 
have started to acknowledge the importance of its spatial distributions to gain additional insights into the 
underlying neuropathology.27 Several studies have therefore examined the spatial patterns of WMHs in 
various populations. The aim of this systematic review is to give an overview of the evidence demonstrating 
pathological, clinical, and cerebrovascular risk factor effects on spatial patterns of WMHs in adult 
populations and clinical cohorts, by summarizing the increasing evidence of spatial specificity with respect 
to WMH burden. Methods This systematic review was performed using the Preferred Reporting Items for Systematic Reviews and 
Meta-Analysis (PRISMA) Statement.28 This review was not registered and no review protocol was 
prepared. The associated PRISMA checklist can be found in the appendix. Search Strategy and Study Selection Criteria Studies have been identified by an advanced search on PubMed. The in STRIVE1 described naming 
conventions for WMHs were utilized as a reference to include the most prominent terms for WMHs. 
Additionally, we restricted our analysis to studies with MRI FLAIR data published before February 1st, 
2023. Due to the lack of consensus of nomenclature for the investigation of spatial WMH burden features, 
the terms “pattern”, “topology”, “topography” and “spatial” were included. The full search string is given 
as: (White Matter Hyperintensity OR White Matter Lesion OR White Matter Disease OR Leukoaraiosis) . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 4 AND (MRI AND (FLAIR OR (Fluid Attenuated Inversion Recovery))) AND (spatial OR pattern OR 
topology OR topography). Only articles written in English were considered.  
 
All abstracts were subsequently screened for eligibility. Studies were limited to human adults (>18 years) 
with sample sizes greater than 20 participants/patients that investigated whole-brain WMH patterns and 
their relation to risk factors and diseases. Studies investigating multiple sclerosis or tuberous sclerosis were 
not considered, following the STRIVE recommendation.1 Descriptive studies without the aim to describe 
the association of the observed patterns were excluded for the purpose of this review. Finally, we extended 
our selection by examining the cited literature of the identified studies for additional relevant articles. Data collection The screening was performed by one reviewer without the use of automated tools (J.B.). The final decision 
over study inclusion was reached in consensus with a second reviewer (M.D.S.). Data were extracted using 
a standardized form that captured (1) disease type(s), (2) the number of patients of each disease type, (3) 
age, (4) sex, (5) the quantification method, and (6) spatial pattern analysis (see Table 1). Results A total of 380 studies were identified by the initial literature search, of which 41 studies satisfied the 
inclusion criteria (see Figure 1 for a detailed description of the selection phase).29–69 These studies included 
cohorts based on mild cognitive impairment (MCI) (15/41), Alzheimer's Disease (AD) (14/41),  Dementia 
(DEM) (5/41), Parkinson's Disease (PD) (3/41), and subjective cognitive decline (SCD) (2/41). Six of the 
41 studies investigated cognitively normal cohorts, two of which were population-based.32,37 Additionally 
one study described the spatial WMH patterns in an acute ischemic stroke (AIS) cohort30 and one in a 
population with reduced cardiac output (RedCO)48. Cohorts ranged from 32 to 882 patients/participants 
(median cohort size: 191.5, 51.6% female (range 17.9 - 81.3 %)). The mean age of the investigated cohorts 
ranged from 30 to 80 years (median: 69.9 years; mean age not reported in two studies50,69). . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 5 Figure 1: Flow-chart of study selection based on the PRISMA statement.28 Studies were identified through 
an advanced search on PubMed. First, abstracts were screened, followed by reading the retrieved 
publications and excluding non-relevant studies. Exclusion criteria - Reason 1: Cohort included less than 
20 subjects. Reason 2: Study was not performed in adult populations (>18 years). Reason 3: Study did not 
investigate spatial patterns of WMHs (e.g., studies investigated spatial patterns of other markers, such as 
activation patterns of functional MRI, only included total WMH burden as a covariate, or fully descriptive 
studies). Reason 4: Study did not assess the whole brain for analysis. Reason 5: Study investigating multiple 
sclerosis70 or tuberous sclerosis71 which do not meet the STRIVE1 definition of WMH. The reference list 
of each retrieved report was additionally examined for eligible studies. WMH Quantification To study spatial features of WMH burden and its association with clinical correlates and risk factors, it is 
necessary to characterize the extent of the burden. In the identified studies, WMHs were assessed by 
employing qualitative/semi-quantitative (n=6), or fully quantitative approaches (n=35). Most studies 
utilized either semi- or fully-automated WMH segmentation methodology (31/35), eleven of which were 
based on respective in-house developed algorithms, six using the Lesion Segmentation Toolbox (LST),72 
and the remainder employed other available tools like the Brain Intensity AbNormality Classification 
Algorithm (BIANCA)73 or the Medical Image Processing, Analysis, and Visualization (MIPAV) software 
package.74 In five studies lesions were manually delineated. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 6 Semi-quantitative/qualitative. Semi-quantitative/qualitative measures represent the degree of lesional 
dispersion and severeness with help of visual rating scales. These scales provide a way to measure WMH 
burden and describe its topology without explicit lesion delineation. The used scales were the Fazekas22 
(and its derivatives; n=5), and Scheltens24 (n=1) scales. The latter incorporates WMH topology in the form 
of local, lobar burden, while the Fazekas scale is employed for whole-brain evaluation. 
 
Fully-quantitative. Fully-quantitative approaches describe the disease burden as a volumetric measure, 
e.g., in cubic centimeters or milliliters. WMHs are delineated either manually, semi-, or fully-automatically. 
Manual WMH segmentation, while often considered the gold standard in the field, is time-consuming and 
shows high intra- and inter-rater variability.75 To address this challenge, many automated assessment 
algorithms have been developed,25 however, no universally applicable algorithm exists.17 Spatial Pattern Analyses Based on the identified literature, we distinguish between three analysis approaches for studying spatial 
patterns of WMH burden: 1) Region of interest (ROI), 2) periventricular (PWMH) and deep WMH 
(DWMH), and 3) voxel-wise analysis. A comprehensive overview of the included studies and their findings 
is given in Table 1. 
 
Regions of interest. In this approach, the brain is subdivided into ROIs, based on brain areas that individual 
studies hypothesize to be associated with specific biomarkers. These ROIs may represent, e.g. cortical 
lobes,29,31–33,35,36,38,39,66–69 or other specific regions that may be relevant to the patient cohort or disease under 
investigation, e.g. using vascular territories.30,66 To identify and define ROIs, studies often rely on image 
registration to a brain template, which is often derived for the general population or a control cohort, and 
on which the ROIs are defined.  
 
We identified eleven studies using ROI analyses.29–39,66–69 Of these studies eight utilized the definition of 
cortical lobes or a modified version of them as ROIs. While increased parietal WHM burden was associated 
with AD,31 reduced balance and postural support,32 and with poor cognitive function, measured with the 
Montreal cognitive assessment (MoCA),35 increased WMH burden in the frontal lobe was related to poor 
executive function and episodic memory34 in patients with PD38 and neurological decompression sickness 
(NDCS).39 Increased temporal WMH burden corresponded with decreased cognitive measures,33 and 
impaired cognitive function,35 while another study associated increased inferior-occipital WMH burden 
with MCI.29 Additionally, Garnier-Crussard et al.68 and Thu et al.69 found WMH in the posterior regions 
(parietal, temporal, and occipital) to be associated predominantly with AD. Thu et al.69 further compared 
the local WMH burden across atypical variants of AD logopenic progressive aphasia (LPA-AD), posterior 
cortical atrophy (PCatro-AD) and typical amnestic AD (tAD). Patients with LPA-AD compared to PCatro-
AD showed a higher left/right posterior lobar WMH burden, and patients with either atypical variants had 
more WMHs in the occipital lobes compared to patients with tAD. In patients with cerebral autosomal 
dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL), Auer et al.36 found 
an increased WMH burden in the temporal lobes, and in those with NDCS, McGuire et al.39 described an 
increased WMH burden in the frontal and temporal lobes. Schirmer et al.30 demonstrated  a shift of 
predominance of WMH burden in a cohort of AIS patients and by using areas of cortical blood flow, i.e. 
vascular territories, that were associated with age, sex, small vessel stroke, hypertension, smoking, and . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 7 hyperlipidemia. Similarly, Wen et al.66 introduced cortical blood flow regions in addition to lobar ROIs. 
They found increased WMH burden in the anterior and middle cerebral artery regions as well as in the 
periventricular regions. The association between advanced age and increased periventricular WMH burden 
was confirmed by a follow-up study67 of younger patients (mean age under 50 years). Finally, using a 
bullseye representation, Tullberg et al.34 showed an increased WMH burden in the deep white matter which 
was  associated with poor executive function whereas increased WMH burden close to the ventricles was 
found to be associated with poor episodic memory. While this approach roughly relates to P/DWMH, 
discussed in the next section, they did not follow the common definitions for stratification in their work, 
resulting in 36 unique ROIs.  
 
PWMH and DWMH. PWMH and DWMH are umbrella terms for multiple definitions in the literature, all 
relating WMH burden to the distance with respect to the ventricular surface.56 In general, PWMHs are 
adjacent or in close proximity to the lateral ventricles, while DWMHs are located at larger distances inside 
the subcortical white matter. While PWMH and DWMH can be considered ROIs, this definition aims to 
reflect different functional, histopathological, and etiological features of WMH burden.   
 
We identified six studies distinguishing PWMH and DWMH.52–56,64 Increased PWMH burden was found 
to be associated with PD,64 AD and brain atrophy,52 faster progression in cognitive decline,54 and impaired 
cognitive function.56 Increased DWMH burden was associated with a decreased mini mental status 
examination score in patients with MCI.53 Another study showed increased DWMH burden in subjects with 
intracerebral hemorrhage and cerebral amyloid angiopathy (CAA), while subjects with intracerebral 
hemorrhage and hypertensive arteriopathy (HA) presented with increased PWMH burden.55  
 
Voxel-Wise. In the voxel-wise approach, each white matter voxel is studied for the presence of WMHs. 
After image registration to a template, the voxel-based frequency can be used to either derive maps showing 
clusters of WMHs within a specific cohort or serve as the basis for voxel-based lesion-symptom mapping. 
The advantage of voxel-wise analysis lies in the hypothesis-free, data-driven localization of lesion clusters 
compared to ROI approaches. Furthermore, it enables studies to combine this analysis with studying 
associations with other localized cerebrovascular information, such as presence of cerebral microbleeds and 
perfusion.  
 
Thirteen studies utilized a voxel-wise analysis approach.40–51,65 In patients with AD and MCI, local WMH 
burden in the frontal and parietal lobes close to the ventricles was associated with an increased amyloid 
burden.42,46,49,65 Increased WMH burden in voxels around the ventricles was found to be associated with 
advanced age,40 poor executive function and episodic memory,41 and with AD.45 Further, juxtacortical 
WMH load was found to be associated with CI, male sex, and CAA, and deep frontal WMH burden with 
hypertension and DM2.65 Quattrocchi et al.50 showed that patients with lung cancer (LC) with brain 
metastases (BM) presented with increased WMH burden in the frontal and occipital lobes, compared to 
those in a control group. They also identified an inverse relationship between the presence of BM and WMH 
burden. Two additional studies measured local perfusion and demonstrated an inverse relationship between 
WMH burden and perfusion, meaning higher WMH burden related to low relative perfusion in the normal 
appearing white matter.47,48 In another study increased WMH burden bilaterally in the anterior thalamic 
radiation was found to be associated with apathy.51 Lastly, two studies reported no regional associations 
between WMH burden and cognitive performance43 or PD.44 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 8 Mixed Approach. Seven studies utilized a mixed spatial analysis approach, two with ROI as well as 
distinguishing between PWMH and DWMH,57,58 and five studies applying ROI and voxel-wise analyses.59– 63  Kim et al.57 found increased PWMH burden associated with impaired cognitive function and increased 
parieto-occipital WMH burden associated with decreased neuropsychological scores. Bea et al.57 found 
increased frontal and deep WMH burden in patients with panic disorder compared to controls. Four of the 
studies using ROI and voxel-wise analyses found local associations between WMH burden and lower 
cerebrospinal fluid (CSF) amyloid burden,59 poor executive function,60 increased vascular risk factors and 
CI,61 and reduced speed and flexibility of executive function.63 Gaubert et al.62 demonstrated associations 
of regional WMH distributions in multiple locations, such as the posterior lobes and the corpus callosum, 
with multimodal brain biomarkers of AD. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 9 Table 1: Overview of the study cohorts investigating spatial WMH burden patterns and summary of their findings. A = automatic, ACA = 
Anterior Cerebral Artery, AD = Alzheimer’s Disease, AIS = Acute Ischemic Stroke, artd = arterial disease, ATR = Anterior, Thalamic Radiation, AV45 = 
Florbetapir, BM = Brain Metastases, C = Control, CAA = Cerebral Amyloid Angiopathy, CADASIL = Cerebral Autosomal Dominant Arteriopathy with Subcortical 
Infarcts and Leukoencephalopathy, CC = Corpus Callosum, CI = Cognitive Impairment, CMBs = Cerebral Microbleeds, CR = Corona Radiata, CSF = Cerebrospinal 
Fluid, DEM = Dementia, DM2 = Diabetes Mellitus type 2, f = female, FDG = Fluordesoxyglucose, FQN = Fully Quantitative, HA = Hypertensive Arteriopathy, 
HLD = Hyperlipidemia, HTN = Hypertension, ICH =  Intracerebral Hemorrhage, LC = Lung Cancer, LPA-AD =logopenic progressive aphasia,  M = manual, 
MCA = Middle Cerebral Artery, MCI = Mild Cognitive Impairment, ME = Method, MoCA = Montreal Cognitive Assessment, N = cohort size, NDCS = Neurologic 
Decompression Sickness, NPI = Neuropsychiatric Inventory, P/D = PWMH/DWMH, PCA = Posterior Cerebral Artery, PCatro-AD = posterior cortical atrophy, 
PD = Parkinson Disease, PDis = Panic Disorder, RedCO = Reduced Cardiac Output, ROI = Region of Interest, S = semi-automatic, SCD =  Subjective Cognitive 
Decline, sd = standard deviation, SLF = Superior Longitudinal Fasciculus, sSAE = sporadic Subcortical Arteriosclerotic Encephalopathy , SUVR = Standardised 
Uptake Value Ratio, tAD = typical amnestic Alzheimer’s Disease, v-w = voxel-wise Study 
Population 
N 
Sex 
(f; %) Age  
mean (sd) 
ME FQN Analysis 
Findings 
ROI P/D v-w Auer et al. 200136 
CADASIL 
sSAE 28 
24 53.6 
62.5 49.9 (-) 
65.0 (-) 
M 
N 
X 
 
 
Increased temporal WMH in CADASIL Tullberg et al. 200434 DEM 
MCI 
C 26 
30 
22 19.2 
26.6 
63.6 76.5 (8.7) 
78.2 (7.7) 
76.6 (6.5) M 
Y 
X 
 
 
Increased WMH in frontal lobe -> Poor executive function and episodic memory Wen et al. 200466 
C 
477 
47.4 
62.6 (1.5) 
A 
Y 
X 
 
 
Increased PWMH across lobes associated with age 
ACA & MCA : Higher prevalence with age Wen et al. 200867 
C 
428 
54.2 
46.7 (1.4) 
A 
Y 
X 
 
 
Increased PWMH across lobes associated with age Bunce et al. 201033 
C 
428 
54.2 
46.7 (1.4) 
S 
Y 
X 
 
 
Increased WMH in right temporal lobe -> Decreased cognitive measures Murray et al. 201032 
C 
148 
56.1 
79.0 (5.2) 
S 
Y 
X 
 
 
Increased parietal WMH -> Reduced balance and postural support McGuire et al. 201339 
NDCS 
C 102 
91 NA 
NA 36.0 (6.2) 
37.0 (6.0) 
M 
Y 
X 
 
 
Increased frontal and temporal WMH in pilots with NDCS Ai et al. 201435 
DEM/MCI 
56 
44.6 
66.8 (5.4) 
S 
Y 
X 
 
 
Increased temporal WMH -> Impaired cognitive function 
Increased parietal WMH -> Decreased MoCA score Brickman et al. 201531 
AD 
303 
69.0 
79.2 (5.3) 
S 
Y 
X 
 
 
Increased parietal WMH in possible AD Schirmer et al. 201930 
AIS 
882 
38.3 
65.2 (14.8) 
M 
Y 
X ACA: Higher prevalence with age, small vessel stroke, HTN, smoking  
ACA: Lower prevalence with HLD 
MCA: Lower prevalence with age 
PCA: Higher prevalence for male sex 
PCA: Lower prevalence with age, small vessel stroke Spatial patterns of WMH 10 Wu et al. 201929 
MCI 
C 22 
98 59.0 
64.0 70.0 (8.5) 
69.9 (8.7) 
A 
Y 
X 
 
 
Increased bilaterally WMH in inferior-occipital white matter in MCI Brugulat-Serrat et al. 202037 
C 
561 
61.0 
57.4 (7.5) 
A 
Y 
X 
 
 
Increased 
DWMH 
-> 
Poor 
executive 
function Increased PWMH -> Poor episodic memory Fang et al. 202138 
PD 
C 21 
33 47.4 
45.5 67.6 (9.2) 
72.5 (10.6) 
M 
Y 
X 
 
 
Increased WMH in frontal lobes  -> impaired motor function Garnier-Crussard et al. 202168 AD C 54 
40 40.7 
50.0 71.0 (9.1) 
69.7 (6.9) 
A 
Y 
X 
 
 
Increased WMH in posterior regions (parietal, temporal, occipital) associated with AD Thu et al. 202269 tAD 
LPA -AD 
PCatro-AD 50 
75 
39 58.0 
58.7 
61.5 - (Med: 63) 
- (Med: 68) 
- (Med: 64) A 
Y 
X Increased WMH in left parietal lobe in LPA-AD compared to PCatro-AD 
Increased WMH in right occipital, parietal and temporal lobes in PCatro-AD compared to 
LPA-AD 
Increased WMH in occipital lobe in LPA-AD and PCatro-AD compared to. tAD Holland et al. 200847 AD/MCI 
CAA 
C 41 
32 
29 48.8 
62.5 
62.1 71.2 (5.1) 
75.1 (7.1) 
72.3 (7.1) S 
Y 
 
 
X Increased PWMH associated with age 
Increased frontal WMH associated with age 
Increased WMH in regions with lower relative perfusion Dalaker et al. 200944 
PD 
C 163 
102 39.3 
54.9 65.7 (9.4) 
66.2 (9.1) 
S 
Y 
 
 
X 
No regional effects in PD patients Jefferson et al. 201148 
RedCO 
32 
37.5 
72.0 (8.0) 
S 
Y 
 
 
X 
Increased WMH in regions with lower normative perfusion Smith et al. 201141 DEM 
MCI 
C 11 
96 
40 45.0 
61.0 
65.0 71.2 (4.7) 
72.9 (5.5) 
74.1 (6.9) S 
Y 
 
 
X 
Increased PWMH -> Poor executive function and poor episodic memory Rostrup et al. 201240 
C 
605 
53.4 
74.1 (5.1) 
S 
Y 
 
 
X 
Increased PWMH -> Increase with age, male, smoking, alcohol consumption 
Increased DWMH -> HTN Quattrocchi et al. 201450 
BM w/ LC 
BM w/out LC 107 
93 29.0 
40.9 - (Med: 63) 
- (Med: 60) 
S 
Y 
 
 
X 
Inverse relationship between WMH and BM 
Increased WMH in occipital and frontal lobes in patients with LC Torso et al. 201551 
MCI 
C 31 
26 54.8 
57.7 67.5 (7.0) 
71.3 (8.1) 
S 
Y 
 
 
X 
Bilateral WMH in anterior thalamic radiation associated with NPI-12 apathy score Al-Janabi et al. 201842 
MCI 
C 36 
26 47.2 
65.4 73.5 (8.0) 
76.8 (6.1) 
S 
Y 
 
 
X 
Increased posterior WMH -> Lower CSF amyloid β 
Increased DWMH -> HTN Altermatt et al. 201943 
CI 
878 
58.7 
68.2 (7.6) 
S 
Y 
 
 
X 
No regional association with cognitive performance Damulina et al. 201945 
AD 
C 130 
130 63.1 
61.5 73.8 (5.3) 
74.6 (7.0) 
S 
Y 
 
 
X 
Increased PWMH in AD Graff-Radford et al. 201946 
AD 
424 
45.0 
75.0 (8.4) 
A 
Y 
 
 
X 
Increased amyloid burden associated with local WMH associated with lobar CMBs Moscoso et al. 202049 
AD 
190 
46.8 
73.1 (6.3) 
S 
Y 
 
 
X 
Increased frontal and parietal WMH -> Faster rates of amyloid accumulation Phuah et al. 202265 
MCI 
DEM 529 
166 
47.7 
72.9 (7.6) 
A 
Y 
 
 
X 
Increased deep frontal WMH -> HTN and DM2 
Increased juxtacortical WMH -> CAA, male, CI Spatial patterns of WMH 11 C 
346 De Groot et al. 200254 
C 
563 
49.9 
73.5 (10.6) 
M 
N 
 
X 
 
Increased PWMH -> Increased rate of cognitive decline Saka et al. 200752 
AD 
MCI 25 
17 44.0 
29.4 68.9 (5.8) 
74.7 (6.4) 
M 
N 
 
X 
 
Increased PWMH in AD 
Increased PWMH -> Increased interuncal distance Stenset et al. 200853 
MCI 
217 
50.2 
69.7 (8.9) 
M 
N 
 
X 
 
Increased DWMH -> Decreased Mini Mental Status Examination Griffanti et al. 201656 
C 
563 
17.9 
69.6 (5.3) 
A 
Y 
 
X 
 
Increased PMWH -> Impaired cognitive function Charidimou et al. 201855 
CAA-ICH 
HA-ICH 319 
137 50.5 
43.1 73.9 (1.8) 
67.2 (3.5) 
M 
N 
 
X 
 
Increased DWMH in CAA 
Increased PWMH in HA Grey et al. 202264 AD+MCI 
PD+PDMCI 
C 40 
42 
55 70.0 
29.0 
69.0 71.5 (7.1) 
63.8 (9.7) 
67.0 (7.3) M 
Y 
 
X 
 
Increased 
PWMH 
in 
PD 
and 
PDMCI Increased DWMH with cognitive performance Bae et al. 201058 
PDis 
C 24 
24 46.8 
50.0 32.3 (6.6) 
30.1 (6.1) 
M 
N 
X 
X 
 
Increased DWMH in PDis 
Increased frontal WMH in PDis Kim et al. 201157 AD 
MCI 
C 37 
23 
22 73.0 
47.8 
72.7 70.5 (7.0) 
66.5 (7.2) 
67.6 (6.2) S 
Y 
X 
X 
 
Increased PWMH -> Impaired cognitive function 
Increased parieto-occipital WMH -> Decreased neuropsychological scores Yoshita et al. 200661 AD 
MCI 
C 26 
28 
33 61.5 
39.3 
69.7 73.4 (8.1) 
74.8 (8.2) 
79.6 (6.8) S 
Y 
X 
 
X 
Increased WMH in CC -> Increase in  CI 
Increased WMH in periventricular ROIs -> HTN Biesbroek et al. 201360 
artd 
516 
18.0 
56.7 (9.4) 
S 
Y 
X 
 
X 
Increased WMH in SLF and ATR -> Poor executive functioning Birdsill et al. 201463 
AD 
349 
68.2 
59.7 (6.4) 
A 
Y 
X 
 
X 
Increased WMH in CC, CR and right cingulum -> Reduced speed and flexibility Weaver et al. 201959 AD 
DEM 
MCI 
SCD 97 
37 
118 
121 52.7 
39.4 
39.8 
46.3 68.5 (7.6) 
66.4 (6.7) 
68.0 (7.4) 
63.5 (7.5) A 
Y 
X 
 
X 
Increased bilateral parieto-occipital periventricular WMH -> Lower CSF Aβ Gaubert et al. 202162 AD 
MCI 
SCD 
C 25 
51 
28 
51 64.0 
56.9 
60.7 
47.1 68.0 (10.2) 
73.3 (7.2) 
67.1 (7.4) 
70.8 (6.4) A 
Y 
X 
 
X 
Increased WMH in CC -> Increased AV45-SUVR 
Increased WMH in temporo-parietal region -> Decreased FGD-SUVR Spatial patterns of WMH 12 Spatial effects of vascular risk factors and clinical correlates 
 
Some of the discussed studies reported spatial effects of vascular risk factors and clinical correlates, such 
as age, sex, HTN, smoking, alcohol, stroke, hyperlipidemia (HLD), and specific neurodegenerative and 
other 
diseases. Advanced age was associated with increased PWMH47,53,56,66,67 and DWMH29,53 burden as well as with 
increased WMH burden in the parietal lobes.47 Furthermore - using vascular territories as ROIs - an 
association was found between advanced age and increased and decreased relative WMH burden in the 
anterior cerebral artery (ACA) and middle cerebral artery (MCA) territory, respectively.30,66 Men showed 
increased PWMH40 and WMH burden in the posterior cerebral artery (PCA) region,30 while women 
presented with increased frontal WMH burden.33 Increased DWMH40,42,61 and ACA territory burden30 were 
associated with HTN. Smoking30 and alcohol consumption40 showed increased PWMH burden, while 
smoking also showed increased WMH burden in the ACA territory.30 Patients with small vessel stroke 
presented with increased WMH burden in the ACA and decreased WMH burden in the PCA territory, while 
HLD led to a decreased WMH burden in the ACA territory.30 Patients with AD showed a broad spectrum 
of spatial effects: they had increased WMH burden in the frontal34,46,49 and parietal31,34,46,49 lobes, in the 
posterior cerebrum,62,68,69 the corpus callosum,61–63 the corona radiata,62,63 the anterior thalamic radiation62,63 
as well as increased PWMH45,46,52,57,59,61 and DWMH53 burden. Patients with MCI showed similar 
distributions with increased WMH burden in the parietal lobe,35 the posterior cerebrum42 and increased 
PWMH burden,41,61 but also increased WMH burden in the temporal lobe,35 which was also found to be 
associated with CADASIL.36 Patients with PD presented with increased WMH burden in the frontal lobe.38 
Other diseases like arterial disease showed increased WMH burden in the superior longitudinal fasciculus 
and the anterior thalamic radiation,60 panic disorder was associated with increased DWMH burden as well 
as with increased WMH burden in the frontal lobe.58 NDCS39 and  LC with BM50 were associated with 
WMH burden in the frontal lobe. CAA and HA were associated with increased DWMH and PWMH burden, 
respectively.55 The associations are summarized in Table 2. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 13 Table 2. Vascular risk factors and clinical correlates affecting spatial patterns. Abbreviations:  ACA = 
Anterior Cerebral Artery, AD = Alzheimer's Disease, ATR = Anterior Thalamic Radiation, artd = arterial disease  , 
BM = Brain Metastases, CAA = Cerebral Amyloid Angiopathy, CADASIL = Cerebral Autosomal Dominant 
Arteriopathy with Subcortical Infarcts and Leukoencephalopathy, CC = Corpus Callosum, CR = Corona Radiata, 
HA = Hypertensive Arteriopathy, HLD = Hyperlipidemia, HTN = Hypertension, LC = Lung Cancer, MCA = Middle 
Cerebral Artery, MCI = Mild Cognitive Impairment, NDCS = Neurologic Decompression Sickness, PCA = Posterior 
Cerebral Artery, PD = Parkinson's Disease, PDis = Panic Disorder, SLF = Superior Longitudinal Fasciculus Vascular risk factors and clinical 
correlates Spatial effect [# Studies] Age 
Increased PWMH [n=5]47,53,56,66,67 
Increased DWMH [n=2]29,53 
Increased parietal WMH [n=1]47 
Increased WMH in ACA [n=2]30,66 
Decreased WMH in MCA [n=2]30,66 Sex 
Increased PWMH (male40,65) [n=2] 
Increased frontal WMH (female33) [n=1] 
Increased WMH in PCA (male30) [n=1] Vascular risk Factors 
Increased DWMH (HTN40,42,61 ) [n=3] 
Increased PWMH (smoking30, alcohol40) [n=2] 
Increased relative WMH in ACA (HTN, smoking, stroke)30 [n=1] 
Decreased relative WMH in ACA (HLD30) [n=1] 
Decreased relative WMH in PCA (stroke30) [n=1] Neurodegenerative diseases 
Increased frontal WMH (AD34,46,49,PD38) [n=4] 
Increased temporal WMH (MCI35, CADASIL36) [n=2] 
Increased parietal WMH (AD31,34,46,49, MCI35) [n=5] 
Increased posterior WMH (AD62,68,69,MCI42) [n=4] 
Increased WMH in CC (AD61–63) [n=3] 
Increased WMH in CR (AD62,63) [n=2] 
Increased PWMH (AD45,46,52,57,59,61,MCI41,61, PD64) [n=9] 
Increased DWMH (AD53,MCI29) [n=2] 
Increased WMH in ATR (AD51) [n=1] Other diseases 
Increased DWMH (PDis58,CAA55) [n=2] 
Increased PWMH (HA55) [n=1] 
Increased juxtacortical WMH (CAA65) [n=1] 
Increased frontal WMH (PDis58, NDCS39, DM265,  LC&BM50) [n=4] 
Increased WMH in SLF (artd60) [n=1] 
Increased WMH in ATR (artd60) [n=1] Discussion WMH burden has been linked to various impairments, diseases, pathologies, as well as (cerebro)vascular 
risk and clinical factors. Subsequently, it has been used as a biomarker in multiple studies, often . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 14 summarized as total WMH burden or load.8 However, WMHs demonstrate spatial distributions in the brain 
that are specific to diseases and other clinical and risk factors. Investigation of the topographical aspects of 
this marker of cerebral small vessel disease, rather than summarizing it as a single, volumetric measure, 
can therefore give new insights into the underlying pathophysiology and studied correlates of interest.  
 
This systematic review of 41 studies gives new insights into the topography of WMHs as a biomarker and 
provides strong evidence for the importance and benefit of studying their spatial distribution. In general, 
we differentiate between qualitative and semi-/quantitative WMH characterization approaches. While 
qualitative approaches are easy to perform manually in smaller cohorts, other approaches, such as 
probabilistic segmentation methods, can be utilized to extend the analyses to bigger cohort sizes. 
Importantly, with the event of deep learning enabled pipelines for white matter hyperintensity segmentation, 
fully automated, quantitative characterization has become more prominent in the literature. The latter has 
the benefit of removing inter-rate variability within a study, however, comparability of segmentation 
between studies may not be given, if different algorithms are used, due to the high heterogeneity of 
performance.25  
For spatial stratification, we identified three general approaches to studying the spatial patterns of WMH. 
These included ROI, PWMH/DWMH, and voxel-wise stratification. While a generally accepted 
methodology on how to categorize WMH is lacking, the described relationships with risk factors and 
clinical correlates demonstrated consistent trends. In general, it is difficult to define such a gold-standard 
for spatial analyses, as both hypothesis (ROI) and hypothesis-free (voxel-wise) approaches have merit and 
depend on the specific research question that is being answered. An ROI approach may yield new insights 
and potentially simplify the analysis, however, the selection of ROI definition needs to be based on the 
underlying biological hypothesis. A voxel-wise approach provides a more granular topographical 
investigation through a data driven approach, however, the resulting interpretation of the results should 
consider the anatomical structure of the brain, and potentially relate its findings back to commonly used 
ROIs to facilitate the interpretation of the findings. Despite the different approaches and use of varying 
WMH quantification techniques, the discussed findings presented in this review agreed in the direction of 
the found association, suggesting the presence of a biologic signal.  
 
We defined WMH in this review according to the STRIVE criteria1, i.e. hyperintense signal in the white 
matter of presumed vascular origin, assessed using FLAIR imaging. However, this systematic review 
covered populations with various diseases, including AD, DEM and PD, for which, at least in part, it is still 
unclear whether SVD is etiologically involved in the neurodegenerative processes or occurs coincidentally. 
For example, it has been suggested that SVD might precede Parkinsonism related pathology,76 whereas 
WMH and amyloid accumulations might be independent, but additive processes.77 Nevertheless, it is 
established that WMH are highly prevalent in these neurodegenerative diseases and play a key role in 
cognitive impairment, warranting their investigation.7–9  
 
 
The studies included in this systematic review investigated spatial patterns of WMH in populations with 
different diseases. Spatial stratification was predominantly performed in studies investigating cognitive 
impairment (n=23), identifying differences in spatial patterns of WMH in PWMH (n=7; for AD and 
MCI),41,45,46,52,57,59,61 as well as in the parietal (n=5; for AD and MCI),31,34,35,46,49 posterior (n=4; for AD and 
MCI),42,62,68,69 and frontal (n=3; for AD)34,46,49  lobar regions. In this particular study population, spatial . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 15 patterns of WMH appear across the entire brain. However, given the high overall global disease burden in 
these patient groups, as well as heterogeneity of confounding factors that are present and methodological 
approaches for characterizing WMH burden used, it is difficult to identify a consensus on distinct, disease 
specific spatial patterns.  
 
Three studies identified distinctive spatial WMH patterns for different subtypes of SVD.36,55,65 While 
juxtracortical WMH were associated with probable CAA,55 deep (frontal) WMH were predominantly found 
in patients with risk factors for arteriosclerosis,65 and temporal WMH with CADASIL.36 Again, the 
methodological approaches differed across studies, hindering a direct comparison. Another three studies 
explored spatial patterns of WMH in patients with PD.38,44,64 While one of them  did not identify spatial 
patterns specific to patients with PD,44 Fang et al.38 found an increased WMH burden in the frontal lobe, 
and Grey et al.64 identified increased PWMH burden. The differences in these findings may be explained 
due to the variations in age and sex distribution of the study populations, as well as the different 
methodology used for characterizing WMH. Additional studies are needed for these and other common 
diseases, such as stroke, to fully explore the topography of the cerebrovascular disease burden. Future 
research may also consider elucidating differences, not only between patient cohorts compared to the 
general population, but between patient cohorts of various diseases to further highlight the applicability of 
spatially specific disease burden as biomarkers for early disease prognostication.  
 
Epidemiologic research over the past decades has highlighted the importance of vascular risk factors in 
WMH.3,19,75 Overall, studies have identified spatial heterogeneity of WMHs related to risk factors and 
clinical correlates. Localized WMH burden was found to be positively correlated with vascular risk factors, 
such as HTN, alcohol consumption, and history of smoking, as well as clinical correlates, such as sex (see 
Table 2). The studies described in this review also demonstrated that patients with HTN mainly presented 
with an increase in DWMH burden, whereas smoking and alcohol consumption were linked to an increase 
in PWMH. This supports the underlying assumption that deep and periventricular WMH are different 
etiologies that are individually affected by different vascular risk factors.  
 
While only a few studies specifically investigated sex differences, the research findings highlighted in this 
systematic review suggest that it plays an important role in the spatial distribution of WMH, with men 
showing higher WMH in periventricular and PCA regions and women with higher frontal burden. Sex 
differences in total WMH burden, as well as differences in women pre- and post-menopausal, have been 
shown previously in the literature.3,14,78–80 These findings, with the above described spatial differences in 
disease burden, however,  have not been fully explored, but should be accounted for in future studies. 
 
Our systematic review further identified different spatial patterns of WMHs associated with poor overall 
cognitive outcomes. The results, however, are inconsistent and warrant further, systematic studies. Only a 
few studies (n=3) investigated different cognitive domains, specifically looking into spatial patterns in 
relation to executive function and episodic memory. While there were no distinctive WMH patterns in 
studies of older participants who ranged from cognitively normal to demented, a study in middle-aged, 
cognitively normal participants found a distinct disease burden topography. Specifically, Brugulat-Serrat 
et al.37 showed that cognitively unimpaired, middle-aged adults with an increase in DWMH burden 
performed poorer in tasks relating to executive memory, whereas increased PWMH burden was related to 
poor episodic memory. In other studies in older adults, Tullberg et al.34 demonstrated that poor executive . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 16 function and episodic memory was related to increased frontal WMH, whereas Smith et al.41 found that 
these were associated with increased PWMH. While these findings are not exclusive of one another, 
however, the use of different definitions for spatial stratification hinders a direct comparison. Both studies 
suggest, however, that this spatial distinction might be an early marker for cognitive impairment in younger 
people, where the overall WMH burden in the brain is still relatively low. Nonetheless, further studies 
across the lifespan are needed to fully examine the described association. 
 
While research into spatial patterns of WMH is a rapidly developing field that first emerged around 20 
years ago with most publications in the last decade, a commonly agreed terminology does not yet exist, 
which hinders the identification of related literature. While our research terms included a large variety of 
descriptive words for WMH and topology, it is possible that studies were missed due to the large variety of 
nomenclature. Efforts reflecting other standardization approaches, such as STRIVE,1  may be warranted to 
significantly enhance the development of the field. In this systematic review, the varying definitions for 
spatial WMH burden did not allow to pool sufficient data for additional meta-analysis. Future research and 
meta-analyses would also significantly benefit from a consensus for standard terminology and of image 
analysis methodology for spatial patterns analyses of WMH. 
 
In this systematic review, we discussed the evidence in the selected literature for spatial WMH patterns. 
While the data of the included studies did not allow for a quantitative synthesis and risk assessment of bias 
of results, intrinsic biases of the discussed studies exist. Investigating spatially stratified disease burden 
necessitates large cohort sizes to account for the general heterogeneity of WMH presentation. Here, cohort 
sizes varied significantly, ranging from 32 to 882 patients/participants, with most studies having cohort 
sizes of N<200 (median cohort size 191.5). While smaller cohorts may be sufficient to provide first insights 
into spatial patterns of WMH, larger cohort sizes are likely necessary to fully uncover the driving factors 
in observed WMH topography, its associated risk factors and clinical correlates. Moreover, studies 
significantly varied in populations under investigation, with a total of 16 different diagnoses, in addition to 
“control” groups, with a range of 1 - 15 studies per diagnosis. Studies including MCI patients were most 
prevalent. As described above, more disease specific evidence is required to remove uncertainties in the 
results that are present due to intrinsic biases in the study cohorts. Additionally, most studies included in 
this systematic review were conducted in older populations (median cohort age: 69.9 years), specifically 
unraveling spatial patterns of WMH with respect to AD, MCI and normal cognitive function. In general, 
WMH is most prominent in aging populations, as age is a key determinant of overall WMH burden, 
enabling easier automated quantification, where small errors in segmentation accuracy often only relate to 
minute variations in relative burden. Only a few studies investigated spatial patterns of WMH in younger 
participants, however, these investigations focused on rather specific study populations, e.g. presenting with 
panic disorders or pilots. The results presented here may therefore not generalize well to the general 
population. Studies are warranted to disentangle spatial effects of WMH in younger people and populations 
with lower WMH burden from the general population, as well as overall population studies which include 
a 
wide 
age 
range. This systematic review provides an overview of the state of spatial patterns of WMHs across cohorts in 
adults. While in this review, we only extracted literature from a single database (PubMed), it provides a 
good starting point for the emerging studies. Here, we focused on adults, but WMH can emerge earlier in . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 17 life and is often linked with disease state. Overall this systematic review has met its expectations and 
achieved its objectives. 
 
Conflicts of Interest 
Authors report no conflicts of interest. 
 
Acknowledgements 
This project has received funding from the BONFOR program of the Medical Faculty of the Friedrich-
Wilhelms University Bonn (O-194.0001). VL is supported by the Marga and Walter Boll Foundation, 
Kerpen, Germany. 
 
Author contributions 
All authors had full access to all the data in the study and take responsibility for the integrity of the data 
and the accuracy of the data analysis. Conceptualization, JB, VL, MS; Methodology, JB, VL, MS; 
Investigation, JB, MS; Formal Analysis, JB; Writing – Original Draft, JB, VL, MS; Writing – Review & 
Editing, JB, VL, MS; Visualization, JB; Supervision, MS; . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 21, 2023. 
; 
https://doi.org/10.1101/2023.02.13.23285878
doi: 
medRxiv preprint Spatial patterns of WMH 18",1
"Detection of aberrantly spliced genes is an important step in RNA-seq-based rare disease
diagnostics. We recently developed FRASER, a denoising autoencoder-based method for aberrant
splicing detection that outperformed alternative approaches. However, as FRASER’s three splice
metrics are partially redundant and tend to be sensitive to sequencing depth, we introduce here a
more robust intron excision metric, the Intron Jaccard Index, that combines alternative donor,
alternative acceptor, and intron retention signal into a single value. Moreover, we optimized model
parameters and filter cutoffs using candidate rare splice-disrupting variants as independent evidence.
On 16,213 GTEx samples, our improved algorithm called typically 10 times fewer splicing outliers
while increasing the proportion of candidate rare splice-disrupting variants by 10 fold and substantially
decreasing the effect of sequencing depth on the number of reported outliers. Application on 303 rare
disease samples confirmed the reduction fold-change of the number of outlier calls for a slight loss of
sensitivity (only 2 out of 22 previously identified pathogenic splicing cases not recovered). Altogether,
these methodological improvements contribute to more effective RNA-seq-based rare diagnostics by
a drastic reduction of the amount of splicing outlier calls per sample at minimal loss of sensitivity. 1 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. INTRODUCTION The regulation of splicing is important to control isoform expression and cellular function (1–3).
Defects at the level of the pre-mRNA splicing process represent a major cause of human disease. It is
estimated that 15–50% of all variants leading to disease in humans alter splicing (4, 5). Different
methods that predict the impact of a variant in splicing from sequence alone have been developed
(6–12). However, even with increasing precision, their accuracy remains imperfect, especially for
diagnostics and for variants located far from the splice sites (9, 12). Even the ACMG guidelines for
variant pathogenicity require additional functional evidence such as RNA-seq for variants predicted to
disrupt splicing (13). Importantly, current prediction models do not inform on the consequence of a
potential splicing defect on the resulting transcript isoform (e.g. frameshift or exon truncation).
Identifying splicing aberrations on RNA sequencing (RNA-seq) data provides more direct evidence of
the presence of splicing defects and reveals the resulting transcript isoforms. This approach has been
successfully used to diagnose patients with rare genetic disorders in large cohorts (14–20). Following
this initial success, computational tools specialized on detecting aberrant splicing from RNA-seq data
have been developed including LeafcutterMD (21), SPOT (22), and FRASER (23).
FRASER (23) models three metrics that are computed from split reads and unsplit reads detected at
de novo identified splice sites. These three metrics are the percent-spliced-in to test the splice
acceptor site (𝜓5) and donor site (𝜓3), and splicing efficiency (𝜃). These metrics capture different types
of aberrant splicing, namely aberrant acceptor site usage, aberrant donor site usage and aberrant
splicing efficiency. FRASER uses a denoising-autoencoder approach to control for potentially
unknown sources of covariation between samples, and calculates beta-binomial P-values to identify
splicing outliers. Benchmarks on rare variant enrichment among reported splicing outliers showed that
FRASER outperformed LeafcutterMD and SPOT. Despite FRASER’s improvements, the number of outlier calls per sample often remains very large.
Notably, these splice-site-centric metrics can lead to reporting outliers that reflect local aberrations
that may only have minor effects on the abundance of the canonical splicing isoform. Figure 1A
illustrates such a case, where an exon elongation appearing in one sample is supported by 1,017
reads, while there are 32,018 supporting the annotated intron. Taking as reference the newly created
acceptor site, this exon elongation which is present in only 6 out of 582 samples, appears as an
outlier event. Consistently, FRASER detected this event significantly and with a differential 𝜓5 = 0.85.
However, this event does not strongly affect the canonical isoform distribution because the vast
majority of the reads still support the annotated intron. To address this issue, we here introduce a new intron-based metric which we named the Intron
Jaccard Index. It is defined as the proportion of reads supporting the splicing of an intron of interest
among all reads associated with either splice site of the intron (Figure 1B). The Intron Jaccard Index is
computed using both split and non-split reads, thus allowing to capture several types of aberrant
splicing, including exon skipping, exon truncation, exon elongation, exon creation and full intron
retention (Figure 1C). The Inton Jaccard Index is conceptually similar to the intron excision ratio
concept underpinning LeafCutter (24). However, the statistical model of LeafCutter models multiple
introns of a same locus jointly, which leads to modeling complications and does not allow for modeling
sample co-variation. Here we model the Intron Jaccard Index of individual introns separately, using
the same beta-binomial autoencoder approach as in FRASER. We furthermore perform a systematic
evaluation of model parameters including pseudocounts and filtering criteria. Collectively we refer to
this new method as FRASER 2.0. We next benchmark FRASER 2.0 against FRASER, LeafCutterMD,
and SPOT using the multi-tissue GTEx dataset. Finally, we apply FRASER 2.0 to independent rare
disease cohorts and validate it on previously reported pathogenic splice defects. 2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Figure 1. Intron Jaccard Index metric improves splicing outlier calling. (A) Sashimi plot of three
GTEx skin not-sun-exposed RNA-seq samples showing exons 4 and 5 of the KRT1 gene. A splicing
outlier was detected in the top sample using the 𝜓5 metric of FRASER (red). The position of the donor
site of the outlier intron is indicated with a blue dashed line. While this intron is not expressed in most
other samples (dark blue) and therefore detected with a high Δ𝜓5 value, its functional impact is
probably minor because the canonical intron remains largely dominant. (B) Schematic definition of the
Intron Jaccard Index metric. (C) Representation of different types of aberrant splicing events that can
be captured with the Intron Jaccard Index metric. The right column contains the formulae to compute
the Intron Jaccard Index metric of the canonical intron (black dotted line) from the split (s) and
non-split (u) reads of the involved introns in each scenario. (D) Recall of rare splice-disrupting
candidate variants (as defined by VEP, MMSplice, SpliceAI and Absplice) versus the rank of nominal
P-values from FRASER (light blue) and from an adaptation of FRASER using the Intron Jaccard Index
metric (dark blue) on the GTEx skin not-sun-exposed dataset (N=582). Different nominal P-value
cutoffs are indicated with shapes. MATERIAL AND METHODS Datasets The GTEx dataset consists of 17,350 RNA-seq samples from 54 tissues of 948 assumed healthy
individuals of the Genotype-Tissue Expression Project V8 (25). The GTEx data used for the analyses
described in this manuscript were obtained from dbGaP accession number phs000424.v8.p1.
Samples with a RIN number < 5.7 and tissues with less than 100 samples were discarded. This
resulted in a total of 16,146 samples and 48 tissues.
The Yépez et al. dataset consists of 303 individuals affected with a rare mitochondrial disorder
described in (20). The intron counts were downloaded from Zenodo (26, 27).
The Undiagnosed Disease Network (UDN) (28) dataset consists of individuals suffering from a rare
genetic disorder, as well as unaffected controls collected from different centers in the United States.
We downloaded it from dbGaP (study accession phs001232.v4.p2). It contains 821 RNA-seq samples 3 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint extracted from blood (N=370), fibroblasts (N=398), and other tissues (N=53) which were not further
considered. All RNA-seq samples are stranded except for 1 which was removed. Poly-A sequenced
samples with a high-quality exonic rate lower than 0.7 were removed. Samples with size factors, a
sequencing depth metric (29), larger than 3 or lesser than 0.25 were also discarded. The resulting
dataset consists of 252 poly(A) blood, 104 total RNA blood, and 391 fibroblast samples. Splicing outlier detection with FRASER The integrated workflow DROP v1.1.3 (30) was used to count split and non-split reads from the BAM
files (from the GTEx and UDN datasets) and to detect splicing outliers with FRASER v1.2.2 (23) on all
datasets. Default cutoffs (FDR < 0.1, |Δ𝜓| ≥0.3, minimal intron coverage ≥5 reads) and default intron
filtering settings (95% of samples with N ≥1 and at least one sample with an intron count ≥20) were
used. Splicing quantification using the Intron Jaccard Index metric We defined the Intron Jaccard Index (J) as the Jaccard index of the sets of donor-associated reads D
and acceptor-associated reads A. For a given sample i and intron j, the set of donor associated reads
Dij was defined as the set of all split-reads with the same donor site as intron j as well as all non-split
reads spanning the exon-intron boundary at that donor site of sample i. The set of
acceptor-associated reads Aij was analogously defined for the split and non-spit reads at the acceptor
site of intron j (Figure 1B).
The Intron Jaccard Index Jij for sample i and intron j was calculated as follows: ,
𝐽𝑖𝑗=  
|𝐷𝑖𝑗 ∩ 𝐴𝑖𝑗| |𝐷𝑖𝑗 ∪ 𝐴𝑖𝑗| =
𝑠𝑖𝑗 𝑑 ∈ 𝐿𝑗
∑𝑠𝑖𝑑 + 
𝑎 ∈ 𝑅𝑗
∑𝑠𝑖𝑎 +
𝑡 ∈ {𝑑𝑗,𝑎𝑗}
∑
𝑢𝑖𝑡 − 𝑠𝑖𝑗 where sij denotes the count of split-reads mapping to intron j in sample i, dj is the donor site of intron j,
aj is the acceptor site of intron j, Lj is the the set of introns using dj, Rj is the set of introns using aj, and
uit denotes the count of non-split reads spanning the exon-intron boundary at a splice site t. Denoising autoencoder As for the original FRASER, we modeled and controlled for sample covariation using an autoencoder
that takes as input a matrix X consisting of the logit-transformed splice metrics of each intron i and
sample j using the following formula: xi,j = logit(
) 𝑘𝑖,𝑗 + α 𝑛𝑖,𝑗 + 2·α where kij and nij are the numerator and the denominator of the Intron Jaccard Index of intron j in
sample i, and α corresponds to a pseudocount needed to avoid taking the logarithm of or dividing by
zero. Originally, in FRASER a pseudocount of 1 was used as a default. Here we assessed various
possible pseudocount values. Annotation of genes to introns To report results on the gene level, FRASER uses a user-provided gene annotation file to assign
introns to genes. Each intron is assigned to the gene(s) overlapping either the donor or the acceptor
site of the intron in a strand-specific manner. As such, an intron may contribute to the gene-level
P-value of several genes. Genes fully contained in an intron but not overlapping either splice site are
not assigned to the intron. 4 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Multiple testing correction and effect size cutoff As in FRASER, P-values are obtained for each intron by modeling the Intron Jaccard Index using
beta-binomial regression on the autoencoder latent space (23). We used the same strategy as in
FRASER to obtain gene-level P-values. Specifically, intron-level P-values are corrected using Holm’s
method per gene and sample, and the minimal corrected P-value is selected. Then, false discovery
rate (FDR) correction of those selected P-values is applied across genes per sample using the
Benjamini-Yekutieli method. This FDR correction step is by default done transcriptome-wide across all
expressed genes. Genes are considered to be expressed if they have at least one intron assigned to
them in the dataset. FRASER 2.0 introduces the additional option to restrict the FDR correction to
user-provided lists of genes. The provided genes can differ between samples. To define outlier status,
the FRASER 2.0 default cutoffs are FDR ≤0.1 and effect size 𝛥J ≥0.1. FRASER 2.0 additionally
introduces an option to flag results located in blacklist regions of the genome (33) as those results
may be less trustworthy. Rare variant recall benchmarks To perform rare variant recall analyses, four different sets of splice affecting variants were considered,
each defined by a different variant effect prediction tool. The first set contained all splice-site and
splice-region variants as defined by VEP (31), corresponding to 1–3 bases of an exon or 1–8 bases of
an intron. For the second set, we used SpliceAI (9) and considered variants with a SpliceAI score ≥
0.5. For the third set, we used MMSplice (7) and considered variants with an |Δ logit 𝛹| score ≥2. The
fourth set contained variants predicted to cause aberrant splicing by AbSplice, specifically those with
a maximum score across tissues except testis ≥0.05, which corresponds to the medium suggested
cutoff. Rare variants were defined by a gnomAD MAF < 0.1% and present in at most 2 individuals of
GTEx. In each GTEx tissue, genes were ranked based on the nominal gene-level P-value, and the recall of
rare splice affecting variants was calculated at each rank. To facilitate comparison across tissues and
ensure that the reported outliers are relevant, we used two different measures of performance for
each tissue: 1) the recall and precision of ranking the nominal P-values of FDR significant results, and
2) the recall at the rank corresponding to a mean of 20 outliers per sample for each tissue. Correlation with mapped reads The number of mapped reads was computed using the function idxstats from SAMtools (32). Then,
the spearman correlation coefficient and P-value between the mapped reads and its splicing outliers
per sample were calculated using the cor.test function in R. Tissue reproducibility analysis To assess the reproducibility of our splicing outlier calls, we called aberrant splicing events across all
48 GTEx tissues both with FRASER and FRASER 2.0, as well as SPOT and LeafcutterMD, using
default settings for all methods. This analysis was done on the set of individuals with at least 20 out of
the 48 tissues and genes with available P-values in at least 25 out of the 48 tissues. This led to
14,707 genes for FRASER 2.0, 16,104 genes for FRASER, 12,154 genes for SPOT and 14,001
genes for LeafcutterMD to be considered. For all methods, we applied three nominal P-value cutoffs
of 10-5, 10-7 and 10-9 and computed for each gene-sample combination that passes this cutoff in at
least one tissue in how many other tissues it could be reproduced with a nominal P-value < 10-3. 5 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Benchmark on previously reported pathogenic splice events For the Yépez et al. dataset (N=303), 26 cases were found to have a splice defect in the disease
causal gene with FRASER. Out of those 26 cases, 4 harbored large deletions which were excluded
from the benchmark, resulting in a set of 22 cases with validated pathogenic splice defects. RESULTS Introduction of the Intron Jaccard Index as a robust splice metric To be less sensitive to local splice site aberrations that do not have a strong effect on the canonical
splice isoform, we introduce the Intron Jaccard Index, a new intron-centric metric that integrates
alternative donor usage, alternative acceptor usage and intron retention signal (Figure 1B). We define
the Intron Jaccard Index Jij as the Jaccard Index of the set of donor-associated reads Dij and the set of
acceptor-associated reads Aij for a given sample i and intron j: ,
𝐽𝑖𝑗=  
|𝐷𝑖𝑗 ∩ 𝐴𝑖𝑗| |𝐷𝑖𝑗 ∪ 𝐴𝑖𝑗| where Dij contains, for sample i, both the split-reads with the same donor site as intron j and the
non-split reads spanning the exon-intron boundary at that donor site and analogously for Aij (Figure
1B, Materials and methods). The intersection of those two sets represents the split-reads mapping
exactly to intron j while the union captures all reads mapping to any intron with the same donor or
acceptor site as intron j in addition to non-split reads at those splice sites. The inclusion of both split and non-split reads in the metric allows retaining FRASER’s capability to
capture several types of aberrant splicing, including partial or full intron retention, within a single
metric (Figure 1C). As in FRASER, we model the Intron Jaccard Index with a beta-binomial denoising
autoencoder and calculate P-values with the beta-binomial distribution to assess the statistical
significance of each Intron Jaccard Index value. Outliers are then called based on the multiple testing
corrected P-values and effect size (ΔJ). We ran FRASER with the Intron Jaccard Index as the splice metric on several GTEx tissues.
Following the rationale that rare variants in the vicinity of splice sites are likely to disrupt splicing (33),
we evaluated our new metric on the recall of rare (MAF < 0.1%) splice-disrupting candidate variants
using a combination of variant annotation tools (Materials and methods) . On this benchmark,
adopting the Intron Jaccard Index metric increased the recall compared to the three metrics from
FRASER together and individually (Figure 1D, Supplementary Figure 1). Optimization of FRASER 2.0 parameters After having established the Intron Jaccard Index, we evaluated several FRASER parameters to
identify their optimal values with respect to the new metric. As FRASER’s autoencoder works with
values in the logit space, which is defined for values greater than 0 and less than 1, a pseudocount
needs to be added to both the numerator and denominator when calculating each metric on raw read
counts. So far we had set the pseudocount to 1. Here we investigated a range of possible values and
found that reducing the pseudocount improved the recall of rare splice-disrupting candidate variants
across 15 GTEx tissues, with the optimum being reached at 0.1 (Supplementary Figure 2A,B).
We further investigated the effect size cutoff. The highest recall of splice-disrupting candidate variants
was achieved for a difference between the observed and expected value, denoted ΔJ, of 0.2
consistently across different filtering settings (Supplementary Figure 3). However, the more
permissive cutoff ΔJ = 0.1 was similarly optimal. We therefore decided to adopt the Δ = 0.1 cutoff as
the default (Supplementary Figure 2C,D). 6 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Furthermore, we investigated the intron filtering criteria. FRASER uses three parameters, denoted k,
n, and q to filter out lowly expressed introns. The parameter k is the minimal value of the splice metric
numerator required in at least one sample. The parameter n is the minimal value of the splice metric
denominator at percentile q across samples. Among parameter values with optimal performance, we
opted for the most permissive setting (k=20, q=25% and n=10, Supplementary Figure 2E). This
parameter setting means that to be considered by the algorithm in a first place, an intron must be
supported by at least 20 split reads in at least one sample (Intron Jaccard Index numerator) and shall
have a total of more than 10 donor-site and acceptor-site related reads (Intron Jaccard Index
denominator, Figure 1B) in at least 25% of the samples. Adopting these cutoffs resulted in testing
287,943 introns and 16,548 coding and non-coding genes on average for each GTEx tissue
(Supplementary Figure 2F).
Finally, we investigated whether discarding outlier calls in introns with a poor goodness-of-fit would
improve the performance further. Generally, a poor fit can indicate a violation of the modeling
assumption, making the P-value estimates not trustworthy. FRASER models the data of each
individual intron using a beta-binomial regression on the latent space. We observed some instances
for which the modeling assumption was violated, in particular cases of splicing quantitative trait loci
(sQTL) in cis that were not captured by the latent space (Supplementary Figure 4C,D). To
systematically capture those cases, we considered the beta-binomial overdispersion parameter (𝜌) as
a measure of the goodness-of-fit. However, filtering out introns with a high overdispersion parameter
had minimal effects on the overall recall of rare variants. Therefore, we decided to not implement this
filtering (Supplementary Figure 4A,B). Overall, we call this new approach FRASER 2.0. All the parameters for which we explored the optimal
default values here can be changed by the user. Improved performance and robustness of FRASER 2.0 on GTEx We ran FRASER 2.0 on 48 GTEx (V8) tissues and benchmarked it against FRASER (23),
LeafcutterMD (21) and SPOT (22). Notable sample-sample correlations were present in raw Intron
Jaccard Index values analogous to what has been previously described in FRASER and the
autoencoder of FRASER 2.0 was able to correct for them (Supplementary Figure 5). Introns that were
detected as outliers in FRASER but not with FRASER 2.0 tended to have a high 𝛹5 or 𝛹3 value and a
low Intron Jaccard Index value, whereas the splice metric values of introns that were found as outliers
by both methods were similar (Supplementary Figure 6). FRASER 2.0 P-values were better calibrated
than FRASER’s P-values for each metric (Figure 2A, Supplementary Figure 7), confirming that the
Intron Jaccard Index metric is more robust.
We then evaluated each method on their ability to identify splicing outliers in genes predicted to be
affected by a rare splice-disrupting candidate variant. FRASER 2.0 consistently outperformed
FRASER, SPOT and LeafCutterMD and increased the recall of such genes throughout all ranks and
across tissues (10 fold increase in precision at FDR=0.1, Figure 2B and Supplementary Figure 8).
Importantly, FRASER 2.0 reported significantly less outliers per sample than FRASER on all tissues
(10.0 ± 2.6 times less, Figure 2C,D). While the overlap of outliers from FRASER and FRASER 2.0 is
only 8% across all tissues (Figure 2C), outliers identified by FRASER 2.0 only were more enriched for
candidate rare splice-disrupting variants than those identified by FRASER only (Supplementary Figure
9), highlighting that FRASER 2.0 allows removing outlier calls from FRASER that are not biologically
meaningful. 7 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint 8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Figure 2. FRASER 2.0 increases recall of rare splice-disrupting candidate variants on GTEx. (A)
Quantile-quantile plots of the P-values for the different splice metrics from FRASER (𝜓3, 𝜓5, 𝜃, shown
in shades of blue) and the Intron Jaccard Index metric from FRASER 2.0 (purple) on the GTEx skin
not sun exposed dataset. The red line depicts the diagonal and the gray ribbon around it the 95%
confidence interval. (B) Recall of rare splice-disrupting candidate variants as defined by the variant
annotation tools VEP, MMSplice, SpliceAI, and AbSplice (facets) versus the rank of nominal P-values
combined across GTEx tissues for FRASER (blue), FRASER 2.0 (purple), LeafcutterMD (yellow), and
SPOT (green). Nominal P-value cutoffs are indicated with shapes. (C) Venn diagram of the overlap of
splicing outliers at the gene-level found with FRASER (blue) and FRASER 2.0 (purple). (D) Boxplots
of the number of splicing outliers (gene-level) per sample (y-axis) called by FRASER (blue) and
FRASER 2.0 (purple) for each GTEx tissue (x-axis). All brain tissues have been combined for
readability. In addition, splicing outliers from FRASER, LeafcutterMD, and SPOT were more likely to arise with a
higher sequencing depth than ones detected with FRASER 2.0 (Figure 3), thus confirming the
robustness of the latter. FRASER 2.0 outliers were also more often reproducible across tissues than
competitor methods (Supplementary Figure 10).
Overall, FRASER 2.0 reports less, but more biologically-relevant splicing outliers than FRASER as
well as LeafcutterMD and SPOT. Figure 3. FRASER 2.0 is
less sensitive to
sequencing depth than
previous methods. (A)
Scatterplot of the number of
splicing outliers at the gene
level against the total
mapped reads per sample on
the GTEx skin
not-sun-exposed dataset for
LeafcutterMD, SPOT,
FRASER and FRASER 2.0
(facets). Spearman
correlation coefficients (rho)
are shown. All are significant
(Spearman test, P < 3 x 10-3)
(B) Boxplots of the Spearman
correlation coefficients
(y-axis) between the mapped
reads and the number of
splicing outliers at the gene
level called by LeafcutterMD,
SPOT, FRASER and
FRASER 2.0 (x-axis) for each
GTEx tissue (N=48). P-values
of Wilcoxon tests are shown
above brackets. 9 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Application of FRASER 2.0 to rare disease cohorts To evaluate FRASER 2.0 in a diagnostic setting, we applied it to 747 samples from the Undiagnosed
Disease Network (28) and to the cohort composed of RNA-seq samples from 303 individuals
suspected to be affected with a mitochondrial Mendelian disorder (20). As with GTEx, FRASER2
reported less splicing outliers than FRASER in all cohorts (3 to 9.5 times less outliers on median,
Figure 4A).
Frequently in diagnostics, researchers are not interested in testing all genes, but only those that could
cause the disease (e.g. OMIM or curated lists for each disease). In addition, interpretation of variants
revealed by panel, whole exome (WES) or whole genome sequencing (WGS) is often performed prior
to RNA sequencing, yielding a restricted list of candidate genes per sample. Such prior information
has the potential to reduce the multiple testing burden when analyzing splicing outlier calls from
RNA-seq. To implement this strategy, we extended the FDR correction step of FRASER 2.0 to allow
considering a subset of genes specific for each sample. Testing OMIM genes on both cohorts and
OMIM genes harboring at least one rare variant (gnomAD MAF < 0.1%, in cohort allele frequency <
1%, within the gene body except for the UTRs) called by WES in the Yépez et al. dataset consistently
led to less outliers per sample to be manually inspected (median of 2 outliers per sample, Figure 4A).
The latter approach resulted in a median of 5,427 introns and 149 OMIM genes to be tested per
sample, compared to 140,230 introns and 16,846 genes in the transcriptome-wide setting
(Supplementary Figure 11). In the Yépez et al. dataset, 22 cases with aberrant splicing on the disease causal gene were identified
by FRASER. FRASER 2.0 reported 20 out of those 22 diagnosed cases with the default cutoffs
(Figure 4B). The two missing cases were both exon truncations caused by synonymous variants and
showed a large deviation from canonical splicing (ΔJ = -0.55 and ΔJ = -0.39), which was reflected in
low nominal P-values (1.08 x 10-4 and 1.87 x 10-5), but due to the multiple-testing burden from testing
all expressed introns and genes, the resulting FDR was 1 in both cases. Testing only OMIM genes
with rare variants recovered these 2 cases at 10% FDR (FDR = 0.087 and 0.094). However, 5 other
cases were not detected with this approach because the gene affected by the splice defect was not in
the list of tested genes. In one case, the causal variant had a MAF of 0.0096 which is above the 0.1%
cutoff we adopted here, and the 4 other cases had splicing defects caused by intronic variants that
were not detected by WES (Figure 4B). Calling variants from WGS could help to overcome this issue
and further improve the usefulness of this feature. Finally, we investigated the sensitivity of FRASER 2.0 to sample size as the number of samples is
often limited in rare disease cohorts. We used the Yépez et al. dataset and the 22 known pathogenic
splicing events to estimate the required dataset size to reach significance for most of the clinically
relevant events. As expected, the percentage of recovered pathogenic events dropped with reduced
sample size (Figure 4C, Supplementary Figure 12). With already 50 samples, we recovered 78% of
cases (17 out of 22, on average). Altogether, these results show the general applicability of FRASER 2.0 to various RNA-seq cohorts
and that it reports considerably less outliers than FRASER. In addition, the new functionality to test a
preselected number of genes per sample can further help reduce the search space. FRASER 2.0 is
publicly available at github.com/gagneurlab/fraser and integrated into the workflow DROP (30). 10 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Figure 4. Application of FRASER 2.0 to rare disease cohorts. (A) Distribution of the splicing
outliers per sample at the gene level on the UDN (N=391, N=252, N=104 for Fibroblasts, Blood
poly(A) and Blood totalRNA) and the Yépez et al. dataset (N=303) for FRASER (blue) and FRASER
2.0 applied to three gene sets considered for FDR-correction: expressed genes (dark purple),
expressed OMIM genes (light purple), and expressed OMIM genes with a rare variant (violet red,
Methods). (B) Size (bars) of all non-empty intersections (linked dots) between four outlier sets from
the Yépez et al dataset: i) the 22 originally reported pathogenic events, ii) the transcriptome-wide
significant FRASER 2.0 calls, iii) the significant FRASER 2.0 calls when only considering OMIM genes
with a rare variant and iv) the transcriptome-wide significant FRASER calls. (C) Fraction of recovered
pathogenic splicing outliers from the Yépez et al. dataset (y-axis, total N=22) when subsampling to
different sample sizes (x-axis). Each sample size was randomly sampled 5 times. RV: rare variant. DISCUSSION Here we introduced FRASER 2.0, a method to detect aberrant splicing using a novel intron-centric
metric, the Intron Jaccard Index. In a single metric, the Intron Jaccard Index captures former metrics
of splicing efficiency as well as alternative donor and acceptor site choice. The use of a single metric
in FRASER 2.0 translates into easier interpretation of results, reduced runtime, computational
resources, and storage. Benchmarks on assumed-healthy samples of the multi-tissue dataset GTEx
showed that FRASER 2.0 decreases the number of reported splicing outliers by one order of
magnitude, recovers splicing outliers associated with candidate splice-disrupting rare variants more
accurately than competitor methods, and is more robust to variations in sequencing depth. Application
to two unrelated rare disease cohorts further confirmed the relative advantage of FRASER 2.0 over its
predecessors. 11 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint Motivated by applications with various collaborators, we have here introduced the option to filter
genes based on prior information when performing multiple-testing correction. In practice, it is often
the case that RNA-sequencing is performed after an inconclusive WES or WGS, which may have
yielded variants of unknown significance for which RNA-seq constitutes the needed functional assay.
The possibility to focus on candidate genes and variants in such a “DNA-first, RNA-second” mode of
operation can be valuable as we have shown on the mitochondrial rare disease dataset application. Initially, we introduced the Intron Jaccard Index to focus on more functionally relevant outliers than
with the original three splicing metrics of FRASER. However, our benchmarks are based on candidate
splice-disrupting variants without consideration for canonical splice isoform abundance. Perhaps,
these benchmarks, along with a stronger robustness to sequencing depth, show that the main
advantage of the Intron Jaccard Index is merely statistical. An explanation for this could be that the
Intron Jaccard Index may be more stable as its denominator accounts for a larger set of reads
compared to the three splicing metrics of FRASER. One limitation of FRASER 2.0 and other methods designed to detect outliers is that a sufficiently large
cohort is needed. In the rare disease field, attaining these minimal requirements can be especially
challenging. Integrating unaffected samples or samples from individuals suffering from other disorders
can help overcome this. In addition, count matrices are provided for a variety of tissues in DROP
which can be downloaded and integrated with the local samples. In any case, the aggregated
samples should have been probed from the tissue and sequenced using a similar protocol. Another
limitation of calling aberrant splicing in RNA-seq data is that the gene might not be sufficiently
expressed in the probed tissue, and the choice of tissue is usually limited to clinically-accessible ones
(e.g. blood or skin). FRASER2 output, however, could be used as an input on the recently-developed
AbSplice-RNA method (12) to predict aberrant splicing in multiple tissues. Another limitation of this study is that we used the GTEx dataset for tuning FRASER 2.0’s parameters
as well as comparison to competitor methods. However, even the version of FRASER 2.0 without
optimized parameters clearly outperformed previous methods (Figure 1D). Moreover, due to
insufficient available data, we have not assessed in other datasets whether the parameters fitted on
GTEx remain optimal on other datasets. As we share our analysis pipeline, users with a large enough
cohort could reconduct the parameter optimization on their own dataset. Long-read sequencing shall eventually lead to more direct measurements of isoform expression and
to more accurate quantifications of the expressed isoforms, especially at complex loci (34). Such
direct estimations of the abundance of functional splice isoforms on every locus hold the promise to
advantageously substitute local splicing metrics including the intronic Jaccard index. However,
because of the higher cost and limited sequencing depth of long-read sequencing, which is
particularly limiting for RNA-seq due to the high dynamic range of gene expression, short-read based
methods such as FRASER 2.0 will probably remain relevant for rare-disease diagnostics in the
coming years. In conclusion, FRASER 2.0 can be readily used in the context of rare disease diagnostics as it has the
same sensitivity of FRASER and reports less, but more relevant outliers thus facilitating manual
candidate gene inspection. We anticipate the option to include prior knowledge and allow the
restriction of the FDR correction to sets of candidate genes to be especially useful in diagnostic
strategies where DNA sequencing is routinely done as a first step and RNA-seq is used as a
follow-up. As splicing-based therapeutics are on the rise (3, 35), we hope FRASER 2.0 will be a useful
tool for the identification of the exact aberrant splicing event for these new targeted therapies. 12 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint AVAILABILITY No new data was generated for this study. For a description of the used datasets, refer to Materials
and Methods. FRASER 2.0 is an open-source R/Bioconductor package available in the GitHub
repository https://github.com/gagneurlab/fraser (version 1.99.0 and above). It is also integrated into
the workflow DROP (version 1.3.0 and above) available at https://github.com/gagneurlab/drop. The
code to reproduce the figures in this manuscript can be found at
https://github.com/gagneurlab/FRASER-2.0-analysis. SUPPLEMENTARY DATA Supplementary Data are available at NAR online. AUTHOR CONTRIBUTION Conceptualization: I.S., C.M., V.A.Y., J.G.; Methodology: I.S., C.M., V.A.Y., J.G.; Software: I.S.;
Validation: I.S., K.L., V.A.Y.; Formal analysis: I.S., K.L., V.A.Y.; Data curation: I.S., C.M., V.A.Y.; Writing
- original draft: I.S., V.A.Y., J.G.; Writing - review & editing: all authors; Visualization: I.S., C.M., V.A.Y.,
J.G.; Supervision: C.M., V.A.Y., J.G.; Project administration: V.A.Y., J.G.; Funding acquisition: J.G. ACKNOWLEDGMENT We would like to thank Nicholas H. Smith and Felix Brechtmann for valuable advice throughout the
project, as well as the different people that have tested and used the software.
The Genotype-Tissue Expression (GTEx) project was supported by the Common Fund of the Office of
the Director of the National Institutes of Health and by the National Cancer Institute, National Human
Genome Research Institute, National Heart, Lung, and Blood Institute, National Institute on Drug
Abuse, National Institute of Mental Health, and National Institute of Neurological Disorders and
Stroke. The GTEx data used for the analyses described in this manuscript were obtained from dbGaP
accession number phs000424.v8.p1. The Undiagnosed Diseases Network (UDN) is supported in part
by the Intramural Research Program of the National Human Genome Research Institute and by
grants U01HG007674, U01HG007703, U01HG007709, U01HG007672, U01HG007690,
U01HG007708, U01HG007530, U01HG007942, U01HG007943, U01TR002471, U54NS093793,
U54NS108251, U01HG010215, U01HG010233, U01HG010217, U01HG010230, U01HG010219, and
U01TR001395 from the National Institutes of Health Common Fund, through the Office of Strategic
Coordination/Office of the NIH Director. The content is solely the responsibility of the authors and
does not necessarily represent the official views of UDN investigators or the National Institutes of
Health. The datasets used for the analyses described in this manuscript were obtained from dbGaP at
http://www.ncbi.nlm.nih.gov/gap through dbGaP accession number phs001232. FUNDING This work was supported by the German Bundesministerium für Bildung und Forschung (BMBF)
through the ERA PerMed project PerMiM [01KU2016B to IS, VAY and JG]; by the Deutsche
Forschungsgemeinschaft (DFG, German Research Foundation) – via the projects “Identification of
host genetic variation predisposing to severe COVID-19 by genetics, transcriptomics and functional
analyses” [466168909 to VAY and JG], “Identification and Characterization of Long COVID-19
patients using whole-blood transcriptomics” [466168626 to KL and JG], and Nationale
Forschungsdateninfrastruktur (NFDI) 1/1 “GHGA - German Human Genome-Phenome Archive”
[441914366 to CM and JG]. Funding open access charge: BMBF [01KU2016B]. 13 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287997
doi: 
medRxiv preprint",1
"Background Earlier reviews documented the effects of a broad range of climate change outcomes on sleep but have not yet evaluated the effect of ambient temperature. This systematic review aims to identify and summarize the literature on ambient temperature and sleep outcomes in a warming world. Methods For this systematic review, we searched online databases (PubMed, Scopus, JSTOR, GreenFILE, GeoRef and PsycARTICLES) together with relevant journals for studies published before February 2023. We included articles reporting associations between objective indicators of ambient temperature and valid sleep outcomes measured in real-life environments. We included studies conducted among adults, adolescents, and children. A narrative synthesis of the literature was then performed. Findings The present systematic review shows that higher outdoor or indoor ambient temperatures, expressed either as daily mean or night-time temperature, are negatively associated with sleep quality and quantity worldwide. The negative effect of higher ambient temperatures on sleep is stronger in the warmest months of the year, among vulnerable populations and in the warmest areas of the world. This result appears consistent across several sleep indicators and measures. Interpretation Although this work identified several methodological limitations of the extant literature, a strong body of evidence from both this systematic review and previous experimental studies converge on the negative impact of elevated temperatures on sleep quality and quantity. In absence of solid evidence on fast adaptation to the effects of heat on sleep, rising temperatures induced by climate change pose a planetary threat to human sleep and therefore human health, performance and wellbeing. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Introduction There is accumulating evidence that climate change is increasing health risks and notably heat-related illnesses and mortality.1–5 Beyond mortality, hotter ambient temperatures and heat extremes are associated with increased injuries,6,7 hospitalizations,8 mental health issues,9 health-care costs,10 as well as worsened cognitive performance,11 sentiment,4 labor productivity,12 and activity days.13 One proposed pathway of the association between ambient temperature and health outcomes is disrupted sleep.14–16 Shorter sleep duration, poor sleep quality and sleep disorders (e.g., insomnia, sleep apnea) are prospectively associated with the development of cardiovascular17 and metabolic diseases,18 cancer risks,19 mental health disorders,20 and accidents.21 Although the environmental causes of sleep disruption are multi- factorial (e.g., light and noise pollution),22,23 it is likely that rising ambient temperatures due to ongoing climate change will impair sleep in the hottest seasons of the year at a global scale, barring further adaptation.14,24 Exposure to hot and cold ambient thermal conditions demand the human body to mount a thermoregulatory response to maintain a core body temperature rhythm within the normal range required to support physiological functioning and sound sleep.25–27 Extreme heat can alter human core body temperature outside its normal range when air temperatures exceed that of fully vasodilated skin (35°C), with elevated heat-health risks apparent well-below this threshold.3,28–30 Further, sleep onset is closely coupled with night-time core body temperature decline.25 In hot sleeping environments, heat production can exceed heat loss beyond tolerable levels, increasing core body temperature and disturbing the natural sleep–wake cycle with increased wakefulness.26 In daily life, ambient heat can impact core body temperature (and thus sleep) through at least two plausibly interacting pathways: (i) direct exposure during the day imposing thermal strain, cardiovascular strain and/or dehydration which may carry over into the nocturnal resting period,3 and (ii) exposure at night via a combination of nighttime ambient weather conditions and environmental heat transfer (i.e., the energy accumulated in the built environment, conducted and re-emitted in the bedroom at night) reducing the thermal gradient between the body and ambient environment.26,31,32 The largest investigation of the effect of ambient temperature on sleep thus far – a study based on billions of repeated sleep measurements from sleep-tracking wristbands collected in 68 countries over two years – found that increased nighttime ambient temperature shortens sleep duration, primarily through delayed sleep onset, with stronger negative effects during summer . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint months, in lower-income countries, in warmer climate regions, among older adults, females and after controlling for individual and spatiotemporal confounders.24 These results confirmed those from two previous large-scale national analyses of self-reported sleep outcomes from the United States,33,34 the one including the most representative sample showing that a +1°C increase in monthly nighttime ambient temperatures produces an increase of approximately three nights of subjective insufficient sleep per 100 individuals per month.33 Minor et al. (2022) and Obradovich et al. (2017) also included climate change impact projections, and both estimated that rising ambient temperatures may negatively impact human sleep through the year 2100 under both moderate and high greenhouse gas concentration scenarios, with impacts scaling with the level of emissions barring further adaptation.24,33 Critically, climate change and other anthropogenic environmental changes related to increased heat exposure, such as urban heat island, are altering outdoor ambient temperatures where populations reside. Although sound and sufficient slumber underpins human functioning, the current prevalence of insufficient or poor sleep is already elevated in high-income countries (e.g., above 50% in the US, 31% in Europe and 23% in Japan).35 Although insufficient sleep prevalence estimates are relatively lower for low (7%)- and middle (8.2%)-income countries,36 a recent study found that habitants from those countries disproportionally suffered greater sleep loss due to heat,24 suggesting heightened vulnerability to elevated temperatures (i.e., reduced or missing access to personal or collective cooling strategies).37 In parallel, the 1.5°C global temperature threshold above the pre-industrial climate is expected to be exceeded by 2040 under most scenarios of the Intergovernmental Panel for Climate Change, including the increasingly plausible “Shared Socioeconomic Pathway” SSP2-4.5.38 Although there is some evidence of adaptation to increasing temperatures in high-income countries for heat-attributed mortality,39 temperature projections for the coming decades remain concerning for both current and future generations.40–44 In this context, having a precise and comprehensive understanding of the influence of ambient temperature and extreme heat on sleep is crucial. Earlier reviews documented the effects of a broad range of climate change outcomes on sleep (see14 which included six studies about the specific role of ambient temperature and heat), the influence of the bed micro-environment on sleep physiology,26,45,46 as well as the specific role of humidity in sleep regulation measured mostly in laboratory settings.47 However, no previous systematic review has described the state of the literature on the effect of ambient . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint temperature on sleep in humans under real-life conditions (i.e., observational studies conducted in real-life environments), in contrast with laboratory studies that experimentally manipulate both behavior and temperature in the micro-environment.48–50  This systematic review aims to identify and summarize the literature on ambient temperature, notably heat, and sleep outcomes in a warming world. Specifically, we aim to synthesize the available evidence and research gaps on this topic to inform researchers seeking to explore new facets of the temperature-sleep association, as well as decision makers and interventionists trying to promote climate adaptation. Methods Methods for collecting and summarizing data met the standards of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.51 The study protocol was registered in PROSPERO (CRD42021284139). Inclusion and Exclusion Criteria Studies were included if they: (i) reported associations between objective indicators of ambient temperature and valid sleep outcomes measured in real-life, environments; (ii) included adults, adolescents, or children; and (iii) were peer-reviewed. Eligible measures of sleep included: self-reported (subjective) sleep questionnaires; accelerometer-based actigraphy and commercial-grade activity monitors (e.g., fitness bands and sleep-tracking wearable devices); sleep sensors and polysomnography. This selection criteria thus excluded sleep-adjacent articles such as those interested in the association between ambient temperature and the prescription of hypnotics, which are only indirectly related to sleep issues.52 Eligible measures for temperature were narrowed to objective records via weather stations, climate reanalysis data or indoor temperature sensors; this criterion excluded articles interested in the associations between sleep outcomes and seasons without measuring ambient temperature,53 subjective perceptions of temperature54 or using climate classifications (i.e., Koppen’s weather climate classification).55 We excluded articles with valid measures of both sleep and temperature when the association between the two outcomes was insufficiently reported (e.g., association plotted with insufficient details to be numerically interpreted and reported),56 or simply not tested (i.e., some articles include measures of temperature and sleep but focus on other outcomes).57,58 Because we focused on ambient temperature and sleep outcomes measured in real-life contexts, experimental studies manipulating in-laboratory temperatures were also excluded.48–50 Beyond heat manipulation, those experimental studies often include . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint invasive skin and rectal temperature measures as well as behavioral constraints and thus can neither be considered ecologically valid in the context of daily life nor the present review. Data Sources and Searches Studies were identified by searching PubMed, Scopus, JSTOR, GreenFILE, GeoRef and PsycARTICLES between March and April 2022, and then updated in February 2023. Search strategies and algorithms are available in the supplemental material. Relevant reviews and articles cited in the introduction were also scanned. Specific journals were also inspected, in the sleep literature (e.g., Sleep; Sleep Medicine, Journal of Sleep Research) and the environmental health domain (e.g., Environment International, Environmental Research Letters, Environmental Health Perspectives, The Lancet Planetary Health). After duplicates were removed, titles and abstracts of all studies identified were examined independently by three authors (GC, KM, PB) to determine those meeting the selection criteria. Data Extraction and Synthesis An a priori data extraction form was developed and tested with 5 articles. Data were coded from each paper by three coders (KM, EC, PB) and double checked by a single coder (GC). The following information was extracted: first author’s name, sample region and period, study design, main research question, estimation strategy, temperature and sleep assessment methods and outcomes, the inclusion of relevant control variables, main results, conclusion, and relevant additional information. A narrative synthesis of the literature was then performed. We did not perform meta-analyses given the diversity of study designs, temperature and sleep outcomes and the statistical strategies used. A preliminary synthesis was conducted by the first authors (GC, KM) and then discussed and revised by all authors. Quality rating and risk of bias We used the items from the Quality Assessment Tool for Observational Cohort and Cross- Sectional Studies59 as a basis to develop a custom list of 14 quality criteria relevant to the question of ambient temperature and sleep. For each criterion, we indicated whether or not the study met the requirement (i.e., binary outcome, yes/no). Criteria are displayed in Table 1 below. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Table 1. Quality criteria Item 1 
For exposures that can vary in amount or level, did the study examine different 
levels of the exposure as related to the outcome and allow for a non-linear or 
semi-parametric response? Item 2 
Was the association between temperature and sleep assessed more than once 
over time at the within-participant level? Item 3 
Was within-participant variance accounted for in the analyses? Item 4 
Were sleep and temperature outcomes measured over the whole year? Item 5 
Were seasons and/or day length measured and adjusted statistically for their 
impact on the relationship between exposure(s) and outcome(s)? Item 6 
Were temporal variables, such as the day of the study, day of the year or day of 
the week, measured and adjusted statistically for their impact on the 
relationship between exposure(s) and outcome(s)? Item 7 
Was humidity measured and adjusted statistically for its impact on the 
relationship between exposure(s) and outcome(s)? Item 8 
Was wind speed measured and adjusted statistically for its impact on the 
relationship between exposure(s) and outcome(s)? Item 9 
Was cloud cover measured and adjusted statistically for its impact on the 
relationship between exposure(s) and outcome(s)? Item 10 
Was precipitation measured and adjusted statistically for its impact on the 
relationship between exposure(s) and outcome(s)? Item 11 
Were indoor and outdoor temperatures measured and analyzed together? Item 12 
Was the association between the exposure and the outcome tested at different 
lags? Item 13 
Was the utilization of air conditioning, fans and/or other indoor personal 
cooling technologies measured and analyzed? Item 14 
Was sleep measured with both subjective and device-derived indicators? Role of the funding sources The funders of the study had no role in the study design, data collection, data analysis, data interpretation, or writing of the report. Results Descriptive findings As depicted in the study flowchart (Figure 1), a first iteration resulted in a total of 1735 independent records. After screening title and abstract, 58 articles were inspected in closer detail and 27 articles were included in the present review. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Figure 1. Study flowchart Included articles were published in journals from diverse academic disciplines, including public health, sleep, physiology, engineering, economics, environmental science and medicine. The oldest article retrieved was published in 1992 and the most recent one 30 years later, in 2022. Figure 2 illustrates the countries in which the studies were performed; the US is the most represented country (42%). In terms of participants’ characteristics, studies included samples of various ages from ~13 years old to nearly 80 years old. As for identified sex, most studies reported at least ~30% female sample composition, while females were not included in two studies.60,61 All studies were correlational but six articles used intensive repeated measurements and appropriate statistical methods to estimate covariation between changes in temperatures and sleep outcomes within-participants, and thus are labelled as quasi- experimental studies here24,60,62–65. 1716 records 
identiﬁed from 
databases 1148 Scopus
355 PubMed
213 Others 19 records identiﬁed 
from speciﬁc journals 1597 screened 138 duplicates 
removed 58 assessed for eligibility 1539 excluded for 
irrelevancy 27 studies included 31 studies excluded 16 invalid sleep outcome
6 invalid temperature outcome
4 studies with experimental manipulation
3 association not tested
1 results can’t be reported 
1 review paper . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Figure 2. World map of included studies Note.  Multi-center studies are represented here at the national and local (city) scale. Table 2 and Table 3 provide, respectively, a brief and detailed description (constrained by the scope of the current review) of each individual study included. Outdoor ambient temperature was measured via weather stations in 17 articles and indoor temperature was measured using local sensors installed in the home environment (e.g., HOBO temperature data logger, Thermochrons iButtons logger) in 11 articles. One study measured and reported results for both indoor (via local sensor) and outdoor (via weather station) temperature outcomes (see Table 2).66 Sleep measures showed a greater diversity of assessment methods with ten articles using self-reported questionnaires or diaries,33,34,64–71 eight studies using commercial activity monitors,24,62,63,71–75 five studies using polysomnography61,69,76–78, four studies using research- grade accelerometers64,79–81 and three studies using specific sleep sensors60,82,83. Beyond assessment methods, 15 articles used daily (24-hour) aggregated measures of temperature, the remaining 12 articles focused on average nighttime temperature (see Table 3, column “sample period”). For sleep, outcomes ranged from subjective sleep duration34, perception of insufficient sleep33 or sleep quality70 to accelerometer-derived signals providing information about sleep efficiency or wakefulness after sleep onset64 as well as sleep-related outcomes measured via polysomnography such as sleep stages78 and sleep disorders (e.g., episodes of sleep apnea).76 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Table 2. Short summary of included studies Study 
Country 
Temperature assessment Sleep assessment Sleep outcome 
Impact of 
increased temperature An, 2018 
N = 12,000 China 
Weather 
station Questionnaire 
Sleep duration 
Positive Cassol, 2012 
N = 7,523 Brazil 
Weather 
station PSG 
AHI 
Positive Cedeño L., 
2018 
N = 44 US 
Local sensor 
Activity monitor 
Sleep duration 
Negative Cepeda, 2018 
N = 1,166 The 
Netherlands Weather 
station Research-grade 
accelerometer Sleep duration 
Negative Hashizaki, 
2018 
N = 1,856 Japan 
Weather 
station Other sensor 
Sleep timing; WASO; SE 
Negative Lappharat, 
2018 
N = 68 Thailand 
Local sensor 
Questionnaire  
+ PSG Subjective sleep; Sleep latency; 
Sleep duration; 
AHI Negative Li, 2020 
N = 98 US 
Weather 
station Research-grade 
accelerometer 
+ Questionnaire WASO; SE; Sleep duration; 
Subjective sleep; Sleep latency Negative Liu, 2022 
N = 5,204 Taiwan 
Weather 
station PSG 
WASO; SE; AHI; sleep stages 
Negative Mattingly, 
2021 
N = 216 US 
Weather 
station Activity monitor 
Sleep duration and timing 
Not explicit Milando, 2022 
N = 22 US 
Local sensor 
Activity monitor 
Sleep duration 
Not significant Minor, 2022 
N = 47,628 68 countries  
Weather 
station Activity monitor 
Sleep duration and timing 
Negative Montmayeur, 
1992 
N = 6 Niger 
Weather 
station PSG 
Sleep duration; SE; Sleep stages 
and awakenings Not explicit Mullins, 2019 
N = 4,120,514 US 
Weather 
station Questionnaire 
Subjective sleep; Sleep duration 
Negative Obradovich, 
2017 
N = 765,000 US 
Weather 
station Questionnaire 
Subjective sleep 
Negative Ohnaka, 1995 
N = 40 Japan 
Local sensor 
Other sensor 
Body movements 
Negative Okamoto, 
2010 
N = 19 Japan 
Local sensor 
Research-grade 
accelerometer Sleep timing; Sleep duration; 
WASO; SE Negative Pandey, 2005 
N = 43 US 
Weather 
station Questionnaire 
Sleep latency; Awakenings; 
WASO; sleep duration Negative Quante, 2017 
N = 669 US 
Weather 
station Research-grade 
accelerometer Sleep duration and timing; 
WASO; SE Negative . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Quinn, 2016 
N = 40 US 
Weather 
station  
+ Local sensor Questionnaire 
Subjective sleep 
Negative van Loenhout, 
2016 
N = 113 The 
Netherlands Local sensor 
Questionnaire 
Subjective sleep 
Negative Wang, 2022 
N = 40,470 China 
Weather 
station Questionnaire 
Subjective sleep 
Negative Weinreich, 
2015 
N = 1,773 Germany 
Weather 
station Other sensor 
AHI 
Negative Williams, 
2019 
N = 51 US 
Local sensor 
Activity monitor 
Body movements 
Negative Xiong, 2020 
N = 48 Australia 
Local sensor 
Activity monitor 
SE; Sleep stages 
Negative Xu, 2021 
N = 41 China 
Local sensor 
Activity monitor 
SWS 
Negative Yan, 2022 
N = 40 China 
Local sensor 
Activity 
monitor; 
Questionnaire Sleep duration; REM; WASO; 
SE; Light sleep; Deep sleep; 
Subjective sleep Negative Zanobetti, 
2009 
N = 3,030 US 
Weather 
station PSG 
RDI 
Negative Abbreviations. PSG: polysomnography; AHI: apnea-hypopnea index; WASO: wake after sleep onset; SE: sleep efficiency; TST: total sleep time; NREM: non-rapid eye movement; RDI: respiratory disturbance index. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Table 3. Detailed summary of the studies included in the systematic review First author, 
year Sample region 
[Latitude]  
(Spatial unit) Sample period 
(Time unit) Study design, 
(Sample 
characteristics) Main research 
question  
(Design) Main 
estimation 
strategy Temperature 
assessment method(s), 
Outcome(s) 
(Measurement range) Sleep assessment 
method(s),  
Outcome(s)  
(Measurement range) Time-varying, spatial 
and stable control 
variables Main results  
(Temperature vs. sleep 
outcomes) Additional 
information An, 2018 
Beijing, China  
[39°N] 
(City) All seasons, 2012-
2015 (repeated 
sleep assessments 
with weekly 
average 
temperature 
measured the week 
preceding sleep 
assessment) Prospective cohort 
study with 2 to 4 
measurement points 
per participant  
(N = 12, 000 
students; 33% 
Female; Mean age 
= 18 years) Association 
between air 
pollution and health 
behaviors 
(Correlational) Linear fixed-
effect 
regression Weather station; last 7 
days average of daytime 
temperature preceding 
the sleep assessment 
(range = 12-26°C) Self-reported questionnaire 
(custom single item); daily 
average hours of 
night/daytime (including 
naps) sleep duration in the 
last week (range = 6.3-8.4 
hours) Control variables included 
individual fixed effects, 
average ambient PM2.5 
concentration, average 
daytime temperature, 
average wind speed and 
percentage of rainy days 
during the last 7 days 
before the sleep 
assessment, and sex Daytime temperature 
positively associated with 
sleep duration in last 
week; β = .14, 95% CI 
[.14, .14] 
 
Overall conclusion: Study 
focuses on pollution, no 
explicit conclusion about 
the effect of temperature 
variability on sleep Air pollution, wind 
speed and % of rainy 
day positively and 
significantly 
associated with the 
sleep outcome. Did 
not account for 
plausibly 
confounding seasonal 
factors or social 
temporal factors. Cassol, 2012 
Porto Alegre, Brazil 
[30°S] 
(City) All seasons, 2000-
2009 (single sleep 
assessment with 
daily average 
temperature lagged 
up to 4 days 
preceding the sleep 
assessment) Cross-sectional 
study (N = 7, 523 
patients referred to 
sleep clinic to 
investigate 
suspected sleep 
disorders; 35% 
Female; Mean age 
= 46 years) Association 
between seasons 
and obstructive 
sleep apnea  
(Correlational) Bivariate 
nonparametric 
correlation Weather station; daily 
temperature 
(measurement range not 
reported, annual range 
for Porto Alegre: 10-
31°C) PSG; AHI expressed as 
number of events per hour 
(with higher score 
representing higher severity; 
range = 15-18 events per 
hours, indicating moderate 
sleep apnea on average) No control variables 
Ambient temperature 
negatively and 
significantly correlated 
with AHI (i.e., higher 
temperatures associated 
with less severe apnea 
events); r = -.25, P = .005 
 
Overall conclusion: 
Higher temperatures 
associated with better 
sleep outcomes (sleep 
apnea) Atmospheric 
pressure, relative air 
humidity and carbon 
monoxide levels 
positively and 
significantly 
associated with AHI. 
Cross-sectional 
design susceptible to 
confounding by 
omitted variables Cedeño 
Laurent,  
2018 Boston, United 
States  
[42°N]  
(Bedroom) July 2016 (repeated 
sleep assessments 
over a week with 
ambient indoor 
temperature 
measured the night 
of the sleep 
assessment) Longitudinal study 
with 12 time points 
following one 
group of students 
with residential AC 
access and one 
without (N = 44 
students; 50% 
Female; Mean age 
= 20 years) Effect of heat 
waves on cognitive 
functions  
(Quasi-
experimental) Mediation 
analyses with 
sleep being the 
mediator, 
cognitive 
variables the 
outcome and 
temperature the 
exposure Sensor; average 
nighttime temperature 
(participants’ bedroom 
average temperature day 
and night = 26 ±2.5°C in 
a group of participants 
without access to air 
conditioning and 21.4 ± 
1.9°C in a group with air 
conditioning;  ±3 SD 
study temperature range: 
16 to 33°C) Commercial activity 
monitor (accelerometer-
based, wrist-worn); Sleep 
duration (measurement 
range not reported) Control variables include 
hydration, caffeine intake, 
presence of air 
conditioning and the time 
from waking up to taking 
the cognitive test An increase in 1°C in 
overnight indoor 
temperature resulted in a 
2.7-minute decrease in 
sleep duration (95% CI 
−2.77 to −2.71) 
 
Overall conclusion: sleep 
may be an intermediate 
variable in the causal 
mechanism between 
indoor temperature 
exposures and cognitive 
function Low powered study; 
The mediation 
analysis was 
significant for one of 
five cognitive tests 
showing that 
disrupted sleep 
explained a part of 
the variance in the 
negative association 
between temperature 
and reaction time Cepeda, 2018 
Rotterdam, The 
Netherlands 
[52°N] 
(City) All seasons, 2011-
2016 (one to two 
weeks of daily 
sleep assessments 
with daily average 
temperature 
measured the day 
of the sleep 
assessment) Prospective cohort 
study with 1 to 2 
measurement weeks 
per participant (N = 
1166 among which: 
394 middle-aged 
adults [49% 
Female; Mean age 
= 59 years]; 449 
younger-elderly Seasonality of 
physical activity, 
sedentary behavior, 
and sleep in a 
middle- aged and 
elderly population 
(Correlational) Linear mixed-
effects 
regression Weather station; daily 
average temperature 
(range = -9.8-27°C) Research-grade 
accelerometer (wrist-worn); 
Sleep duration (Median 
middle-aged group = 370 
minutes per day; Median 
young-elderly = 374 
min/day; Median old-elderly 
= 381 min/day) Control variables included 
individual and study wave 
random effects, seasons 
(also a study outcome) and 
day of the week (weekday 
vs. weekend), sex, age, 
BMI, comorbidities, 
smoking behavior and 
alcohol intake, housing 
status, occupation and a Seasonal effect significant 
among middle-aged 
participants with longer 
night duration in winter 
(seasonal effect not 
significant among the two 
other age groups); Among 
middle-aged adults the 
seasonality of sleep 
duration was mainly Other weather 
variables were not 
significantly 
associated with sleep 
duration (wind speed, 
sunlight, relative 
humidity, 
precipitation and 
visibility) adults [43% 
Female; Mean age 
= 71 years]; 323 
older-elderly adults 
[42% Female; 
Mean age = 79 
years]) disability index. Although 
other weather variables 
were assessed, they were 
not included as controls 
with temperature in the 
same regression. driven by ambient 
temperature with higher 
ambient temperature 
associated with lower 
sleep duration [-17 
minutes (95% CI -32.3 to -
1.0) for temperatures 
between 14.1 to 27 °C 
compared to the reference 
range (-10 to 6.6°C)] 
 
Overall conclusion: 
seasonality of sleep 
duration mainly explained 
by ambient temperature Hashizaki, 
2018 Tokyo, Japan  
[40°N] 
(City) All seasons, 2013-
2015 (repeated 
sleep assessments 
with hourly 
ambient 
temperature 
averaged between 
23:00 and 8:00 on 
the night of the 
sleep assessment) Intensive 
longitudinal study  
(k = 691, 161 
observations; N = 
1, 856 adults from 
the general 
population; 9% 
Female; Median 
age = 50 years) Association 
between sleep 
timing and quality 
with seasons and 
environmental 
changes  
(Quasi-
experimental) Linear mixed-
effects 
regression Weather station; 
nighttime temperature 
averaged between 23:00 
and 8:00 (measurement 
range not reported but 
appeared to vary between 
0 and 30°C as plotted in 
the article) Sleep sensor (located near 
participants’ pillow and 
detected body and 
respiratory movements); 
Sleep onset time; Sleep 
offset time; Mid-sleep time; 
WASO; SE (measurement 
range not reported) Control variables included 
random effects for each 
user, seasonal controls, an 
indicator for whether the 
day was a weekday or 
weekend, age and sex Statistical coefficients not 
reported, models are only 
plotted and discussed;  
 
- Sleep onset: not 
correlated with 
temperatures;  
- Sleep offset: is earlier in 
the range 5 to 25°C; 
- Mid-sleep time: is earlier 
in the range 5 to 25°C; 
WASO: U-shape 
correlation with 
temperature with WASO 
being at the minimum 
around 15°C and higher 
for cold and notably high 
temperatures; 
- SE: inverse U-shape 
correlation with SE being 
at the maximum around 
15° and decreased 
gradually with colder 
temperatures and 
decreased very rapidly 
with higher temperatures 
 
Overall conclusion: 
Higher temperatures 
associated with poorer 
sleep quality WASO and SE 
showed significant 
seasonal changes 
with the lowest sleep 
quality occurring 
around mid-summer 
during the hottest 
days of the year Lappharat, 
2018 Bangkok, Thailand  
[14°N]  
(Bedroom) May to August, 
2016 (single sleep 
assessment with 
bedroom 
temperature 
recorded during 3 
consecutive nights 
following the sleep 
assessment) Cross-sectional 
study 
(N = 68 adults; 
27% Female; 
Median age = 42 
years) Association 
between bedroom 
environmental 
temperature and, 
respectively, 
obstructive sleep 
apnea and 
subjective sleep 
quality 
(Correlational) Logistic 
regression 
(subjective 
sleep quality), 
and linear 
regression 
(PSG) Sensor; average 
nighttime temperature 
sampled at the 5-min 
level (participants’ 
bedroom average 
temperature =  
26 ±1.89°C; study 
temperature range 
(±3SD) 20 to 32°C) Self-reported questionnaire 
(PSQI); Sleep latency 
(Median = 20 min); Sleep 
duration (Median = 6 
hours); SE (Mean = 89%); 
PSQI score (Median = 7 
PSQI index) 
 
PSG; AHI (Mean = 46 ±27 
events per hour); respiratory 
disturbance index-RDI (48 
±26 events per hour) Control variables included 
age, sex, BMI, alcohol 
consumption, smoking, 
secondhand smoke 
 
Note: AHI also included as 
control for subjective sleep 
analyses Bedroom temperature 
during sleep positively and 
significantly associated 
with poorer subjective 
sleep quality (OR = 1.46, 
95% CI [1.01, 2.10]), and 
not significantly 
associated with short sleep 
duration, poor sleep 
efficiency and long sleep 
latency; 
 
In patients with 
Obstructive Sleep Apnea 
(AHI ≥ 5 events/h), 
bedroom temperature not 
significantly associated 
with elevated AHI (β = 
0.69 [−2.42, 3.81]) nor 
elevated RDI (β = 0.92 
[−2.08, 3.93]) 
 
Overall conclusion: 
Higher temperatures 
associated with poorer 
subjective sleep quality All polysomnography 
sleep outcomes were 
measured in a sleep 
laboratory, and 
preceded home 
environmental 
exposure 
measurements; Air 
pollution (PM 10) 
was significantly 
associated with AHI 
and RDI in patients 
with obstructive sleep 
apnea. Cross-
sectional study 
susceptible to omitted 
variable bias. Li, 2020 
Boston, United 
States  
[42°N]  
(City) March 2016 to 
October 2017 
(repeated sleep 
assessments with 
daily temperature 
averaged on the day 
preceding the sleep 
assessment) Intensive 
longitudinal study 
(k = 4406 
observations; N = 
98 healthy adults 
with episodic 
migraine; 88% 
Female, Mean age 
= 35 years) Associations of 
daily weather and 
ambient air 
pollution with sleep 
duration and 
fragmentation 
(Quasi-
experimental) Linear fixed-
effects 
regression Weather station; daily 
average ambient 
temperatures (Mean = 14 
±8.9°C; Study 
temperature range 
derived from plotted 
temperature series (-10 to 
31°C)) Research-grade 
accelerometer (wrist-worn); 
WASO (Mean = 45 ±17 
minutes); SE (Mean = 
90±3.3 %); Sleep duration 
(Mean = 7±1.2 hours);  
 
Self-reported questionnaire 
(PSQI); Sleep latency (Mean 
= 28 ±30 min); PSQI Index 
(Mean = 5±3) Control variables included 
individual fixed effects, 
day of the week and day of 
the year Significant associations 
between temperatures and 
WASO (5.6°C higher 
daily average temperature 
associated with 0.88 
minutes longer WASO 
95% CI [0.06, 1.70]), as 
well as sleep efficiency 
(5.6°C higher daily 
average temperature 
associated with 0.14%, 
95% CI [0.01, 0.30] lower 
sleep efficiency on that 
night);  
 
Non-significant 
associations between 
temperatures and sleep 
duration, self-reported 
sleep latency and the PSQI 
index 
 
Overall conclusion: 
Higher temperatures 
modestly associated with 
poorer sleep quality Association between 
temperature and 
WASO was no longer 
significant when 
using a lagged two-
day moving window; 
did not consider 
potential lagged 
associations 
exceeding 2 days; did 
not account for 
spatially correlated 
errors. Liu, 2022 
Taipei, Taiwan  
[25°N]  
(City) All seasons, 
January 2015 to 
April 2019 (single 
sleep assessment 
with daily 
temperature for the 
day, 7-day, 1-
month, 6-month 
and 1 year 
preceding the sleep 
assessment) Cross-sectional 
study  
(N = 5204 adults 
with sleep 
disorders; 30% 
Female, Mean age 
= 50 years) Association 
between ambient 
humidity and 
temperature with 
sleep parameters  
(Correlational) Linear 
regression Weather station; daily 
average temperature 
(Mean = 23°C; range = 5 
to 33°C) PSG; Sleep efficiency 
(Mean = 76 ±16.5%); 
WASO (Mean = 62 ±50.0 
minutes); Snoring index 
(Mean = 227 ±221.4 events 
per hours); AHI (Mean = 31 
± 27.5 events per hours); 
Percentage of TST in 
NREM sleep stage I (N1; 
Mean = 14 ± 11.8%); in 
NREM sleep stage II (N2; 
Mean = 71 ± 12.6%); in 
NREM sleep stage III (N3; 
Mean = 3 ± 6.6%);  in the 
REM sleep stage (REM; 
Mean index = 12 ± 7.2%); 
Supine AHI (Mean = 37 ± 
30.2 events per hours); non-
supine AHI (Mean = 26 ± 
43.4 events per hours) Control variables included 
age, sex, and BMI (Study’s results only 
partially reported here for 
parsimony); Main 
significant results 
observed between 
temperatures and sleep 
parameters were between 
yearly-average 
temperatures and WASO 
(β = 2.53, 95% CI [.32, 
4.74]), snoring index (β = 
24.9, 95% CI [15.2, 34.6]) 
and AHI (β = -1.17, 95% 
CI [-2.25, -0.09]) 
 
Overall conclusion: 
Higher temperatures 
associated with decreased 
sleep quality Authors conclude that 
ambient relative 
humidity and 
temperature were 
associated with 
alterations in most 
PSG-derived sleep 
parameters; 
alterations may be 
mediated by the sleep 
cycles. The study did 
not account for 
seasonality, social 
temporal factors, and 
other meteorological 
factors. Mattingly, 
2021 Multiple cities, 
United States  
[33°N - 44°N]  
(City/Household) All seasons, 
February 2018 to 
March 2019 
(repeated sleep 
assessments with 
temperature 
measured on the 
day preceding the 
sleep assessment) Intensive 
longitudinal study  
(k = 51, 836 
observations; N = 
216 adult 
information 
workers from four 
tech companies; 
32% Female; Mean 
Age = 35 years) Effects of seasons 
and weather on 
sleep duration, 
bedtime and wake 
up time (Quasi-
experimental) Linear mixed-
effects 
regression Weather station; 
principal component 
mainly consisting of 
daily average 
temperature 
(measurement range not 
reported, annual range 
derived for plotted 
locations in study: -9 to 
33°C) Commercial activity 
monitor (accelerometer-
based, wrist-worn); Sleep 
duration; Bedtime; Wake 
time (numerical average 
values and ranges not 
reported but seems to 
fluctuate graphically 
between 7 and 7.5 hours for 
sleep duration, 23:15 and 
23:30 for bedtime and 6:15 
to 7:00 for wake time) Control variables included 
individual random effects, 
latitude and longitude of 
home location, as well as 
day length and seasons, 
principal components 
mainly consisting of wind, 
humidity and cloud cover, 
as well as age, sex, work 
location, job 
characteristics, 
psychological variables 
(affect and personality), 
subjective sleep quality 
(PSQI), and chronotype 
(MEQ) Temperature principal 
component not 
significantly associated 
with sleep duration. For 
each unit increase in 
temperature, bedtime and 
wake time were 
significantly later (+ 0.6 
min, 95%CI [0, 1.2] for 
bed time and 95%CI [0, 
0.6] for wake time) 
 
Overall conclusion: Minor 
effects of temperature 
variability on sleep 
(bedtimes and wake times 
tend to be slightly later as 
outdoor temperature 
increases) Day length and 
spring season were 
significantly 
associated with a 
decrease in sleep 
duration; day length 
is significantly 
associated with 
earlier bedtimes, 
while Spring and 
Summer seasons are 
associated with 
delayed bedtimes; 
day length associated 
with earlier wake 
times; Spring and fall 
are associated with 
earlier wake times, 
summer season 
associated with 
delayed wake times; 
study excluded all 
weekend sleep entries 
and did not control 
for social temporal 
factors. Participants 
were information 
workers with flexible 
hours in thermally 
controlled offices. Milando, 
2022 Boston, United 
States,  
[42°N] 
(City/Bedroom) Summer 2020 
(repeated sleep 
assessments with 
temperature 
measured during 
the sleep period) Longitudinal study 
(k = 14 nights on 
average at the 
individual level; N 
= 22 adults; 75% 
Female; Age range 
= 22 to 78 years) Characterization of 
heat exposure in 
urban residents 
with heat 
adaptation practices 
and association of 
indoor heat and 
sleep duration 
(Correlational) Linear mixed-
effects 
regression; 
qualitative 
thematic 
analysis Sensor; Average 
nighttime temperature 
sampled at the 10-min 
level in participants’ 
bedroom (night 
temperatures not 
reported; inspection of 
the plots provided 
indicate a temperature 
range between 20-30°C) Commercial activity 
monitor (accelerometer-
based, wrist worn); Sleep 
duration (descriptive 
statistics not reported; 
inspection of the plots 
provided tend to indicate a 
sleep duration range 
between 2 and 10 hours) Control variables included 
the distinction between 
weekend-days and 
weekdays Temperature not 
significantly associated 
with sleep duration, 95% 
CI [-0.16, 0.05]; the slope 
was negative (-.05 hours = 
-3.0 minutes) of sleep per 
1°C increase in indoor 
temperature) 
 
Overall conclusion: the 
small sample size 
precludes conclusion on 
the effect of heat on sleep Results from 
qualitative interviews 
about sleep quality 
are presented, 
indicating varied heat 
concerns and heat 
adaptation strategies 
(although not sleep-
specific). Did not 
control for 
seasonality or other 
meteorological 
factors. Minor, 2022 Multiple (68) 
countries from all 
permanently 
populated 
continents   
[38° S to 65° N] 
(City/Households) All seasons, 
September 2015 to 
October 2017 
(repeated sleep 
assessments with 
temperature 
measured on the 
night of the sleep 
assessment) Intensive 
longitudinal study 
(k = ~7.41 million 
nights; N = 47,628 
adults; 31% 
Female; Age = 91% 
in 25-65 years old 
range) Effect of ambient 
temperature and 
weather on sleep 
duration, short 
sleep probability, 
sleep onset, 
midsleep, offset 
and nighttime 
awakenings in real-
world settings, 
projected impact of 
different climate 
change scenarios on 
global sleep 
erosion.  
(Quasi-
experimental) Multivariate 
fixed effects 
panel 
regression Weather station; 
nighttime minimum 
temperature (temperature 
ranged between -36 to 
35°C) Commercial activity 
monitor (accelerometer-
based, wrist-worn); Sleep 
duration (Median = 7.1 
hours); Sleep offset (00:00-
15:00); Sleep onset (19:00-
08:00) Control variables included 
fixed effects for each 
individual, each first-level 
administrative division 
(e.g., state) by month of 
study, the unique date of 
study, as well as daily 
precipitation, diurnal 
temperature range, 
percentage cloud cover, 
relative humidity, average 
wind speed, local daily 
climate normals (1981-
2010) for each 
meteorological variable; 
subgroup analyses 
conducted by age, sex, 
income, and climate decile Hot nights delayed sleep 
onset and midsleep, 
advanced sleep offset and 
reduced sleep duration. 
For nights (>=10 °C), 
sleep declined by 0.62 
minutes per +1°C (β = 
0.618, 95% CI [0.549, 
0.687]), for nights (<10°C) 
sleep declined by 0.11 
minutes per +1°C (β = 
0.107, 95% CI [0.070, 
0.144]). The probability of 
short sleep (<7h | <6hr | 
<5hr) increased steeply 
beyond 10°C). Per +1°C 
increase, sleep loss greater 
in the elderly, residents of 
lower income countries, 
and in females, as well as 
during summer months 
and in hotter regions. 
 
Overall conclusion: 
Increases in nighttime 
temperature erode human 
sleep duration across the 
global range of observed 
temperatures, with sleep 
loss progressively 
increasing beyond 10°C. No evidence of 
adaptation within 
days, between days, 
between months or 
across climate 
regions. 
 
Projections indicate 
that climate change 
may erode sleep at a 
faster rate in the 
warmest global 
regions, with higher 
greenhouse gas 
concentrations 
producing larger 
global inequalities in 
temperature-
attributed sleep loss.  
Additional analyses 
did not reveal 
significant 
associations between 
ambient temperature 
and nighttime 
awakenings (a marker 
of sleep quality) Montmayeur, 
1992 Niamey, Niger 
[13°N] 
(City) February to May; 
Year not specified 
but probably late 
80’s/early 90’s 
(single sleep 
assessment with 
temperature 
averaged for the 14 
days before the 
sleep assessment) Longitudinal study 
with 3 time points 
(N = 6 military 
men; Mean age = 
35 years) Association 
between ambient 
temperature 
and sleep in 
expatriates 
(Correlational) Bivariate 
correlation Weather station; daily 
maximum, minimum and 
average temperature 
(means ranged from 30 to 
33°C), preceding 14-day 
average mean, min and 
max temperature PSG; Total sleep time 
(Mean for March = 358 ± 
16.2 minutes); Sleep 
efficiency (Mean for March 
= 86 ± 4.6%); Number of 
stage changes (Mean for 
March = 254 ± 17); Number 
of awakenings (Mean for 
March = 89 ± 8); Sleep 
latency (Mean for March = 
8 ± 5.5 minutes); 
Wakefulness (Mean for 
March = 59 ± 6 minutes) No control variables  
(Study’s results only 
partially reported here for 
parsimony); Significant 
and positive correlations 
between temperatures 
(average daily maximum 
last 14 days) and various 
sleep parameters, 
including number of 
awakenings (r = .44), 
shortened sleep stage 2 (r= 
-0.44) 
 
Overall conclusion: No 
explicit conclusions on the 
effect of temperature on 
sleep Underpowered study; 
Did not estimate 
within-person 
outcomes. 
Polysomnographic 
measures were 
carried out in air-
conditioned 
laboratory (ambient 
temperature: 23-
25°C). Mullins, 2019 
Multiple cities, 
United States [25°N 
- 50°N] (County) All seasons, 1993-
2012 (single 
individual-level 
sleep assessment 
with ambient 
temperatures 
averaged over the 
last 7-days and 30-
days preceding the 
sleep assessment) Repeated cross-
section study (N = 
4, 120, 514 adults 
randomly sampled 
from the general 
population; 
descriptive 
statistics not 
provided) Effect of ambient 
temperature on mental 
health and subjective 
sleep outcomes 
(Correlational/Quasi-
Experimental) Linear fixed-
effects 
regression Weather station; 
daily temperature 
averaged at the 
county level 
(temperatures ranged 
from < -1°C to > 
27°C) Self-report questionnaire 
(single item from the 
BRFSS); Number of nights 
with self-reported 
insufficient sleep over the 
prior 30 days (descriptive 
statistics not clearly 
reported) 
 
Time use survey (ATUS); 
self-reported sleep duration 
(descriptive statistics for 
sleep outcomes not reported) Control variables included 
fixed effects for each 
county-by-month, state-by-
year, day of week, and 
other climatic variables 
such as precipitation as 
well as local demographics 
on age, race, gender, 
education, marital status 
and income. Standard 
errors were clustered at the 
state level. Results indicate that 
warmer temperatures led 
to worse outcomes for 
both subjective sleep 
quality (increases in 
nights of poor sleep) and 
self-reported duration 
(decreases in minutes 
slept); in each case the 
relationship is roughly 
linear, cold temperatures 
led to improvements in 
both sleep measures. A 
+1°C increase in mean 
daily temperature 
produced ~1 extra night 
of recalled insufficient 
sleep over the prior 
30days per 100 
individuals (β = 0.008) 
and reduced self-reported 
sleep duration by 0.45 
minutes per person (β = 
0.448).  
 
Overall conclusion: 
Higher temperatures 
associated with decreased 
subjective sleep quality 
and quantity Minimum (nighttime) 
temperature had a 
stronger effect than 
maximum (daytime) 
temperature on 
mental health 
outcomes (ED visits 
and suicide). Did not 
examine the effect of 
very cold (<-1°C) 
temperatures on sleep 
outcomes.  Did not 
assess within person 
effects. Obradovich, 
2017 Multiple cities, 
United States [25°N 
- 50°N] (City) All seasons, 2002-
2011 (single 
individual-level 
sleep assessment 
with nighttime 
temperature 
averaged over the 
last 30-nights 
corresponding to 
the sleep 
assessment) Repeated cross-
section study 
(N = 765, 000 
adults randomly 
sampled from the 
general population; 
descriptive 
statistics not 
provided) Effect of ambient 
nighttime temperature on 
subjective sleep, projected 
impacts of climate change 
on poor sleep 
(Correlational/Quasi-
Experimental) Linear fixed-
effects 
regression Weather station; 30-
day average of 
minimum nighttime 
temperature 
deviations from their 
normal daily values 
(reference period 
1981 to 2010; 
minimum 
temperature range -
21 to 29°C) Self-report questionnaire 
(single item from the 
BRFSS); Number of nights 
with self-reported 
insufficient sleep over the 
prior 30 days (descriptive 
statistics not reported) Control variables included 
fixed effects for the city-
by-season and each 
calendar date, as well as 
controls for precipitation 
anomalies, daily climate 
normals, temperature 
range, cloud cover and 
humidity. Standard errors 
clustered on city and day. 
Heterogeneity analyses 
stratified by seasons, age 
and income. A +1°C deviation in 
nighttime temperatures 
produced an increase of 
approximately three 
nights of self-reported 
insufficient sleep per 100 
individuals per month (β 
= 0.028, P = 0.014).  
 
Overall conclusion: 
Higher temperatures 
associated with increased 
reports of insufficient 
sleep; climate change 
might disrupt human 
sleep Negative association 
between temperatures 
and sleep stronger 
during summer 
months and among 
both lower-income 
and elderly 
individuals; 
projections indicate 
that climate change 
might disrupt human 
sleep across the US, 
producing relatively 
more monthly nights 
of insufficient sleep 
in southeastern states 
and along the west 
coast. Assessed the 
influence of monthly 
temperature 
anomalies. Did not 
assess within-person 
effects. Ohnaka, 1995 
Maebashi, Japan  
[36°N]  
(Bedroom) July and August, 
year not specified 
but probably early 
90’s (repeated 
sleep assessments 
with bedroom 
temperature 
recorded during the 
night of the sleep 
assessments) Cross-sectional 
study (k = between 
250/300; N = 20 
older adults and 20 
young adults; 
Mean age = 73 and 
21 years 
respectively; sex 
not specified) Association between 
summer bedroom 
temperature and sleep in 
two age groups 
(Correlational) Bivariate 
correlation Sensor; Average 
nighttime 
temperature sampled 
at the 2-min level 
(participants’ 
bedroom average 
temperature ranged 
between 20 and 
30°C) Sleep sensor (bed equipped 
with accelerometers 
measuring body 
movements); Number of 
significant body movements 
per 30 minutes after a 
subject went to bed (Mean = 
2 ± 2.29 counts and 3 ± 2 
counts in the young and 
older adult groups 
respectively) No control variables 
A significant association 
was found between 
bedroom temperature and 
body movements for both 
groups (r = .30 and r = 
.28 for older adults and 
young adults 
respectively) 
 
Overall conclusion: sleep 
disruptions increase with 
ambient room 
temperatures; the effect 
being stronger in older 
adults compared to young 
counterparts Study duration is 
unclear. The study 
did not account for 
seasonality, social 
temporal factors, and 
other meteorological 
factors. Cross-
sectional study 
susceptible to omitted 
variable bias. Okamoto 
Mizuno, 2010 Tsukuba, Japan 
[36°N]  
(Bedroom) Winter, summer 
and fall, year not 
specified (repeated 
sleep assessments 
with temperature 
recorded on the 
night of the sleep 
assessments) Longitudinal study 
(5-consecutive 
days per seasons; 
N = 19 older 
adults; 32 % 
Female; Mean age 
= 66 years) Association between 
seasonal variations, 
bedroom temperature, 
sleep and skin 
temperature 
(Correlational) Bivariate 
correlation Sensor; Average 
nighttime 
temperature sampled 
at the 1-min level 
(average outside and 
bedrooms’ 
temperatures ranged 
between 1 and 25°C 
and 10 and 28°C 
respectively) Research-grade 
accelerometer (wrist-worn); 
(descriptive statistics only 
reported for fall-months for 
parsimony) Bedtime (Mean 
= 22:25 hour); Wake-up 
time (Mean = 6:03 hour); 
Time in Bed (Mean = 459 
minutes); TST (Mean = 414 
minutes); WASO (Mean = 
45 minutes); SE (Mean = 
90%); Sleep latency (Mean 
= 16 minutes); Longest 
Wake episode (Mean = 16 
minutes); Activity Index 
(Mean = 39) No control variables 
(Study’s results only 
partially reported here for 
parsimony); A significant 
correlation was observed 
between bedroom 
temperature in summer 
and the following 
outcomes: wake episode 
(r = 0.46; P < 0.05); SE (r 
= −0.47; P < 0.05) and 
WASO (r = 0.49; P < 
0.01). No significant 
correlation between 
bedroom temperatures 
and sleep parameters was 
observed in other seasons 
 
Overall conclusion: 
higher sleep disruptions 
in summer compared to 
fall and winter Skin temperature 
discussed as a 
potential mechanism 
but no mediation 
analysis was 
performed; Did not 
assess within-person 
effects; Associations 
between outdoor 
temperature and sleep 
parameters were not 
tested. Did not 
account for 
seasonality, social 
temporal factors, and 
other meteorological 
factors. Pandey, 2005 
Rochester, United 
States, [43°N] 
(City) January to May, 
year not specified 
(repeated sleep 
assessments with 
temperature 
averaged for the 
whole study 
period, i.e., 105 
days) Intensive 
longitudinal study 
analyzed as a 
cross-sectional 
study (all 
observations 
averaged; N = 43 
undergraduate 
students; 72 % 
Female; Mean age 
= 23 years) Association between 
multiple ambient 
meteorological factors 
and ability to initiate and 
maintain 
sleep  
(Correlational) Bivariate 
correlation Weather station; 
daily temperature 
averaged over the 
study period 
(measurement range 
not reported; annual 
average range 
derived for reported 
months in Rochester: 
-8 to 20°C) Self-report questionnaire 
(daily diaries); Sleep 
latency; Number of 
awakenings; WASO and 
TST (descriptive statistics 
not reported) No control variables 
Significant negative 
associations between 
temperatures and TST (r 
= -0.21, P = 0.03); non-
significant associations 
with the other sleep 
outcomes 
 
Overall conclusion: lower 
temperature associated 
with improved sleep Analytical strategy 
did not account for 
within-participants 
effects; largest 
association found 
between precipitation 
and SE (r = -.42, P 
<.0005). Did not 
control for other 
meteorological 
factors when 
estimating the 
association between 
temperature and 
sleep. Quante, 2017 
Boston, United 
States,  
[42°N] 
(City) All seasons, 2012-
2015 (repeated 
sleep assessments 
averaged with 
temperature 
measured the day 
preceding the sleep 
assessment) Cross-sectional 
study 
(N = 669 children; 
51% Female; Mean 
age = 13 years) Association between 
outdoor weather variation, 
sleep and physical activity 
in 12-14-year-old children 
(Correlational) Generalized 
Estimating 
Equation Weather station; 
daily temperature 
(temperature range: 
−2 to 24°C) Research-grade 
accelerometer (wrist-worn); 
Sleep duration (Mean = 466 
min), sleep midpoint time 
(Mean = 3:20 hours), 
WASO (Mean = 54 min) 
and SE (Mean = 89 %) Control variables included 
windspeed, precipitation, 
day length, snowfall, 
maternal education status, 
household income, child 
race/ethnicity, sex, age, 
BMI, pubertal 
development scale, and 
school attendance Temperature negatively 
associated with sleep 
duration (β = -.33, 95% 
CI [-.50, -.17]), and SE (β 
= -.02, 95% CI [-.03, -
.01]); association 
borderline with WASO (β 
= -09, 95% CI [.00, .18]) 
and non-significant with 
sleep midpoint time (β = 
.10, 95% CI [-.14, .34]) 
 
Overall conclusion: 
weather-related factors 
modestly associated with 
sleep Snow falls positively 
associated with 
WASO and sleep 
duration. Cross-
sectional study 
susceptible to omitted 
variable bias. Quinn, 2016 
New York, United 
States, [41ºN] 
(Households) Winter and 
summer, 2013-
2015 (repeated 
sleep assessments 
with temperatures 
averaged across the 
past 24-hours 
preceding the sleep 
assessment) Longitudinal study 
(N = 40 
households; 
Mean household 
members = 2.3 
individuals; 
Median age = 29 
years) Association between 
indoor temperature, 
humidity, and health 
symptoms  
(Correlational) Mixed-effects 
logistic 
regression Sensor (indoor 
temperature) and 
weather stations 
(outdoor 
temperature).  
(Average daily 
outdoor and indoor 
temperatures 
appeared to range 
respectively between 
approximately -15°C 
to 33°C (outdoor) 
and 10 to 38°C 
(indoor); 
temperature ranges 
derived from the 
graphs displayed in 
the article) Self-report questionnaire 
(custom single item); 
deviation from the normal 
subjective sleep quality 
(dichotomized value “better” 
or “worse” than usual) Control variables included 
random effects for 
households, controls for 
household size and the 
number of surveys 
completed by the 
household Self-reported sleep 
problems (rating last 
night’s sleep as worse 
than usual) were 
significantly and 
positively associated with 
the last 24-hour average 
indoor temperature in the 
summer season only (OR 
= 
2.28) 
 
Overall conclusion: Sleep 
quality was adversely 
affected by increased 
temperature but only in 
the summer season Self-reported sleep 
problems were also 
significantly and 
positively associated 
with perceived 
(subjective) 
temperature (OR = 
3.47); unclear 
whether the 
associations between 
outdoor temperatures 
and sleep were 
modelled and how. 
The study did not 
account for 
seasonality or social 
temporal factors. van Loenhout, 
2016 Arnhem and 
Groningen, The 
Netherlands, 
[51 - 53ºN] 
(Households) Spring and 
summer, April to 
September 2012 
(repeated sleep 
assessments with 
temperature 
averaged across the 
past 24-hours 
preceding the sleep 
assessment) Longitudinal study 
(N = 113 elderly 
individuals; 49% 
Female; Mean age 
= 74 years) Association between 
indoor and outdoor 
temperature and heat-
related symptoms in 
elderly individuals during 
the summer months 
(Correlational) Logistic 
regression and 
Generalized 
Estimating 
Equation Sensor; mean daily 
and weekly 
temperature (indoor 
temperatures ranged 
between 15 and 30ºC 
and outdoor 
temperatures ranged 
between 11 and 
24ºC) Self-report questionnaire 
(custom tool); subjective 
sleep disturbances (item and 
descriptive statistics not 
reported) Control variables included 
gender and age Bedroom daily 
temperatures significantly 
associated with sleep 
disturbances rated during 
the day, B = 1.21 (95% 
CI, 1.07, 1.48); outdoor 
temperature not 
significantly associated 
with sleep disturbances 
when controlling for 
bedroom temperatures 
 
Overall conclusion: The 
relationship with heat-
related health problems in 
the elderly is stronger 
with indoor (living room 
and bedroom) 
temperature than with 
outdoor temperature Did not control for 
social temporal 
factors, other 
meteorological 
factors or seasonality Wang, 2022 
Multiple counties, 
China,  
[18 - 52ºN] 
(County) All months in 2012 
and 2016, with 
most of the 
assessments 
performed in 
August, July and 
December 
(repeated sleep 
assessments with 
temperature 
assessed the week 
preceding the sleep 
assessment; 
sensitivity analyses 
include lagged 
associations for the 
past 2 and 3 weeks, 
and the past 1, 2 
and 3 months, rural 
vs urban, 
demographic 
variables) Prospective cohort 
study with up to 2 
measurement 
points per 
participant  
(N = 40, 470 
adults; 51% 
Female; Mean age 
= 49 years) Effect of ambient air 
temperatures on 
subjective poor sleep 
experience 
(Quasi-Experimental) Linear fixed-
effects 
regression Weather station; 
average daily 
temperature (range ~ 
-20 to 35ºC; Mean = 
21ºC) Self-report questionnaire 
(custom tool); subjective 
sleep quality “experience” in 
the past 7 days (Mean score 
= 0.74, with 0 = less than 
one day where sleep was 
restless in the past week and 
3 = five to seven days.) A 
score of 3 was considered as 
a poor sleep experience. Control variables included, 
for the main model, 
average relative humidity, 
sunshine duration, wind 
speed, pressure, and 
cumulative precipitation 
along with their quadratics 
during the past week 
(county level), and the day 
of the interview as well as 
individual fixed effects; 
other variables included in 
sensitivity analyses (e.g., 
lagged effects) No significant association 
between average 
temperature and poor 
sleep (full sample 
analysis) 
 
Compared with the 16-
20°C bin, every 
additional day increase in 
the bin of 28°C 
significantly increases the 
likelihood of having poor 
sleep experience for all. 
 
Significant association 
between temperature and 
sleep in elderly and 
participants with lower 
educational level; 
 
Overall conclusion: high 
temperature can increase 
the probability of 
reporting poor sleep 
experience, notably in 
elderly, lower educated 
and rural residents and 
those who do not own air 
conditioners Sensitivity analyses 
show: (1) The effect 
of average 
temperature on sleep 
was significant in 
rural residents 
(probability of 
reporting poor sleep 
experience increases 
by 2.34 % when the 
temperature increases 
by 1 SD; (3) only the 
past one week, two 
weeks, three weeks, 
and one month’s daily 
temperature 
significantly affect 
sleep experience; (4) 
compared with 
participants who did 
not own AC, a +1 SD 
increase in avg. 
temperature 
decreased the chance 
of poor sleep 
experience by 1.12 
percentage points 
among people who 
owned AC Weinreich, 
2015 Essen, Bochum and 
Mülheim, 
Germany, 
[51ºN] 
(City) All seasons, 2006-
2008 (single sleep 
assessment with 
temperature 
assessed the day 
preceding the sleep 
assessment as well 
as averaged the 2- 
and 3-days 
preceding the sleep 
assessment) Cross-sectional 
study (N= 1773 
adults; 49% 
Female; Mean age 
= 64 years) Association between air 
pollution, 
temperatures and sleep 
disordered 
breathing in the general 
population 
(Correlational) Logistic 
regression Weather station; 
daily temperature 
(range = -4 to 29ºC; 
Mean = 13ºC) Screening device for apnea-
hypopnea index; AHI and 
sleep disordered breathing 
(Mean = 11 events per hours 
indicating mild sleep apnea) Control variables included 
seasons, daily relative 
humidity and air pollution 
as well as age, sex, BMI, 
education, smoking habits, 
alcohol consumption and 
physical activity Daily mean of outdoor 
temperature was not 
associated with SDB 
severity.  
One IQR increase in 
temperature (8.6°C) 
associated with a 10.2% 
(95% CI 1.2–20.0%) 
increase in AHI 
 
Overall conclusion: short-
term increases in ozone 
concentration and 
temperature are 
associated with higher 
sleep disordered 
breathing Association between 
temperature and AHI 
stronger in summer 
(32.4% (95% CI 0.0–
75.3%) increase in 
AHI per 8.6°C). 
Sensitivity analyses 
suggested that 
associations for 
temperature was not 
significant when air 
pollution were added 
in the model. Cross-
sectional study 
susceptible to omitted 
variable bias. Williams, 
2019 Cambridge, United 
States,  
[42ºN] 
(City/Households) June to August 
2015 during three 
extreme heat 
events 
(Repeated sleep 
assessments with 
bedroom 
temperature 
recorded the night Longitudinal study 
(20 days of 
observations; total 
number of 
observations not 
reported; N = 51 
low-income older 
adults spread in 
two groups 
residing in two Characterizing the indoor 
environmental quality in 
public housing and its 
association with health 
parameters 
(Correlational) Poisson 
generalized 
additive mixed 
model Sensor: average 
bedroom nighttime 
temperature sampled 
at the 5-min level 
(Mean temperature 
during the survey for 
the two groups = 26 
and 23ºC; indoor 
temp range: 18-32 
ºC) Commercial activity monitor 
(accelerometer-based, wrist-
worn); movements at night 
(tosses and turns; descriptive 
statistics not reported) Control variables included 
nested random effects for 
individuals and the 
buildings in which the 
participants were recruited Number of tosses and 
turns increased 
significantly with indoor 
mean temperatures 
(results not numerically 
reported but plotted); no 
significant association 
with self-reported or 
monitored-derived sleep Self-reported 
hydration did not 
increase with 
temperatures 
suggesting that 
participants lack 
adaptive behavioral 
strategies. Did not 
control for social 
temporal factors, of the sleep 
assessments) different buildings; 
54 and 59% 
Female; Mean age 
= 65 and 66 years) duration (coefficients not 
reported).  
 
Overall conclusion: 
higher indoor 
temperatures is associated 
with higher sleep 
disruption other meteorological 
factors or seasonality. Xiong, 2020 
Sydney, 
Australia, 
[33ºS] 
(Households) January to March 
2019 (repeated 
sleep assessments 
with bedroom 
temperature 
recorded the night 
of the sleep 
assessments) Cross-sectional 
study (k = 217 
observations; N = 
48 adults; 54% 
Female; Mean age 
= 36 years) Association between 
bedroom temperature, 
ventilation and sleep 
parameters and quality 
(Correlational) Linear 
regression Sensor; average 
bedrooms’ nighttime 
temperature sampled 
at the 5-min level 
(Mean temperature 
during the survey = 
26ºC ± 1.7; 
temperature range ± 
3SD: (21-31ºC)) Commercial activity monitor 
(accelerometer-based, wrist-
worn); SE (Mean = 88%); 
Light, REM and deep sleep, 
number of awakenings. 
Self-reported measures: 
Satisfaction and calmness of 
sleep, ease of falling asleep 
and of awakening 
 (descriptive statistics not 
reported for these outcomes) Not explicit whether – and 
if so which – controls were 
used Significant negative 
association between 
bedroom temperature and 
SE, and significant and 
positive association 
between temperature and 
percentage of light sleep 
(correlations plotted, 
descriptive statistics 
unclear, effect sizes look 
small) 
 
Overall conclusion: 
higher bedroom 
temperature associated 
with device measured 
poorer sleep but not self-
reported sleep measures Higher bedroom CO2 
concentration 
significantly 
associated with lower 
deep sleep (%) and 
self-reported sleep 
quality; no results 
provided between 
temperature and self-
report sleep 
outcomes. Cross-
sectional study 
susceptible to omitted 
variable bias. Did not 
control for social 
temporal factors, 
other ambient 
environmental factors 
or seasonality. Xu,  
2021 Shanghai, China,  
[31ºN] 
(Bedroom) Assessment 
performed in 
summer in air-
conditioned 
bedrooms, year(s) 
not explicitly 
reported (repeated 
sleep assessments 
with bedroom 
temperature 
recorded the night 
of the sleep 
assessments) Longitudinal study 
(7 days of 
observations; N = 
41 healthy adults; 
32% Female; Age 
ranged from 22 to 
52 years) Association between 
bedroom temperature and 
sleep (Correlational) Bivariate 
correlation and 
linear 
regression Sensor: average 
bedroom nighttime 
temperature sampled 
at the 1-min level 
(Average mean 
temperature during 
the survey ranged 
from 24 to 30.5 ºC) Commercial activity monitor 
(accelerometer-based, wrist-
worn); 
Slow wave sleep (Mean = 
73.8 min) and sleep 
efficiency (Mean = 86.7%) Control variables included 
the objective measures of 
relative humidity, carbon 
dioxide concentration, and 
noise level Significant correlation 
between temperature and 
slow wave sleep (r = -
0.38); association not 
significant between 
temperature and SE (r = 
0.06).  
 
Overall conclusion: 
higher air temperature 
associated with poorer 
sleep quality in summer 
in air-conditioned 
bedrooms Air temperature and 
carbon dioxide 
concentration might 
have a greater impact 
on the sleep quality of 
males, while noise 
level might have a 
greater impact on the 
sleep quality of 
females. Analytical 
strategy did not assess 
within-participant 
associations. Did not 
control for social 
temporal factors or 
seasonality. Yan, 2022 
Shanghai, China,  
[31ºN] 
(Bedroom) Summer 
assessment 
conducted from 
July – August 2018 
in bedrooms with 
access to AC units 
and fans (repeated 
sleep 
measurements with 
bedroom air 
temperature 
recorded the night Longitudinal study 
(6 days of 
observations; N= 
40 independently 
living elderly 
adults with no 
chronic disease; 
56% Female; Mean 
age = 71 ± 5.0 
years; only a third 
of participants 
reported using their Association between 
bedroom temperature 
during sleep period, 
objective sleep parameters 
and subjective sleep 
parameters (Correlational) Generalized 
linear mixed 
model 
regression Sensor; bedroom air 
temperature, relative 
humidity and CO2 
concentration 
measured at 5-min 
intervals during 
sleep period; skin 
temperature at wrist 
measured at night in 
3-min intervals. 
(Average bedroom 
temperature during Commercial activity monitor 
(accelerometer-based, wrist-
worn); Time in bed (TIB) 
Median = 432.5 (IQR 380.0-
468.8), total sleep time 
(TST) = 363.0 (314.0 - 
393.0), sleep efficiency (SE) 
84.5 (81.2-87.9), time awake 
after sleep onset (WASO) 
66.5 (50.0-82.3), light sleep 
duration 230.5 (194.3-
267.8), deep sleep duration Control variables included 
sensor measures of relative 
humidity, carbon dioxide 
concentration Significant associations 
between bedroom 
temperature and several 
objective and subjective 
sleep outcomes. A 1ºC 
increase in bedroom 
temperature associated 
with decreases in total 
sleep time (-5.1min), 
sleep efficiency (-0.7%), 
REM sleep (-2.1 min), 
light sleep (-7.0min), Does not indicate 
whether individual 
random effects were 
included. Did not 
control for social 
temporal factors or 
seasonality. Bedroom 
temperature not 
significantly 
associated with deep 
sleep, time in bed or 
ease of awakening, of the sleep 
assessments) AC during the 
sleep period) sleep period ranged 
from 26-32 ºC; 
relative humidity 
ranged from 36–
88%; 
CO2 concentration 
ranged from 36–
88%; 
Outdoor temperature 
during the study 
ranged from 26-35ºC 
but not included in 
analysis) 52.0 (37.3-67.0), REM sleep 
duration 69.5 (44.3-85.0). 
Self-report questionnaire on 
five-point Likert scales; 
calmness of sleep 4 (3-5), 
ease of falling asleep 4 (3-4), 
ease of awakening 4 (3-4.8), 
freshness after awakening 4 
(3-4) and sleep satisfaction 4 
(3-4). subjective calmness of 
sleep (-1.6), ease of 
falling asleep (-1.1), 
freshness after awakening 
(-1.6), satisfaction about 
sleep (-2.3) and an 
increase in time awake 
after sleep onset (2.3min).   
Overall conclusion: 
Bedroom temperature 
negatively associated 
with sleep quantity and 
quality among elderly 
with air conditioning and 
fan access. although all 
coefficients were 
negative. Control 
variables vary across 
outcomes. 
 
Sleep duration was 
negatively correlated 
with 
CO2 concentration. 
TST decreased by 
11min per 100ppm 
increase in 
CO2 concentration 
(adjusted for air 
temperature). Shorter 
TST correlated with 
higher wrist skin 
temperature. Zanobetti, 
2009 Seven cities, United 
States [25°N - 
50°N] (Households) All seasons, 1995-
1998 (single sleep 
assessment with 
365-day moving 
average of 
temperature and 
the difference 
between the daily 
temperature 
outcome and the 
365-day average 
preceding the sleep 
assessment) Cross-sectional 
(N = 3030 adults; 
52% Female; 
Mean age = 63 
years old) Association between air 
pollution and sleep 
disordered breathing and 
sleep efficiency  
(Correlational) Mixed effects 
regression Weather station; 
daily temperature 
averaged over a year 
(averaged daily 
temperatures ranged 
from 7 
(Minneapolis) to 
23ºC (Tucson)) PSG; Respiratory 
disturbance index (RDI), 
computed as the ratio of the 
count of all apneas and 
hypopneas to the total sleep 
time, expressed as hours 
(Mean = 14; interpretation 
unclear); SE (Mean = 83%) Control variables included 
age, sex, ethnicity, 
educational level, smoking 
history, alcohol 
consumption and 
consumption or coffee, tea 
and soda. Random effects 
for each city were 
included. Significant association 
found between 
temperature (short-term) 
and RDI with +4ºC 
associated with 12% 
increase 95%CI [1.96, 
22.01] in RDI (effect not 
modified by seasons); 
other associations 
between long-term 
temperature or with SE 
not significant 
 
Overall study’s 
conclusion: increases in 
the respiratory 
disturbance index are 
associated with increases 
in short-term temperature RDI also significantly 
associated with 
increase in pollution 
levels in summer 
months. Cross-
sectional study 
susceptible to omitted 
variable bias. Did not 
control for social 
temporal factors, 
other meteorological 
factors or seasonality. Abbreviations. PSG: polysomnography; AHI: apnea-hypopnea index; PM (2.5/10): particulate matter (density in micron); WASO: wake after sleep onset; SE: sleep efficiency; TST: total sleep time; NREM: non-rapid eye movement; MEQ: morningness-eveningness questionnaire; BRFSS: behavioral risk factor surveillance questionnaire; PSQI: Pittsburgh sleep quality index; RDI: respiratory disturbance index. Figure 3 illustrates the range of observed ambient temperatures and latitudes for each of the studies in this review. Aggregating these ranges suggests a greater observational density for hotter compared to colder temperatures (bottom heat bar, Figure 3A) and for northern latitudes compared to both the equatorial region and the southern hemisphere (right heat bar, Figure 3B). Notably, just four (15%) of the studies investigated temperature-related sleep responses in the Tropics, even though the region is home to approximately 40% of the global human population (blue histogram, Figure 3B).24,61,65,69 Figure 3. Range of observed ambient temperatures (A) and latitudes (B). of included studies . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Table 4. Methodological quality of included studies As shown in Table 4, study quality was generally low as assessed by the 14-item research quality checklist developed for this systematic review (i.e., the quality criteria were only met by ~30% of all cells, marked in the table in green). Although there was a high degree of heterogeneity between studies in terms of evaluated quality, some specific criteria were rarely met by the extant literature such as the combined measurement and analysis of both indoor and outdoor temperatures (estimated by only one prior study), the inclusion of personal cooling strategies (e.g., fans), or other time-varying meteorological controls that might otherwise confound inference between temperature and sleep. Item 1: 
non-
linearity 
inspected Item 2: 
exposure 
assessed 
more 
than once Item 3: 
within-
participant 
variation 
analyzed Item 4:  
sleep and 
temperatures 
measured over 
the whole year Item 5: 
seasonality 
or day 
length 
controlled Item 6: 
temporal 
variables 
controlled Item 7: 
humidity 
controlled Item 8: 
wind speed 
controlled Item 9: 
cloud cover 
controlled Item 10: 
precipitation 
controlled Item 11: 
indoor and 
outdoor 
temperature 
measured and 
analyzed Item 12: 
lagged 
temperature 
effects 
inspected Item 13: 
personal 
cooling 
technologies 
measured and 
analyzed Item 14: sleep 
measured with 
questionnaires 
and devices An, 2018 
0 
1 
1 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 Cassol, 2012 
0 
0 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 Cedeño L, 2018 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 Cepeda, 2018 
0 
1 
1 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 Hashizaki, 2018 
1 
1 
1 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 Lappharat, 2018 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 Li, 2020 
1 
1 
1 
1 
0 
1 
0 
0 
0 
0 
0 
1 
0 
1 Liu, 2022 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 Mattingly, 2021 
0 
1 
1 
1 
1 
0 
1 
1 
1 
0 
0 
0 
0 
0 Milando, 2022 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 Minor, 2022 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
0 
1 
0 
0 Montmayeur, 1992 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 Mullins, 2019 
1 
0 
0 
1 
1 
1 
1 
0 
0 
1 
0 
0 
0 
0 Obradovich, 2017 
1 
0 
0 
1 
1 
1 
1 
0 
1 
1 
0 
0 
0 
0 Ohnaka, 1995 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 Okamoto M, 2010 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 Pandey, 2005 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 Quante, 2017 
1 
0 
0 
1 
1 
0 
0 
1 
0 
1 
0 
0 
0 
1 Quinn, 2016 
0 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 Van Loenhout, 2016 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 Wang, 2022 
1 
1 
1 
0 
1 
1 
1 
1 
0 
1 
0 
1 
1 
0 Weinreich, 2015 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
1 
0 
0 Williams, 2019 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 Xiong, 2020 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 Xu, 2021 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 Yan, 2022 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
1 Zanobetti, 2009 
0 
0 
0 
1 
1 
0 
1 
1 
1 
1 
0 
0 
0 
0 Narrative synthesis for the association between ambient temperature and sleep Overall, most studies (i.e., 22 articles, 80%) concluded that higher temperatures were associated with poorer sleep, with two articles explicitly characterizing the association as “modest” in terms of effect size.64,79 Among the five remaining studies: (i) two observed a positive effect of temperature on sleep, with one study performed in China reporting a positive association between temperature and self-reported sleep duration,68 another, performed in Brazil, showing that apnea-hypopnea index was inversely correlated with ambient temperature;77 (ii) two other studies were not explicit in their conclusion about this specific temperature-sleep association, with one not observing a significant association between temperature and sleep duration (but delayed sleep timing),62 and another observing a significant association between temperature and the number of awakenings but focusing on sleep adaptation in expatriates (not the impact of heat);61 and (iii) one study showing a non- significant association (neither positive nor negative) between indoor temperature and sleep duration.75 In regards to sleep apnea specifically, the abovementioned result77 contradicts two other studies included in the present review showing a significant and positive association between temperature and apnea-hypopnea indexes;76,78 while a fourth study did not observe significant relationships between temperature (indoor) and apnea-hypopnea indexes.69 Results are less contrasted with other sleep outcomes and assessment methods. For example, the four studies using research-grade accelerometers64,79–81 all found that higher temperature was associated with poorer sleep (i.e., notably reduced sleep duration and sleep efficiency, as well as increased WASO). In the same vein, four of the six studies using commercial-grade activity monitors found negative associations between ambient temperature and sleep duration,24,63,71 efficiency71,73 (but see74,75 for a non-significant association), WASO and REM sleep,71 nighttime body-movements72 and slow wave sleep.74 Eight studies using questionnaires reported that higher temperature was associated with poorer subjective sleep.33,34,65–67,69–71 One study did not observe a significant association between temperature and subjective sleep in adults with episodic migraine,64 and another study reported a positive association between temperature and sleep duration in a large cohort of students.68 Among the five higher quality studies that fulfilled at least half of the quality criteria,24,33,62,64,65 all concluded that higher temperature was associated with poorer sleep outcomes. For the six studies that identified significant negative effects of temperature on sleep duration, estimated effect sizes appeared to vary in magnitude from modest to large, with effects . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint scaling across the temperature distribution.24,34,63,67,71,79,80 For instance, a multi-country study found that at the colder end of local temperature distributions, a +1ºC increase reduced sleep by just 0.20 minutes during winter months but sleep was reduced by 0.97 minutes per +1ºC during the last month of summer,24 whereas a US-based national time use diary study conducted across all seasons estimated a linear sleep reduction of 0.45 minutes per +1ºC.34 By comparison, a Boston-based study conducted during a summer heatwave estimated a larger sleep reduction of 2.7 minutes per +1ºC increase in bedroom temperature for a sample of young adults,63 while a Shanghai-based study estimated a large magnitude reduction of 5.10 minutes in total sleep time per +1ºC increase for a sample of elderly participants.71 Taken together, these results suggest that ambient temperatures may exact both cumulatively and progressively larger sleep impacts at higher ambient temperatures, with one study24 estimating that nights exceeding 25ºC push 4,600 additional people to experience a short (<7 hour) night of sleep per 100,000 adults compared to the cold optimum identified. Other relevant results Other notable results that further contextualize the temperature-sleep association include (i) the role of seasons, (ii) the impact of other weather and environmental variables, (iii) the presence of non-linear associations between temperature and sleep, (iv) the potential mechanisms explaining how temperature impacts sleep, (v) the specific role of indoor temperature beyond outdoor temperature, and (vi) the role of adaptation measures. Seasonality In regards to seasonality, several studies showed that the negative impact of ambient temperature on sleep was more pronounced in the hottest months of the year as discussed earlier in this review,24,60,81,82 although one study failed to replicate this effect.76 A study conducted in the Netherlandshighlighted that the seasonality of sleep duration – with people sleeping less during summer and more during winter – appeared to be mainly driven by ambient temperature (i.e., the percentage of variance explained by seasons decreased significantly when temperature was controlled for).80 However, another study conducted across the US, found that seasons and day length were the only significant variables associated with sleep duration over 3 principal components mainly representing temperature, wind, humidity and cloud coverage (all of these derived components were not significantly associated with sleep duration).62 However, all subjects in this last study were information . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint technology workers working in thermally controlled indoor offices, potentially buffering against outdoor thermal exposures. Effect modification with other weather variables Other weather outcomes such as humidity, precipitation, cloud cover or wind speed were included as control variables in several studies.24,34,62,68 In the largest device-based study included in this review,24 authors showed that (i) the impact of temperature on sleep duration was independent from other weather variables and that (ii) higher levels of precipitation, wind speed and cloud coverage marginally increase sleep duration while both low and high levels of humidity significantly reduce sleep duration. Additionally, high diurnal temperature range (the difference between daily maximum and minimum temperature) further reduced sleep duration, albeit to a lesser degree than high night-time temperature.24 Concerning humidity specifically, several experimental studies previously showed that the combination of high levels of indoor humidity and heat create the worst conditions for sleep.32,84 At high ambient temperature, high levels of humidity compromise the body’s evaporative cooling thermoregulatory response and thus increase the risk of heat stress and hyperthermia, possibly challenging the nocturnal core body temperature decline.47 Although evidence remains sparse, one article included in our review investigated the combined effect of heat and humidity on sleep, finding that higher heat index values progressively reduced sleep duration.24 Functional form of the temperature-sleep association Regarding the functional forms that the temperature-sleep association may take, most studies assumed a linear response but three studies found non-linear associations.24,33,60 For example, one global-scale study observed a monotonic decline in sleep duration as night-time temperature increased, but uncovered an inflection point (i.e., a steeper decline) at 10°C, with progressively larger effects at higher temperatures.24 Interestingly, the same study found that the marginal effect of ambient temperature on sleep loss was over twice as large in the warmest climate regions compared to the coldest areas, consistent with the kinked functional form identified. A second study found a U-shape association between nighttime temperature and wake after sleep onset, as well as an inverse U-shape functional form for sleep efficiency, with the lowest level of wake after sleep onset and higher sleep efficiency in the range 10- 15°C.60 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Potential mechanisms Several researchers have discussed putative causal mechanisms linking temperature and sleep or the role of sleep as a mediator of the association between heat and other health and behavioral outcomes,14,15,85,86 although evidence remains limited in this regard. Two studies inferred that body skin temperature may be one of the mechanisms linking temperature and sleep outcomes in the elderly, with higher skin temperature associated with poorer sleep in studies conducted in Japan and China.71,84 A second study investigated the interplay between temperature, sleep and mental health.34 This last study proposed that sleep loss may be a plausible mechanism explaining the effect of higher temperature on emergency department visits for mental disorders and suicide attempts – both of which exhibited consistent functional forms – but the authors did not perform a proper mediation analysis to test this hypothesis. Another study showed that shorter sleep duration induced by higher temperature may be a mediating factor of the negative association between temperature and cognitive functions.63 However, the mediation analysis was only significant for one of five cognitive outcomes assessed. The authors concluded that a small sample size precluded their analysis from yielding conclusive results about the mediating role of sleep in cognitive effects. Indoor and outdoor temperatures Only two studies included in the systematic review combined measures and simultaneous analyses of indoor ambient temperature in parallel with outdoor temperature.66,70 A first study, performed in The Netherlands, showed that outdoor temperature was no longer associated with self-reported sleep disturbances when also including indoor (bedroom) temperature in their model specification, with the latter being significantly associated with sleep disturbances.66 Interestingly, the second study, conducted in New York, showed that indoor ambient temperature was systematically higher than outdoor temperature, even in summer and with 92 % of the sample reporting air conditioning ownership.70 However, this study focused on the effect of indoor temperature and did not statistically control for outdoor temperature when testing the association with sleep, rendering it impossible to disentangle the effect of indoor versus outdoor temperature on sleep. A third study measured both indoor and outdoor temperature for elderly residents in Shanghai but only included indoor temperatures in the final analysis.71 The range of reported indoor temperatures closely approximated the outdoor range. A separate study found greater sleep loss on days with larger diurnal temperature ranges and cumulatively larger lagged negative effects of outdoor ambient temperature on . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint sleep loss, suggesting that interior environments may trap ambient heat and prolong temperature-related sleep loss.24 Adaptation measures Finally, only a minority of studies investigated behavioral or technological adaptations that might protect sleep from heat. One study, conducted in Sydney, Australia, showed that air conditioning was rarely operated compared to open windows and the use of fans, and that air conditioning did not interact with the study results showing that higher bedroom temperature was associated with poorer sleep.73 Similarly, a separate study found that only a third of elderly participants activated their AC units to cool their rooms at night.71 Despite uniform AC and fan access, higher bedroom temperatures were associated with large reductions in sleep quantity and quality. Another study showed that older adults did not report higher levels of hydration (i.e., drinking episodes) when temperature increased, suggesting that participants lacked adaptive behavioral strategies.72 One study also investigated whether people adapt to night-time sleep impacts with compensatory sleep during the day (napping), week (catch up sleep) or across summer months (intra-annual acclimatization), but did not find any evidence of sleep adaptation.24 This same study also found that residents already living in warmer climate regions were more affected per degree of temperature increase than those living in colder areas, suggestive of limited long-run adaptation. This may indicate an upper threshold for human physiology and appears similar to the pattern observed for the temperature- mortality relationship in Europe.43 Narrative synthesis for available climate change projections Two studies investigated whether warming nighttime temperatures due to climate change would increase the incidence of insufficient sleep in the future.24,33 Obradovich et al. 2017 calculated nighttime temperature anomalies for 2050 and 2099 for the Representative Concentration Pathways “high greenhouse gas (GHG) concentration” scenario (RCP8.5; IPCC) and the United States based on a large empirical self-reported sleep dataset.33 Assuming no further adaptation and that the same functional sleep response persists in the future climate, these authors inferred that climate change may cause between 6 to 14 additional nights of insufficient sleep per 100 individuals by 2050 and 2099, with the greatest increase in climate change-induced nights of insufficient sleep evident in areas of the western and northern United States. Assuming that future adaptation responses do not exceed those observed across the diverse global climate regions examined in the recent historical record, . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Minor et al. 2022 projected the impact of climate change on sleep for two scenarios: the end- of the century GHG stabilization scenario (RCP4.5) and the increasing GHG concentration scenario (RCP8.5), using empirical sleep data from 68 countries.24 Their globally averaged, population-weighted projections indicate that by 2099, sleep loss might vary between approximately 50 hours per year in a stabilized GHG scenario (RCP4.5) to 58 hours under a less plausible increasing GHG scenario (RCP8.5). By the end of the century, the authors separately estimate that individuals might experience 13 (RCP4.5) to 15 (RCP8.5) excess short (<7 hours) nights of sleep per person per year. These last simulations also indicate that global inequalities in the effect of climate change on sleep loss may scale with future greenhouse gas concentrations, with the warmest regions of the world disproportionately impacted. The authors reported that future sleep loss may also be larger for certain demographics that were less represented in their sample composition, referring to their subgroup analyses that found larger marginal effects for residents from lower-income countries (by a factor of approximately 3), older adults (by a factor of 2) and women (~25% higher).24 Discussion Projection studies estimate that, with ongoing climate change, the number of nights with insufficient sleep may significantly increase by the end of the century.24,33 Since the global prevalence of poor sleep is already high, it is crucial to develop a detailed and comprehensive understanding of the effect of temperature on sleep. The present systematic review, which includes 27 original articles, shows that higher outdoor or indoor ambient temperatures, expressed either as daily mean or nighttime temperature, are negatively associated with various sleep outcomes worldwide. This negative effect of higher ambient temperatures on sleep is stronger in the warmest months of the year, among vulnerable populations, notably in the elderly, and in the warmest areas of the world. This result seems consistent across various sleep indicators including sleep quantity, timing or quality and measured via various means including questionnaires, polysomnography, research-grade or commercial activity monitors (see Tables 2 and 3 for a summary of the results). Although the heat-related results are in accordance with those from previous reviews focused on experimental studies manipulating indoor temperatures,26 studies investigating both cold and hot outdoor ambient temperatures have found elevated sleep duration during colder temperatures, suggesting that people may be better at adapting to low ambient temperature than to ambient heat. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Nonetheless, the methodological quality of most studies included in the present systematic review is low (see Table 4). The current literature is notably limited by a relatively poor consideration of key potential individual, spatiotemporal and social confounders. For instance, only 41% (11/27) of the studies statistically adjusted for location-specific seasonality (see Table 4), even though seasonality is associated with changes in daylight, environmental characteristics and behaviors that may also influence or otherwise spuriously associate with sleep. Although the impact of temperature on sleep appears robust – even when controlling for other weather variables (i.e., precipitation, cloud cover, humidity, wind speed, diurnal temperature range), only few studies properly handle these covariates.24,34,62 The negative association between ambient temperature and sleep also remains significant when controlling for adaptation measures such as the utilization of air conditioning, but this should also be further explored.34,65,70,71 Additionally, only 18%  (5/27) of the studies assessed the plausible lagged effects of ambient temperature conditions on sleep in addition to the contemporaneous effect (see Table 4).24,61,64,82 Moreover, the relative importance of indoor and outdoor ambient temperatures remains remarkably unclear and virtually unassessed. According to the only study that accounted for both measures, indoor temperature (i.e., measured in the bedroom), more than outdoor temperature, appeared to drive the relationship between ambient heat and sleep.66 It is worth highlighting that climate change is shifting the underlying distribution of local outdoor temperatures, yet adaptation will continue to transpire both outdoors and indoors. Thus, both ambient temperature measures likely impact human sleep through potentially distinct and/or overlapping pathways that should be investigated in future research, both independently – and where possible – in combination. Similarly, it’s unclear how daytime and nighttime temperatures interact to impact sleep. To our knowledge, only one study tested this effect and found that a higher diurnal temperature range was independently associated with decreased sleep duration.24 These limitations and the quality assessment performed for this review help to draw a set of recommendations for future studies. First, researchers and funding agencies should pursue large-scale cooperative projects leveraging repeated person-level sleep measures (including, but not limited to personal sensing technologies) and longitudinal study designs across larger, and more globally diverse populations, and for longer periods of unobtrusive observation.87 The geographic distribution of studies conducted so far does not cover the global distribution . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint of the human population (Figures 2, 3B). Drawing on these timeseries data, researchers should explicitly control for time-invariant between-individual differences to identify within- person temperature-sleep responses. Second, key spatiotemporally-varying factors should be more consistently controlled in future studies, including daily precipitation, percentage of cloud cover, relative humidity, average wind speed, local climatological conditions for these meteorological variables, and potentially, other relevant ambient environmental factors (e.g., air pollution, day length, etc.).24 Further, for quasi-experimental study designs that seek to identify plausibly causal effects from as good as random variation in ambient temperature fluctuations in real-life environments, researchers should control for location-specific seasonality as well as socio- temporal trends by accounting for day of study-specific shocks due to calendar-induced behavioral changes and macro events that might spuriously associate with both temperature and sleep outcomes. Third, future studies should strive to investigate the effects of indoor versus outdoor temperatures and diurnal versus nighttime temperatures on sleep.66 Fourth, studies should consider the lagged and cumulative effects of temperature and other meteorological variables on sleep outcomes. Fifth, behavioral and technological adaptation measures should be more consistently measured and included in analyses; this includes hydration, behaviors related to sleep hygiene and the utilization of fans or air conditioners.31,63,72 Sixth, non-linearity in the association between temperature and sleep, as well as other meteorological controls, should be systematically inspected and reported.24 Seventh, a priority should be given to vulnerable populations who received scant attention so far, including habitants of low-income countries, individuals with low financial resources within high-income countries, women in peripartum period,88 developing infants and children,79,89 residents living in the tropics (Figure 3B), residents living in extremely cold and hot environments (Figure 3A), incarcerated populations with limited environmental controls, individuals with mental health disorders, and those with sleep disorders such as insomnia and restless legs syndrome.54 Eighth, and as argued before,15,85 more mechanistic studies are still very much needed to both better understand (i) the potential mediators of the temperature-sleep association beyond physiological parameters (e.g., mental health)9, and (ii), although not the main focus of this systematic review, the contribution of sleep issues in the pathway between ambient temperatures and health outcomes (e.g., mortality).29,90 Given the congruence between the recently identified . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint temperature-sleep functional response and temperature-mental health functional forms,11,24,33,34,91 carefully designed field experiments that enable rigorous assessments of mediation while also experimentally shutting down other temperature-sensitive pathways are needed to inform well-targeted policy responses that bolster heat and sleep resilience. Finally, only 30% (8/27) of the studies in this review investigated temperature-sleep relationships across multiple cities or geographic regions,24,33,34,62,65,66,76,82 and only one featured multiple countries.24 Since spatial autocorrelation is likely high within single county and city studies – likely introducing bias to results and interpretation92 – researchers should strive to carry out research investigations and multi-country collaborations across diverse geographic regions while statistically accounting for spatially correlated errors. Fostering adaptation Beyond observational studies, there is an urgent need for interventional studies aiming to foster heat adaptation at different levels, from interventions focused on individuals to environmental and structural modifications.93 At the individual level, evidence-based sleep hygiene measures should be tested to see whether such behavioral measures can foster adaptation to ambient heat. This includes general sleep health measures, such as the avoidance of caffeine, nicotine, alcohol and daytime naps, stress management, sleep timing regularity, management of bedroom noise and artificial light.94 Additionally, heat-specific behavioral adaptations should also be assessed, including cool showers before bedtime, the use of fans (when relative humidity <30%),95,96 water sprays, daytime hydration, reduced bedding, and light cotton clothing.88 Traditional lifestyle interventions, such as the promotion of regular physical activity, are also crucial given the role of physical fitness towards heat adaptation.97,98 These interventions could be implemented through traditional randomized controlled trials or using innovative designs such as just-in-time interventions using weather forecasts that might be particularly relevant for heatwaves.99,100 At the societal level, equitable adaptation should be promoted101. These efforts should ideally be combined with environmental measures such as urban greening,102 urban water features, passive cooling and the improvement of buildings’ insulation and ventilation systems.31,103 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint Conclusion The present systematic review shows that higher temperatures are generally associated with poorer sleep outcomes worldwide. Given the absence of solid evidence on fast sleep adaptation to heat, rising average temperatures induced by climate change pose a serious threat to human sleep and therefore human health, performance, and wellbeing. Although this work identified several methodological limitations of the extant literature, a strong body of evidence from both this systematic review and previous experimental studies converge on the negative impact of elevated temperatures on sleep quality and quantity. Pertinent to policymakers, planners and sleep researchers, the intensity of night-time warming is projected to continue to exceed daytime warming in most populated areas,29,104–106 while urbanization will likely further exacerbate night-time ambient heat exposure for most of humanity.107 Even if these relationships and their associated pathways can be refined further through future well- designed observational studies as we advise here, we argue that interventional studies are now urgently needed to foster adaptation and safeguard the essential restorative role of sleep in a hotter world. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.03.28.23287841
doi: 
medRxiv preprint",1
"Background: Since nursing job is perceived as personally and professionally demanding, internal resources such as resilience and coping skills are essential to improve nurses’ health and wellbeing and therefore work productivity and quality of patient care. Objective: To assess the effect of social support on nurses’ resilience. Moreover, we investigated the impact of demographic characteristics of nurses on their resilience. Methods: We conducted an on-line cross-sectional study in Greece. Data were collected during October 2022. We used the Multidimensional Scale of Perceived Social Support to measure social support, and the Brief Resilience Scale to measure resilience. We measured the following demographic characteristics of nurses: gender, age, self-perceived health status, COVID-19 diagnosis, MSc/PhD diploma, and clinical experience. Results: Study population included 963 nurses with a mean age of 37.9 years. Nurses experienced moderate levels of resilience and high levels of social support. Multivariable linear regression analysis identified that increased significant others support and increased friends support were associated with increased resilience. Moreover, we found a positive relationship between age and resilience. Also, nurses with good/very good health had higher levels of resilience compared to nurses with very poor/poor/moderate health. Finally, resilience was higher among nurses with MSc/PhD diploma. Conclusions: We found a positive relationship between social support and resilience among nurses. Understanding of factors that influence nurses’ resilience can add invaluable knowledge to develop and establish tailored programs. Peer support is essential to improve nurses’ resilience and promote patient healthcare. Keywords: nurses, social support, resilience, Greece . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 4 Introduction A high demand job as nursing in combination with nursing shortage decrease job satisfaction and increase stress and burnout levels (Gi et al., 2011; Toh et al., 2012). Moreover, psychological issues such as depression, anxiety, stress, sleep disturbances, and burnout are prevalent among nurses especially during the COVID-19 pandemic (Al Maqbali et al., 2021; Chen & Meier, 2021; Galanis et al., 2021). Since nursing job is perceived as personally and professionally demanding, internal resources such as resilience and coping skills are essential to improve nurses’ health and wellbeing and therefore work productivity and quality of patient care. Resilience is the ability to bounce back from adverse situations (Jackson et al., 2007). Especially, in case of nursing job resilience is related with subjective motivation to cope, self- efficacy, and emotion regulation (Stacey et al., 2019). Formal and informal support can assist nurses to promoting resilience and coping skills (Gillman et al., 2015). Nurses should be aware of the coping strategies in order to promote their resilience (Dahl et al., 2022). Moreover, there is a positive correlation between compassion satisfaction and resilience among nurses (Unjai et al., 2022). Several strategies can be implemented in order to improve nurses’ resilience, such as improved connections within the professional team, educational programs to develop behaviors that help nurses to control or limit their stress, and tailored interventions to support nurses’ wellbeing (Gillman et al., 2015; Unjai et al., 2022). Social support is defined as the protection and assistance given by family members, significant others and friends. A recent systematic review found a positive relationship between social support from supervisors and coworkers and burnout syndrome in nurses (Velando-Soriano et al., 2020). Several other studies confirm the positive effect of social support on nurses’ mental health such as anxiety, depression, stress, hypochondriasis, and quality of life (Chen & Meier, 2021; Li et al., 2022; Yan . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 5 et al., 2022). Moreover, social support improves positive attitudes among nurses such as COVID-19 booster vaccination willingness (Galanis et al., 2022). Until now, only three studies in Haiti, Taiwan, and China has investigated the impact of social support on nurses’ resilience (Caton, 2021; Hsieh et al., 2017; Wang et al., 2018). Therefore, no studies in developed countries have been conducted on this issue. Thus, the aim of our study was to assess the effect of social support on nurses’ resilience. Moreover, we investigated the impact of several demographic characteristics of nurses on their resilience. Methods Study design We conducted an on-line cross-sectional study in Greece. Data were collected during October 2022. First, we used the Google forms to create an on-line version of the study questionnaire. Then, we disseminated the study questionnaire through the social media. Nurses that are working in healthcare services can participate in our study. We measured the following demographic characteristics of nurses: gender (females or males), age (continuous variable), self-perceived health status (very poor, poor, moderate, good or very good), COVID-19 diagnosis (no or yes), MSc/PhD diploma (no or yes), and clinical experience (continuous variable). We used the Multidimensional Scale of Perceived Social Support to measure social support that nurses receive (Zimet et al., 1988). The scale consists of 12 items and includes three factors: family support, friends support, and significant others support. Each factor takes values from 1 to 7. Higher values indicate higher levels of support. In our study, Cronbach’s alpha for the three factors ranged from 0.808 to 0.928. Also, we measured nurses’ resilience with the Brief Resilience Scale (Smith et al., 2008). The scale includes six items and total resilience score ranges from 1 to 5. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 6 Higher values indicate higher levels of resilience. In our study, Cronbach’s alpha for the scale was 0.848. Ethics We did not collect personal data of nurses. Also, we informed nurses about the aim and the design of our study and they gave their informed consent. The study protocol was approved by the Ethics Committee of Faculty of Nursing, National and Kapodistrian University of Athens (reference number; 417, 7 September 2022). Also, we applied the guidelines of the Declaration of Helsinki in our study. Statistical analysis We use numbers and percentages to present categorical variables. Also, we use mean, standard deviation, median, minimum value and maximum value to present continuous variables. We calculated Cronbach’s alpha to estimate reliability of the questionnaires. Social support and demographic characteristics of nurses were the independent variables, while resilience was the dependent variable. We performed univariate and multivariable linear regression analysis to assess the impact of independent variables on nurses’ resilience. We presented unadjusted and adjusted coefficients beta, 95% confidence intervals (CI) and p-values. P-values less than 0.05 were considered as statistically significant. We used the IBM SPSS 21.0 (IBM Corp. Released 2012. IBM SPSS Statistics for Windows, Version 21.0. Armonk, NY: IBM Corp.) for the analysis. Results Study population included 963 nurses with a mean age of 37.9 years (standard deviation=9.6). The majority of nurses was females (88.4%) and had a good/very good level of health (88.3%). Among nurses, 71.8% have been diagnosed with COVID-19 during the pandemic. More than half of nurses (54.6%) possessed a . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 7 MSc/PhD diploma. Mean years of clinical experience was 12 (standard deviation=9.2). Table 1. Demographic characteristics of nurses. Characteristics  
N 
% Gender Females  
851 
88.4 Males  
112 
11.6 Age (years)a 
37.9 
9.6 Health status Very poor 
26 
2.7 Poor   
16 
1.7 Moderate 
70 
7.3 Good  
580 
60.2 Very good 
271 
28.1 COVID-19 diagnosis No  
272 
28.2 Yes  
691 
71.8 MSc/PhD diploma No  
437 
45.4 Yes  
526 
54.6 Years of clinical experiencea 
12.0 
9.2 a mean, standard deviation Table 2 presents descriptive statistics for the scales in our study. Nurses experienced moderate levels of resilience and high levels of social support. Moreover, significant others support was higher (mean=6.15) than family support (mean=6.00) and friends support (mean=5.86). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 8 Table 2. Descriptive statistics for the scales. Characteristics  
Mean 
Standard deviation Median Minimum value Maximum value Resilience  
3.45 
0.65 
3.5 
1 
5 Family support 
6.00 
1.42 
6.5 
1 
7 Significant others support 
6.15 
1.26 
6.8 
1 
7 Friends support 
5.86 
1.38 
6.3 
1 
7 Predictors of nurses’ resilience are shown in Table 3. Multivariable linear regression analysis identified that increased significant others support (b=0.106, 95% CI=0.059 to 0.154, p<0.001) and increased friends support (b=0.047, 95% CI=0.007 to 0.087, p=0.02) were associated with increased resilience. Moreover, we found a positive relationship between age and resilience (b=0.012, 95% CI=0.003 to 0.021, p=0.007). Also, nurses with good/very good health had higher levels of resilience compared to nurses with very poor/poor/moderate health (b=0.245, 95% CI=0.154 to 0.335, p<0.001). Finally, resilience was higher among nurses with MSc/PhD diploma (b=0.194, 95% CI=0.113 to 0.275, p<0.001). Table 3. Univariate and multivariable linear regression analysis with nurses’ resilience as the dependent variable. Independent variables  
Univariate model 
Multivariable model Unadjusted coefficient beta (95% CI) P- value Adjusted coefficient beta (95% CI)a P- value Gender (females vs. males) 
-0.099 (0.228 to 0.030) 
0.131 
-0.018 (-0.149 to 0.113) 
0.786 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 9 
 
Age (years) 
0.003 (-0.001 to 0.007) 
0.202 
0.012 (0.003 to 0.021) 
0.007 Health status (good/very good vs. very poor/poor/moderate) 0.323 (0.234 to 0.413) 
<0.001 
0.245 (0.154 to 0.335) 
<0.001 COVID-19 diagnosis (yes vs. no) 
-0.069 (-0.161 to 0.023) 
0.139 
-0.058 (-0.147 to 0.032) 
0.208 MSc/PhD diploma (yes vs. no) 
0.171 (0.089 to 0.253) 
<0.001 
0.194 (0.113 to 0.275) 
<0.001 Years of clinical experience 
0.002 (-0.002 to 0.007) 
0.318 
-0.006 (-0.015 to 0.003) 
0.169 Family support 
0.089 (0.060 to 0.117) 
<0.001 
-0.022 (-0.060 to 0.016) 
0.252 Significant others support 
0.151 (0.120 to 0.183) 
<0.001 
0.106 (0.059 to 0.154) 
<0.001 Friends support 
0.118 (0.089 to 0.147) 
<0.001 
0.047 (0.007 to 0.087) 
0.020 CI: confidence interval a p-value for ANOVA<0.001; R2 for the final multivariable model was 13.5% Discussion We conducted a cross-sectional study in Greece to investigate the impact of social support on nurses’ resilience. To the best of our knowledge this is the first study on this issue in the developed countries. Additionally, we examined the impact of several demographic characteristics on nurses’ resilience. We found that significant others support and friends support improves nurses’ resilience. Literature supports this finding since three studies in China, Haiti, and Taiwan found a positive relationship between social support and resilience (Caton, 2021; Hsieh et al., 2017; Wang et al., 2018). In particular, Wang et al. (2018) found that coworker support improves nurses’ resilience in a sample of 747 nurses in China. Also, Hsieh et al. (2017) identified a positive relationship between family and peer support and resilience, while Caton (2021) found the same relationship between social support and resilience. Social support improves the ability of nurses to better adapt to difficult conditions at the personal and professional level. Moreover, social support gives nurses the opportunity to manage effectively negative feelings that might experience during their work (Kılınç & Sis Çelik, 2021). Especially, support from . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 10 significant others and friends decrease stress and anxiety that nurses experience when they work with their colleagues under the stressful environment of hospital. Moreover, older nurses can help younger colleagues to better cope with difficulties in clinical context by providing them professional help and emotional support (Welsh, 2014). Thus, peer support increases nurses’ resilience enabling them to deal with their work more effectively and better manage demanding work circumstances (Liu et al., 2018). Among demographic characteristics, age, health status, and educational level were associated with resilience. In particular, increased age and educational level were associated with increased resilience. Also, resilience was higher among nurses with better health status. Literature supports our findings since several studies found that older age was associated with higher resilience (Cohen et al., 2014; Terrill et al., 2016; Weitzel et al., 2021). For example, a recent study in Germany found that 20% of adults over 65 years old had high resilience (Weitzel et al., 2021). Moreover, individuals whose health status was very good/excellent were more probable to exhibit higher levels of resilience compared to individuals whose health status was poor/fair/good (Hopkins et al., 2015; McKibbin et al., 2016; Soundararajan et al., 2023). Our study had several limitations. First, we collected data through a self-administrated questionnaire. Thus, an information bias is probable in our study. Second, we used a convenience sample and therefore it is less likely to be representative of the population of nurses in Greece. Third, we performed an on-line study through social media. Therefore, selection bias is probable since nurses without accounts on social media cannot participate in our study. Fourth, we conducted a cross-sectional study and a causal relationship between social support and resilience cannot be established. Fifth, we measured several demographic characteristics of nurses but many others could be examined in future research. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 11 In conclusion, we found a positive relationship between social support and resilience among nurses. Moreover, older nurses, those with better health, and those with higher educational level had higher levels of resilience. Since nurses work under demanding and stressful circumstances policy makers should develop a harmonious working environment in order to improve resilience and thus job productivity. Understanding of factors that influence nurses’ resilience can add invaluable knowledge to develop and establish tailored programs. Peer support is essential to improve nurses’ resilience and promote patient healthcare. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288089
doi: 
medRxiv preprint 12",1
"Background Debriefing is a team discussion in a constructive, supportive environment. Barriers exist to consistent, 
effective, operative team debriefing. Aim To identify barriers to debriefing and their potential solutions as articulated by operating room personnel. Design Qualitative study. Methods Between December 2021 and February 2022 we interviewed operating room workers in a tertiary 
children’s hospital. We used purposive sampling to interview a variety of professions and specialties who 
work in the operating room environment. Interviews were audio-recorded, transcribed, and coded. The 
qualitative approach was reflexive thematic analysis with the theoretical framework was critical realism. Results Interviews were analysed from 40 operating room staff: 14 nurses,7 anaesthetic technicians,  7 
anaesthetists, and 12 surgeons; 25 (62%) were female. The five key themes were: 1) “commitment to 
learning” − healthcare workers are committed to teamwork, quality improvement, and teamwork; 2) “it’s a 
safe space” – psychological safety is a pre-requisite for, and is enhanced by, debriefing; “natural born 
leader” − the value of leadership and also the limitations caused by psychological constructs about what 
and who is a leader; 4) “space-time” – finding time to debrief after routine operations and after critical 
events; and 5) “doing the basics well” – debriefing needs structure without being over-complicated. Conclusion Psychological safety is both a prerequisite for and a product of debriefing. Leadership, if viewed as a 
collective responsibility, could help break down power structures. Given the results of this study and 
evidence in the literature, it is likely that routine debriefing, if well done, will improve psychological safety, 
facilitate team learning, reduce errors, and improve patient safety. Relevance to Clinical Practice Debriefing is challenging to perform, requires leadership and training, but is worth the effort. Nurses can 
take a leading role in promoting routine debriefing in healthcare. What does this paper contribute to the wider global clinical community? •
Debriefing, if done well, promotes teamwork, psychological safety in the workplace, quality and 
safety, and organisational learning. •
Setting up an effective debriefing programme is challenging but worth the effort. •
Debriefing requires collaboration and nurses are well placed to be leaders in debriefing policy 
design and to lead multidisciplinary debriefs. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 3 Introduction & Background Surgery is a high-risk undertaking in which professionals from a variety of specialties work together on a time-critical operation in which safety is of the utmost importance. Similar to teams in aviation, motor racing and sport, surgical teams can be described as action teams. Where surgical teams differ is the diversity of roles within the group: nurses, anaesthetists, technicians, surgeons, from students to seniors. To perform effectively, team members must feel safe about sharing observations and opinions with the rest of the team (A. C. Edmondson, 2019); however, diverse surgical teams are often beset by intergroup rivalries based on profession, gender, and seniority. The resulting power dichotomies inhibit effective teamwork. One way to counter this problem with surgical teams may be through regular team debriefing. Debriefing is a process that allows individuals to discuss team performance in a constructive, supportive environment (Fanning & Gaba, 2007). Debriefing has been linked to improved performance, teamwork and communication, and can assist with error identification (Zuckerman et al., 2012). Debriefing is one component of the World Health Organisation Surgical Safety Checklist (SSC). Initially a three-point checklist involving a sign-in, time-out, and sign-out (Haynes et al., 2009), the SSC was expanded to include five steps by adding a briefing discussion at the beginning of the operating list, and a debriefing at the end of the list (Vickers, 2011). The sign-in, time-out, and sign-out are now performed with a high level of compliance in operating rooms throughout the world (Abbott et al., 2018). In our tertiary paediatric hospital, compliance with the SSC is 100% but compliance with debriefing (which is not mandated at the present time) is <10% (personal communication, Starship Children’s Hospital Operating Rooms, 2022). Other authors have also found debriefing challenging to implement (Brindle et al., 2018), possibly because it involves a coordinated team discussion rather than a simple checklist, or for reasons related to power dynamics. Debriefing has roots in experiential learning theory (A. Y. Kolb & Kolb, 2006) and is essential to team learning (D. A. Kolb, 2015; Kolbe et al., 2020; Vashdi et al., 2013). Given that debriefing is central to other fields including medical simulation (Fanning & Gaba, 2007; Raemer et al., 2011), the aviation industry (Mavin et al., 2018), the space industry (Landon et al., 2018), and the military (Crane et al., 2006), we appear to be missing out on debriefing’s benefits in clinical healthcare. Our surgical unit consists of seven operating rooms within a tertiary referral paediatric hospital providing cardiac, ear nose and throat (ENT), orthopaedic, thoracic, urology, and general paediatric surgical services. We introduced debriefing into our operating rooms in 2017. Despite the positive feedback, debriefing did not become routine practice throughout our operating rooms, being performed in only a minority of operating lists. Given the potential value of debriefing for surgical teams but the challenges of putting it . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 4 into practice, we were interested in the views of our operating room staff on the how, why, when and where to debrief. The purpose of this study was to gain insights from frontline workers on how to set up an effective debriefing policy for our operating room suite, to explore operating room workers’ experiences with debriefing, and to critically reflect on the meaning of debriefing to psychological safety, hierarchy, and teamwork. Methods Ethics and Trial Registration This study was registered with the institutional research office and received ethical approval from the Auckland Health Research Ethics Committee (#3228). Context The setting was the operating room of a children’s hospital. Operating room workers had some experience with debriefing since it was informally introduced in 2017. A simple debriefing structure was developed which consisted of three phases: 1) introductions and ground rules (confidentiality, respect, share the air- time); 2) discussion (what went well, what could have gone better); and 3) take home actions. This structure was used mostly in paediatric surgery and in ear, nose and throat operating lists. Paediatric cardiac surgery regularly debriefed all operations using an extended checklist structure. Apart from cardiac surgery, debriefings were performed inconsistently and were not part of the operating room culture. Researcher Characteristics & Reflexivity Researchers brought together a mix of experience and fresh eyes on the subject. Two researchers were senior nurses who had been instrumental in introducing debriefing in 2017. At the time of the study, one was a senior operating room nurse (LH) and the other was the nurse manager of perioperative services at Starship Children’s Hospital (NM). One researcher was a psychology masters graduate with background in neuroscience (MM). Two researchers were medical students (CM and ES). Two researchers were doctors: JW is an anaesthetist with extensive experience in simulation training, debriefing and multidisciplinary teamwork; and JH is a paediatric surgeon with experience in debriefing in simulation training and in operating room. CM and ES performed all interviews; as students, they came from a more independent stance than some of the other researchers who were co-workers and colleagues of the participants. Theme development and critical interpretation was performed by a student (CM), psychologist (MM), and operating room worker (JH), bringing a variety of perspectives on the topics. Qualitative Approach & Research Paradigm . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 5 The qualitative methodology used for this research was reflexive thematic analysis (Braun & Clarke, 2022a). The theoretical framework was one of critical realism (Bhaskar, 2013). According to Pilgrim (Pilgrim, 2014), in critical realism “both causes and meanings are given due weight in social scientific enquiries and the entwined relationship between facts and values emphasised.” According to Yucel (Yucel, 2018), critical realism “avoids many of the problems of positivism and social constructivism by finding a middle ground between them.” Operating room personnel work in a technological environment. We wanted to “give voice” to what people told us in the interviews while recognising that how we understand these “truths” is focused through the psychosocial lens of the interviewers and the interviewees. As operating teams, we deal in reality –  “life and limb” – so reality matters; however, we recognise that social constructs determine how team members relate to each other, hence the reason for choosing a critical realism paradigm. Our purpose was to shed light on the barriers to debriefing and to identify factors that might facilitate routine debriefing. Critical realism and reflexive thematic analysis also allowed us to recognise the transitive nature of the research where the researchers are part of the study. Sampling Strategy & Data Collection The inclusion criteria were operating room staff including nurses, anaesthetic technicians, anaesthetists, and surgeons, who worked regularly in the children’s operating suit. Excluded were students and other temporary visitors to the operating rooms. The sampling strategy was purposive, with care taken to interview a range of professions and specialty groups. The sample size was determined pragmatically by the number of interviews able to be performed over the study period, December 2021 to February 2022, but also guided reflexively as recommended by Braun and Clarke (Braun & Clarke, 2021a, 2022a), and Malterud’s “information power” concept helped guide the decision as to when to stop recruiting (Malterud et al., 2016).  Our considerations in determining the information power of our sample were: 1) the aims of this study was quite broad, favouring a larger sample; 2) the specificity of the sample was relatively broad, favouring a larger sample; 3) there was no established theory, favouring a larger sample; 4) the quality of the dialogue in our interviews was strong, favouring a smaller sample; and 5) the analysis was across cases, favouring a larger sample. Operating room staff were made aware of the project through presentations at meetings. Two researchers (CM and ES) invited participants to take part in an interview. Participants gave written informed consent. Interviews were conducted between December 2021 and February 2022. All interviews took place in person in a private space using a semi-structured interview guide (available in Appendix 1 in supplementary material). The interview guide was developed by the researchers to cover the items of interest based on our experience with surgical debriefing (e.g., issues of timing and structure). Limited testing of the interview guide was undertaken by mock interviews between three researchers. Interviews began with . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 6 making personal connections and explaining the study, then proceeded with open-ended questions with an emphasis on letting the interviewee tell their story and do most of the talking. Interviews were audio recorded using a mobile device and transcribed verbatim. Participants received a copy of their transcript for editing and approval. Transcripts were de-identified and labeled with a random number generated from the random sequence generator service of random.org (Haahr, 2023). Data Processing & Analysis We followed the process of reflexive thematic analysis described by Braun and Clarke (Braun & Clarke, 2022b). Briefly, coding and theme development were recursive processes involving immersion in data, review of relevant literature, and deep reflection. Immersion in the dataset was by transcribing (performed by CM and ES), reading, and re-reading interview transcripts. Two researchers (CM and JH) coded the data separately and met regularly to discuss and mould themes. The online version of Taguette (Rampin & Rampin, 2021) was used as an aid to coding. Taguette is an open source tool used to import transcripts, highlight and tag quotes. We drew mind maps to help visualise the diversity and connectedness of themes as they developed. We took both a semantic and latent approach to coding, meaning that some codes came directly from what interviewees said (semantic) while we developed other codes based on our understanding of teamwork and psychological safety (latent). While some of our analysis was inductive (coming straight from the data) much was deductive (our interpretation of the data as it related to debriefing in keeping with the “critical” aspect of the critical realism paradigm). Through a process of team discussion and reflection, we refined codes into initial themes. Mapping themes graphically helped demonstrate connections and interactions between initial themes. Through a process of reflection, discussion, re-reading transcripts, reflecting on initial themes, refined and renaming, we developed the main themes of the research. Techniques to Enhance Trustworthiness To enhance trustworthiness of this report, we took a lead from Braun and Clarke’s guidance on quality and reporting (Braun & Clarke, 2021b) and from the guidelines for publication of qualitative research by Elliot et al. (Elliott et al., 1999). First, we applied Braun and Clarke’s 15-point checklist of criteria for good thematic analysis to our study (Braun & Clarke, 2006) as shown in Appendix 2 in supplementary material. Next, we used Nelson’s conceptual depth scale to self-evaluate robustness of our conceptual categories (Nelson, 2017) as shown in Appendix 3 in supplementary material. We used Braun and Clarke’s 20 questions for evaluating thematic analysis manuscripts for publication (Braun & Clarke, 2021b) as shown in Appendix 4 in supplementary material. The present report follows the Standards for Reporting Qualitative Research (O’Brien et al., 2014) as shown in Appendix 5 in supplementary material. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 7 Results Of 41 interviews conducted, one recording failed leaving 40 interviews to be transcribed and analysed. Interviewees were predominantly female (n = 26, 65%). Interviewee characteristics are shown in Table 1. Each interview lasted approximately 10−20 minutes. Interview transcripts amounted to 84748 words, an average of over 2,100 words per transcript. A map of initial themes is presented in Fig. S1 in supplementary material. Through a process of reflection, discussion, and re-reading transcripts, we refined the initial themes to five final themes. The development of these themes is shown in Fig. 1. A final table of themes is presented in Table 2. In the following paragraphs, we illustrate each theme with selected quotations from the interviews. Theme 1. Committed to learning A strong theme was how committed frontline workers are to learning and quality improvement. Learning was seen as one of the most important things we do in healthcare and debriefs were seen as a vehicle for learning. “It’s just a way to kind of come together with the team at the end of the day and talk about any issues and try and think of possible solutions, so that when we do this, when we do the list again, we can work better and not have those issues.” (Participant 10) “Sometimes there’s quite good key learning points and from theatre, well if every theatre did a decent debrief at the end of the day, I think that there might be things that might come up consistently, repeatedly, that actually we can learn from – learning debriefs…” (Participant 36) “Is there anything that we can learn from this, you know that’s almost the most important thing in modern medicine and healthcare, isn’t it? You know it's, ‘what can we do better next time so that we can avoid this situation?’” (Participant 4) It was striking that staff are not just committed to individual and team learning, they want to see organisational change too. They wanted debriefs to be incorporated into a quality improvement process so that issues raised were dealt with at an organisational level. If debriefs did not lead to quality improvement, then they were seen as a waste of time. Conversely, if debriefs lead to learning they were seen as immensely valuable. “It’s kind of like you just get stuck in Groundhog [Day] with the same − you’re not coming to a conclusion or fixing the problem; it’s just you’re continuously talking about it.” (Participant 26) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 8 “… although if there is any big issue, I think if there's anything that needs improving, it could be good to record… otherwise it won't get fixed.” (Participant 13) “Can you close that? Can you come back to us in two weeks to know how you progress?” (Participant 14) “I’ve found it absolutely one of the most valuable things that we’ve had instituted here.” (Participant 38) Theme 2. It’s a Safe Space The importance of being psychologically safe in the team was a strong theme throughout the interviews. Psychological safety was promoted by debriefing, but also a pre-requisite for effective debriefing. “And then also just making sure that if someone does say something they’re not going to be shot down or they’re not going to be – yeah, just making sure it’s a safe space to do so.” (Participant 11) Debriefing Promotes Psychological Safety Interviewees felt that consistent debriefing would facilitate better communication, help flatten the hierarchy, and create a more inclusive work environment. Many participants recognised the importance of acknowledging positives during debriefs. Positive feedback is an effective way to encourage speaking up. Developing a structure that makes space for positives (“what went well”) was helpful. Several respondents commented that debriefing is a good way to unpack what had happened during the day, unwind a bit, and to go home leaving work at work, which would clearly carry co-benefits for mental health and resilience. Debriefing critical events was also seen as essential for people’s mental health. “By doing briefing and debriefing it actually helps to break down barriers between different subgroups.” (Participant 2) “So sometime, often when the list is run really well, I think it’s really important to acknowledge that too, everybody to kind of be grateful to each other.” (Participant 36) “I think the debriefing actually finishes off the day and creates a good work environment and lets people kind of put work aside.” (Participant 36) “Everybody responds very differently to trauma and to, you know, critical incidents, and sometimes people just need that little bit of help and support to sort of express how they feel. Because otherwise you may end up losing these people because they just don’t want to do this job anymore.” (Participant 4) Psychological Safety Facilitates Debriefing . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 9 Operating room staff needed to feel they were in a safe place to debrief effectively. Safety took various forms: assurance of confidentiality, a non-judgmental and respectful environment, and a flat hierarchy. People need to feel safe from being judged. Debriefs need to be conducted in a climate of respect. Creating an environment in which everyone, including inexperienced staff and students, can feel free to speak up is a challenge when there is an obvious hierarchy arrangement. There was an interplay between the tone set in the pre-operative briefing and that of the post-operative debriefing, and an interplay with the general culture of the operating room environment. Responses suggest that the attitude and approach to preoperative briefings impacts on the postoperative debriefings. Most interviewees felt that psychological safety is largely determined by broader operating room culture and attitudes in the workplace. “I think that starts from before the brief. So, I think having everyone there from the beginning, having an expectation that there’s a flattened hierarchy, we would welcome people speaking up, and that we use individual names, we introduce each other at the brief, we highlight how important each team member is.” (Participant 36) “In terms of like if people want to – anyone can say something, like it’s not a judgmental environment.” (Participant 26) “I think also that needs to be emphasised that it’s confidential.” (Participant 38) “I guess sometimes that’s where there might be issues with people not wanting to speak up if they feel that there’s a hierarchy.” (Participant 22) “It's a longer-term thing... I think there’s an old-school way of practicing that you might not have even experienced that I experienced as a junior doctor – it’s fading out where there’s quite a lot of barriers to communication with senior people. And I think that’s improving, definitely, but I think it’s something that we generate every day through our interactions with other staff members. So I think it’s on the ward, it’s in pre-op, it’s in the clinic, it’s in the operating room, you just want to have an open, positive environment for discussion.” (Participant 28) Theme 3. Natural born leader The theme of leadership considers interviewee perspectives on nominating a leader of the debrief session. Responses indicate that leadership is necessary to facilitate an efficient and effective debrief. In the following extract, an interviewee discussed options for leadership. “I think that’s a difficult one, actually. Because it could be different people on different days…  it shouldn’t really be a set person every time. Because sometimes the surgeons change, sometimes the nurse in charge will pop out and then come back at the end. So really, there should be a natural . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 10 leader that emerges, and sort of leads the debrief. But probably the person that needs to instigate it should be the nurse in charge, because otherwise surgeons might just forget to do it. So, it should be instigated by the nurse in charge, but then the natural leader should just do the chatting…” (Participant 18) Surgical Leadership in Surgery Many interviewees viewed senior doctors, the surgeon or the anaesthetist, as the main leaders in the OR. “In my experience, I think it's best to have a surgeon or anaesthetist be onboard and leading it. I feel like they're more of like a leader in the room and… in my experience people appear a lot more engaged when it's an anaesthetist or a surgeon or someone quite senior in the room to be leading it.” (Participant 31) Nursing Leadership is Structured and Comprehensive Other interviewees felt that nurses were best suited to provide a comprehensive overview of operating room events and tended to follow a more structured and systematic approach. “The experience that generally I have is that the nurse that's coordinating the list for the day runs it, and I think that allows for a lot more of an inclusive approach.” (Participant 20) It’s About Skills Many interviewees felt that anyone within the multidisciplinary operative team should be able to lead an operative team debrief. Responses suggest that an individual’s experience and communication skills may be more relevant to a leadership position than their title or specialty. Therefore, the leadership role can be flexible and negotiated. “I don't think it should be any particular person, I don't think. But I think the person needs to be able to lead it, so they need a sense of confidence, and they need to be able to understand when the most appropriate time is for debriefing… So, you have to be experienced enough to know − pick up on those sorts of things, but also subtle clues as well that maybe something is going on, it’s not quite the right time. So, you need a bit of experience I think to lead the debrief, and very much so, confidence. So, you need to be a little bit more experienced, but I don't think necessarily it should be one person's role.” (Participant 7) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 11 Theme 4. Space-time There was little debate about the right place to debrief − consistently people found the practical place to be in the same operating room where the case(s) had been performed. Finding “space” in the sense of time in the day, however, was a challenge. One Foot out the Door − Routine Debriefing Some participants suggested that debriefing should occur whilst closing the skin of the final case. Interviewees felt that this approach would be the most achievable way to ensure that all team members are still present and engaged. At this point, the most challenging parts of the operative procedure have been completed, and the anaesthetic team is yet to begin waking up the patient. Responses suggest that during this time window, team members were still focused on the current session, which may be optimal for recalling learning points from the preceding list. Additionally, this time avoids the challenge of trying to bring team members back, which is practically challenging after the list is finished. “Because pretty much as soon as we start closing people start wandering off, and the surgeon has already got one foot out the door by the time the registrar or the fellow gets on to closing the skin layer. So I would have thought towards the end of the last operation would be the best time to catch everyone who was in that session without delaying people who want to get away at the end of the day.” (Participant 6) Other participants felt that the ideal time to debrief is at the end of the list, after the patient has been taken to recovery. Responses suggest that this approach is likely to be associated with fewer distractions and result in greater focus on the debrief itself. Interviewees also felt that because the whole list is finished, the debrief would be able to cover all aspects of the day, including the closing of the final case. Responses revealed the main drawback to this approach is the challenge to get all team members back to the operating room for a debrief. “I really think it should be done at the very end, when the patient has left the room. Because, and everyone should be, you should sort of say at the end of the case. ‘we’re going to do a debrief, so can we all meet back here in ten minutes’. And then it just means that everyone’s mind is on that rather than on the patient. And it also makes it a bit more, sort of, real, if everyone’s just focusing on that rather than focusing on other things.” (Participant 18) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 12 Time to Debrief After a Critical Event Debriefing critical events was seen as valuable but, again, there was a timing debate. Some believed that critical event debriefs were best performed straight after the event (“hot”); others felt that critical event debriefs should wait until things settle down (“cold”). “If there's something happened, if there's something wrong, like baby died on the table or something happened like an emergency or crisis that happens on the table, it should be right after. Not tomorrow, not the next day. Yeah, like on the day when everyone's still around and you don't have to look for them.” (Participant 41) “It should probably be dealt with a little bit later in a setting where everybody feels safe. And preferably the timing, depending on the personalities and how well you know the people, to be able to give a bit of time for people to be able to process it after a case, personally.” (Participant 5) All the Time One facet of the collective effort theme is that debriefing should be done consistently across all teams. This creates a universal standard of practice. Interviewees felt that collective support would be necessary to facilitate this cultural change. “The more they see people doing it... the more they feel that they can contribute. Because they’ve seen it before, it becomes more familiar, more routine, and they realise that there’s no negative repercussions for it.” (Participant 33) “It actually breaks down those barriers, and people feel that they can ask those questions... by doing them consistently is actually how we break down the barriers as well.” (Participant 2) Theme 5. Doing the Basics Well Interviewees felt that debriefing techniques should be kept simple but implemented in an effective and efficient manner. There was clearly a need for some framework in which to construct a purposeful conversation. One aspect of structure was the “reactions” or feelings phase which commonly occurs near the start of simulation debriefs. “Try to keep doing the basics well. We’re not overcomplicating too much stuff; I think that would be the way to go.” (Participant 14) “I think it needs to be structured because otherwise it descends into waffle and stories and anecdotes and opinions rather than actually exchanging information that is relevant to how the day went and to use our time efficiently and particularly at the end of the day… I do think that it’s better . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 13 having some structure, and it being led by someone with a particular interest in getting some useful information.” (Participant 6) “It also gives people an opportunity to tell how they’re feeling although often people won’t necessarily say how they’re feeling, but I think that’s the thing of a good debrief is to make people feel comfortable to do that and I think it’s good to do it even when everything went smoothly and everything went fine.” (Participant 38) No-one is Born Knowing how to do this Some participants expressed the need for teaching and modelling of debriefing skills. Critical event debriefing would be easier if there was a culture of routine debriefs and if staff had training in how to facilitate a debrief. Other interviewees expressed that ongoing training and education would be useful for debrief leaders. “No one is born knowing how to do this, you actually do need to have some training.” (Participant 14) “I think that hot debriefs are very difficult, and you need to be very very careful that they’re done safely. I wonder if we could... have a format and have a culture of how those should be carried out and maybe some specific people that might champion doing the hot debriefs that I think everyone should probably have some training on doing it safely.” (Participant 36) “I think some form of support for the people that lead the debrief would be useful, as well as teaching of certain skills saying that you know like, when we do this, these are the aims, and let's try to keep things running that way.” (Participant 14) Discussion By giving voice to frontline workers, we have attempted to understand what is important when working within an operating room environment. The themes reflected a rich sense of the value of debriefing for learning, culture, and togetherness as a team. Debriefing’s influence on a culture of gratitude and psychological safety came through strongly, as did the contribution debriefing could make to team performance. Our research has established a foundation for various factors that relate to psychological safety including power hierarchies and perceptions of leadership. Committed to Learning The commitment to quality, safety, and organisational learning by frontline staff is not always appreciated and organisations can miss opportunities to tap into the intellectual value of employees. The literature on . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 14 commitment of healthcare workers to organisational learning is sparse. In a qualitative study of frontline healthcare workers in London, Lalani et al. (2020) found that “participants from across health and social care expressed their ambition to work collaboratively and maximise opportunities for formal and informal learning” (Lalani et al., 2020). Fathy Ahmed et al. (2023) demonstrated a strong positive correlation between goal orientation in staff nurses (commitment to learning and performance) and organisational learning culture (dialogue, team learning, collaboration, empowerment, etc.) and recommended that nurses are empowered to participate in decision making, problem solving, and in achieving organisational goals (Fathy Ahmed et al., 2023). Our research showed that people working in theatre are committed to learning and improvement. Debriefs were seen as a vehicle for individual and team learning, with the potential to contribute to organisational change. Participants saw debriefs as a way to identify key learning points and improve the quality of care. However, they wanted debriefs to be incorporated into a quality improvement process to ensure that issues raised were dealt with at an organisational level. If debriefs did not lead to quality improvement, then they were seen as a waste of time. Overall, committed staff in healthcare are eager to learn and improve, and organisations would do well to value their input and incorporate their suggestions into quality improvement processes. Psychological Safety Our study uncovered a prominent theme concerning psychological safety in the theatre environment, which was viewed by participants as paramount for effective debriefing. Despite this, we found concerning evidence that some theatre staff members did not perceive themselves to be in a psychologically safe working environment. According to Edmondson (2008), psychological safety ensures that individuals are not punished or humiliated for expressing ideas, questions, concerns, or mistakes. Our study reveals that creating an environment where individuals can be open and confident in group situations depends on the prevailing attitudes and culture in the workplace. Therefore, psychological safety is shaped by broader cultural norms within the organisation. Research shows that teams who welcome feedback have the healthiest work environment by supporting staff that report problems without fear of retribution and accountability (A. C. Edmondson, 2003). This creates an environment focused on proactively identifying and addressing actual and potential safety concerns, building a safer health system for patients, and increasing psychological safety for staff (Leonard et al., 2004). To create trust, some institutions incorporate programmes that acknowledge or give positive recognition for reporting (Trossman, 2019). Positive recognition can be implemented by thanking individuals for speaking up and acknowledging why reporting is necessary, reinforcing trust between groups (Trossman, 2019). Eliminating the fear of consequence is crucial to building trust and establishing that reporting will not . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 15 negatively affect the staff member voicing their concerns or the member of staff involved in a report (Dekker, 2016). During our research, we found that people felt comfortable to speak up in a debrief about anything that had gone wrong during surgery. It was a process of team reflecting, rather than an outlet of blame on an individual, which coincided with an elimination of fear of consequence. This elimination of fear of consequence further builds on staff trust, ensures a positive feedback loop, and provides a learning opportunity from the event that has been reported (Dekker, 2016; Waring, 2005). Further, our research recognised the importance of acknowledging positives during debriefs. Positive feedback was seen as an effective way to encourage individuals to speak up and developed a structure that makes space for positives. Developing a structure that makes space for positive feedback was seen as an effective way to encourage individuals to speak up. In a healthcare setting, psychological safety and just culture are essential to promote a safe and effective workplace culture. However, social power and perceived hierarchies in the workplace can impede the implementation of these principles (Vanstone & Grierson, 2022). The idea of power hierarchies in the medical field stems from an understanding of hierarchies that can present themselves in various areas, with different levels of authority given to individuals based on their perceived rank (Hughes & Salas, 2013). These hierarchies can start implicitly from the day students start their professional training and can reflect biases held from a young age (Gopal et al., 2021). Research has shown that these power hierarchies can diminish the effectiveness of critical work teams and team performance (Hughes & Salas, 2013). One of the primary ways that power operates in the workplace is through the establishment and maintenance of hierarchies (Vanstone & Grierson, 2022). Hierarchies are a well-recognised feature of the clinical environment, and research indicates that they can impede patient safety by contributing to a reluctance to speak up if a superior makes an error (Brennan & Davidson, 2019; Vanstone & Grierson, 2022). Furthermore, hierarchies can obstruct interprofessional teamwork and collaboration, leading to a hostile work environment that impacts individual and team psychological safety (A. Edmondson, 2008; McClintock & Fainstad, 2022). However, hierarchies can also provide a template for expectations of interaction that generate and sustain the structure and stability necessary to enable the formation and efficiency of the team (Knight & Mehta, 2017; O’Shea et al., 2019; Vanstone & Grierson, 2022). To promote a just culture and psychological safety, it is crucial to recognise and address the negative effects of power hierarchies in the workplace. By doing so, healthcare professionals can foster an environment that values open communication, teamwork, and safety (Vanstone & Grierson, 2022). While hierarchy can hinder processes, it can also be necessary for achieving a team goal. Those in higher hierarchical positions in a surgical environment can not only set the mood for the day but can also create a positive team environment and initiate or delegate team debriefing, which has been found to have a positive impact on teamwork and communication and break down barriers of hierarchy (Berenholtz et al., . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 16 2009; Paull DE et al., 2010; Robinson et al., 2010). Furthermore, hierarchy can be understood in terms of leadership, where leaders in healthcare organisations play a crucial role in promoting a culture of safety and just culture by addressing power imbalances and promoting open communication (A. C. Edmondson & Lei, 2014; Nembhard & Edmondson, 2006). Leadership Our research showed that participants had varying perspectives on leadership in the context of medical debriefing. Some equated leadership with hierarchy, while others saw it as a ""natural"" skill. Interestingly, our findings suggest that the notion of leadership is subjective and lacks consensus in the medical field. However, the idea of a ""natural leader"" can reinforce hierarchical constructs and encourage dominant individuals to take over, which can hinder team performance and psychological safety (Adams & Anantatmula, 2010; Gamble & Christensen, 2022). Although limited research exists on what constitutes a leader in medical debriefing, our study sheds light on the importance of addressing leadership and hierarchy in promoting effective teamwork and creating a culture of safety. Dominance can be a problem if an individual perceives themselves as someone who is not a natural leader and, therefore, does not want to speak up about an adverse event or discuss something they were not satisfied with (Adams & Anantatmula, 2010).  Based on hierarchical bias (Vanstone & Grierson, 2022), some interviewees perceived senior doctors, the surgeon, or the anaesthetist as the primary leaders in the operating room, and as such, considered them to be ""natural leaders."" This view reinforces the traditional power hierarchy in healthcare settings, which can impede effective teamwork and hinder psychological safety. Moreover, many interviewees felt that anyone within the multidisciplinary operative team should be able to lead an operative team to debrief. Responses suggest that an individual's experience and communication skills may be more relevant to a leadership position than their title or speciality. Therefore, the leadership role can be flexible and negotiated. Ideas that those who take the debrief ""should"" have leadership skills, however, is not necessarily true. Research suggests that those who are perceived to be leaders due to their hierarchical standing may not necessarily be capable of leading teams due to insufficient training and are therefore no more capable than anyone else at taking a debriefing (Greer et al., 2017). Understanding leadership leads to different ideas of what leadership should be in a medical context. Although there are different understandings of who should be in charge, considerable research indicates that leadership is entirely necessary when used correctly (Berenholtz et al., 2009; Brennan & Davidson, 2019; Sammer et al., 2010).  Leadership aims to achieve a collective purpose (A. Edmondson, 2008; Vanstone & Grierson, 2022), and can be a crucial factor in achieving safety and promoting cultural change in . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 17 various industries (Berenholtz et al., 2009; Brennan & Davidson, 2019; A. C. Edmondson, 2019; McClintock & Fainstad, 2022). To break down hierarchical barriers and promote effective teamwork, it may be useful to consider transformational leadership, collectivistic leadership, and flat hierarchies. Transformational leadership involves leaders engaging with individuals to achieve common goals (Matheson et al., 2020; Smith, 2015). The purpose of transformational leadership is to elevate, inspire, and ensure individuals become more active in themselves (Matheson et al., 2020; Smith, 2015). Transformational leadership is an empirically supported approach understood as a relationship of mutual stimulation and elevation that raises the level of aspirations of both the leader and those led, thereby transforming both (Matheson et al., 2020; Smith, 2015). Research indicates that transformational leadership in healthcare settings has favourable outcomes with individuals realising their capabilities to reach higher levels of performance and personal meaning (Matheson et al., 2020; Smith, 2015). Another aspect of leadership is a collective approach, where roles and responsibilities are shared, distributed, or rotated amongst team members (Lv & Zhang, 2017; West et al., 2014). By distributing leadership across teams, formal and informal leaders work together to generate actions, promoting involvement in decision-making and a sense of belonging within a group (Lv & Zhang, 2017; West et al., 2014). Flat hierarchies are gaining popularity in healthcare, as they provide the flexibility and equality needed in a caring environment where all individuals should feel safe to raise concerns and voice opinions (Green et al., 2017). With flat hierarchies, employees have more responsibility for each staff member, as there are more people available for support and guidance (Green et al., 2017). Despite various leadership approaches introduced to healthcare, there is no ""one size fits all."" Space-time One of the primary obstacles to debriefing was finding a suitable time and place for the whole team to meet and discuss. Participants demonstrated a trade-off between idealism and pragmatism when deciding when to debrief. According to Kivetz and Tyler (2007), people shift between these positions based on their timeframe. A distal time perspective focuses on the core aspects of the self, allowing for the expression of the idealistic self. In contrast, a proximal time perspective directs attention towards situational contingencies that are incidental to one’s true self, leading to the activation of the pragmatic self (Kivetz & Tyler, 2007). When it comes to debriefing, theatre staff may prefer an idealistic approach if it happens in the future, but a more pragmatic approach if it needs to occur immediately. Our observations of debriefs confirmed this trade-off between the ideal and practical. Debriefing after the patient left for recovery and all the staff returned to the room appeared to be better structured, with fewer encumbrances and distractions, than debriefs that occurred while the patient was still present. However, as noted in several . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 18 interviews, getting everyone back into the theatre after the last patient left was unlikely for most operating lists. Learning to Debrief Debriefing is a vital component of critical event management in healthcare settings, requiring specific skills for effective implementation (Abbott et al., 2018); however, as expressed by one participant, ""no-one is born knowing how to do this”. As Abbott et al. (2018) note, no one is born with an innate ability to debrief, highlighting the need for training and modelling of debriefing skills. From a social constructionist perspective, debriefing skills are acquired through training, education, and modelling from experienced professionals who provide feedback and guidance (Adams & Anantatmula, 2010). Consequently, debriefing skills can be enhanced through practice and reflection with support from colleagues and mentors. Therefore, debriefing training and education should be included in healthcare organisations' continuous professional development programs to enhance the quality and safety of patient care. Healthcare professionals must have access to ongoing training and education to develop and improve their debriefing skills. The development of a culture of routine debriefs and standardised debriefing programs can facilitate the acquisition and improvement of debriefing skills and ultimately improve patient safety in healthcare settings. Limitations Debriefing had been performed on a relatively ad hoc basis in our operating rooms, so interviewees had varying degrees of experience. Further research on the experience of operating room workers with regular debriefing would be of value. The study was set in a tertiary paediatric hospital. Subsequent research across a range of hospitals and specialties could provide further insights. Voluntary participation in this study could introduce sampling bias. Although the results of this research were shared with the theatre unit where the research took place, formal reviewing of the themes with interviewees (member checking) was not performed. In keeping with the critical realist paradigm, some of the themes were more descriptive while others were developed (in the discussion) along a more constructionist (critical) line; however, we believe this will enable readers to reflect on some of the practical issues as well as the social issues around debriefing. Conclusion In conclusion, this study of the insights and experiences of frontline workers in the operating room shows that regular, structured debriefs could improve psychological safety, help counter the negative effects of hierarchies and power structures, and promote teamwork. Debriefs were seen as a vehicle for individual and team learning, with the potential to contribute to organisational change. However, to set up an . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 19 effective debriefing program, it is essential to create a psychologically safe working environment where individuals can be open and confident in group situations without fear of retribution. Creating trust by incorporating programs that acknowledge positive recognition for reporting is crucial to building trust and establishing that reporting will not negatively affect the staff member voicing their concerns or the member of staff involved in a report. Hierarchies can impede the implementation of just culture and psychological safety. Therefore, it is crucial to recognise and address the negative effects of power hierarchies in the workplace. By doing so, healthcare professionals can foster an environment that values open communication, teamwork, and safety, ultimately leading to improved patient care and safety. Acknowledgements The authors sincerely thank Dr. Haare Williams for his expert advice and support. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 20 Table 1. Interviewee characteristics. n  = 40 Gender Female
25 Male
15 Profession Nursing
14 Anaesthesia
7 Anaesthetic technician
7 Surgeon
12 Surgical specialty Paediatric surgery
7 Orthopaedic surgery
2 Cardiac surgery
2 ENT
1 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 21 Table 2. Themes and description of each theme. Theme
Explanation Committed to learning
Health workers are committed to learning, quality improvement, and to the performance of the organisation. It’s a safe space
Debriefing promotes psychological safety, psychological safety is needed in debriefs, and constructs around power dynamics in teams. Natural born leader
Who should lead debriefs and constructs of the meaning of leadership in teams. Space-time
Practicalities on when and where to debrief, the problem of people needing to leave, and constructs on the meaning of time. Doing the basics well
Realist reflection on the right amount of structure in performing debriefs and on training how to debrief. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 22 Figure 1. Mind map of themes developed from the study. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.09.23.22280268
doi: 
medRxiv preprint 23",1
"Reports suggest that the potential long-lasting health consequences of SARS-CoV-2 infection may involve persistent dysregulation of some immune populations, but the potential clinical implications are unknown. In a nationwide cohort of 2,430,694 50+-year-olds, we compared the rates of non-Covid-19 infectious disease inpatient hospitalizations (of ≥5 hours) following the acute phase of SARS-CoV-2 infection in 930,071 individuals with rates among SARS-CoV-2 uninfected from 1 January 2021 to 10 December 2022. The post-acute phase of SARS-CoV-2 infection was associated with an incidence rate ratio of 0.90 (95% confidence interval 0.88-0.92) for any infectious disease hospitalization. Findings were similar for upper- (1.08, 0.97-1.20), lower respiratory tract (0.90, 0.87-0.93), influenza (1.04, 0.94-1.15), gastrointestinal (1.28, 0.78-2.09), skin (0.98, 0.93-1.03), urinary tract (1.01, 0.96-1.08), certain invasive bacterial (0.96, 0.91- 0.1.01), and other (0.96, 0.92-1.00) infectious disease hospitalizations and in subgroups. Our study does not support an increased susceptibility to non-Covid-19 infectious disease hospitalization following SARS-CoV-2 infection. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 3 INTRODUCTION The introduction of Covid-19 into human society has had a profound impact on global health, with hundreds of millions of people infected worldwide.1 A considerable number of individuals infected with SARS-CoV-2 continue to suffer from persisting symptoms following the acute phase of their illness.2–6 These long-term symptoms have been reported to be multifaceted including both physiological and psychological manifestations such as debilitating fatigue, dyspnea, chest pain, headaches, and cognitive dysfunction.6–8 Collectively, known as Post-Covid-19 condition or long Covid and shown to have significant impact on quality of life.5–9 While our knowledge of the long-term health consequences of Covid-19 continues to advance, the full range of implications is not yet fully understood. In addition, SARS-CoV-2 infection may have a prolonged effect on both the innate and adaptive immune system.10–19 The affected immune compartments and duration of dysregulation after Covid-19 include reduced absolute numbers of CD4+ T cells, CD8+ T cells and NK cells for up to 2.5-3 months10–12,14, increased activation of CD4+ and CD8+ T cells for up to 12 months12–15, and reduced plasmacytoid dendritic cells for up to 7 months16. Moreover, post-acute Covid-19 immune perturbations seem most pronounced up to 3 months after SARS-CoV-2 infection10, associated with older age and severe Covid-19.12,13 Nonetheless, the clinical consequence of these potential immunological dysregulations in terms of risk of other severe non-Covid-19 infectious diseases remains unclear. We leveraged the comprehensive Danish test- and surveillance system for Covid-19 together with nationwide healthcare- and demography registers to investigate the associated risk of non-Covid-19 infectious disease hospitalizations after the acute phase of SARS-CoV-2 infection in a large national cohort of adults aged ≥50 years. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 4 RESULTS Among the 2,430,694 included individuals (mean age 66.8 years, standard deviation 11.3 years), 930,071 individuals were infected with SARS-CoV-2 (ascertained by first positive PCR test) during the study period from 1 January 2021 to 10 December 2022 (Table 1 and Supplementary Figure S1). Follow-up for the entire study cohort totaled 4,519,913 person-years. Slightly more than half (51.8%) of the population were females. Demography-, comorbidity-, and vaccination status characteristics were similar between the entire study cohort and those acquiring a SARS-CoV-2 infection. We identified a total of 78,555 infectious disease hospitalizations of any type during follow-up (Figure 1 and Figure 2). Examining the individual secondary outcomes, lower respiratory tract infections were the most common cause of hospitalization, followed by the other types of infection-, urinary tract infection-, and certain invasive bacterial infection categories. Comparing the main risk period of day 29+ following SARS-CoV-2 infection with the SARS-CoV-2 uninfected reference period for any infectious disease hospitalization, the incidence rate ratio (IRR) was 0.90 (95% confidence interval [CI] 0.88-0.92) (Figure 2). Likewise, no increased risks were observed for secondary outcomes when individually assessed. The IRR was 1.08 (0.97-1.20) for upper respiratory tract, 0.90 (0.87- 0.93) for lower respiratory tract, 1.04 (0.94-1.15) for influenza, 1.28 (0.78-2.09) for gastrointestinal, 0.98 (0.93-1.03) for skin, 1.01 (0.96-1.08) for urinary tract, 0.96 (0.91-1.01) for certain invasive bacterial, and 0.96 (0.92-1.00) other types of infectious disease hospitalization. No major differences in the risks were identified when stratifying the cohort according to sex and age subgroups (Table S1). For the age group consisting of individuals aged ≥80 years, we observed an IRR of 0.96 (0.92-1.00) for any infectious disease hospitalization, but the IRR was 1.22 (1.02-1.45) for influenza, 1.16 (1.03-1.30) for skin, 1.17 (1.07-1.27) for urinary tract, and 1.13 (1.04-1.23) for certain invasive bacterial infectious disease hospitalization. Examining the risk of infectious disease hospitalizations according to vaccination status at time of SARS- CoV-2 infection was not suggestive of consistent differential risks (Figure 3). The IRR for any infectious disease hospitalization was 0.95 (0.88-1.02) for unvaccinated, 0.93 (0.87-0.99) for primary course All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 5 vaccinated, and 0.88 (0.86-0.91) for booster vaccinated at time of SARS-CoV-2 infection as compared with SARS-CoV-2 uninfected with similar vaccination status. Comparing the risk of any infectious disease hospitalization according to vaccination status did not change across sex and age subgroups (Table S2). For the individual infectious disease hospitalizations, however, we observed tendencies toward significant associations in some of the sex and age subgroup analyses among unvaccinated and primary course vaccinated individuals; however, these estimates were based on lower numbers of cases among SARS-CoV- 2 infected. Among booster vaccinated, no associated risks were seen for SARS-CoV-2 infected, including among those individuals aged ≥80 years. Splitting the post-acute period into day 29-180 and >day 180 risk windows since the SARS-CoV-2 infection date, showed similar results to those of the primary analysis; the IRR for any infection was 0.93 (0.90-0.96) for day 29-180 and 0.86 (0.83-0.89) for >180 days (Table 2). Likewise, estimates were largely unchanged in sensitivity analyses deferring the start of the main risk period to day 90 since SARS-CoV-2 infection (IRR 0.86, 0.84-0.89 for any infectious disease hospitalization, Table S3) and applying a pre-risk period of -7 days before the first SARS-CoV-2 infection date (IRR 0.93, 0.91-0.95 for any infectious disease hospitalization, Table S4). To inform previous findings20, we observed no increased risk of infectious disease hospitalization for tonsillitis in the 29-180- and >180-day periods following SARS-CoV-2 infection, nor when stratified by sex or age subgroups (Table S5). As expected, the rates of the infectious disease hospitalization outcomes following day 90+ of Covid-19 hospitalization were universally increased compared with SARS-CoV-2 uninfected reference period rates (IRR 2.28, 2.08-2.49 for any infectious disease hospitalization, Table S6). All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 6 DISCUSSION We estimated the subsequent risk of hospitalization with non-Covid-19 infectious diseases following recovery from SARS-CoV-2 infection in a nationwide cohort of Danish adults aged ≥50 years from 1 January 2021 to 10 December 2022. Our results do not support that SARS-CoV-2 infection increases the susceptibility to non-Covid-19 infectious diseases in this population. Given the observed persistent dysregulations of innate and adaptive immune populations in SARS-CoV-2 infected compared to healthy controls,10,15–18 the theoretical concern is not unjustified. The reduced levels of dendritic cells may imply an impaired ability to initiate immune responses against new pathogens, while prolonged lymphopenia may increase risk for hospitalization with infectious diseases.21 However, studies of the immunological sequelae observed after an SARS-CoV-2 infection are limited to peripheral immune cells in unvaccinated individuals, with immune dysregulation primarily occurring after severe Covid-19.10,12,14–19 It remains unclear if post-acute Covid-19 modulation extends to tissue resident immune cells, that are present at the site of new infections, or vaccinated individuals with breakthrough SARS-CoV-2 infections. Data to evaluate how post-acute Covid-19 immunological perturbations (known and unknown) in unvaccinated and vaccinated individuals may translate into clinical consequences, however, are not well- established. Compared with SARS-CoV-2-test-negative controls, a Danish population sample of 9893 unvaccinated, non- hospitalized SARS-CoV-2 positive cases from 27 February 2020 to 31 May 2020, reported no increased risk of any medication use or diagnoses related to infectious disease from 14 to 180 days after the SARS-CoV-2 test in a hypothesis-free screening analysis (screened on the fifth level of the Anatomical Therapeutical Chemical classification for drug substances and second level of ICD-10 codes for diagnoses).22 One retrospective Israeli cohort study examined the risk of conjunctivitis, streptococcal tonsillitis, pneumonia, Epstein-Barr virus, and herpes virus infections (that is, not restricted to inpatient hospitalization) among other long Covid-19 outcomes in an unvaccinated SARS-CoV-2 infected (a total of 170,280 infected during Index or Alpha subvariant predominance periods) vs an uninfected population, consisting primarily of All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 7 younger individuals (median age 24 years, interquartile range 13-42 years; 10,835 [6.4%] were aged >60 years).20 While their main results observed a hazard ratio of 1.18 (95% CI, 1.09-1.28) and 1.12 (1.05-1.20) for streptococcal tonsillitis for the assessed intervals of day 30-180 and day 180-360 since SARS-CoV-2 infection, respectively, no other increased risks of other infections were found. In individuals aged 41-60 and >60 years, no increased infection risks were observed (except for tonsillitis among 41-60-year-olds from 180-360 days following infection).20 Thus, with the exception of a modest increase in streptococcal tonsillitis, these studies of unvaccinated individuals do not support an overall increased risk of microbial infectious diseases during the period of post-Covid-19 lymphopenia and other immune perturbations. Rather, since the immune memory responses against non-SARS-CoV-2 are maintained following a SARS- CoV-2 infection14,23, these responses may contribute to sustained protection against infectious diseases24; unlike the measles virus-associated loss of adaptive immune memory, that associates with a long-lasting effect on the risk of acquiring infections with other pathogens.25–30 None of the aforementioned studies included Covid-19 vaccinated individuals.10–20,22 We found no evidence of an increased risk of non-Covid-19 infectious disease hospitalization associated with SARS-CoV-2 infection in individuals having received one or two Covid-19 vaccine booster doses. As such, our study provides a pertinent evaluation that may inform booster vaccinations. The inherent methodological boundaries of observational data should be reflected in the clinical interpretation of our findings. Particularly, the possibility that behavioral and environmental differences between SARS-CoV-2 infected and uninfected individuals may exist. One possible factor could be differences in risk behavior (such as being less precautious and more socially interactive) of SARS-CoV-2 infected individuals relative to uninfected at that time, leading to differences in the susceptibility of acquiring other transmittable diseases. Additionally, patterns in healthcare seeking behavior may also be different between SARS-CoV-2 infected and uninfected. Similarly, given the increased public as well as clinical awareness of symptoms related to SARS-CoV-2 infection, risk estimates of other infectious diseases with similar clinical manifestations to those of SARS-CoV-2, such as other upper respiratory tract infections, would likely be mostly influenced by potential differences in healthcare seeking patterns. In the context of All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 8 examining the risk of severe infections these potential biases from differences in risk- and healthcare seeking behavior are mitigated, but would otherwise most likely tend to skew estimates toward increased risks for SARS-CoV-2 infected. Still, our inpatient hospitalization definition of hospital contacts of ≥5 hours was most likely subject to some outcome misclassification (i.e., also capturing some less severe outcome events). Our exposure definition relied on a registered positive PCR test for SARS-CoV-2, and hence, holds potential for exposed being misclassified as unexposed; however, extensive free-of-charge national SARS- CoV-2 testing strategies were implemented in Denmark (e.g., the number of SARS-CoV-2 PCR tests performed per capita far exceeds most other countries31) during the Covid-19 pandemic. Moreover, given the observational nature of our study residual confounding cannot be ruled out to the extent that confounders not adjusted for were differently distributed between compared periods. Our study results were primarily null findings; however, we did observe tendencies toward some signals among individuals aged ≥80 years as well as for those hospitalized for Covid-19, that is, subgroup populations with general greater background prevalence of comorbidity and frailty that may be difficult to adequately capture with our utilized observational design. As such, while our inclusion of individuals of these older ages increases the generalizability of our results, the degree of residual confounding was likely greater for this specific subgroup. Similarly, as expected, the risk of hospitalization for other infectious diseases was universally increased among Covid-19 hospitalized compared with uninfected. Lastly, our secondary outcome analyses for individuals infected with SARS-CoV-2 infected among unvaccinated and after receiving a primary vaccination course-only was limited by lower statistical precision, but we observed no consistent risk patterns across outcomes or subgroups. Notably, comparing the risk in booster vaccinated only, an analysis where the internal validity was maximized, detected no associations for SARS-CoV-2 infected in any outcome or subgroup analysis. Owing to the nationwide coverage of the longitudinal individual-level Danish healthcare register data, selection bias is minimized and allows for a high degree of generalizability to similar populations. As we studied the risk of infectious disease hospitalization among Danish adults aged ≥50 years, our results may have less applicability in evaluations of associated risks in younger or demographically different All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 9 populations. Likewise, the relevancy of our results to other infectious diseases or clinical scenarios not studied, such as individuals having had multiple SARS-CoV-2 infections, is unknown. In conclusion, in this nationwide cohort study from 1 January 2021 to 10 December 2022 of Danish adults aged ≥50 years, SARS-CoV-2 infection was not associated with an increased long-term susceptibility to inpatient hospitalization with non-Covid-19 infectious diseases. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 10 ONLINE METHODS Setting and study population We cross-linked several Danish test- and surveillance system for Covid-19 with healthcare- and demography registers, all with nationwide coverage, using the unique identifier assigned to all residents in Denmark at either birth or immigration.32–35 This allowed us to obtain data on SARS-CoV-2 infection, hospitalization with physician-assigned diagnoses (according to the International Classification of Disease, revision 10), Covid-19 vaccination status, and other covariates on an individual level. Supplementary Tables S7 and S8 provide details on the utilized data sources and variable definitions. According to Danish law, register-based research in Denmark is exempt from ethics committee approval. During the study period 1 January 2021 to 10 December 2022, we constructed a cohort representative of the general Danish population born in 1972 or earlier (i.e., equal to turning at least 50 years in 2022). Additionally, to be included in our study cohort, we required individuals to have Danish residency at baseline and not having been infected with SARS-CoV-2 prior to study entry. SARS-CoV-2 infection was defined as a registered positive PCR test for SARS-CoV-2 and was used to create a time-varying exposure. We designated the first 28 days after the infection day (day 0) as the acute phase of individuals’ first SARS- CoV-2 infection while our main risk period of interest was day 29 following infection and onwards. Time as SARS-CoV-2 uninfected constituted as the reference period (see Figure S2 for a graphical scheme of our study design). Outcomes Study outcomes in the form of incident events of infectious disease hospitalization were defined by primary or secondary diagnoses of infectious disease (adapted and modified from previous work36; Table S8) assigned during hospital contacts of ≥5 hours. The main outcome was any infectious disease hospitalization, while secondary outcomes were constructed by subclassifying the diagnoses into upper respiratory tract, lower respiratory tract (influenza not included), influenza, gastrointestinal, skin, urinary All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 11 tract, certain invasive bacterial (i.e., sepsis, meningitis, endocarditis, and osteomyelitis), and other types of infectious diseases. We used the date of admission as the date of the event and studied each individual outcome separately. We excluded individuals who had a recent history of the respective outcome during a washout period from 1 January 2018 to 31 December 2020. Statistical analysis We followed individuals from study entry (i.e., 1 January 2021 or turning age 50 years, whichever occurred last) until first outcome event, second positive PCR test for SARS-CoV-2, emigration, death, receipt of a fifth vaccine dose (as not rolled out to the general Danish population during the study period), or 10 December 2022, whichever occurred first. We also right-censored individuals upon the day of receiving an Ad26.COV2-S Covid-19 vaccine as these were few within our study cohort and/or not recommended for the studied population. Infected individuals could contribute with person-time during both the SARS-CoV-2 uninfected (reference), acute SARS-CoV-2 infection phase (1 to 28 days following first SARS-CoV-2 infection date; estimates not reported), and main risk (≥29 days following first SARS-CoV-2 infection; i.e., post-acute phase) period. Comparing the outcome rates during the main risk (≥29 days since infection) and uninfected reference period, we estimated adjusted incidence rate ratios (IRRs) with corresponding 95% confidence intervals (CI) from a log-linear Poisson regression with the logarithm of the follow-up time as the offset. The model was adjusted for sex, age (in 5-year bins), ethnicity (Nordic, Western, non-Western), region of residence (5 levels), considered at high risk of severe Covid-19 (binary), calendar time (biweekly), vaccination status (5 levels), and number of comorbidities (defined as asthma, other chronic respiratory disorders, chronic cardiac disorders, renal disorders, diabetes, autoimmune-related disorders, epilepsy, malignancies, and psychiatric disorders; summed to: 0, 1, ≥2 comorbidities). Age, calendar time, and vaccination status was treated as time-varying covariates, the others were ascertained at baseline. In secondary analyses, we examined the associations by sex and age (50-64, 65-79, and ≥80 years) subgroups, by Covid-19 vaccination status at time of SARS-CoV-2 infection (separate analyses were made for unvaccinated, primary course vaccinated [received two vaccine doses], and booster vaccinated All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 12 [received one or two booster doses]) and by splitting the main risk period at day 180 (i.e., day 29-180+ and >180+) after the SARS-CoV-2 infection date. For the analyses of Covid-19 vaccination status, individuals were excluded or right-censored if acquiring SARS-CoV-2 infection prior to or after the particular vaccination status-time analyzed, respectively (see Figure S3 for a graphical illustration of these analyses). In sensitivity analyses, we deferred the time of start of follow-up to day 90 since the day of SARS-CoV-2 infection and applied a pre-risk period of -7 days to the date of first SARS-CoV-2 infection. For the latter, this pre-risk period was omitted from the uninfected (reference) period (to assess for any influence by the uncertainty of the specific order of an outcome and exposure during this period [including day 0]). To inform and expand on previous study findings,20 we examined the risk of infectious disease hospitalization for tonsillitis (day 29-180+ and >180+ after SARS-CoV-2 infection) separately. Lastly, we compared the rates of the outcomes following Covid-19 hospitalization (≥90+ days from the admission date) with the uninfected outcome rates. Statistical tests were 2-sided; associations were considered statistical significance if the 95% CI did not overlap with 1. SAS version 9.4 and R version 4.0.2 was used for data management, while statistical analysis was completed in R version 4.0.2. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288102
doi: 
medRxiv preprint 13",1
"PURPOSE: Coffin-Siris and Nicolaides-Baraitser syndromes, are recognisable neurodevelopmental disorders caused by germline variants in BAF complex subunits. The SMARCC2 BAFopathy was recently reported. Herein, we present clinical and molecular data on a large cohort. METHODS: Clinical symptoms for 41 novel and 24 previously published cases were analyzed using the Human Phenotype Ontology. For genotype-phenotype correlation, molecular data were standardized and grouped into non-truncating and likely gene-disrupting variants (LGD). Missense variant protein expression and BAF subunit 
interactions 
were 
examined 
using 
3D 
protein 
modeling, 
co- immunoprecipitation, and proximity-ligation assays. RESULTS: Neurodevelopmental delay with intellectual disability, muscular hypotonia and behavioral disorders were the major manifestations. Clinical hallmarks of BAFopathies were rare. Clinical presentation differed significantly, with LGD variants being predominantly inherited and associated with mildly reduced or normal cognitive development, while non-truncating variants were mostly de novo and presented with severe developmental delay. These distinct manifestations and non- truncating 
variant 
clustering 
in 
functional 
domains 
suggest 
different pathomechanisms. In vitro testing showed decreased protein expression for N- terminal missense variants similar to LGD. CONCLUSION: This study improved SMARCC2 variant classification and identified discernible SMARCC2-associated phenotypes for LGD and non-truncating variants, which were distinct from other BAFopathies. The pathomechanism of most non- truncating variants has yet to be investigated. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 5 INTRODUCTION The BAF (BRG1/BRM-associated factor) complex is an ATP-dependent chromatin remodeling complex that repositions nucleosomes and increases the accessibility of regulatory 
DNA 
sequences.1 
""BAFopathies"" 
encompass 
a 
spectrum 
of neurodevelopmental delay disorders (NDDs) caused by germline pathogenic variants in BAF complex subunit genes including SMARCA4, SMARCA2, ARID1A/B, SMARCB1/E1, DPF2, and ARID2. The most well-defined BAFopathies with overlapping clinical presentations are Coffin-Siris (CSS; MIM 135900) and Nicolaides-Baraitser (NCBRS; MIM 601358) syndromes. 2–7 Recent studies reporting affected individuals with mainly de novo missense/in-frame and a few likely gene-disrupting (LGD) pathogenic variants in another BAF-subunit, SMARCC2 (BAF170, MIM *601734), expanded the spectrum of BAF-related NDDs. 8–13 Machol et al. described a cohort of 15 cases with variable clinical manifestations resembling CSS and NCBRS. 8 The Online Mendelian Inheritance in Man (OMIM) database now classifies the SMARCC2-associated phenotype as Coffin-Siris syndrome 8 (MIM #601734). The described phenotype included neurodevelopmental delay (DD), mild to severe intellectual disability (ID), profound speech delay, behavioral abnormalities, muscular hypotonia, feeding disorders in infancy, and dysmorphic facial features. SMARCC2 contains four well described and highly conserved functional domains, namely the SWIRM domain, the SANT domain, and two domains in a coiled-coil region, termed dimerization (DR) and core assembly region (CAR) (see Figure 1A). Constitutional abolition of Smarcc2 during postnatal and adult hippocampal neurogenesis in mice increased astrogenesis, resulting in an abnormal spatial distribution of radial glial-like cells, ultimately linked to behavioral and learning impairments. 14 Additionally, intact Smarcc2 expression determines cerebral cortex volume, thickness, forebrain and cortex development. 14,15 To date, reports of SMARCC2 variants have been mostly part of individual case reports or large NDD studies with limited clinical information. 9–13,16  In an attempt to better characterize the clinical and molecular spectrum of SMARCC2-associated . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 6 NDD, a large cohort of 65 cases was collected, including 41 novel individuals with de novo or inherited variants, whose clinical and molecular findings were systematically described, and 24 previously published individuals, whose data were thoroughly curated. Additionally, Human Phenotype Ontology (HPO) and automated facial recognition were used to investigate genotype-phenotype correlations between non- truncating and LGD variants, and structural modeling as well as functional assays to investigate missense variants. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 7 MATERIALS AND METHODS Cohort and ethical considerations A cohort of 65 individuals with SMARCC2 variants, including 24 previously reported and 41 novel, was collected (File S1 and S2). Two individuals (Ind-18; c.172C>T p.(Gln58*), Ind-25; c.1094_1097del p.(Lys365Thrfs*12)) previously described in other publications, albeit with incomplete clinical or molecular characterization, were included in the novel series. 9,10 Novel SMARCC2 individuals were recruited using GeneMatcher 17 and an international collaborative network. This study follows the Declaration of Helsinki. Genetic testing was done in routine diagnostic settings (n = 30) or in research settings (n = 11) after ethical review board approval. Legal guardians gave written informed consent for genetic and clinical data, including photos and brain images, to be published. See File S2 sheet ""clinical_table"" for setting. Genetic analysis The majority of SMARCC2 variants was found by exome sequencing (singleton n=14, duo n=4 and trios n=20) in the collaborating centers using different analysis platforms based on BWA/GATK pipelines. 18,19 Chromosomal microarray revealed a complete SMARCC2 gene deletion in Ind-12. RT-PCR for Ind-29 was performed using standard methods. Real-time PCR was used to measure SMARCC2 expression levels of Ind-19 (Fam-18). See File S2 sheet ""clinical_table"" for genetic analyses and File S1 for method details. Variant annotation and scoring Variants were standardized to the SMARCC2 reference transcript NM_003075.5 (GRCh37/hg19) using Mutalyzer 3 20 and annotated using the Ensembl Variant Effect Predictor (VEP) 21. All SMARCC2 variants were subsequently reclassified based on the recommendations of the American College of Medical Genetics and Genomics (ACMG) 22 and subsequent updates. An a priori and a posteriori classification system was used based on either prior evidence or our findings, such as new mutational hotspots, clustering in functional domains, recurrence, and . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 8 functional results supporting pathogenicity. Compare File S2 sheets ""clinical_table"" and ""ACMG criteria"". Clinical Information Clinical manifestations were systematically described and standardized according to the HPO terminology (File S2 sheet ""clinical_table"" and File S1 “Clinical reports”). 23 Information on clinical abnormalities and facial dysmorphic features, which were available for 58 SMARCC2 subjects, including all novel ones, were summarized and compared (Table 1 and 2, Table S1). When available, cranial magnetic resonance imaging (MRI) data were evaluated by an experienced pediatric neuroradiologist. The facial overlays shown in Figure 2 were generated by applying the Face2Gene research application (FDNA Inc., Boston, MA, USA) to a total of 23 novel and previously published SMARCC2 individuals (Figure 2 and File S1 “Clinical information”). Fisher's exact test was used to calculate p-values for novel cases compared to reviewed and published cases. Multiple testing was adjusted for using false discovery rate (FDR with threshold < 0.05). Comparing missense/in-frame and LGD causative carrier groups followed the same method. All computations were done in R 4.1.3 using RStudio (Table 1 and 2, Table S1). Protein model analysis of missense variants We used the published structures PDBDEV_00000056 24 (PDB-Dev) and 6LTH 25 (protein data bank; PDB) to map disease associated variants of BAF complex subunits. We utilized the published crystal structure of the SMARCC1 N-terminus (6YXO 26) to generate a homology model (File S4) of the paralogous SMARCC2 region with PHYRE2 27 (One-to-One threading option), which we used to investigate disease associated variants in this region. Structures were visualized with the Pymol software (OpenSource Version 2.5.0; Schrodinger, LLC). Missense variants in other BAF base complex subunits (File S2 sheet ""missense_other_BAF"") were reviewed from published literature reports (Table S2), standardized to fit the model transcript and used to analyze spatial proximity in the BAF complex model. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 9 Functional analyses of missense variants FLAG-tagged SMARCC2 was obtained from Addgene (plasmid #19142) 28 and variants were introduced using the In-Fusion HD Cloning Kit (Clontech). Seven mutants harboring missense variants in different protein domains were generated: p.(Pro77Leu), p.(Thr214Ala) and p.(Phe248Ser) in the N-terminal module, p.(Arg443Trp) in the SWIRM domain, p.(Leu640Pro) in the SANT domain and p.(Ala868Pro) and p.(Glu893Gly) in the CAR. Plasmids were transfected into HEK293T cells using JetPrime (Polyplus Life Science). Immunofluorescence staining was performed as previously described. 29 Proximity ligation assay (PLA) was performed using Duolink In Situ Reagents (Sigma). Co-immunoprecipitation (CoIP) was carried out as previously described. 30 Protein stability was assessed by transiently co-expressing FLAG-tagged SMARCC2 variants together with a different- sized control protein (HA-tagged TBX1), followed by quantitative western blot analysis and normalization of SMARCC2-FLAG to TBX1-HA. File S1 “Supplemental methods” contain experimental details, oligonucleotide sequences (Table S4), and antibodies (Table S5). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 10 RESULTS Description of SMARCC2 variants Of the 45 SMARCC2 variants described in this study, 25 are novel. Two alterations have been reported in both this and previous studies. 8,12 Three novel missense substitutions (c.2697G>T p.(Leu899Phe), c.640A>G p.(Thr214Ala), and c.1327C>T p.(Arg443Trp)) and one previously described splice variant (c.1651-2A>G p.?) 13 could not be further classified due to lack of conclusive evidence (no strong segregation information, recurrence, or strong effects in functional studies). Thus, carriers Ind-03, Ind-06, Ind-40, and Gofin_Subject 5 were excluded from further clinical analysis. The linear model (Figure 1A) shows the variants, and File S2 sheet ""clinical_table"" and File S1 “clinical reports” describe these individuals clinically. The present study describes 27 probably non-truncating variants, including 19 missense (11 novel), three in-frame (one novel), and five splice (two novel). All non- truncating variants arose de novo. Segregation could not be determined for the splice changes c.1651-2A>G and c.1833+1G>T. The splice variants c.1311-1G>A, c.1651-2A>G 
and 
c.1833+2T>C 
were 
computationally 
predicted (http://autopvs1.genetics.bgi.com 31) to result in an in-frame deletion. RNA samples were unavailable for further analysis. The splice donor variant 1833+1G>T causes an in-frame deletion of exon 19, creating an aberrant product that escapes NMD. 8 Messenger RNA sequencing for the novel intronic variant c.1834-7C>G revealed an aberrant transcript with retention of the last 6 bp of intron 19 (r.1833+1_1834- 1_ins1834-6) 
and 
an 
in-frame 
insertion 
of 
two 
amino 
acids (p.Glu611_Ala612insHisGln) that was less stable (Figure S4A-C). Seven previously reported missense/in-frame causative variants clustered in the highly conserved SANT domain. Variant c.1833+2T>C has been reported in two unrelated individuals in a previous study 8 and once in the present study. Each of the amino acid substitutions c.1829T>C p.(Leu610Pro) and c.1826T>C p.(Leu609Pro) was detected in two different families in previous studies. The clustering of pathogenic variants in this functional domain was confirmed by the discovery of four novel non-truncating variants, two missense and two in-frame. Consequently, de . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 11 novo missense alterations in the SANT domain could be classified a priori as likely pathogenic (PS2_Supporting, PM2_Supporting, PP2_Supporting, PM1_Moderate) upon meeting the cutoffs for computational evidence (PP3). Machol et al. 8 reported two non-truncating variants in the CAR domain, here classified as a clustering hotspot due to four new amino acid substitutions, one of which was found in two unrelated families (c.2678A>G p.(Glu893Gly)). De novo non- truncating variants in this domain were classified as likely pathogenic according to the PM1_Moderate criterion if computational predictions (CADD PHRED v1.6 score ≥28.1) indicated they were pathogenic. Variants in the DR and SWIRM domains are rare and lack molecular/functional evidence, thus classified as variants of unknown significance (VUS). N-terminal missense variants included one previously reported 8 and three novel ones. The substitution c.230C>T p.(Pro77Leu) in three unrelated families was post hoc interpreted as likely pathogenic after considering variant recurrence and functional assays. The remaining N-terminal and variants outside of SMARCC2 domains remained VUS due to lack of evidence. Overall, the 18 identified LGD variants were dispersed across SMARCC2. Seven of these variants have been previously reported (two nonsense, four frameshifting, and one splice). The remaining 11 variants were novel consisting of six nonsense, three frameshifting, and two splice. Most LGD alterations were inherited from unaffected parents. Four LGD variants were de novo, and seven individuals had unknown inheritance patterns. All LGD variants could be classified as (likely) pathogenic (PVS1_VeryStrong, PM2_Supporting) assuming loss-of-function (LOF) as disease mechanism. Ind-12 had a microdeletion that included SMARCC2 and 14 other genes, including RPS26, which is linked to dominantly inherited Diamond-Blackfan anemia 
10 
(MIM 
#613309). 
The 
splice 
variants 
c.317+2T>A 
p.? 
and c.3135_3139dup p.[(Gly1047Alafs*16); p.?] were computationally predicted to cause out-of-frame effects. Real-time PCR previously demonstrated that the variant c.1311- 3C>G reduces SMARCC2 expression. 8 Variants c.327C>G p.(Tyr109*), c.574C>T . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 12 p.(Arg192*), c.805C>T p.(Arg269*), c.2059C>T p.(Arg687*) were each found in two unrelated individuals and variant c.3279del p.(Pro1094Hisfs*9) in two siblings in the novel cohort. Variant c.3129del p.(Gly1044Aspfs*17) was prior reported in a twin pair 12 as well as identified in five siblings and one unrelated individual in the present cohort. Also, variant c.3129del p.(Gly1044Aspfs*17) was the only LGD variant of the combined SMARCC2 cohort to be listed in gnomAD (seven heterozygous carriers). We used the well characterized GeneDx cohort to examine the prevalence of this variant in individuals with DD/ID (10/97.993) and healthy controls (13/400.200). The odds-ratio (OR) was calculated using Fisher's exact test and showed a moderate to strong association (OR: ~ 3.14, p-value < 0.008, 95% CI: 1.23 - 7.76). Real-time PCR on peripheral blood from one Fam-18 carrier (Ind-19) confirmed NMD with reduced SMARCC2 expression level at nearly 68% (Figure S4D). Three individuals (8.5% of this cohort) carried a second (likely) pathogenic variant in an NDD-related gene (Ind-25, Ind-33, Ind-34) (see also File S1 “Supplemental Results” and File S2 sheet ""clinical_table"" for these and additional VUS). Linear and 3D protein model Annotation of variants on the linear gene model revealed that truncating variants were dispersed throughout SMARCC2, whereas non-truncating variants clustered within the SANT and core assembly domains (Figure 1A). Furthermore, a correlation between these clusters and high computational prediction scores for missense variants in the annotated domains was found (Figure S1). Mapping of the amino acid residues to the three-dimensional protein structure of the BAF complex, demonstrated that missense variants in the core assembly domain are located in the five-helix bundle of the base module, which potentially impedes interaction with other subunits forming the base scaffold (Figure 1B). The homology model revealed that missense variants in the N-terminal region, which is not covered in any of the two published BAF complex structures 24,25, are generally dispersed throughout the globular domain, except variants p.(Pro77Leu) and p.(Phe248Ser) which are found in close proximity to each other, as well as the cancer-related variant p.Glu250Lys . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 13 (Figure 1C). Exclusively the p.(Pro77Leu) variant in the N-terminus was predicted to cause structural changes using Missense3D 32 (File S3 sheet “cohort_variants”). Functional analysis of SMARCC2 missense variants As missense variants in other BAF subunits have been linked to protein misfolding and aggregate formation, we initially investigated cellular SMARCC2 protein localization in a subset of missense alterations, but all of them exhibited normal nuclear localization (Figure S5). We next analyzed whether the missense variants affect the BAF complex combinatorial assembly. Both PLA and quantitative CoIP showed no impairment of SMARCC2 interaction with the subunits ARID1B, SMARCA4, SMARCC1 and SMARCE1 (Figure S6). Co-immunoprecipitation experiments showed a trend towards higher interaction of mutant SMARCC2-FLAG with  SMARCC1 as compared to the wildtype protein, however this effect did not reach statistical significance. Finally, we asked whether SMARCC2 variants affect protein 
stability. 
We 
included 
three 
cancer-related 
missense 
alterations (p.(Ser89Pro), p.(Cys91Phe), p.(Glu250Lys)) located in close proximity to the herein identified N-terminal variants. These were computationally predicted to cause structural defects of the SMARCC2 protein in a previous study. 26 While variants in other protein regions did not adversely affect protein stability, four of the investigated N-terminal variants showed an impact: the recurrent variant p.(Pro77Leu) from this cohort and the cancer variant p.(Glu250Lys) significantly reduced protein levels (> 80%). Likewise, cohort variant p.(Phe248Ser) and cancer variant p.(Cys91Phe) each resulted in a 20% reduction of protein levels, although only the former was statistically significant (Figure 1D). Clinical presentation of SMARCC2 individuals Global developmental delay and/or intellectual disability (ID) was described in 85% of SMARCC2 individuals, with 38% being mildly and 47% moderately/severely affected, whereas 15% had no cognitive or speech/motor deficits. Gross motor delay . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 14 and fine motor deficits in infancy and late childhood were reported in 55% of cases. Muscular hypotonia was found in 69% of the individuals, and behavioral abnormalities in 60%, with autistic behavior being the most common (34%). Anxiety, aggression, attention deficit hyperactivity disorder (ADHD), fixations, tantrums, and obsessive-compulsive behaviors were also common. Generalized, tonic, tonic-clonic, focal, absence seizures, and Lennox-Gastaux syndrome were diagnosed in 28% of the cohort. Visual defects, primarily due to refraction anomalies such as hypermetropia, hyperopia, astigmatism and myopia were present in 33% of the individuals. Strabismus and ptosis were the most common structural eye abnormalities (27%). Neuroimaging studies in 34 subjects (brain MRI in 32 and CT in two) revealed abnormalities in 21 individuals (61%). Neuroimaging features included non-specific white matter signal alterations, intracranial arachnoid cysts and/or small inferior cerebellar vermis (34%), corpus callosum hypoplasia and/or dysplasia (23%), white matter volume loss and/or anterior commissure agenesis/hypoplasia (14.7%) and  enlargement of cerebrospinal fluid spaces (14.7%)  (Figure 2C, File S2 sheet ""clinical_table"",  and Figure S3). Seven individuals had low birth weight and length, while seven had high birth weight and normal length. Two individuals showed oligohydramnios and six intrauterine growth retardation. Reduced body weight and short stature were found in 30% and 24% of the individuals, respectively. Clinodactyly, camptodactyly, brachydactyly, long fingers, and persistent fingertip pads were found in 31% of subjects. Notably, prominent interphalangeal joints were rare (4%) and absent phalanges of the 5th finger were not reported. Only Ind-43 presented with shorter distal phalanx and hypoplastic nail of the left thumb. Pes planus was the most common foot deformity (29%), while only two individuals had hypoplastic toenails (4%). Ectodermal anomalies such as sparse/thin scalp hair and hypertrichosis were noticed in 16% and 17% of individuals, respectively. The most common skeletal malformation was scoliosis (28%). Aside from genitourinary (23%) and gastrointestinal abnormalities (22%), other congenital disorders, such as heart defects, were relatively uncommon. Feeding difficulties or failure to thrive were reported by half of the cohort (51%). Finally, 21% of SMARCC2 individuals reported sleep disturbances, and 16% had recurrent infections (Table 2 and Table S1). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 15 Detailed dysmorphic facial features were observed in 45% of SMARCC2 subjects (Figure 2A and 2B, Table 2, Table S1, and File S2 sheet ""clinical_table""). Apart from structural eye and outer ear anomalies, the prevalence of clinical and dysmorphic features between previous studies and this report was consistent (see also Table 1 and 2, File S1 “Supplemental results”). Genotype-phenotype correlation in LGD and non-truncating variants DD/ID was found in 100% of missense/in-frame and 72% of LGD variant carriers. Missense/in-frame variants were associated with moderate/severe DD/ID (81% vs. 17%) with severe speech deficits and normal to moderate motor development. LGD individuals showed predominantly mild DD (55% vs. 19%) with mild/borderline ID or normal cognitive development, mild expressive/receptive language deficits or unaffected speech development, and motor abilities that were normal to mildly restricted in the majority of the cases. Notably, behavioral disorders occurred at equal frequency in both groups. Missense/in-frame variants more frequently caused muscular hypotonia (88% vs 52%). Compared to LGD carriers, missense/in-frame variant carriers had shorter stature (43% vs. 7%) and lower body weight (52% vs. 11%). Structural eye (55% vs. 8%) and outer ear (71% vs. 30%) abnormalities, were more common in individuals with non-truncating alterations (Table 1 and 2, Table S1). Facial recognition revealed a facial gestalt, which was dominated by coarse facies in missense/in-frame carriers (Figure 2A and 2B). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 16 DISCUSSION The present study provides novel insight into the SMARCC2-associated phenotype, via a comprehensive analysis of a large cohort of individuals with SMARCC2 variants, both known and novel. Functional assays revealed reduced protein expression as a pathomechanism in a subset of SMARCC2 N-terminal missense variants. Moreover, the systematic characterization of clinical traits suggests that non-truncating and LGD variants are associated with clinical entities of variable severity. SMARCC2 is highly intolerant to non-truncating (Z-score of 3.91) and LGD variants (pLI score of 1 in gnomAD). We found that de novo missense/in-frame variants clustered mainly in the SANT and the C-terminal CAR domains, enabling their classification as (likely) pathogenic. SWIRM and DR domain variants are rare. Nevertheless, considering that computational analysis indicated high conservation and constraint of amino acid substitutions for both regions (Figure S1), future studies addressing non-truncating changes in these domains could help identify additional clusters. All LOF changes were (likely) pathogenic. We focused on the pathogenicity of the c.3129del p.(Gly1044Aspfs*17), as this variant was the only one listed in public databases, and exceeded the allele frequency cutoff (2 in gnomAD). RNA expression analysis in one carrier showed the expected NMD, although it was incomplete. A suspected milder impact of this alteration due to residual expression requires protein analysis, but no further material from this individual or other LGD variant carriers was available to compare expression levels. In the well-studied GeneDx cohort, a moderate to strong effect of this frameshifting variant on DD/ID risk (OR ~ 3.14, p-Fisher < 0.008) was found. To exclude the possibility that milder effects of this variant confounding the genotype-phenotype analysis, clinical data were re-analyzed after excluding all eight carriers of this alteration in this cohort, and did not detect any deviations as compared to the previous analysis of the non- truncating/LGD group (Table S3). This suggests that the effect of this variant does not differ from that of other LGD counterparts. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 17 A subset of SMARCC2 variants was dispersed across the N-terminal module, whose function is not well defined to date. A recent study on the SMARCC2 ortholog SMARCC1 found several structural domains in its N-terminus, including a MarR-like helix-turn-helix, chromo-, and BRCT domain, which regulate transcription, histone modifications, and complex activity, respectively. 33,34 Variants in this module were predicted to cause protein folding defects and destabilization. 26 SMARCC2's N- terminal structure shares 66% homology with SMARCC1, suggesting a similar domain composition. 3D protein modeling suggested a significant alteration (contraction of the cavity encompassed by the BRCT and N-terminal domains) only for the recurrent variant p.(Pro77Leu) in this cohort. Functional analysis confirmed the pathogenicity of p.(Pro77Leu) by showing significantly reduced protein stability and almost complete SMARCC2 protein loss, similar to LGD variants. Notably, the synthetic variant p.(Glu250Lys), previously suggested to cause a structural defect,  26 resulted in a similar effect, while the variant p.(Phe248Ser) in this cohort, which is structurally close to both, had a weaker effect. These findings emphasize the structural importance of the N-terminal region without, however, excluding the possibility that N-terminus variants that do not affect protein stability may modify, disrupt, or attenuate other SMARCC2 functions. A clustering in this domain was not observed, thus the remaining N-terminal variants were classified as VUS due to lack of evidence. Since the analysis confirmed a LOF effect for p.(Pro77Leu), the carriers were included in the LGD group for genotype-phenotype analysis. 3D model analysis of SMARCC2 interaction with other BAF subunits indicated impairment due to missense variants in SWIRM, SANT, and CAR domains, but this could not be confirmed experimentally. Intriguingly, all mutants showed a tendency for increased interaction with SMARCC1, which, although not significant, could indicate increased formation of SMARCC1/SMARCC2 heteroduplexes or altered BAF complex assembly dynamics. No effect was seen on SMARCC2 protein localization or stability. These findings suggest that alterations in these domains have a more complex molecular pathomechanism, which is consistent with the extreme compositional complexity of the BAF complex, with over 1400 possible combinations. 
25 Further research is needed to determine the molecular underpinnings and pathomechanisms of missense variants in these regions. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 18 NDD with cognitive impairment, speech and motor deficits, behavioral disorders, muscular hypotonia, brain malformations, feeding difficulties or failure to thrive, short stature, and skeletal anomalies were the main clinical manifestations of the SMARCC2-related BAFopathy (Table 1 and 2). Facial dysmorphisms included a prominent forehead, thick eyebrows, broad/short philtrum, thin upper and thick lower lip vermilion, and outer ear malformations. Despite certain similarities to CSS and NCBRS, 8 the overall clinical and phenotypic manifestations of SMARCC2 individuals appear to be non-recognizable and phenotypic hallmarks of CSS and NCBRS, including finger-/toenail hypoplasia or absence of 5th finger distal phalanges and prominent interphalangeal joints, respectively, were either absent or very rare.  Less than 20% of SMARCC2 subjects had other frequent findings like microcephaly, sparse/thin scalp hair, and hypertrichosis. The characteristic CSS brain anomaly, agenesis/dysgenesis of the corpus callosum, was also only found in a small subset of this cohort (8/34) (Table S1 and File S2 sheet ""clinical_table"").  In summary, the SMARCC2-associated phenotype has only minor resemblance to CSS and NCBRS, thus challenging the current classification as CSS8 in OMIM. Moreover, such limited resemblance complicates clinicians and geneticists to clinically suspect this BAFopathy without genetic testing. The large number of causative variants identified in this study allowed the identification of two clinical patterns associated with truncating and non-truncating variants, respectively. Carriers of missense/in-frame variants had a more severe phenotype, especially in neurodevelopmental (Table 1), growth parameters (Table 2), and facial dysmorphisms (Figure 2A and 2B). The de novo occurrence of the vast majority of non-truncating variants in this cohort corroborates their severe effect. On the contrary, the impact of LGD variants were milder, explaining their frequent inheritance from a phenotypically healthy parent and their possible presence in public databases (compare p-MvT and OR-MvT in Tables 1 and 2). This association is further confirmed by the fact that the almost complete SMARCC2 protein loss, attributed to the missense variant c.230C>T, p.(Pro77Leu), was associated with milder clinical symptoms in subjects Ind-7, Ind-8, Ind-43. Overall, these results support an incomplete penetrance of loss-of-function SMARCC2 variants, which is also found in other rare monogenic developmental disorders. 33 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 19 Three affected individuals (Ind-25, Ind-33, Ind-34) with SMARCC2 LGD variants deviated from the expected clinical phenotypes. Their severe clinical presentations can be explained by a second pathogenic variant identified in other NDD-related genes. Our study suggests that loss of SMARCC2 alone cannot explain a severe phenotype. Thus, clinicians should be alerted that a severely affected individual with a SMARCC2 LGD alteration requires further analysis for a possible second molecular diagnosis. The two distinct neurodevelopmental presentations of SMARCC2-related disease suggest that missense/in-frame variants not affecting protein stability may follow a pathomechanism other than loss-of-function. One possibility is that such variants inhibit or attenuate interactions between SMARCC2 and other BAF subunits or SMARCC2 targets. Due to their clustering in evolutionarily conserved regions, these variants may exert a dominant-negative, gain-of-function or change-of-function effect. A similar mechanism has been proposed or shown for non-truncating variants in other BAF-subunits such as SMARCA4/A2, SMARCB1/E1, and DPF2. 6,34 This study demonstrates that large cohorts are essential for improved characterization, standardized ascertainment of disease-associated variants and genotype-phenotype correlations in genetic diseases. SMARCC2 clustering hotspots and recurrent variants allowed the reclassification of newly and previously reported variants, improving genetic diagnostics. Functional studies showed that N-terminal missense variants can destabilize SMARCC2 protein, although further research is needed to identify the pathomechanism for variants in other domains. Overall, individuals with SMARCC2-NDD exhibit non-specific clinical manifestations and lack the defining clinical characteristics of CSS and NBRS, thus requiring genetic testing for identification. By systematically analyzing and reviewing clinical data, two distinct SMARCC2-associated phenotypes were found: a more severe phenotype due to de novo non-truncating variants, and a milder phenotype due to predominately inherited LGD variants with possibly incomplete penetrance. In view of such a sharp contrast, the appropriate nomenclature allowing to distinguish the two associated clinical entities remains to be determined. The presented findings also support two distinct disease pathomechanisms underlying the corresponding clinical manifestations. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287962
doi: 
medRxiv preprint 20",1
"Purpose: The purpose of this study is to compare two libraries dedicated to Markov chain Monte Carlo method: pystan and numpyro. Materials and methods: Bayesian item response theory (IRT), 1PL-IRT and 2PL-IRT, were implemented with pystan and numpyro. Then, the Bayesian 1PL-IRT and 2PL-IRT were applied to two types of medical data obtained from a published paper. The same prior distributions of latent parameters were used in both pystan and numpyro. Estimation results of latent parameters of 1PL-IRT and 2PL-IRT were compared between pystan and numpyro. Additionally, the computational cost of Markov chain Monte Carlo method was compared between the two libraries. To evaluate the computational cost of IRT models, simulation data were generated from the medical data and numpyro. Results: For all the combinations of IRT types (1PL-IRT or 2PL-IRT) and medical data types, the mean and standard deviation of the estimated latent parameters were in good agreement between pystan and numpyro. In most cases, the sampling time using Markov chain Monte Carlo method was shorter in numpyro than that in pystan. When the large-sized simulation data were used, numpyro with a graphics processing unit was useful for reducing the sampling time. Conclusion: Numpyro and pystan were useful for applying the Bayesian 1PL-IRT and 2PL-IRT. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint Keywords item response theory; Markov chain Monte Carlo; graphics processing unit. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint 1 Introduction Item response theory (IRT) is a statistical framework used for analyzing test results and evaluating test items and test takers quantitatively. While IRT is commonly used in educational and psychological research (1–3), there are several applications of IRT to medical research. For example, Choi et al. used IRT for constructing the computer adaptive testing system of short- form patient-reported outcome measures with the data from the Patient-Reported Outcomes Measurement Information System project (4). Gershon et al. used IRT to build the quality of life item banks for adults with neurological disorders (5). Generally, IRT is applied to the results of binary responses to the test items (e.g., correct and incorrect answers). In medical diagnosis, the results of various diagnostic procedures are frequently defined as binary responses. Therefore, it is possible to apply IRT to the data of medical diagnosis. To apply IRT to the data of medical diagnosis, the following correspondence is assumed: (i) the patient as the test item, (ii) the doctor as the test taker, and (iii) the results of the binary responses obtained through medical diagnosis as test results. For example, Nishio et al. used IRT for analyzing the results of medical diagnoses by radiologists (6). The Bayesian IRT can be implemented using probabilistic programming languages or dedicated libraries (e.g., JAGS, Stan, pystan, and numpyro) (7–10). For example, previous studies used Stan for the implementation of the Bayesian IRT, graded response model, and . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint nominal response model (6,11,12). The recent advances in hardware and software make it possible to use the Bayesian IRT efficiently. However, there is no study comparing the efficiency of the Bayesian IRT from the viewpoint of computational cost. The purpose of the current study was to compare the results of the Bayesian IRT implemented with two dedicated libraries (pystan and numpyro). In the current study, the Bayesian 1PL-IRT and 2PL-IRT implemented with pystan and numpyro were applied to the two types of medical data obtained from the published paper (6). The estimation results of latent parameters and the computational cost were compared between pystan and numpyro. For reproducibility, our implementation of the Bayesian IRT in pystan and numpyro used in the current study is disclosed as open source through GitHub (https://github.com/jurader/irt_pystan_numpyro). 2 Materials and methods Because this study used the medical data obtained from the published paper, institutional review board approval or informed consent of patients was not necessary. 2.1 Medical Data . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint The two types of medical data (BONE and BRAIN data) were obtained from the published paper (6). Table 1 shows the characteristics of the two types of medical data. The BONE data include binary responses from 60 patients (test items) and 7 radiologists (test takers), and the BRAIN data include those from 42 patients and 14 radiologists. The total numbers of the binary responses were 420 and 588 in the BONE and BRAIN data, respectively. 2.2 1PL-IRT IRT is a statistical model for analyzing the results of binary responses. While there are several types of IRT models (13), 1PL-IRT and 2PL-IRT were used. In the current study, latent parameters of IRT are estimated based on the results of medical diagnoses by test takers. In 1PL-IRT, one latent parameter (𝛽𝑖) is used to represent the difficulty of test item i, and another latent parameter (𝜃
𝑗) is used to represent the ability of test taker j. The following equations represent 1PL-IRT. Pr(𝑟𝑖𝑗= 1) = 
1 1 + exp (−𝑧𝑖𝑗 ) 𝑧𝑖𝑗 = 𝜃
𝑗−𝛽𝑖 Here, . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint ⚫ 
Pr(𝑟𝑖𝑗= 1) represents the probability that the response of test taker j to test item i is correct, ⚫ 
𝛽𝑖 is the difficulty parameter of test item i, ⚫ 
𝜃
𝑗 is the ability parameter of test taker j. 2.3 2PL-IRT In 2PL-IRT, two latent parameters (𝛼𝑖 and 𝛽𝑖) are used to represent test item i. The following equations represent 2PL-IRT. Pr(𝑟𝑖𝑗= 1) = 
1 1 + exp (−𝑧𝑖𝑗 ) 𝑧𝑖𝑗 = 𝛼𝑖(𝜃𝑗−𝛽𝑖) Here, ⚫ 
𝛼𝑖 and 𝛽𝑖 are the discrimination and difficulty parameters of test item i. 2.4 Experiments We used Google Colaboratory to run the experiments. The following software packages were used on Google Colaboratory: pystan, version 3.3.0; jax, version 0.4.4; jaxlib, version . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint 0.4.4+cuda11.cudnn82; numpyro, version 0.10.1. Two cores of Intel(R) Xeon(R) (2.20 GHz) and NVIDIA(R) Tesla T4(R) were used as CPU and a graphics processing unit (GPU), respectively. 2.4.1 Experiments for agreement of latent parameters The Bayesian 1PL-IRT and 2PL-IRT, implemented with pystan and numpyro, were applied to the BONE and BRAIN data. For 1PL-IRT, the following prior distributions were used: ⚫ 
𝛽𝑖 ~ 𝑁(0, 2) , ⚫ 
𝜃
𝑗 ~ 𝑁(0, 2) , where N represents a normal distribution in which the ﬁrst and second arguments are the average and variance of normal distribution, respectively. For 2PL-IRT, the following prior distribution was used in addition to those of 1PL-IRT: ⚫ 
𝑙𝑜𝑔(𝛼𝑖 )~ 𝑁(0.5, 1). The same prior distributions of the latent parameters were used in both pystan and numpyro. . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint The following parameters were used for sampling using Markov chain Monte Carlo method in both pytan and numpyro: num_chains=6, num_samples=8000, num_warmup=2000. For numpyro GPU version, chain_method='parallel' was used. After the sampling, the posterior distributions of the latent parameters were obtained for the Bayesian 1PL-IRT and 2PL-IRT. The posterior distributions of the latent parameters were then compared between pystan and numpyro. 2.4.2 Experiments for computational cost To evaluate the computation cost of 1PL-IRT and 2PL-IRT implemented with pystan and numpyro, simulation data were generated from the medical data (BONE and BRAIN data) and numpyro. The computer simulation was performed in the following steps: (i) estimating the posterior distributions of the latent parameters of 1PL-IRT and 2PL-IRT for the two types of medical data, and (ii) generating binary responses from the IRT equations and the estimated posterior distributions for the two types of medical data. The total number of binary responses in the simulation data were as follows: 420, 840, 2100, 4200, 8400, 21000, 42000, 84000, 210000, and 420000 for the BONE data; 588, 1176, 2940, 5880, 11760, 29400, 58800, 117600, 294000, and 588000 for the BRAIN data. To evaluate the computational cost in pystan and numpyro, the sampling time using Markov chain Monte Carlo method was measured. In numpyro, both CPU . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint version and GPU version were used for the sampling. The following parameters were used for the sampling in both pytan and numpyro: num_chains=2, num_samples=3000, num_warmup=500. For numpyro GPU version, chain_method='parallel' was used. Due to the limitation of Google Colaboratory, it was not possible to evaluate the sampling time in several large-sized simulation data. 3 Results In the current study, we focused on the ability parameters of test takers, and the estimation results of test items were omitted. Tables 2–5 present the estimation results of the ability parameters of test takers. In addition, Figures 1 and 2 show representative scatter plots of the estimation results between pytan and numpyro, which are obtained from values of Tables 2 and 5, respectively. Tables 2–5 show the mean, standard deviation, and credible interval (94% highest density interval) as the estimation results of the ability parameters of test takers. Based on Tables 2–5 and Figures 1 and 2, we found that there was good agreement between pystan and numpyro for 1PL-IRT and 2PL-IRT of the BONE and BRAIN data. From Tables 2–5, Lin’ s concordance correlation coefficients (CCC) of the estimated mean of the ability parameters were calculated between (a) pystan v.s. numpyro CPU version, . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint (b) pystan v.s. numpyro GPU version, and (c) numpyro CPU version v.s. numpyro GPU version (14). The following criteria were used to evaluate CCC (15,16); low CCC values (< 0.900) were considered to represent poor agreement, whereas higher CCC values represented moderate (0.900–0.950), substantial (0.951–0.990), and almost perfect agreement (> 0.990). In the current study, the CCC values of the estimated mean of the ability parameters were as follows: (a) 1.000, (b) 1.000, and (c) 1.000 for 1PL-IRT and 2PL-IRT of the BONE and BRAIN data, indicating almost perfect agreement. Figures 3–6 show the sampling time for the simulation data of the BONE and BRAIN data. When original-size simulation data were used, the sampling time was shorter in pystan than numpyro CPU version. However, in the simulation data of the BONE and BRAIN data except for the original size, the sampling time was shorter in numpyro CPU version than pystan. Moreover, when the large-sized simulation data (total number of binary responses >30000– 50000) were used, the sampling time was shorter in numpyro GPU version than numpyro CPU version. 4 Discussion The current study aimed to compare the estimation results of the ability parameter of test takers . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint using two different libraries (pystan and numpyro) for two different types of IRT models and medical data (BONE and BRAIN data). The study found that there was good agreement between pystan and numpyro for all the combinations of the IRT models and medical data; there was almost perfect agreement between pystan and numpyro in the CCC values of the estimated mean of ability parameters. The current study also compared the sampling time for the simulation data of the BONE and BRAIN data. We found that while the sampling time was shorter in pystan than numpyro CPU version for the original-size data, it was shorter in numpyro CPU version than pystan for the simulation data except for the original size. For the large-sized simulation data, the sampling time was shorter in numpyro GPU version than numpyro CPU version. Our results show that there was almost perfect agreement in the ability parameters of the Bayesian IRT between pystan and numpyro. This suggests that researchers can choose either library for implementing the Bayesian IRT. While numpyro requires only Python, both Python and Stan (two different programming languages) are necessary for pystan. Many practitioners and researchers may find numpyro to be simple and straightforward. Although we used the simulation data, our results of sampling time show that the fastest libraries differed based on the total number of binary responses. Specifically, pystan was the fastest for the original-size simulation data, while numpyro CPU version was the fastest for the . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint small-sized and medium-sized data. For the large-sized simulation data, the sampling time was shorter in numpyro GPU version than numpyro CPU version. This implies that practitioners and researchers should select either pystan or numpyro based on the data size. Tables 3 and 4 show that the total number of latent parameters may affect the usefulness of GPU for reducing the sampling time. In complex models, such as 2PL-IRT used in this study, numpyro GPU version may tend to be faster than numpyro CPU version. The effect of GPU on the sampling time should be evaluated in future studies. This study had several limitations. First, we evaluated only Bayesian 1PL-IRT and 2PL- IRT. Further studies are needed to investigate the effectiveness of pystan and numpyro in other types of Bayesian models. Second, although we evaluated the sampling time, we used the simulation data instead of real-world data. Future studies should use real-world data to evaluate the sampling time. Third, we used Google Colaboratory. Although Google Colaboratory has several merits (e.g., ease of use and availability), our experiments were performed using limited types of hardware provided by Google Colaboratory. In conclusion, the current study demonstrated that both pystan and numpyro were effective in the estimation for 1PL-IRT and 2PL-IRT of the BONE and BRAIN data. Moreover, the study provides useful information about the sampling time for the different data and the libraries using the computer simulations. The results of this study may be helpful for researchers and . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint practitioners who use these models for large-scale data. Overall, the findings of this study contribute to the growing body of research on the application of the Bayesian methods in medical data. . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.03.29.23287903
doi: 
medRxiv preprint",1
"This study aimed to develop a classification model predicting incident bipolar
disorder (BD) cases in young adults within a 5-year interval, using sociode-
mographic and clinical features from a large cohort study. We analyzed 1,091
individuals without BD, aged 18 to 24 years at baseline, and used the XGBoost
algorithm with feature selection and oversampling methods. Forty-nine individ-
uals (4.49%) received a BD diagnosis five years later. The best model had an
acceptable performance (test AUC: 0.786, 95% CI: 0.686, 0.887) and included
ten features: feeling of worthlessness, sadness, current depressive episode, self-
reported stress, self-confidence, lifetime cocaine use, socioeconomic status, sex
frequency, romantic relationship, and tachylalia. We performed a permutation
test with 10,000 permutations that showed the AUC from the built model is sig-
nificantly better than random classifiers. The results provide insights into BD as
a latent phenomenon, as depression is its typical initial manifestation. Future
studies could monitor subjects during other developmental stages and investi-
gate risk populations to improve BD characterization. Furthermore, the usage
of digital health data, biological, and neuropsychological information and also
neuroimaging can help in the rise of new predictive models. Keywords: Bipolar Disorder, Supervised Machine Learning, Incidence, Risk
Factors, Young Adult, Cohort Studies ∗Corresponding author
Email address: bmontezano@hcpa.edu.br (Bruno Braga Montezano) Preprint submitted to MedRχiv . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 1. Introduction Bipolar disorder is a chronic psychiatric condition associated with high mor-
bidity and mortality (McIntyre et al., 2020). The global lifetime prevalence of
bipolar disorder is approximately 2.4%, being 0.6% for bipolar I disorder, 0.4%
for bipolar II disorder and 1.4% for individuals with subthreshold presentations
(Merikangas et al., 2011). Previous investigations describe that these patients
present a significant reduction of life expectancy of about 10 years relative to
the general population (Kessing et al., 2015).
Cardiovascular disease is the
main factor associated with premature mortality in bipolar disorder; nonethe-
less, deaths by suicide are more commonly reported in bipolar disorder than
in other mental health conditions, with these patients presenting a twenty to
thirty times higher chance of dying by suicide (McIntyre et al., 2020; Plans
et al., 2019; Kessing et al., 2015). In addition, bipolar disorder patients present
significant functional and psychosocial impairment, also representing an impor-
tant economic cost (McIntyre et al., 2020). For instance, evidence from the
United States described that the total costs associated with bipolar I disorder
exceeded $200 billion in the year of 2015 (Cloutier et al., 2018).
Even though the majority of the patients with BD present clinical symptoms
before the age of 25, there is a significant delay of 6-10 years between the
onset of the symptoms and the correct diagnosis (Yatham et al., 2018; Scott
& Leboyer, 2011). Furthermore, delayed diagnosis is associated with longer
duration of untreated illness, which is ultimately linked to a poorer prognosis
in terms of hospitalizations, functioning and recurrence of episodes (Altamura
et al., 2015). High rates of psychiatric comorbidity, difficulty in the differen-
tial diagnosis, usual onset with depressive symptoms, and reduced help-seeking
behavior are some of the reasons for the delay in the proper recognition of bipo-
lar disorder (McIntyre & Calabrese, 2019; McIntyre et al., 2020; Yatham et al.,
2018). Nevertheless, the diagnosis of bipolar disorder is eminently clinical, with
limited evidence to support the use of neuroimaging or laboratory biomarkers
during clinical investigation (McIntyre et al., 2020).
Taking into account this context, the rise of the concept of precision psychi-
atry, with the use of big data and machine learning tools represents a promise,
which may ultimately bring a revolution in terms of diagnosis, treatment selec-
tion and prognosis in the field of mental health (Fernandes et al., 2017; Passos
et al., 2016). To this date several studies have explored the use of these tech-
niques in bipolar disorder, based on distinct data sources (including neuroimag-
ing, clinical and sociodemographic data, peripheral biomarkers, neuropsycho-
logical tests, genetics, among others), with the majority of these models being
focused on classification tasks that help in the differential diagnosis between
bipolar disorder and other psychiatric conditions such as schizophrenia, major
depression and healthy individuals (Librenza-Garcia et al., 2017; Passos et al.,
2019). Nevertheless, most of these studies present modest classification perfor-
mances, are based on small and clinical samples, originary from cross-sectional
procedures of data collection, or present short periods of follow-up (Librenza-
Garcia et al., 2017). All these limitations may compromise the generalizability . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint and the translation of the results of such investigations to clinical and public
health settings (Passos et al., 2019).
Thus, considering these gaps, the present study aims to create a binary clas-
sification model capable of predicting incident cases of bipolar disorder in a
5-year interval through sociodemographic and clinical features in a sample of
young adults, from a large and population-based cohort study. 2. Methods 2.1. Participants
This was a prospective cohort study that collected sociodemographic and
clinical information from a population-based sample of young adults aged be-
tween 18 and 24 years, living in the urban area of the city of Pelotas, located
in southern Brazil. The first phase took place between 2007 and 2009, and
the sample was selected through cluster sampling, considering eighty-nine ran-
domly selected census-based sectors from 448 total sectors (Brazilian Institute
of Geography and Statistics, 2010).
The following inclusion criteria were considered at baseline: (1) age be-
tween 18 and 24 years old; (2) live in the urban area. Severe cognitive dis-
ability (assessed through clinical judgement) that could cause difficulties in un-
derstanding study instruments was considered the only exclusion criteria. All
eligible subjects (n = 1762) were invited to participate, of which 1560 accepted
and consented to participate. Trained interviewers conducted a face-to-face in-
terview at the participants’ homes, so that data confidentiality was ensured.
Data were collected through printed paper questionnaires with research instru-
ments and diagnostic criteria for mental disorders.
The follow-up occurred from 2012 to 2014, that is, an average interval of
five years after the first assessment. The participants from baseline (n = 1560)
were invited for a second data collection. All interviewers met weekly to discuss
the assessments, focusing on those who were uncertain about the BD diagnosis.
In these situations, a psychiatrist was recruited to carry out the reassessment.
1244 individuals were located and consented to be reevaluated (79.7% of re-
tention), and 14 (0.9%) were lost due to death. Since the present study aims to
predict BD incidence, subjects who met diagnostic criteria for a lifetime manic
or hypomanic episode were excluded. Unlike the baseline, data were collected
through tablets using Open Data Kit (ODK), an open-source mobile data collec-
tion platform (Hartung et al., 2010). The forms were filled out offline, and the
data were later backed up to computers through secure data transfer protocols.
This study was approved by the Research Ethics Committee of Universidade
Cat´
olica de Pelotas under protocol number 2008/118. The subjects who pre-
sented any psychiatric diagnosis in the clinical interview were referred for spe-
cialized treatment in the local health system. All participants signed a printed
informed consent form and could withdraw from the study at any time. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint 2.2. Outcome
The BD diagnosis was built with modules A and D from Mini International
Neuropsychiatric Interview 5.0 (MINI), in order to assess current or past de-
pressive episodes and current or past manic or hypomanic episodes, respec-
tively. The BD diagnoses were reassessed in those cases where the diagnosis was
questionable. MINI is a short-term diagnostic interview designed for clinical as-
sessment of mental disorders according to Diagnostic and Statistical Manual of
Mental Disorders — Fourth Edition (DSM-IV) and ICD-10 criteria (American
Psychiatric Association, 1994; World Health Organization, 1993). Despite eval-
uating several disorders, MINI psychometric properties for the diagnosis of life-
time manic episode (sensitivity: 81.0%; specificity: 94.0%; positive predictive
value: 76.0%; negative predictive value: 95.0%) and major depressive episode
(sensitivity: 96.0%; specificity: 88.0%; positive predictive value: 87.0%; neg-
ative predictive value: 97.0%) are reliable when compared to DSM Structured
Clinical Interview (Amorim, 2000). 2.3. Predictors
One hundred and ninety features were included in the original dataset be-
fore preprocessing steps. These variables include demographic, social, clinical,
and environmental characteristics. The following features were included in the
modeling pipeline: a) Sociodemographic and environmental variables: Sex, skin color, age, socioe-
conomic status (3 levels and 5 levels), current occupation, currently study-
ing, worked for money, has a partner, has a religion, access to psychotherapy,
knows someone who attempted suicide or committed suicide, involvement
in physical fights, family gun ownership, social support, has divorced par-
ents, has any deceased parents, has someone close by already deceased,
individual and family stress problems, lives with parents, family suicide at-
tempts, seat belt wearing, helmet use when riding a motorcycle, suffered an
accident that led to an emergency room, drove or took a ride with a drunk
driver.
b) Substance use variables: Indicative of substance abuse or dependence (to-
bacco, alcohol, cannabis, cocaine, crack, amphetamines, inhalants, seda-
tives, hallucinogens, opioids, illicits, any other substances) assessed by Alco-
hol, Smoking and Substance Involvement Screening Test (ASSIST) and life-
time use features (same substances cited above), age that first used drugs,
injected drugs use, use of medication for stress problems in the last 30 days.
c) Clinical variables: mental disorder diagnoses (anxiety, mood and personal-
ity disorders), eating disorders, current suicide risk, serious organic disease,
lifetime psychiatrist or psychologist visit, lifetime psychotherapeutic treat-
ment, interrupted treatment.
d) Sex-related variables: age of first sexual intercourse, sexual intercourse in
the last week (sex frequency), condom use, alcohol use before sexual inter-
course, number of sexual partners, number of pregnancies, lifetime sexual
abuse, lifetime sexual intercourse (binary). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint e) Psychometric instrument items: Beck Depression Inventory (BDI) [21 items],
Hypomania Checklist (HCL-32) [32 items], Social Readjustment Rating Scale
(SRRS) [26 items], Beck Scale for Suicide Ideation (BSS) [21 items]. 2.4. Machine learning analysis
Aiming to predict new cases of bipolar disorder in young adults, using fea-
tures previously described, we created an ML pipeline to generate a predictive
model using supervised learning. We used a vastly used machine learning algo-
rithm for tabular data called tree gradient boosting, implemented through the
XGBoost library (Chen & Guestrin, 2016) in the R programming language on
version 4.2.1 (R Core Team, 2022).
Tree gradient boosting is part of what is called ensemble algorithms —
joining many models to make predictions together — in statistical learning
methods. Boosting improves this concept by building a sequence of originally
weak models into progressively more powerful models. Additionally, in gradi-
ent boosting techniques, the gradient of a loss function is used to choose the
best approach to improve a weak learner (James et al., 2021). In the context of
gradient-boosted trees, weak learners are decision trees.
The following tree boosting hyperparameters were tuned (Kuhn & Vaughan,
2022a; Chen & Guestrin, 2016): a) mtry: Number of predictors that is randomly sampled at each split.
b) trees: Number of trees contained in the ensemble.
c) min n: Minimum number of observations in a node required for the node to
be split further.
d) tree depth: Maximum depth (number of splits) of each tree.
e) loss reduction: Reduction in the loss function required to split further.
f) learn rate: Step size at each iteration while moving toward a loss function
optimization.
g) sample size: Proportion of the data set used for modeling within an iteration. When it comes to tabular data, gradient boosting decision trees (GBDT) are
seen as the state-of-the-art, reinforced by several competitions in the ML sce-
nario. In addition, a study found that GBDT perform better than deep learning
models across multiple tabular datasets, and also requires less hyperparameter
tuning (Shwartz-Ziv & Armon, 2021).
The implementation of the data modeling routines was carried out using
the tidymodels framework (Kuhn & Wickham, 2020).
The tidymodels is a R
metapackage made up of multiple packages that assist in different stages of a
machine learning pipeline. In order to split the data and create cross-validation
resamples, rsample package was used (Silge et al., 2022), parsnip was used
to access XGBoost functions in an unified manner (Kuhn & Vaughan, 2022a),
recipes for preprocessing functions (Kuhn & Wickham, 2022), workflows to bun-
dle the preprocessing, modeling and post-processing routines (Vaughan, 2022),
yardstick to easily calculate performance measures (Kuhn & Vaughan, 2022b). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint In the cross-validation, fifteen hyperparameter combinations were used as can-
didate parameter sets. The values for each hyperparameter were randomly cho-
sen based on an algorithm that attempts to maximize the determinant of the
spatial correlation matrix between coordinates (Santner et al., 2003). 2.5. Preprocessing
Before any preprocessing routine was performed, the data was divided into
two subsets. A training set, consisting of 70% of the sample, and a test set
with the remaining samples (30% of the total samples). The entire test set was
isolated until the end of all tuning and validation procedures to build the model,
to then be used to simulate the model performance on new data.
Some preprocessing techniques were adopted to clean and tidy data prior to
modeling. The following preprocessing steps were applied: 1) Remove all features with more than 10% of missing values.
2) Impute categorical features with mode.
3) Impute numeric features with median.
4) Remove near-zero variance features (few unique values relative to the num-
ber of observations and also a ratio of the frequency of the second most
common value is large [ratio of 10]).
5) Create dummy variables with C −1 categories from categorical features. 2.6. Feature selection
The feature selection process aims to automatically filter variables from the
data matrix considering their relevance to the predictive modeling problem.
Therefore, we can build more accurate and parsimonious models while, at the
same time, saving computational resources through the use of less data in the
next model fitting steps.
The Boruta system was implemented for feature selection in the present
pipeline. It consists of a random forest based algorithm that iteratively removes
features that are statistically less important than random synthetic features (ar-
tificial noise). For each iteration, removed variables are prevented from being
considered for the next iteration (Kursa et al., 2010). Boruta is considered a
wrapper method as it takes into account a subset of variables with different
combinations in each iteration. 2.7. Class imbalance
Class imbalance is a common problem in classification modeling. It happens
when we face a set of examples that presents a given level way more frequently
than other.
Since most ML classifiers assume data equally distributed, they
tend to be more biased towards the majority class, causing bad performance on
minority class classification.
This concept is especially important in the context of predicting mental dis-
orders, as subjects who will present the disease will be exposed to greater health . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint risk. Therefore, it is necessary that the classifiers of such outcomes can ad-
equately predict this portion of the population. BD still has the aggravating
factor of having a complex prognosis regarding the neuroprogression, which
can be worsened by the length of disease (Librenza-Garcia et al., 2021).
For this paper, an algorithm named ROSE (Random Over-Sampling Exam-
ples) was used. ROSE is a smoothed-bootstrap-based technique that creates new
artificial observations in data in order to minimize or eliminate class imbalance
(Menardi & Torelli, 2014). The themis and ROSE R packages were adopted to
implement the previously described algorithm (Hvitfeldt, 2022; Lunardon et al.,
2014). 2.8. Cross-validation
The cross-validation (CV) process was used to tune XGBoost hyperparame-
ters described earlier. We used the k-fold cross-validation technique with 5 folds
repeated five times. In order to optimize the hyperparameter combinations, we
used a racing method proposed by Kuhn (2014). It consists in calculating the
area under the receiver operating characteristic (ROC) curve for each param-
eter set across validation folds. After evaluating the parameter combinations
for three resamples, a repeated measure ANOVA model is fitted. The combina-
tions that are statistically different (based on α level for one-sided confidence
interval of 5%) from the best setting are excluded from further validation pro-
cedures. The ANOVA racing method was implemented via finetune R package
(Kuhn, 2022).
In Figure 1, the cross-validation procedure can be visualized inside the or-
ange area. For each fold, the Boruta and ROSE algorithms were applied just
in training folds, leaving the testing fold untouched in order to properly esti-
mate model error. A maximum of 25 runs (5-fold CV repeated up to five times)
was considered for hyperparameter tuning. At the end, the remaining models
of the ANOVA racing process were evaluated, and the model with the highest
validation AUC was chosen to be tested in testing set from the initial data split. 2.9. Performance measures
Aiming to evaluate the performance of the algorithm, some evaluation met-
rics were used.
Firstly, the area under the receiver operating characteristic
(ROC) curve (AUC) was used to diagnose the classifier ability to predict cor-
rectly across multiple discrimination thresholds (Fawcett, 2006).
Sensitivity and specificity were also used to assess model ability to correctly
detect subjects with BD who have the disorder, and correctly detect subjects that
did not present BD who actually does not have BD, respectively (Yerushalmy,
1947). Positive and negative predictive values (PPV and NPV) show the pro-
portion of positive and negative predictions that are truly positive or negative,
correspondingly.
Accuracy measures the proportion of correct predictions among the total
number of observations evaluated (Metz, 1978). In order to take into account
the class imbalance previously described, we also used balanced accuracy as it
inputs both sensitivity and specificity into its formula: . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Training set
n = 763
Training set
n = 763 70% 30% XGBoost model
XGBoost model
Best model based on
highest AUC XGBoost model
Feature importance
SHAPley values based
on final model Feature selection
Boruta algorithm
1% confidence level 
100 runs Performance metrics
Area Under the ROC Curve
Balanced Accuracy
Accuracy
Sensitivity
Specificity
Positive Predictive Value
Negative Predictive Value
F1 Score Oversampling
ROSE algorithm Dataset
n = 1091
Dataset
n = 1091 Testing set
n = 328
Testing set
n = 328 Preprocessing
Feature removal
(missing values and
near-zero variance)
Feature imputation
Dummy features 5x XGBoost algorithm
Hyperparameter tuning
5-fold cross-validation
ANOVA racing method Figure 1: Machine learning pipeline flowchart. The figure shows data splitting, preprocessing rou-
tine, feature selection, cross-validation, model fitting, model assessment and feature importance
steps using XGBoost algorithm. Balanced accuracy = Sensitivity + Specificity 2
(1) The F1 score is defined as the harmonic mean of PPV and sensitivity (Equa-
tion 2). This metric is able to demonstrate another model accuracy measure,
being more robust in class imbalance scenarios. F1 = 2 · Positive Predictive Value · Sensitivity Positive Predictive Value + Sensitivity
(2) 2.10. Model interpretability
In order to make model predictions more interpretable, we used SHAPley
values. SHAPley values shows how to fairly distribute the total output among
all features. Beyond that, SHAP (SHapley Additive exPlanations) allow for ex-
planation on individual predictions (Lundberg & Lee, 2017). In the present
study, the SHAPley values were obtained using the R package SHAPforxgboost
(Liu & Just, 2021). This package provides functions to create SHAP-related
visualizations from a XGBoost model object.
In addition to the use of feature importance visualization, partial depen-
dence plots (PDP) were also employed. They are able to show the marginal
effect a feature has on the predicted outcome of a machine learning model
(Friedman, 2001). The PDP were built with the pdp (Greenwell, 2017) and
SHAPforxgboost (Liu & Just, 2021) R packages, along with the ggplot2 (Wick-
ham, 2016) and patchwork (Pedersen, 2020) packages for plot composition. 3. Results The present study aimed to create a model to predict bipolar disorder onset
on young adults based on 5-year follow-up data. We assessed 1,091 subjects at . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint follow-up interview who had no current or past episode of mania or hypomania
at the first assessment.
Of these, 4.49% (n = 49) young adults received a
diagnosis of BD five years later. Descriptive tables of demographic features at
baseline are presented in Table 1. Absolute and relative frequency of missing
values in each feature are described in Table 2. Table 3 shows the selected
hyperparameter set from the cross-validation.
XGBoost showed an acceptable performance predicting BD five years before
the diagnosis with a test set AUC of 0.786 [95% CI: 0.686, 0.887] (Figure 2).
The other performance metrics using a cut-off of 0.5 for class decision bound-
ary1 can be seen in Table 4.
The six most relevant baseline features in BD prediction were feeling like
a failure (BDI item 3), sadness (BDI item 1), current depressive episode, self-
reported stress problems, self-confidence (HCL-32 item 3) and lifetime cocaine
use. Feature importance can be seen in more detail in Figure 3. Given the im-
portance of interpreting the model trajectory to a given prediction, to visualize
the influence of each feature on the prediction of a specific sample, a force plot
was built. The SHAPley values for each training sample is shown in Figure 4.
Partial dependence plots can be seen in Figure 5.
In addition to the main pipeline, 1,000 different random training and test-
ing splits were sampled in order to fit the final model. The estimates can be
visualized in Figure 6. In this way, an adequate AUC can be seen in the model
performance — within the estimated confidence intervals — including a robust-
ness in the predictive power shown through the resamples. Along with the ran-
dom splits, we also performed a permutation test as proposed by Fisher (1935)
to compare the distribution of ROC AUC performance of random rearrange-
ments of the outcome with the original test data using 10,000 permutations.
We observed statistical difference between the original and permuted models
(p<0.001). The distribution of the permuted AUCs is available in Figure 7. This
result shows that our model predictions for BD incidence after the five years are
more accurate than random classifiers. 1Class decision boundary separates the data points into classes, where the algorithm switches
from one class to another. In the present paper, a threshold of 0.5 was used. If a prediction had a
probability ≥0.5, it was classified as a positive instance, otherwise, as a negative one. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint 0.00 0.25 0.50 0.75 1.00 Sensitivity 0.00
0.25
0.50
0.75
1.00 1 - Specificity 0.00 0.25 0.50 0.75 1.00 Precision 0.00
0.25
0.50
0.75
1.00 Recall Figure 2: Receiver operating characteristic (ROC) curve and precision recall (PR) curve of the
final model fitted on the training set with best parameter combination from cross-validation step,
assessed on test set, with an area under the ROC curve of 0.786 and area under the PR curve of
0.208. 0.887
0.628
0.368
0.231
0.184
0.082
0.075
0.051
0.047
0.024
0.020
Middle Socioeconomic Status Tachylalia (HCL-32 item 18) Current Romantic Relationship Sex Frequency (At Least Once a Week) Lower Socioeconomic Status Lifetime Cocaine Use Self-Confidence (HCL-32 item 3) Self-Reported Stress Problems Depressive Episode at Baseline (MINI) Sadness (BDI item 1) Feeling like a Failure (BDI item 3) -1.0
-0.5
0.0
0.5
1.0 SHAP value Figure 3: SHAPley values for each feature included in the final model. Each y-axis tick represents a
feature, sorted by the highest absolute contribution across all observations, regardless of the direc-
tion of the association. Each dot represents a participant in the training set (n = 763). Observations
with SHAPley values lower than zero behaved as protective factors, otherwise they were risk fac-
tors. The fill color represents the value of the variable for a given individual (purple corresponds to
higher values, and yellow corresponds to lower values). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint -3 -2 -1 0 1 2 0
200
400
600
800 Observation SHAP values by feature Feature Feeling like a Failure (BDI item 3)
Sadness (BDI item 1)
Depressive Episode at Baseline (MINI)
Self-Reported Stress Problems
Self-Confidence (HCL-32 item 3)
Lifetime Cocaine Use
Lower Socioeconomic Status
Sex Frequency (At Least Once a Week)
Current Romantic Relationship
Tachylalia (HCL-32 item 18)
Middle Socioeconomic Status Figure 4: SHAPley force plot. The y-axis demonstrates the influence of each feature on current
prediction based on SHAPLey values.
The x-axis represents all samples used to train the final
model. The whole training set (n = 763) is presented. Table 1: Sociodemographic features measured at baseline grouped by bipolar disorder diagnosis in
the follow-up. Features
With BD (n = 49)
Without BD (n = 1042)
Overall (n = 1091)
p-value Sex
0.302
Male
16 (32.7%)
457 (43.9%)
473 (43.4%)
Female
33 (67.3%)
585 (56.1%)
618 (56.6%) Age
0.353
Mean (SD)
20.9 (2.28)
20.5 (2.10)
20.5 (2.11)
Median [Min, Max]
21.0 [17.0, 25.0]
20.0 [16.0, 26.0]
20.0 [16.0, 26.0]
Missing
0 (0%)
8 (0.8%)
8 (0.7%) Socioeconomic status
0.635
Upper
14 (28.6%)
401 (38.5%)
415 (38.0%)
Middle
26 (53.1%)
509 (48.8%)
535 (49.0%)
Lower
9 (18.4%)
132 (12.7%)
141 (12.9%) Skin color
0.800
White
34 (69.4%)
765 (73.4%)
799 (73.2%)
Non-white
15 (30.6%)
273 (26.2%)
288 (26.4%)
Missing
0 (0%)
4 (0.4%)
4 (0.4%) Currently working
0.965
No
8 (16.3%)
245 (23.5%)
253 (23.2%)
Yes
14 (28.6%)
380 (36.5%)
394 (36.1%)
Missing
27 (55.1%)
417 (40.0%)
444 (40.7%) Has a partner
0.953
No
13 (26.5%)
247 (23.7%)
260 (23.8%)
Yes
33 (67.3%)
696 (66.8%)
729 (66.8%)
Missing
3 (6.1%)
99 (9.5%)
102 (9.3%) The p-values were calculated from t-tests for numeric features and χ-squared tests for categorical features. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Table 2: Missing values of each feature in the entire dataset (n = 1091), before data splitting procedure. Only features with at least one missing value are
present in the table. All features were collected at baseline. Feature
Number of missing values
% of missing values Age
8
0.73
Skin color
4
0.37
Currently working (last year)
444
40.70
Deceased parents
1
0.09
Has a religion
1
0.09
Psychiatric hospitalization
2
0.18
Stress problems
2
0.18
Psychiatric medication use
8
0.73
Maternal stress problems
1
0.09
Paternal stress problems
1
0.09
Sibling stress problems
2
0.18
Grandparents stress problems
1
0.09
Partner suicide attempt
753
69.02
Children suicide attempt
785
71.95
Paternal suicide attempt
725
66.45
Maternal suicide attempt
718
65.81
Sibling suicide attempt
725
66.45
Friend suicide attempt
712
65.26
Lifetime sexual intercourse
8
0.73
Worked for money
6
0.55 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Table 2 continued from previous page Feature
Number of missing values
% of missing values Has a significant disease
2
0.18
Has ever visited a psychiatrist of psychologist
3
0.27
Has ever been treated by a psychiatrist or psychologist
107
9.81
BSS item 1
4
0.37
BSS item 2
2
0.18
BSS item 3
4
0.37
BSS item 4
1
0.09
BSS item 5
1
0.09
BSS item 6
1
0.09
BSS item 7
1
0.09
BSS item 8
1
0.09
BSS item 9
1
0.09
BSS item 10
1
0.09
BSS item 11
1
0.09
BSS item 12
1
0.09
BSS item 13
1
0.09
BSS item 14
1
0.09
BSS item 15
1
0.09
BSS item 16
1
0.09
BSS item 17
1
0.09
BSS item 18
1
0.09 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Table 2 continued from previous page Feature
Number of missing values
% of missing values BSS item 19
1
0.09
BSS item 20
3
0.27
BSS item 21
1057
96.88
Knows someone who tried suicide
1
0.09
BDI item 1 (sadness)
1
0.09
BDI item 2 (hopelessness about the future)
1
0.09
BDI item 3 (feeling like a failure)
1
0.09
BDI item 4 (anhedonia)
1
0.09
BDI item 5 (sense of guilt)
1
0.09
BDI item 6 (feel that you are being punished)
1
0.09
BDI item 7 (self-hatred)
1
0.09
BDI item 8 (blame yourself for everything)
1
0.09
BDI item 9 (death thoughts)
1
0.09
BDI item 10 (excessive crying)
1
0.09
BDI item 11 (irritability)
1
0.09
BDI item 12 (loss of interest in personal relationships)
1
0.09
BDI item 13 (impaired decision making)
1
0.09
BDI item 14 (self-image)
1
0.09
BDI item 15 (occupational performance)
1
0.09
BDI item 16 (sleep)
1
0.09
BDI item 17 (fatigue)
1
0.09 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Table 2 continued from previous page Feature
Number of missing values
% of missing values BDI item 18 (loss of appetite)
1
0.09
BDI item 19 (weight loss)
1
0.09
BDI item 20 (physical health concern)
1
0.09
BDI item 21 (loss of sexual interest)
1
0.09
Age that first used drugs
139
12.74
Age of first sexual intercourse
111
10.17
Sex frequency (sexual intercourse in the last week)
106
9.72
Condom use during last sexual intercourse
101
9.26
Alcohol use before the last sexual intercourse
101
9.26
Has a partner
102
9.35
Number of people you had sex with in the last year
138
12.65
Number of times you have gotten or made someone pregnant
114
10.45
Have you experienced forced sex?
405
37.12
Current anorexia nervosa
1
0.09
Antisocial personality disorder
2
0.18
Do you have access to psychotherapy (public or private healthcare)?
964
88.36
Current melancholic depressive episode
1
0.09
Past depressive episode
1
0.09 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Table 3: Hyperparameter set chosen for final model based on highest area under the receiver oper-
ating characteristic curve in cross-validation routine. Hyperparameter
Value on final model mtry
9
trees
1429
min n
10
tree depth
14
learn rate
0.0017
loss reduction
0.6120
sample size
0.9075 4. Discussion We proposed to create a model capable of predicting BD in a 5-year inter-
val with acceptable classification performance. Our final model performed with
good metrics (AUC: 78.6%), suggesting good predictive capacity. To the best of
our knowledge, this is the second Brazilian study to investigate the prediction of
bipolar disorder incidence. A previous study investigated the development of a
prediction model, with the use of elastic net algorithms, to identify participants
who would develop bipolar disorder over the follow-up, in a large community
birth cohort, from the city of Pelotas in Brazil (Rabelo-da Ponte et al., 2020).
According to the results of this investigation, the model with the best perfor-
mance (AUC of 0.82) predicted bipolar disorder at the age of 22 years, using
clinical and sociodemographic data from the age of 18 years (Rabelo-da Ponte
et al., 2020). A recent systematic review on clinical prediction models in psy-
chiatry pointed to several aspects that predictive models could improve, such as
overfitting prevention, generalizability and clinical utility (Meehan et al., 2022).
The present paper used a larger sample than most studies to predict BD with
statistical learning, despite having a low value of events per variable (EPV) of
approximately 5.8.
The current study corroborates previous findings in which depressive symp-
toms would be one of the main predictors for BD conversion (Hafeman et al.,
2017; Perich et al., 2015). Notably, the three primary factors found by the pre-
diction model for BD developed in this paper are correlated constructs linked
to depression: failure feeling, sadness and current depressive episode. This
finding suggests that these factors could be prodromal symptoms of the disor-
der (Faedda et al., 2019; Van Meter et al., 2016), or even evidence of genetic
predisposition to emotional distress (Smeland et al., 2018). It also reinforces
the perspective of BD as a worsening trajectory, and the first mood episode as
a milestone signaling for a complex disorder onset (Duffy et al., 2014). In the
vast majority of cases, the first mood episode of a patient with BD is a depres-
sive one, often years prior to a manic episode (Mesman et al., 2017; Duffy et al., . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint Table 4: Performance metrics from the XGBoost model applied on testing set using 0.5 as threshold
for positive classification. Area under the receiver operating characteristic curve: 0.786. Performance metrics
XGBoost on test set Sensitivity (recall)
0.375
Specificity
0.920
PPV
0.194
NPV
0.966
Balanced accuracy
0.647
Accuracy
0.893
F1-score
0.255 Positive predictive value or precision (PPV); Neg-
ative predictive value (NPV). 20% 30% 40% 0
1
2
3 Feeling like a Failure (BDI item 3) 15% 20% 25% 30% 35% 0
1
2
3 Sadness (BDI item 1) Figure 5: Partial dependence plots for continuous depression-related features. It shows the average
trend of each feature. Other variables are held constant. The plots show an upward trend, which
indicates that the higher the values of the variables of feeling like a failure and sadness, the higher
the predicted probabilities for developing bipolar disorder after five years. The blue line indicates a
regression line using the LOESS (locally weighted polynomial regression) method. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint 0 25 50 75 100 0.5
0.6
0.7
0.8
0.9 Area under the receiver operating characteristic curve # of resamples Figure 6: Histogram of the area under the receiver operating characteristic curve (AUC) based on
1,000 random training and testing data splits. The AUC mean and 95% CI found were 0.723 [0.719,
0.726]. This analysis is able to demonstrate the predictive performance robustness of the selected
boosting model. 0 250 500 750 1000 0.3
0.4
0.5
0.6
0.7
0.8 Permuted Area Under the ROC Curve Count Figure 7: Distribution of the areas under the receiver operating characteristic curves (AUC) from
the permutation test with 10,000 rearrangements. The red line indicates the AUC on the original
test set (AUC = 0.786). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint 2014; Mesman et al., 2013), which turns the model also useful to ease the dif-
ferential diagnosis between unipolar and bipolar depression, because together
with other predictors, it is possible to verify whether there is a greater chance
that a given current depressive episode is from a unipolar or a bipolar clinical
condition.
Lifetime cocaine use is another major predictor evaluated in our study. Sev-
eral studies have investigated the role of substance use in general and its asso-
ciation with the development of mood disorders. A cross-sectional study con-
ducted in 2013 identified subsequent mood disorders developed in individuals
with primary substance use disorder (SUD), and the average time between SUD
onset and mood disorder was 11 years (Kenneson et al., 2013). The odds of de-
veloping bipolar disorder were particularly high among individuals with drug
dependence in this study. A systematic review published in 2021 showed that
substance use is a predictor for BD and (hypo)manic symptoms (Lalli et al.,
2021). Specific data regarding cocaine use and BD has also been published. A
prospective study investigated lifetime cocaine use as a potential predictor for
conversion from major depressive disorder to bipolar disorder (de Azevedo Car-
doso et al., 2020). The study analysis showed that the risk for conversion from
major depressive disorder to BD was 3.41-fold higher in subjects who reported
lifetime cocaine use at baseline. A systematic review also found a five-fold in-
creased risk on the development of BD in individuals with lifetime cocaine use
(Marangoni et al., 2016). Therefore, we consider this finding as part of the
advancement of studies in the area, corroborating the information already es-
tablished in the literature.
This study has some positive points to be noted.
Initially, our sample is
composed of young adults between 18 and 24 years old. According to the lit-
erature, BD symptoms usually appear before the age of 25, so the population
used to build the model allows us to think about an early identification of the
disease. This is possible because the population used to build the model was in
a critical period of development for the onset of symptoms. Additionally, our
team had an external psychiatrist to confirm the diagnosis whenever there were
doubts through the standardized diagnostic interview (MINI), which sets a gold
standard for characterization of the diagnosis. The average period between the
initial interview and the follow-up was an average of five years, higher than in
other studies in this field (Ribeiro et al., 2020).
The study has a large sample, collected through a probabilistic sample, ob-
tained from the population of a city in southern Brazil with approximately
343,651 inhabitants. These factors bring robustness to our model. However,
the outcome presented is difficult to predict due to: 1) the rarity of the out-
come and 2) control participants may develop BD later. Nonetheless, this is a
common challenge in studies in this area and we try to address these issues,
whenever possible, statistically. Another point that must be taken into account
when understanding the results presented here is the generalizability and ap-
plicability of the model. Studies in the area of precision psychiatry are on the
rise. In this work, we aim and manage to present satisfactory results (test AUC
0.78), however, we understand that the data presented are primarily for scien- . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint tific purposes and as a basis for future improvements. This study demonstrates
that, in the near future, it will be possible to think of a calculator capable of
being implemented in basic health systems. The information presented may be
useful especially for patients who present characteristics seen here as of poten-
tial importance in the face of the diagnosis of BD: current depressive episode,
depressive symptoms (mainly related to feelings of failure and sadness) and
lifetime use of cocaine. Such a tool has the potential for robust screening, en-
abling symptomatic treatment, ensuring a better prognosis and preventing more
severe clinical conditions.
In summary, we developed a binary model with a state-of-the-art algorithm
capable of predicting the diagnosis of BD in approximately five years in a specific
population of young adults, through clinical, socio-environmental, substance
use, sex-related variables and demographic data, collected through a probabilis-
tic sample. However, aiming for a better characterization of the BD diagnosis,
future studies should focus on making systematic follow-ups that seek to follow
these subjects during other developmental stages, as well as investing in stud-
ies that use specific risk populations, such as depressed patients or children of
parents with BD. Furthermore, the inclusion of digital health data, biological
and neuropsychological information and the use of neuroimaging can help in
the rise of new models with greater applicability for the future. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.22282507
doi: 
medRxiv preprint",1
"Aims: This study assessed an artificial intelligence (AI) model’s performance in predicting elevated brain natriuretic peptide (BNP) levels from chest radiograms and its effect on human diagnostic performance. Methods and results: Patients who underwent chest radiography and BNP testing on the same day were included. Data were sourced from two hospitals: one for model development, and the other for external testing. Two final ensemble models were developed to predict elevated BNP levels of >= 200 pg/mL and >= 100 pg/mL, respectively. Humans were evaluated to predict elevated BNP levels, followed by the same test, referring to the AI model’s predictions. The 8390 images from 1334 patients were collected for model creation, and 1713 images from 273 patients for tests. The AI model achieved an accuracy of 0.855, precision of 0.873, sensitivity of 0.827, specificity of 0.882, f1 score of 0.850, and receiver-operating-characteristics area-under-curve of 0.929. The accuracy of the testing with the 100 images by 35 participants significantly improved from 0.708±0.049 to 0.829±0.069 (P < 0.001) with the AI assistance (an accuracy of 0.920). Without the AI assistance, the accuracy of the experts was higher than that of non-experts (0.728±0.051 vs. 0.692±0.042, P = 0.030); however, with the AI assistance, the accuracy of the non-experts was rather higher than that of the experts (0.851±0.074 vs. 0.803±0.054, P = 0.033). Conclusion: The AI model can predict elevated BNP levels from chest radiograms and has the potential to improve human performance. The gap in utilizing new tools represents one of the emerging issues. Key Words: Deep learning, Neural network, Machine learning, Brain natriuretic peptide, Heart failure . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 4 Introduction Heart failure is a major cause of visits to medical facilities in both unplanned emergency situations and routine medical checkups. It is also a growing and called heart failure pandemic because of the aging population.1 However, diagnosing heart failure, which should be medically managed, can be challenging due to the variability of symptoms, especially in heart failure that is complicated by other diseases for patients visiting medical facilities or for attending physicians who are not familiar with cardiovascular disease.2 Several tests are used to evaluate heart failure. Chest X-rays are widely available, and the images can be obtained quickly; however, the evaluation of chest X-ray images requires experience and has limited sensitivity and specificity for heart failure.3 Natriuretic peptide levels are useful not only for diagnosing heart failure but also for heart failure management.4-6 However, natriuretic peptide testing requires equipment, and even if the facility has such equipment, it may not be available at all hours, as many facilities do not offer testing at night or on weekends. Furthermore, it is essential to make the decision to perform the test itself. We hope that automated support tools will be developed to assist in our daily practice of heart failure quickly and inexpensively. The rise of artificial intelligence (AI) and the evolution of computer hardware provide novel findings and solutions.7,8 With regard to image recognition, deep neural networks (deep learning) provide relatively good performance compared to those of previous architectures, and some have already been deployed in clinical practice.9-11 The aim of our study was to diagnose heart failure using chest X-ray imaging and an AI model, and to support clinical practice. We hypothesized that AI models that predict elevated brain natriuretic peptide (BNP) levels from chest X-ray images could provide excellent performance compared to experienced cardiologists and could improve the diagnostic performance of humans. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 5 Methods Study Patients and Datasets Patients who underwent chest radiography and BNP testing on the same day at Hiroshima City Asa Hospital and Hiroshima City North Medical Center Asa Citizens Hospital from October 2021 to September 2022 were eligible for this study. Since BNP testing is the first choice for natriuretic peptide testing in these hospitals, and not N-terminal pro-brain natriuretic peptide, we selected BNP for this study. We reviewed the medical records of eligible patients, including periods other than the above, and when chest X-ray and BNP testing were performed on the same day, the chest X-ray images, and plasma BNP values were collected. To increase the robustness and generalizability of the AI models, all conditions of the frontal view of chest X-ray images were collected, including anterior-posterior, posterior-anterior, standing, sitting, and supine positions, with or without inspiration, and any diseases or conditions. Lateral chest radiographs were not included in this study. According to the statement of the Japanese Heart Failure Society, we used a BNP cut-off value of 200 pg/mL for the main study and 100 pg/mL for the sub study.12 The chest X-ray images were assigned a binary label according to the cut-off value. The patients from Hiroshima City Asa Hospital were used for the training and validation datasets, while the patients from Hiroshima City North Medical Center Asa Citizens Hospital were used for the external test dataset. The study patients from Hiroshima City Asa Hospital were randomly divided into two datasets for training and validation. The patients were assigned to datasets in the training: validation: testing dataset ratio of approximately 0.66:0.17:0.17. Many patients had multiple pairs of chest X-ray images and BNP labels, and each patient’s data were assigned to only one dataset to avoid overfitting. The models used in this study, along with the sample code . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 6 for their utilization, will be made available on GitHub following the publication of this study. The study was approved by the local institutional review board. Outline of an AI Model We fine-tuned 31 modified pre-trained image recognition models as weak learners to predict elevated BNP levels, and subsequently created an ensemble model. Details are provided in the Supplemental Methods. The Proposed Requirements for Cardiovascular Imaging-Related Evaluation of Models After obtaining the 31 models (weak learners), we constructed the final soft ensemble model by averaging the probabilities of the 31 models. Probabilities >= 0.5 were considered to represent BNP levels >= the cut-off value. The accuracy, precision, sensitivity (recall), specificity, F1 score, receiver-operating-characteristics (ROC) curves, and precision-recall (PR) curves were calculated using the test dataset. ROC and PR curves were constructed using probability of BNP >= cut-off value, and the area under the curve (AUC) was calculated. To avoid overfitting the test dataset, the results of the performance tests using the test dataset were not used to retrospectively train or select the models. Human Testing We evaluated human performance to predict elevated BNP levels from chest radiography. The subjects were voluntary participants from the hospitals’ staff. The general findings of heart failure seen in chest X-ray images, as well as the characteristics of BNP, were taught to those being evaluated. The test subjects were shown chest X-ray images and their corresponding BNP . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 7 labels in the training dataset for their learning phase. Then, they evaluated the 100 chest X-ray images from the test dataset and provided their binary prediction. The 100 images for human testing comprised 50 images with BNP < the cut-off value and 50 with BNP >= the cut-off value, presented in random order. After the first test, to assess whether the AI assistance could improve human diagnostic performance, the test subjects evaluated the same 100 images again, this time with reference to the predictions of the AI model. The performance of the AI model, which had the accuracy of 86% on the test dataset (approximately 10 to 20% higher than that of humans), was explained before the second test. The accuracy of the AI model for the 100 images and the ratio of the two labels were not disclosed to the test subjects until all tests were completed. An expert was defined as someone with a medical career of >= 10 years. Statistical Analysis and Calculations Continuous variables are presented as medians (with first and third quartiles) or as mean ± standard deviation (SD), and categorical variables are presented as numbers and percentages, as appropriate. We utilized Python 3.10.7 (Python Software Foundation, Delaware, USA) and TensorFlow 2.10.1 (Google LCC, Mountain View, CA, USA) for our machine learning and statistical analysis. The difference in the accuracies of the first and second human tests was tested using Welch’s t-test or paired t-test, as appropriate. A P value < 0.05 was considered statistically significant. Results Baseline Characteristics An overview of this study is shown in Graphical Abstract, and the baseline characteristics of the . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 8 study patients are shown in Table 1. No data were missing. Among the 1607 patients in the study, the diagnoses included heart failure (N = 471), acute heart failure (N = 320), coronary artery disease (N = 517), acute coronary syndrome (N = 176), hypertrophic cardiomyopathy (N = 32), interstitial pneumonia (N = 64), and hemodialysis (N = 23). In total, 10103 chest X-ray images were collected. These images were divided among the training, validation, and test datasets as follow: 1061 patients (66%) with 6697 images (66%), 273 patients (17%) with 1693 images (17%), and 273 patients (17%) with 1713 images (17%). Performance of the AI The 31 models (weak learners) were created and trained (Figure 1). Their performance is detailed in Table 2, Graphical Abstract, Figure 2, and Figure 3. The performance metrics of the final ensemble model were as follows: accuracy was 0.855, precision was 0.873, sensitivity (recall) was 0.827, specificity was 0.882, F1 score was 0.850, ROC AUC was 0.929, and PR AUC was 0.934. Performance of Human A total of 35 participants, including 20 medical doctors of whom 13 were cardiologists, were tested. The duration of medical practice among the participants was 10.2±9.0 years, with 16 identified as experts. The AI model's performance on the 100 images was as follows: the accuracy 0.920, sensitivity 0.880, specificity 0.957, and f1 score 0.917 (Graphical Abstract). Without the AI assistance, the human participants achieved an accuracy of 0.708±0.049, a sensitivity of 0.693±0.128, and a specificity of 0.722±0.144. With the AI assistance, these measures significantly improved to an accuracy of 0.829±0.068 (P < 0.001), a sensitivity of . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 9 0.787±0.113, and a specificity of 0.872±0.097. Even with the AI assistance, no human subjects surpassed the performance of the AI model in terms of accuracy, precision, specificity, or f1 score. The accuracy of the medical doctors and experts was higher than that of non-medical doctors and non-experts in the non-assisted test, respectively (0.725±0.054 vs. 0.687±0.032, P = 0.014; 0.728±0.051 vs. 0.692±0.042, P = 0.030) (Figure 4). However, with the AI assistance, the accuracy of the medical doctors was similar to that of the non-medical doctors (0.818±0.064 vs. 0.843±0.074, P = 0.289), and the accuracy of the non-experts was even higher than that of the experts (0.851±0.074 vs. 0.803±0.054, P = 0.033). In the AI-assisted test, there were 3 non-experts and 1 expert who responded entirely based on the AI model’s predictions, and these four participants achieved the highest accuracy throughout the test, with an accuracy of 0.920. In the non-assisted test, the accuracy had a weak positive correlation with the duration of medical careers (r = 0.414, P = 0.014), while in the AI-assisted test it showed a weak negative correlation (r = -0.347, P = 0.041). For the eight images that were incorrectly predicted by the AI model, the human accuracy was 0.301±0.124. Using majority voting for the hard ensemble prediction, the human accuracy was 0.800 in the initial test and 0.880 with the AI assistance. Sub Study We developed a model to predict BNP values. The mean absolute errors between the predicted and true BNP values were 208 pg/mL, with mean squared errors being 1.01*105 pg2/mL2. No significant difference was observed between the predicted and true BNP values (P = 0.274). The models’ performances in predicting elevated BNP level using a cut-off of 100 pg/mL was comparable to that of the models using a cut-off of 200 pg/mL (Table 2, Figure 2). . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 10 Discussion This study presents the development of a high-performing model that predicts elevated BNP levels from chest X-ray images, thereby improving human diagnostic accuracy. Furthermore, this study has revealed the new issue that there exists a gap among participants in the ability to effectively utilize the new AI tool. The strengths of our study are as follows: First, this is the first report demonstrating that an AI model can predict elevated BNP levels from chest X-ray images, outperforming experienced cardiologists. Our models were predicated on the hypothesis that chest X-ray images contain features associated with elevated BNP levels indicative of heart failure. Prior reports have associated chest X-ray findings such as cardiomegaly, pulmonary venous congestion, interstitial or alveolar oedema, and cephalization with heart failure prediction.1,3 These prior evaluations were human based; our study establishes superior diagnostic performance by the AI models in predicting elevated BNP levels, substantiating the one of our hypotheses. The featured map images revealed that our models capture chest X-ray findings akin to prior human reports on chest radiography and heart failure (Figure 1). Nevertheless, various factors such as age, sex, hemoglobin level, renal function, left ventricular end-diastolic pressure, and left ventricular ejection fraction influence plasma BNP levels, so BNP level cannot be perfectly evaluated solely through chest X-ray imaging.13 Conversely, the predictability, as indicated by the ROC AUC, was above 0.929 in our test dataset population, suggesting a high level predictability. Matsumoto et al. reported that their model could predict heart failure from chest radiography.14 A limitation of their study was that the diagnosis of heart failure was determined by two cardiologists using chest radiographs. Our study highlights the relative inferiority of physician performance in predicting elevated BNP levels compared to the AI model, even among experienced cardiologists. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 11 There are several reports predicting pulmonary arterial pressure, pulmonary hypertension, pulmonary wedge pressure, or extravascular lung water from chest radiographs, including similar studies by the same author groups.15-20 One limitation of their studies is the difference in timing between when the catheterization was performed and when the X-ray was taken, as the pulmonary artery pressure can greatly vary depending on the patient’s condition such as posture. The performance of the models in the studies is suboptimal, possibly due to the small sample size associated with the invasive catheterization procedures. Zou et al. reported that their model predicted pulmonary hypertension from chest radiographs with an ROC AUC of 0.967; however, in their study, a dataset excluding various conditions, such as pleural/pericardial effusion, and pneumothorax, was utilized. In contrast, our study utilized all available chest radiographs, encompassing a wide range of pathological conditions, which may contribute to the robustness and generalizability of our model. Second, the performance of our models was benchmarked against front-line physicians. Although numerous AI studies have reported that the AI models could accurately predict medical features from conventional tests, many of these did not evaluate the performance of physicians. Third, we showed that the AI model’s suggestions could enhance human diagnostic performance, and the utilization gap for new tools is an emerging issue. Regardless of the superior performance of a tool, it is useless if it is not trusted or utilized by users. While many studies have reported that AI models exceed human diagnostic performances, there has yet to be a report detailing the degree of improvement in performance when utilizing AI models, as compared to the standalone performance of the AI models or in the absence of any support. This study discovered that the performance without assistance was positively associated with the duration of medical careers. The AI assistance improved the diagnostic performance of both . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 12 inexperienced and experienced practitioners. Ironically, the inexperienced ones achieved results comparable to or even surpassing those of the experienced ones. This implies that with the aid of a potent diagnostic tool, inexperienced individuals can perform as well as or even surpass experienced ones. The limited improvement among the experts may be attributed to their confidence in their expertise and skepticism towards the AI model, despite being informed of the AI model’s superior performance compared to any human. Conversely, a less experienced individual might readily accept the AI’s prediction due to lack of confidence. Distrust in new technology or findings and self-confidence will be emerging issues in AI; this kind of skepticism towards novel approaches has always existed in other domains. In particular, the usage of generative AI has begun to be actively discussed. We should strive to understand and adapt appropriately to new ideas, technologies, and tools, including AI. Fourth, the models used in this study, along with the sample codes for their application, will be made available on GitHub following the publishment of this article. This implies that anyone can evaluate and refine the models, and challenge old notions with new ideas. Fifth, we enhanced the models using state-of-the-art deep learning techniques used in Kaggle competitions. Our models were based on these technical aspects, and our collected dataset, which was comprehensive and had not been used in previous reports regarding heart failure and chest radiography, is thought to be one of the strengths of our study and may enhance the model’s robustness and generalizability. Ensemble prediction has the potential to improve performance.21 In our study, the accuracy improved from 0.818 in the single model to 0.855 in the final ensemble model. Through the ensemble method, there is a potential enhancement not only in the described performance metrics but also in robustness and generalizability. To enhance the performance of the ensemble, . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 13 the models’ performances should be reasonably good and their prediction correlation should not be overly strong. This parallels the performance of a heart team, where members who consistently agree with others, or remain silent, contribute little to the quality of decisions, and active members lacking a certain level of performance can impair overall performance. Given that sensitivity and specificity exist in a trade-off relationship, a weak learner that may not be the best in terms of overall accuracy could potentially enhance the accuracy and generalizability of the ensemble model due to the diversity it provides. Sixth, good old chest radiography is widely available in numerous medical facilities. Technically integrating software, such as the one used in this study, into X-ray machines or smartphones is not challenging. As advancements in both hardware and software continue, the integration process may become even more streamlined, potentially allowing these tools to be used in diverse ways, with the potential to change the world. Study Limitations Natriuretic peptides are used for diagnosis of heart failure, employing either absolute values or relative changes, and for managing the condition through sequential relative changes. This study primarily conducted with binary prediction, as the main goal was to diagnose heart failure in this time. Notably, the human performance in predicting absolute BNP values from chest X-ray images was extremely poor in preliminary testing (data not shown). The choice of a cut-off BNP value warrants discussion. The cut-off values should be determined based on intended purpose while ensuring a balance between sensitivity and specificity. We set the BNP cut-off value at 200 pg/mL with the aim of diagnosing unrecognized heart failure that requires early management; however, a cut-off value of 100 or 125 pg/mL would be considered for different purposes or . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 14 applications. In this study, models with a cut-off of 100 pg/mL demonstrated performance comparable to those with a cut-off of 200 pg/mL. While we labelled the X-ray images based on the BNP cut-off value, it is important to remember that heart failure is not diagnosed based solely on natriuretic peptide values. One of our goals is to diagnose unrecognized heart failure that needs early intervention, which is not synonymous with diagnosing elevated BNP levels alone. Conclusions The AI model can predict elevated BNP levels from chest X-ray images with superior performance compared to experienced cardiologists and can improve the diagnostic performance of individuals, ranging from non-experts to experienced cardiologists. The gap in utilizing new tools represents one of the emerging issues. Acknowledgements We express our gratitude to all the participants who participated in this study. Sources of Funding None. Conflict of interest None. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted January 22, 2024. 
; 
https://doi.org/10.1101/2023.02.22.23286205
doi: 
medRxiv preprint 15",1
"Background The spread of several SARS-CoV-2 variants of concern (VOC) led to increasing numbers of patients with coronavirus disease 2019 (COVID-19) in German intensive care units (ICU), resulting in capacity shortages and even transfers of COVID-19 ICU patients between federal states in late 2021. Comprehensive evidence on the impact of predominant VOC, in this case Delta and Omicron, on inter-hospital transfers of COVID-19 ICU patients remains scarce. Methods A retrospective cohort study was conducted from July 01, 2021 until May 31, 2022 using nationwide reimbursement inpatient count data of COVID-19 ICU patients and weekly sequence data of VOC in Germany. A multivariable Poisson regression analysis was performed to estimate incidence rates and incidence rate ratios (IRR) for competing events of transfer, discharge and death, adjusted for VOC infection, age group and sex. For corresponding risk estimation, a multistate model for the clinical trajectory in ICU was applied. Results Omicron versus Delta infection yielded estimated adjusted IRR of 1.23 (95% CI, 1.16 – 1.30) for transfer, 2.27 (95% CI, 2.20 – 2.34), for discharge and 0.98 (95% CI, 0.94 – 1.02) for death. For death in ICU, estimated adjusted IRR increased progressively with age up to 4.09 (95% CI, 3.74 – 4.47) for those 90 years and older. COVID-19 ICU patients with Omicron infection were at comparatively higher estimated risk of discharge, whereas the estimated risk of transfer and death were higher for those with Delta infection. Conclusions Inter-hospital transfers and discharges occurred more frequently in COVID-19 ICU patients with Omicron infection than in those with Delta infection, who in turn had a higher estimated risk of death. Age emerges as a relevant determinant for fatal clinical trajectories in COVID-19 ICU patients and imposes close therapeutic care. Keywords Intensive care unit, SARS-CoV-2 variant of concern, inter-hospital transfer, multistate model, competing risk . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 Introduction Since early 2020, the coronavirus disease 2019 (COVID-19) pandemic has posed major challenges to the German health care system both on structural and individual level [1-5]. In particular, intensive care units (ICU) have been severely impacted during periods of increased COVID-19 caseloads with a total daily occupancy of more than 5,000 adult COVID-19 patients distributed across approximately 1,300 adult ICU nationwide [6]. Among these, the proportion of patients who required invasive ventilation or extracorporeal membrane oxygenation exceeded 60% on several occasions. In terms of mortality, the maximum number of deaths of COVID-19 patients in ICU surpassed 200 per day, cumulating to a total number of over 50,000 deaths of COVID-19 patients in German ICU by the end of May 2022 [7]. Given all these immense challenges for critical care in Germany, efficient resource allocation, including transfers of COVID-19 ICU patients between federal states, has become crucial to maintain health care delivery and avoid system collapse [8-12]. One of the main factors leading to periods of critical capacity shortages in ICU is the spread of several SARS- CoV-2 variants of concern (VOC) in the German population [13, 14]. By the end of 2020, the Delta VOC increased the risk of hospitalization and fatal outcome compared with the previously dominant Alpha VOC (15-19). ICU occupancy and respiratory support subsequently increased [15-19]. ICU occupancy and respiratory support subsequently increased [7, 20]. In contrast, the Omicron VOC, first reported in South Africa and rapidly circulated in Germany in late 2021, led to reduced disease severity, but its sub-lineages demonstrated high transmissibility and immune escape [21-27]. As a result, the number of new infections in the German population has risen sharply, and with it the number of COVID-19 ICU patients, which has even necessitated their nationwide allocation [28, 29]. So far, there is limited comprehensive information on the impact of predominant VOC on critical care and especially on inter-hospital transfers of COVID-19 ICU patients in Germany. Because similar situations might occur in the resource-constraint ICU setting, further evidence is important, in particular to promote efficient patient allocation and to reduce ICU mortality. Therefore, the objective of our study was to determine the impact of Delta and Omicron VOC on critical care in Germany with a focus on transfers of COVID-19 ICU patients between hospitals. To do so, we aimed to estimate and compare transfer, discharge and death rates in ICU associated with Delta and Omicron infection. We furthermore attempted to estimate the associated risk of transfer, discharge and death by using the aforementioned rates to model the clinical trajectory of COVID-19 ICU patients [30, 31]. Methods Data sources and structure The data for this retrospective cohort study were provided by the Institute for the Hospital Remuneration System GmbH (InEK) and include the reimbursement data of all hospitals in Germany that are subject to the data transmission obligation [32]. Data consists of nationwide total daily counts of prevalent, transferred, discharged and death COVID-19 cases admitted to a German ICU by age group (0-9, 10-19, …, 90+) and sex (female, male) from July 1, 2021 until May 31, 2022. Since the InEK only provides data on patients who have completed clinical treatment, underreporting of daily counts is to be expected at the end of this observation period, which was therefore limited to April 30, 2022. Based on weekly sequence data published by the Robert Koch Institute (RKI) . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 3 on the number and proportions of VOC in Germany, the entire analysis was further restricted to calendar periods for Delta VOC from September 13, 2021 (begin of calendar week 37) until November 21, 2021 (end of calendar week 46) and for Omicron VOC from February 14, 2022 (begin of calendar week 7) until April 30, 2022 [13]. Within these calendar periods almost exclusively Delta and Omicron VOC were sequenced, including their respective sub-lineages, and we therefore used them as surrogate measures of infection impact in our analysis. Thus, the sequenced VOC were not considered individually but grouped into Delta and Omicron infection categories, hereafter referred to as VOC infection. Basic multistate model Based on InEK data, a multistate approach (Fig. 1) was used to model the clinical trajectory of COVID-19 patients in ICU and considering the competing risk of transfer, discharge and death [33, 34]. Accordingly, patients begin with ICU admission (state 0). They may remain in this state until they are either transferred to another hospital as part of clinical care (state 1), discharged from ICU (state 2) or die in ICU (state 3). Consequently, COVID-19 patients may also be discharged or die in ICU after an inter-hospital transfer (from state 1 to state 2 or state 3). The multistate model depends on the hazard rates (𝛼𝛼01, 𝛼𝛼02, 𝛼𝛼03, 𝛼𝛼12, 𝛼𝛼13) between the states admission, transfer, discharge and death. Given the InEK data structure, these hazard rates depend on VOC infection, age group and sex. However, within each of these categories, the hazard rates were assumed to remain constant over time, discharge and death rates to be independent of inter-hospital transfers (𝛼𝛼02  =  𝛼𝛼12,  𝛼𝛼03  =  𝛼𝛼13) and inter-hospital transfers to occur only once during the clinical treatment of a COVID-19 ICU patient. As just mentioned, we also assumed that discharges (state 2) and deaths (state 3) always occurred in ICU, although this was not unambiguous from the data provided and both events may have also occurred outside the ICU during inpatient follow-up. Fig. 1 Schematic representation of a multistate model for the clinical trajectory of COVID-19 patients in German intensive care units based on data from the Institute for the Hospital Remuneration System GmbH (InEK) . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 4 Statistical analysis Based on our assumption of constant hazard rates, a multivariable Poisson regression analysis was performed to estimate the incidence rates and incidence rate ratios (IRR) of transfer, discharge, and death of COVID-19 ICU patients associated with VOC infection. Hence, a separate Poisson regression model was fitted for these outcomes, adjusted for categories of VOC infection, age group and sex of the InEK data (Table 1). Delta was the VOC infection reference due to the timing. The 60-69 age group was chosen as the age reference since it has the highest count of prevalent COVID-19 cases. For sex, female COIVD-19 ICU patients were the reference but without specific reason. For interpretation, an estimated IRR above 1 (below 1) indicates a higher (lower) event rate of transfer, discharge and death in the categories VOC infection, age group and sex compared with their reference. To estimate the risk of transfer, discharge and death of COVID-19 ICU patients associated with VOC infection, we adapted the approach of von Cube et al. [35] and converted the constant hazard rates between the states (Fig. 1) into estimated cumulative probabilities, i.e., risks adjusted for categories of VOC infection, age group and sex dependent on time (day) since ICU admission, noted as t: 𝛼𝛼0 = −(𝛼𝛼01 + 𝛼𝛼02 + 𝛼𝛼03) 𝛼𝛼1 = −(𝛼𝛼12 + 𝛼𝛼13) 𝑃𝑃
00(𝑡𝑡) = 𝑒𝑒𝛼𝛼0⋅𝑡𝑡 𝑃𝑃
01(𝑡𝑡) =
𝛼𝛼01
𝛼𝛼0 −𝛼𝛼1 (𝑒𝑒𝛼𝛼0⋅𝑡𝑡−𝑒𝑒𝛼𝛼1⋅𝑡𝑡) 𝑃𝑃
02(𝑡𝑡) = 𝛼𝛼01𝛼𝛼12
𝛼𝛼0𝛼𝛼1 (1 −𝑒𝑒𝛼𝛼0⋅𝑡𝑡) +
𝛼𝛼01𝛼𝛼12
𝛼𝛼0(𝛼𝛼0 −𝛼𝛼1) (𝑒𝑒𝛼𝛼0⋅𝑡𝑡−𝑒𝑒𝛼𝛼1⋅𝑡𝑡) −𝛼𝛼02
𝛼𝛼0 (1 −𝑒𝑒𝛼𝛼0⋅𝑡𝑡) 𝑃𝑃
03(𝑡𝑡) = 𝛼𝛼01𝛼𝛼13
𝛼𝛼0𝛼𝛼1 (1 −𝑒𝑒𝛼𝛼0⋅𝑡𝑡) +
𝛼𝛼01𝛼𝛼13
𝛼𝛼0(𝛼𝛼0 −𝛼𝛼1) (𝑒𝑒𝛼𝛼0⋅𝑡𝑡−𝑒𝑒𝛼𝛼1⋅𝑡𝑡) −𝛼𝛼03
𝛼𝛼0 (1 −𝑒𝑒𝛼𝛼0⋅𝑡𝑡) The cumulative probability of being transferred to another hospital was estimated by: 𝐶𝐶𝐶𝐶𝐹𝐹
01(𝑡𝑡) = 𝛼𝛼01 𝛼𝛼0 (1 −𝑒𝑒−𝛼𝛼0⋅𝑡𝑡) The cumulative probability of being discharged from ICU was estimated by 𝐶𝐶𝐶𝐶𝐶𝐶02(𝑡𝑡), equal to 𝑃𝑃
02(𝑡𝑡), and the cumulative probability to die in ICU was estimated by 𝐶𝐶𝐶𝐶𝐶𝐶
03 (𝑡𝑡), equal to 𝑃𝑃
03(𝑡𝑡). In the following, we primarily considered 𝐶𝐶𝐶𝐶𝐹𝐹
01(𝑡𝑡), 𝐶𝐶𝐶𝐶𝐶𝐶
02(𝑡𝑡),  𝐶𝐶𝐶𝐶𝐶𝐶
03 (𝑡𝑡) associated with VOC infection up to day 15 since ICU admission (Fig. 2 – 4), although longer durations of treatment were of course possible, and by age group, as age had already been shown to be a major determinant for COVID-19 related in-hospital mortality [36-38]. All analyses were performed using R version 4.2.2. Results Transfer, discharge and death rates of COVID-19 patients in intensive care During the selected calendar periods, a total of 6,046 transfers, 35,425 discharges and 12,114 deaths were observed. The multivariable Poisson regression analysis showed that Omicron infection was associated with higher transfer and discharge rates in COVID-19 ICU patients than Delta infection, with estimated adjusted IRR of 1.23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 5 (95% CI, 1.16 – 1.30) and 2.27 (95% CI, 2.20 – 2.34), respectively (Table 1). With an estimated adjusted IRR of 0.98 (95% CI, 0.94 – 1.02), Delta and Omicron infection were both associated with nearly similar death rates. Compared with the 60-69 age group, younger COVID-19 ICU patients were predominately associated with higher transfer rates, whereas older patients were predominantly associated with lower transfer rates (Table 1). For discharge, the estimated adjusted IRR peaked at 2.40 (95% CI, 2.15 – 2.67) in the 10-19 age group, then decreased steadily until the 60-69 age group and then increased again for older COVID-19 ICU patients. For death, younger age groups were associated with lower incidence rates compared with the 60-69 age group, whereas older COVID- 19 ICU patients were associated with higher death rates, up to an estimated adjusted IRR of 4.09 (95% CI, 3.74 – 4.47) for the 90+ age group. When comparing sex categories, male COVID-19 ICU patients were associated with lower transfer and discharge rates but with an estimated adjusted IRR of 1.04 (95% CI, 1.00 – 1.08) with a death rate nearly similar to that of female COVID-19 ICU patients (Table 1). Risk of transfer, discharge and death of COVID-19 patients in intensive care The multistate analysis yielded a slightly higher estimated risk of transfer for COVID-19 ICU patients with Omicron infection at the beginning of observed ICU stay compared with those with Delta infection across all age groups (Fig. 2). However, the estimated risk of transfer associated with Omicron infection decreased with longer observed ICU stay and fell below the estimated risk of transfer associated with Delta infection, where estimated cumulative probabilities were still above 0.12 on day 15 after ICU admission at ages 10 to 59 years. For discharge, estimated cumulative probabilities decreased across age groups (Fig. 3). For example, on day 10 after ICU admission, the estimated cumulative probability associated with Omicron infection was 0.77 for the 0-9 age group and 0.53 for 80-89 age group, respectively 0.48 (0-9 age group) and 0.29 (80-89 age group) with Delta infection. Thus, COVID-19 ICU patients with Omicron infection were consistently found to have a higher estimated risk of discharge compared with those with Delta infection across all age groups. For death, estimated cumulative probabilities increased sharply with age (Fig. 4). For example, the estimated cumulative probability on day 10 after ICU admission associated with Omicron infection was 0.01 for the 0-9 age group and 0.26 for the 80-89 age group, respectively 0.02 (0-9 age group) and 0.33 (80-89 age group) with Delta infection. Consequently, COVID-19 ICU patients with Omicron infection were consistently found to have a lower estimated risk of death compared with those with Delta infection across all age groups. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 6 Table 1 Estimated incidence rate ratios with 95% confidence intervals and p-values of transfer, discharge and death of COVID-19 patients in German intensive care units, adjusted for SARS-CoV-2 variant of concern infection, age group and sex a Confidence interval b P-value Transfer 
Discharge 
Death Predictors 
Incidence Rate Ratio 
95% CIa 
pb 
Incidence Rate Ratio 
95% CIa 
pb 
Incidence Rate Ratio 
95% CIa 
pb Delta 
1.00 
Reference 
 
1.00 
Reference 
 
1.00 
Reference Omicron 
1.23 
1.16 – 1.30 
<0.001 
2.27 
2.20 – 2.34 
<0.001 
0.98 
0.94 – 1.02 
0.298 Age 0-9 
0.57 
0.41 – 0.76 
<0.001 
1.88 
1.72 – 2.05 
<0.001 
0.14 
0.08 – 0.22 
<0.001 Age 10-19 
1.54 
1.17 – 1.99 
0.001 
2.40 
2.15 – 2.67 
<0.001 
0.34 
0.21 – 0.51 
<0.001 Age 20-29 
1.54 
1.27 – 1.84 
<0.001 
2.33 
2.15 – 2.53 
<0.001 
0.30 
0.22 – 0.41 
<0.001 Age 30-39 
1.52 
1.33 – 1.72 
<0.001 
1.76 
1.65 – 1.88 
<0.001 
0.41 
0.34 – 0.49 
<0.001 Age 40-49 
1.35 
1.21 – 1.50 
<0.001 
1.40 
1.32 – 1.48 
<0.001 
0.52 
0.46 – 0.58 
<0.001 Age 50-59 
1.19 
1.09 – 1.30 
<0.001 
1.17 
1.12 – 1.23 
<0.001 
0.70 
0.65 – 0.76 
<0.001 Age 60-69 
1.00 
Reference 
 
1.00 
Reference 
 
1.00 
Reference Age 70-79 
0.90 
0.83 – 0.98 
0.015 
1.02 
0.98 – 1.06 
0.319 
1.50 
1.42 – 1.59 
<0.001 Age 80-89 
0.98 
0.90 – 1.07 
0.640 
1.31 
1.26 – 1.37 
<0.001 
2.58 
2.44 – 2.72 
<0.001 Age 90+ 
1.18 
0.99 – 1.41 
0.065 
2.01 
1.87 – 2.16 
<0.001 
4.09 
3.74 – 4.47 
<0.001 Female 
1.00 
Reference 
 
1.00 
Reference 
 
1.00 
Reference Male 
0.90 
0.85 – 0.95 
<0.001 
0.83 
0.81 – 0.85 
<0.001 
1.04 
1.00 – 1.08 
0.075 7 Fig. 2 Estimated cumulative probabilities of transfer of COVID-19 patients in German intensive care units up to day 15 since admission, stratified by SARS-CoV-2 variant of concern infection and age group Fig. 3 Estimated cumulative probabilities of discharge of COVID-19 patients in German intensive care units up to day 15 since admission, stratified by SARS-CoV-2 variant of concern infection and age group . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 8 Fig. 4 Estimated cumulative probabilities of death of COVID-19 patients in German intensive care units up to day 15 since admission, stratified by SARS-CoV-2 variant of concern infection and age group Discussion In this retrospective cohort study, we aimed to determine the impact of Delta and Omicron VOC on critical care in Germany, particularly on inter-hospital transfers of COVID-19 ICU patients. Therefore, we performed a multivariable Poisson regression analysis to estimate the incidence rates and IRR of transfer, discharge, and death in COVID-19 ICU patients associated with Delta and Omicron infection. In addition, to estimate the associated risk of transfer, discharge and death, we applied a multistate model for the clinical trajectory of COVID-19 patients that accounts for these competing events during ICU stay. Our analysis was based on nationwide inpatient reimbursement data provided by the InEK and the calendar periods for Delta and Omicron VOC, which served as surrogate measures of infection impact, were selected using weekly sequence data on the number and proportion of VOC in Germany published by the RKI [13, 32]. Transfer, discharge and death rates of COVID-19 patients in intensive care The results of our multivariable Poisson regression analysis reveal the differential impact of the studied VOC on ICU stay of COVID-19 patients in Germany, for whom Omicron infection was associated with higher transfer and discharge rates compared with Delta infection. One plausible explanation might be comparatively more stable patients with a better prognosis, which made it possible to free up ICU capacities at all during periods of increased resource demand [39, 40]. Accordingly, the finding that ICU patients with Omicron infection were transferred and discharged at higher rates, thus more frequently, compared with those with Delta infection suggests that the severity of COVID-19 disease was also lower and confirms previous evidence [23, 24]. With respect to age, our findings show that older COVID-19 ICU patients were transferred and discharged less frequently, but died more . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 9 frequently. This supports existing findings indicating the relevance of age, particularly in the context of COVID- 19 related in-hospital mortality and the consequent need of close therapeutic care [36-38]. In addition, we found that male COVID-19 ICU patients were transferred and discharged less frequently compared to female COVID- 19 ICU patients, suggesting worse clinical progressions [41, 42]. Risk of transfer, discharge and death of COVID-19 patients in intensive care The differential severity of COVID-19 disease associated with the studied VOC becomes even more apparent when considering the results of our multistate approach: ICU patients with Delta infection had a comparatively higher estimated risk of inter-hospital transfer than those with Omicron infection; across all age groups and the longer the observed ICU stay. In addition, the estimated risk of being discharged from ICU was found to be comparatively higher for COVID-19 patients with Omicron infection, whereas the estimated risk to die in ICU was higher for those with Delta infection. Hence, our findings support previous studies showing the reduced severity of COVID-19 infection with Omicron [24, 43-45]. Irrespective of predominant VOC, the influence of age became obvious and particularly so for the estimated risk of death in ICU, again indicating the close therapeutic demands in hospitalized elderly COVID-19 patients [36-38]. Conversion of rates to risks in the presence of competing events At first sight, one might suspect a contradiction when comparing our results on the rates and risks of the competing events of transfer, discharge and death. According to the multivariable Poisson regression analysis, Omicron infection was associated with comparatively higher transfer and discharge rates in COVID-19 ICU patients than Delta infection, whereas the associated death rates were almost the same for both VOC infections. However, converting rates to cumulative probabilities, i.e., risks, using the multistate model yielded comparatively lower estimated risk of transfer and death for COVID-19 ICU patients with Omicron infection across all age groups. This apparent contradiction is explained by the fact that rate and risk are different measures and, according to the multistate model, the estimated risk for each competing event 𝐶𝐶𝐶𝐶𝐹𝐹01(𝑡𝑡), 𝐶𝐶𝐶𝐶𝐶𝐶
02(𝑡𝑡),  𝐶𝐶𝐶𝐶𝐶𝐶
03 (𝑡𝑡) depend on all corresponding hazard rates [46, 47]. For example, COVID-19 ICU patients with Omicron infection were transferred more frequently than those with Delta infection, with an estimated adjusted IRR above 1.0. However, when the competing event of discharge with an estimated adjusted IRR even further above 1.0 was also considered, this difference in transfer rates converted into a comparatively lower estimated risk of transfer for COVID-19 ICU patients with Omicron infection. Likewise, even though COVID-19 patients with Omicron infection died not less frequently in ICU than those with Delta infection, with an estimated adjusted IRR ~1, their estimated risk of death was comparatively lower since they were discharged more frequently. Thus, the modelled clinical trajectory of COVID-19 ICU patients demonstrates the loss of the one-to-one correspondence between rate and risk, which is a distinctive feature of competing event scenarios [48]. Strengths and Limitations One of the strengths of the present study is the representativeness of the data source, which includes the remuneration data of all hospitals in Germany that are subject to the data transmission obligation to the InEK [32]. These data allowed us to gain further insight into critical care during the dynamic pandemic situation in Germany. In this context, our findings demonstrate differential severity of COVID-19 disease in ICU patients associated with Delta and Omicron infection and the related allocation of critically ill COVID-19 patients between hospitals. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 10 Moreover, the relevance of age for COVID-19 related in-hospital mortality was further substantiated [36-38]. From a methodological perspective, we have shown that the multistate approach is an appropriate means of modeling the state transitions of ICU patients in the presence of competing risks and using aggregate count data [30, 31, 33-35]. To our knowledge, there are no comparable approaches to address the rates and risks of inter- hospital transfers of COVID-19 ICU patients. An important limitation with regard to the chosen methodology of our study are the underlying assumptions of constant hazard rates, independent discharge and death rates with respect to inter-hospital transfers, COVID-19 ICU patients were transferred only once during their inpatient treatment and discharges and deaths occurred exclusively in ICU. This might not reflect the actual course of clinical care. Second, we chose calendar periods as surrogate measures of the impact of Delta and Omicron infection, although other population-based measures, such as vaccination coverage or nonpharmacologic interventions, and patient characteristics may also have an influenced the situation in German ICU during these periods. However, these effects could not be separated by the data used. Adequate adjustment of the multistate model and more sophisticated analysis would therefore require information on patient-level. Conclusions Comparison of the impact of Delta and Omicron VOC on critical care in Germany points toward a reduced severity of COVID-19 disease in ICU patients with Omicron infection, as they were transferred and discharged more frequently than those with Delta infection, who in turn had a substantially higher estimated risk of death. Age emerges as a relevant determinant for fatal clinical trajectories in COVID-19 ICU patients and imposes close therapeutic care. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.31.23287964
doi: 
medRxiv preprint 11",1
"Background: A granular understanding of respiratory syncytial virus (RSV) burden in England is needed to prepare for new RSV prevention strategies. We estimated the rates of RSV hospital admissions in infants before age two in England and describe baseline characteristics. Methods: A birth cohort of all infants born between 01/03/2015 and 28/02/2017 (n=449,591) was established using Clinical Practice Research Datalink-Hospital Episode Statistics. Case cohorts included infants with an admission for 1) RSV-coded, 2) bronchiolitis-coded, 3) any respiratory tract infection (RTI)-coded <24 months and 4) RSV-predicted by an algorithm <12 months. Baseline characteristics were described in case and comparative cohorts (infants without corresponding admission). Cumulative incidence and admission rates were calculated. Multiple linear regression was used to estimate the proportion of RTI healthcare visits attributable to RSV. Results: The RSV-coded/RSV-predicted case cohorts were composed of 4,813/12,694 infants (cumulative incidence: 1.1%/2.8%). Case cohort infants were more likely to have low birth weight, comorbidities and to be born during RSV season than comparative cohort infants, yet >77% were term healthy infants and >54% born before the RSV season. During the first year of life, 11.6 RSV-coded and 34.4 RSV-predicted hospitalizations occurred per 1,000 person-years. Overall, >25% of unspecified lower RTI admissions were estimated to be due to RSV. Conclusions: In England, one in 91 infants had an RSV-coded admission, likely underestimated by ~3-fold. Most infants were term healthy infants born before the RSV season. To decrease the total burden of RSV at the population level, immunization programs need to protect all infants. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288132
doi: 
medRxiv preprint 3 Introduction Respiratory syncytial virus (RSV) causes substantial burden on healthcare systems worldwide.1 Although most infants manifest mild symptoms manageable at home, the clinical manifestations of an RSV infection may range from a mild upper respiratory tract infection (URTI) to a severe lower respiratory tract infection (LRTI) and hospitalization.2 There is emerging evidence that it can also lead to long term respiratory morbidity.3–5 RSV is the main cause of acute LRTI in children globally.1 A recent systematic review estimated that RSV is responsible for 33 million episodes of LRTI, resulting in 3.6 million hospital admissions, around 26,300 in-hospital deaths, and >100,000 overall deaths per year in children aged <5 years worldwide.1 In England, between 2007 and 2017, 7,062 RSV-coded hospitalizations occurred annually in children aged <5 years, and, of those, 6,337 occurred in infants <1 year, corresponding to a rate of 8.6 per 1,000 infants.6 Likely due to lack of testing for RSV7, the estimates of RSV morbidity and mortality based on RSV diagnosis codes alone are most likely underestimated6 and other methods, such as statistical modeling8,9 or the use of proxies for RSV activity like bronchiolitis10–12, have been used. Recognizing this underestimation, Reeves et al. developed an algorithm which estimated that 33,561 RSV-associated hospitalizations occurred annually in children aged <5 years in England between 2007 and 2012, at a rate of 35.1 per 1,000 infants aged <1 year.8 This is ~4 times higher than RSV diagnosis codes alone suggest.6 The same authors concluded that ~78% of hospitalizations for bronchiolitis in children aged <5 years may be RSV related, and increases to 82% in infants <6 months.8 The likelihood of developing severe RSV infection is higher in infants with known risk factors such as chronic lung disease (CLD) of prematurity, congenital heart disease (CHD), neuromuscular disorders, immunodeficiencies, and pre-term birth.13,14 Infants born just before . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288132
doi: 
medRxiv preprint 4 and during the RSV season, which occurs from October to February in England, also have a higher risk of serious RSV infection.15 This study aimed to estimate the rates of RSV hospital admissions in infants up to 24 months old in England, using different approaches, including proxies and statistical modeling, and describe the sociodemographic and clinical characteristics of infants with a hospitalization born before and during the season. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288132
doi: 
medRxiv preprint 5 Materials and Methods Data sources This was a retrospective cohort study, using Clinical Practice Research Datalink (CPRD)- HES (Hospital Episode Statistics) linked data. CPRD is a longitudinal primary care electronic medical record data source, that covers 20% of the English population.16 HES contains details of all admissions to National Health Service (NHS) hospitals in England. HES data can be linked at patient-level for a subset of CPRD-contributing practices. Cohorts’ definitions Diagnosis coded case cohorts A birth cohort of all infants born between 01/03/2015 and 28/02/2017 within the CPRD-HES linked dataset was established (n=449,591). Three diagnosis coded case cohorts were defined, each included infants who had at least one hospital admission of interest up to 24 months of age, coded in HES with: 1) J12.1, J20.5, J21.0, or B97.4 (only included if the infant had also an RTI code in the same admission) - RSV-coded case cohort; 2) J21 - bronchiolitis-coded case cohort; and 3) J00-06, J12-J18, J20-22 - RTI-coded case cohort. RSV-predicted case cohort A complementary RSV-predicted case cohort, based on the Reeves at al.9 published algorithm, was defined, and included infants identified by the logistic regression prediction model defined below. ",1
"Background: Prostate adenocarcinoma (PRAD) is the most common cancer in men worldwide, yet gaps in our knowledge persist with respect to molecular bases of PRAD progression and aggression. It is largely an indolent cancer, asymptomatic at early stage, and slow-growing in most cases, but aggressive prostate cancers cause significant morbidity and mortality within five years. Automated methods to type the aggressiveness of PRAD are necessary and urgent for informed treatment management. Methods: Based on TCGA transcriptomic data pertaining to PRAD and the associated clinical metadata, we used the grading guidelines of the International Society of Urological Pathology (ISUP), and converted the clinical information of a cancer sample to its Gleason grade. To model the distinction between aggressive prostate cancers (Gleason grade IV or V) and indolent prostate cancers (Gleason grade I or II), we performed: (i) Gleason-grade wise linear modeling, followed by five contrasts against controls and ten contrasts between grades; and (ii) 1 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Gleason-grade wise network modeling using weighted gene correlation network analysis (WGCNA). Consensus between the grade-salient genes from the statistical modeling and the trait-specific key genes from network modeling were used as features for learning a ternary classification: benign, indolent or aggressive malignancy. Results: The statistical modeling yielded 77 Gleason grade-salient genes, viz. ten genes in grade-1, two genes in grade-II, one gene in grade-III, 34 genes in grade-IV, and 30 genes in grade-V. Using the WGCNA method, we reconstructed grade-specific networks, and defined trait-specific key genes in grade-wise significant modules. Consensus analysis yielded two genes in Grade 1 (SLC43A1, PHGR1), 26 genes in Grade 4 (LOC100128675, PPP1R3C, NECAB1, UBXN10, SERPINA5, CLU, RASL12, DGKG, FHL1, NCAM1), and seven genes in Grade 5 (CBX2,
DPYS,
FAM72B,
SHCBP1,
TMEM132A,
TPX2,
UBE2C).
PRADclass,
a RandomForest model trained on these 35 consensus biomarkers, yielded 100% cross-validation accuracy on the ternary classification problem. Conclusions:
Consensus
of
orthogonal
computational
strategies
has
yielded
Gleason grade-specific biomarkers that are useful in pre-screening (cancer vs normal) as well as typing the
aggressiveness
of
cancer.
PRADclass
has
been
deployed
at: https://apalania.shinyapps.io/pradclass/ for scientific and non-commercial use. Keywords: Prostate adenocarcinoma, RNASeq transcriptomics, Gleason grading, Cancer aggressiveness, Differential gene expression, Monotonic expression, Pairwise contrasts, Linear modeling, WGCNA, Network reconstruction, Enrichment analysis, Grade-salient gene, Trait-specific key gene, Consensus root biomarker, Machine learning, Random forest, Risk stratification. 2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint INTRODUCTION Over the past decade, global cancer incidence has increased by 33%, affecting 16% of the aged and 13% of the growing population [1]. In 2020, 19.3 million new cancer cases and 10.0 million cancer deaths were reported [1]. Prostate adenocarcinoma (PRAD), more commonly prostate cancer, is the most common cancer diagnosis among men, and as serious a threat to men’s health as breast cancer to women’s health [2]. The rise in prostate cancer cases may be attributed to lifestyle changes, smoking history, diet, and environmental factors [3]. Prostate tumors are very heterogeneous histologically and clinically. Early-stage prostate cancer is largely asymptomatic and progresses slowly, and often has an indolent course that might take more than 20 years to impact the quality of life [4]. Some prostate cancers grow aggressively and cause significant morbidity and mortality within five years. Malignant prostate cancers might necessitate prostatectomy, with a lifetime of treatment-related disabilities [4, 5]. The aggressiveness of prostate cancer is a crucial factor when deciding treatment regimen [6], and detection of prostate cancer in early stages and continued active surveillance could be an advantageous management option [7]. Measurement of prostate-specific antigen (PSA) levels followed by Digital Rectal Examination (DRE) are conventionally used to diagnose prostate cancer, however this method is prone to overdiagnosis and does not stratify cancer as slow-growing (indolent) or aggressive (high-risk). Recognizing this key gap in medical diagnosis coupled with the intrinsic difficulties in manual histopathology, automated deep learning systems have been steadily evolving to stratify prostate cancer according to the Gleason grade [8]. Such methods work with tissue microarray or whole slide images of biopsies, and have been advanced as potential clinical aid to support reproducible pathologist grading [9-16]. Omics approaches could be useful in studying and characterizing the 3 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint progression of prostate cancer and its conversion to an aggressive form [17,18]. Expression profiling (or transcriptomics) is useful to gain an understanding of the genetic factors of prostate cancer pathology [19]. Epigenetic processes also contribute to the development of prostate cancer. DNA hypermethylation of CpG-rich gene promoter regions is widespread in prostate neoplasia [20]. Transcriptomics and mutational status-based genomics of prostate tumors might enable the development of personalized therapeutics [21]. Following these cues, we have designed a study protocol for inferring the biomolecular foundation of the Gleason risk stratification of PRAD. Understanding and identifying Gleason grade-specific gene expression might be useful to achieving precise personalized management of prostate cancers. In this work, we have developed a multi-layered approach to address the above concerns to potentiate the differential diagnosis of aggressive prostate cancers (Gleason grades IV and V) versus indolent prostate cancers (Gleason grades I, II, and III). METHODS The investigative workflow, presented in Figure 1, consisted of two major branches, statistical –omics analysis and network-based WGCNA analysis, which were integrated in the end to yield root biomarkers of Gleason grade-wise PRAD progression. Data acquisition and pre-processing Public-domain TCGA datasets were used in our analysis [22]. RSEM-normalised PRAD RNASeq
gene
expression
data
was
obtained
from
the
firebrowse
portal (gdac.broadinstitute.org_PRAD.Merge_rnaseqv2__illuminahiseq_rnaseqv2__unc_edu__Level_3 __RSEM_genes_normalized__data.Level_3.2016012800.0.0.tar.gz) [23].
Associated clinical data was retrieved from the same portal (PRAD.Merge_Clinical. Level_1.2016012800.0.0tar.gz). 4 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Three Gleason grade-related clinical attributes were of interest: (1) primary Gleason pattern encoded as patient.stage_event.gleason_grading.primary_pattern (2)
secondary
Gleason
pattern
encoded
as patient.stage_event.gleason_grading.secondary_pattern (3)
Gleason
score
(sum
of
the
primary
and
secondary
patterns)
encoded
as patient.stage_event.gleason_grading.gleason_score Information about the primary and secondary patterns is necessary to disambiguate a Gleason score of, say, 7, which could arise from either 3 + 4 or 4 + 3, with the latter having a worse prognosis according to ISUP guidelines [24]. Using these clinical attributes, the Gleason grade was derived (Table 1). The RNA-Seq data was matched with the clinical data using the sample ‘patient_bcr’ information. The genes with nominal variation in expression across samples (σ < 1) were removed. Samples with missing grade annotation (or ‘NA’) were also removed. All data pre-processing was done with R [25]. Table 1: Grading of prostate cancer samples, based on the Gleason pattern and ISUP guidelines. Gleason Pattern
Gleason Score Derived Gleason Grade Primary
Secondary 3
3
6
1 3
4
7
2 4
3
7
3 3 or 4 or 5
5 or 4 or 3, respectively
8
4 4 or 5
5 or >=4, respectively
9 or 10
5 5 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 1. Workflow for identifying root biomarkers based on a consensus between statistical modeling and WGCNA analysis of PRAD transcriptome. Linear modeling with PRAD Grade: A grade-wise linear model of gene expression was constructed using the R limma package [26]: y = α + β1X1 + β2X2+ β3X3 + β4X4 + β5X5
(1) where the intercept α represents the baseline expression from control samples, and the βi signify grade-wise log-fold change (lfc) in expression relative to control samples. Empirical Bayes adjustment [27] and correction for multiple hypothesis testing [28] were carried out to yield a grade-wise account of differentially expressed genes ranked by significance. This model 6 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint facilitated the contrast between each grade and controls (Table 2), and genes with |lfc| < 2 in any grade were eliminated. Pairwise contrasts modeling To identify grade-salient genes, we modified the model of gene expression given by eqn. (1): y = β0x0 + β1x1 + β2x2 + β3x3 + β4x4+ β5x5
(2) where the xi are indicator variables encoding the sample grade, and βi correspond to estimated weights for each grade [29]. The design matrix corresponding to these contrasts is shown in Table 3. Table 2: Contrast matrix with control. Mean expression of samples in each grade are contrasted with the mean expression in control samples, to obtain grade-specific lfc with accompanying significance. Annotation
Grade 1 - Control Grade 2 - Control Grade 3 - Control Grade 4 - Control Grade 5 - Control Control
-1
-1
-1
-1
-1 Grade 1
1
0
0
0
0 Grade 2
0
1
0
0
0 Grade 3
0
0
1
0
0 Grade 4
0
0
0
1
0 Grade 5
0
0
0
0
1 7 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Detection of grade-salient genes Grade-salient genes were identified using a sequence of five filters: (i) A stringent adj. p-value (< 0.001) of the contrast against controls was applied to filter the significant differentially expressed genes. Such genes were assigned to the grade showing maximum |lfc|, producing grade-specific genes. (ii) - (v) Significance of contrast with reference to other grades (adj. P-value < 0.05) was applied to the grade-specific genes, and those that pass all the four relevant contrasts (out of the ten possible pairwise contrasts) were identified as grade-salient genes. Table 3: Contrast matrix for executing between-grade contrasts using the contrastsFit function in limma. Five grades of cancer progression implied 5C2 possible pairwise contrasts, four for each grade. Derived annotation Between-Grades contrast Grade1 - Grade 2 Grade1 - Grade 3 Grade1 - Grade 4 Grade1 - Grade 5 Grade 2 - Grade 3 Grade 2 - Grade 4 Grade 2 - Grade 5 Grade 3 - Grade 4 Grade 3 - Grade 5 Grade 4 - Grade 5 Grade 1
1
1
1
1
0
0
0
0
0
0 Grade 2
-1
0
0
0
1
1
1
0
0
0 Grade 3
0
-1
0
0
-1
0
0
1
1
0 Grade 4
0
0
-1
0
0
-1
0
-1
0
1 Grade 5
0
0
0
-1
0
0
-1
0
-1
-1 Monotonic expression: To ascertain any significance in the ordination of mean expression across grades, we used a model with PRAD grade (X) as a numeric variable: 8 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Y= aX + b
(3) To identify genes whose expression is in step (monotonic) with PRAD progression, we checked for the following two patterns with respect to mean gene expression: (i) control < I < II < III < IV<V; signifying monotonic upregulation (ii) control > I > II > III > IV>V; signifying monotonic downregulation Gleason biomarker identification using WGCNA The TCGA PRAD RNAseq data was normalized using voom and checked for outliers [30]. Genes were ranked based on the median absolute deviation in expression across the samples, and the top 5,000 genes were identified. The expression subset corresponding to these genes was annotated with the sample Gleason grade, and used to construct Gleason-grade specific co-expression networks using WGCNA [31], yielding five networks. For each such network, the smallest scale-free exponent with goodness-of-fit R2 > 0.9 was identified as the β value using WGCNA’s ‘pickSoftThreshold’. With β value fixed, we used the Pearson's ρ to calculate the similarity matrix corresponding to each network, which was then converted sequentially into the adjacency matrix, topological overlap matrix (TOM) and dissimilarity matrix (given by 1-TOM). Hierarchical clustering was applied on the resulting dissimilarity matrix, and a dynamic tree cut algorithm with a cut height of 0.25 was used to partition the gene space into 20 co-expression modules. The
association between each module and the clinical trait of interest (namely, the Gleason grade) was finally analyzed to identify the modules functionally significant for PRAD progression. Based on the module-trait relationships, the following measures were computed: 9 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint (i) gene significance (GS), defined as the Pearson's ρ between gene expression and the trait of interest, i.e, Gleason grade (in other words, cor(genei , Trait) ); (iii) module membership (MM), defined as the degree of association of a gene in a module with all the other genes in that module, i.e., the degree of internal connectivity of some gene in a given module; (ii) module significance (MS), defined as the mean of the unsigned GS of all genes in the module; and (iv)
module eigengene (ME), defined as the first principal component of the module-specific gene expression matrix. Based on the above definitions, among all the modules for a given grade, the one with the largest MS value was designated as the Significant Module of that grade. Among all the genes of a given Significant Module, those with module membership MM > 0.7 and abs(GS) > 0.5 were identified as trait-specific key genes. The trait-specific key genes were ranked by GS. Integrative analysis The grade-salient genes from –omics analysis and trait-specific key genes from WGCNA were investigated, and their overlap designated as the Consensus Genes. KEGG [32] and Gene Ontology were used as reference databases for analyzing functional enrichment [33]. The outlier genes were visualized using volcano plots (-log10 transformed p-value vs. log fold-change), and the grade-salient DEGs were visualized using heatmap dendrograms. The novelty of the identified grade-salient genes was screened against the Cancer Gene Census v84 [34], Network of Cancer Genes [35], and the Clinical Trial Registry (www.clinicaltrials.gov). The top ten 10 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint trait-specific key genes were used to reconstruct networks using STRINGdb [36]. Machine learning model for high-risk aggressive prostate cancer Gleason grade-specific biomarkers might make for good features in pre-screening benign hyperplasia from cancerous samples, and further discriminating between indolent and aggressive prostate cancers. Towards this end, a ternary outcome vector was encoded, differentiating among normal or benign samples, indolent PRAD samples (grades 1, 2, and 3) and aggressive PRAD samples (grades 4 and 5). Multiple machine learning algorithms were used to address the ternary classification problem at hand. Specifically, the consensus genes from integrative analysis were used as the feature space for model-building with Random Forest [37,38], SVM [39], neural networks and XGBoost classifiers [40]. The pre-processed dataset described in the preceding Methods was used for model training, and k-fold cross-validation (k=10) was done to optimize model hyperparameters. Post hyperparameter-optimization, the classifiers were evaluated using average performance on all the folds, and the best-performing model identified. This model was re-built using the full dataset and deployed as an R Shiny app [41]. RESULTS The gene expression matrix consisted of 20,531 genes x 551 samples, with 52 controls. After preprocessing we obtained a final dataset of 18,327 genes x 538 samples, with the following distribution: 50, 149, 95, 70 and 123 samples in ISUP-based I, II, III, IV and V Gleason-grades, respectively. The dataset is available as Supplementary File S1. The dataset was log2-transformed and observation-weighted through voom using the limma package. The linear model (eqn. 1) yielded 15257 significant genes (adj. P < 0.05). A more 11 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint stringent significance cutoff (adj. P < 1E-05) yielded 7965 significant genes. The top ten genes of linear modeling, ranked by adj. P-value, are all upregulated, and their lfc with respect to control samples are shown in Table 4. The detailed results of linear modeling for all genes are provided in Supplementary File S2. Fig. 2 presents the expression patterns of the top ten differentially expressed genes from linear modeling. Principal components extracted from the expression patterns of the top 100 genes were effective in discriminating cancer samples from control samples (Figure 3a), in contrast to the principal components of the expression patterns of a random subset of 100 genes that failed to demonstrate any such separation (Figure 3b). Violin plots of all the top 200 genes could be found in Supplementary File S3. Table 4: Top ten genes of the linear model, ranked by significance. The lfc of mean gene
expression in each grade relative to control samples is given, followed by adj. P and the inferred
regulation status in tumor. GENE Log fold-change Adj. P-val
Regulation
Status
Grade 1 β1 Grade 2 β2 Grade 3 β3 Grade 4 β4 Grade 5 β5 HOXC6
3.300889
3.599581
4.321112
4.13026
4.325551
1.25E-52
UP SIM2
3.063839
3.055925
2.992888
3.223932
2.958961
4.16E-51
UP EPHA10
2.054480
2.256922
2.353943
2.590398
2.337854
1.62E-50
UP HPN
2.782889
2.830453
3.143309
3.049693
2.577042
7.29E-49
UP PYCR1
1.476672
1.524151
1.676698
1.669362
1.590206
3.37E-47
UP SLC19A1
1.723341
1.636842
1.660923
1.617484
1.484677
2.47E-46
UP TROAP
1.27276
1.487522
1.897887
2.767342
2.943353
2.98E-46
UP LRFN1
1.622767
1.564225
1.686172
1.795652
1.786367
2.98E-46
UP EZH2
1.390663
1.442683
1.596514
2.098142
2.108934
1.04E-45
UP CGREF1
2.354448
2.340171
2.591191
2.508152
2.340949
7.61E-44
UP 12 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 2. Violin plots of top 10 linear model genes. For each gene, notice that the trend in expression could be either upregulation or downregulation relative to the control. All the top ten genes are upregulated. It could be seen that the linear trend does not enforce lfc to attain a maximum in grade-5. 13 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint To identify grade-specific genes, we binned the significant DE genes into partitions representing different grades, represented in Figure 4 as an UpSet plot [42]. To establish their salience, we identified genes that passed all the filter criteria described in the Methods section, and obtained ten grade I-salient, two grade II-salient, one grade III-salient, 34 grade IV-salient and 30 grade V-salient genes. The top 10 grade-salient genes in each grade are shown in Table 5; information on all the grade-salient genes is documented in Supplementary File S4. A heatmap
of the grade-salient genes, shown in Figure 5, revealed a mostly systematic trend in their expression relative to
the controls, with a mixture of up- and down-regulation processes. The map was clustered based on differential expression across grades and reflected a separation between high-grade and low-grade cancers. A dendrogram of the grade-salient genes yielded further insights into clustering among the grades, with a co-clustering among genes specific to indolent cancers (grades 1, 2, and 3) as well as among genes specific to aggressive cancers (grades 4 and 5). A volcano plot of the differentially expressed genes in PRAD is shown in Figure 6. 14 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 3. Principal components analysis of cancer vs control. (A) The first two principal components of the top 100 genes from linear modeling are plotted. It could be seen that control samples (red) clustered independent of the cancer samples (colored by grade). (B) The same analysis repeated with 100 random genes failed to effect a clustering of the control samples relative to the cancer samples. 15 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 4. Gleason-grade distributions of the computationally identified grade-salient biomarkers, sorted by cardinality. An enrichment in the advanced grades is observable. Table 5: The top grade-salient genes, by grade. Rank
Grade 1
Grade 2
Grade 3
Grade 4
Grade 5 1
PHGR1
PPM1E
PLA2G2A
UBXN10
CBX2 2
OR52R1
PPYR1
ATCAY
FAM72B 3
PCOTH
ANGPTL1
SLC7A4 4
SLC43A1
DGKG
ASPN 16 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint 5
ANXA8L2
NECAB1
UBE2C 6
RNF157
ASB2
SHCBP1 7
MUC15
KIAA1644
TPX2 8
SLC46A2
LOC2824276
IL1F5 9
PTPRZ1
PCYT1B
CTHRC1 10
SERPINB5
RNF175
CD38 The genes in each grade are ranked by adj. p-values of the linear modeling analysis. Figure 5. Heatmap of the expression of grade-salient genes, with clustering in both axes. Only the top ten grade-salient genes in each grade were used to construct the heatmap, for a total of 33 17 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint genes (as shown in Table 5). The downregulation of all the ten grade-4 salient genes is apparent. The top 200 genes from the numeric model (eqn. 3) are provided in Supplementary File S5. Applying the definition of monotonicity, we obtained 307 monotonically expressed genes, with 238 significantly monotonic (112 up and 126 down). The MEGs are also included in Supplementary File S5, ranked by significance. The intersection of significant ME genes with grade-specific genes yielded three genes: one grade-4 specific gene SERPINA5, and two grade-5 specific genes: DPYS and NKX6-1. Eleven genes were common to the top 200 genes from the numeric model and the significant MEGs, namely RCBTB2, VPS36, PLCD1, HRNBP3, NUPL2, SPAG5, SCRIB, AGL, CAMK2G, WHSC1, and ZNF706. Gene set enrichment analysis using the grade-salient genes was performed on GO and KEGG to probe network effects. The grade-wise enrichment results are presented in Supplementary File S6. We screened the grade-salient genes against the CGC, but did not find any hits, suggesting that our findings could be novel to cancer biology, especially the Gleason-specified progression of prostate cancer. On screening the grade-salient genes against the NCG curated database of cancer drivers and healthy drivers, we found two hits, namely ANXA8L2 (grade-1 salient), and TPX2 (grade-5 salient). The annotation of TPX2 (‘putative oncogene’) agrees with the upregulation observed here. Searching the grade-salient genes in ClinicalTrials.gov, we found eight genes used as either targets or endpoints of clinical trials (Supplementary File S7). Three among these eight (namely MUC15, UBE2C, and CD38)
were targeted in prostate cancer-related clinical trials, lending weight to our findings. The top 200 genes from linear model (eqn. 1) genes were screened against the Human Protein Atlas using the search: “cancer related genes” or “found as prognostic markers in other cancers” and the following hits were obtained: VAV2, BRAF, EZH2,
MEN1, BUB1B, and MNX1. EZH2 and BRAF have been 18 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint recently shown to promote castration-resistant prostate cancers [43,44], concordant with the upregulation in our analysis. VAV2 has been tied to poor prognosis in prostate cancer, corroborated by its oncogenic role observed here [45]. MEN1 has been noted as a
tumor suppressor gene with protective effects in aggressive tumors, via increased gene expression in prostate cancer cells, thanks to inclusion in a high copy number gain region [46], which is corroborated by the upregulation in its expression here. Oncogenic BUB1B was necessary for fast proliferation of tumor cells [47], while oncogenic MNX1 has been associated with aggressive disease [48], both results supported by the upregulation found here. The monotonic grade-salient genes in the advanced aggressive stages of PRAD, namely DPYS, NKX6-1, and SERPINA5 have been documented in the prostate cancer literature. Aberrant hypermethylation of DPYS has been found to have prognostic value in stratifying early prostate cancers [49], discordant with the overexpression observed here. NKX6-1, observed here as an oncogene with respect to prostate adenocarcinoma, has been documented with opposing functional roles depending on cancer cell-type: as tumor suppressor in colorectal cancer, gastric cancer, acute lymphoblastic leukemia, B-cell lymphoma; and as oncogene in hepatocellular carcinoma and breast cancer [50]. Loss of expression of SERPINA5 is correlated with high-grade prostate tumors and adverse prognosis [51,52], concordant with the significant downregulation observed in the cancer samples here. The overlap of the top 200 monotonic genes with the top 200 genes from the linear model yielded three genes (SPAG5, ZNF706, SERPINA5), and with the top 200 of ordinal model (eqn. 3) yielded eleven genes: SPAG5, ZNF706, RCBTB2, VPS36, PLCD1, HRNBP3, NUPL2, SCRIB, AGL, CAMK2G, and WHSC1. A discussion of the grade-salient genes with corresponding violin plots is provided in Supplementary File S8 document. 19 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 6. Volcano plot of the significant differentially expressed genes, with the grade-salient genes highlighted. WGCNA Analysis Grade-specific weighted co-expression networks were constructed using R WGCNA. The β parameter was identified as 14, 9, 7, 3, and 4 for Gleason grade-I, grade-II, grade-III, grade-IV, and grade-V networks, respectively. Identification of the optimal scale-free network is illustrated for grade-I in Figure 7. Modular decomposition was effected by the dynamic tree cut algorithm, and the outlier genes in each grade were binned into a gray module. This enabled the calculation of GS and MM of each gene (along with their P-values). This information is presented 20 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint grade-wise in Supplementary File S9. Figure 8 shows the module-trait correlation for all the modules in each grade as a heatmap, with the source data given in Supplementary File S10. Figure 7. Determination of soft-thresholding power in WGCNA analysis, illustrated for Grade-I network. (A) Scale independence. Identifying the optimal soft-thresholding exponent β using the scale-free fit of degree distribution (i.e., p(k) ~ k−γ) for various values of β. (B) Mean connectivity at different β. 21 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 8. Heatmap of grade-wise module-trait relationships, depicting correlation (with P-value) between module eigengene (ME) and grade for each module in the given grade. (A) Grade-I, (B) Grade-II, (C) Grade-III, (D) Grade-IV, and (E) Grade-V. The Significant Module in each grade is highlighted. Strength of correlation is visualized using a color gradient. Based on the per-module GS values, the Significant Module of each grade was identified, and a scatterplot of MM vs GS for all the Significant Modules was created (Figure 9). A strong Pearson’s correlation between MM and GS was observed for all Significant Modules, suggesting intrinsic importance for the respective grades. The trait-specific key genes in each Significant Module were identified, obtaining 67 genes in Grade-I,
46 genes in Grade-II, 204 genes in Grade-III, 603 genes in Grade-IV, and 83 genes in Grade-V represented in Table 6. 22 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Table 6: Summary of Gleason grade-wise WGCNA analysis of PRAD transcriptome. β
No. of
Modules Significant
Module # Genes
in Sig.
Module Key
Genes in
Sig.
Module ME-Trait
correlation Significance
of trait corr.
(P) Grade 1
14
10
Pink
79
67
0.44
1e-05 Grade 2
9
12
Yellow
381
46
-0.58
6e-19 Grade 3
7
12
Blue
843
204
-0.63
1e-16 Grade 4
3
12
Turquoise
1879
603
-0.72
3e-18 Grade 5
4
11
Yellow
336
83
0.68
3e-24 Figure 9. Grade-wise module significance, along with the scatter of genewise correlation 23 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint between MM and GS in the grade’s Significant Module. (I) Grade-I; (II) Grade-II; (III) Grade-III; (IV) Grade-IV; and (V) Grade-V. The top ten trait-specific key genes in the significant module in each grade were used to reconstruct driver networks using STRINGdb. The results are detailed in Supplementary File S11. The trait-specific key genes of Grade-1 WGCNA network were enriched in the catabolic processes of fatty acids and lipids (P < 1E-04). The trait-specific key genes of Grade-2 WGCNA network showed a KEGG enrichment of PPAR signaling pathway (P < 1E-2), altering androgen activity thereby driving PRAD progression [53].
The trait-specific key genes of Grade-3 WGCNA network revealed a Reactome enrichment [54] for neddylation pathway (P < 1E-04), which increases androgen receptor transcription and promotes the growth and invasion of the prostate cancer cells [55]. The trait-specific key genes of Grade-4 WGCNA network showed an enrichment
in
post-translational
SUMOylation
modifications
in
the
pTEN/AKT
and androgen-receptor signaling pathways (P < 2E-3) [56]. An analysis with Reactome revealed enrichment in PPARA gene expression (P < 0.05), known to drive advanced prostate cancer [57]. The top ten trait-specific key genes of Grade-5 WGCNA network were enriched in pTEN transcription regulation (P < 1E-04), whose loss of function is well-known to be associated with aggressive and metastatic prostate cancer [58]. An analysis with Reactome showed significance for HCMV early events (P < 1E-3), known to be involved in the etiology of metastatic prostate carcinoma [59]. DISCUSSION We have executed independent workflows for the statistical and network-based modeling of the TCGA PRAD transcriptome, stratified by Gleason grade. Based on the outcomes of these 24 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint workflows, we investigated the areas of concurrence between the results that would represent consensus and robust findings. The following observations were notable: (i) All the grade-salient genes were located either in the significant modules or with a module highly correlated with the grade; (ii)
Grade-salient genes displaying significant upregulation with Gleason progression almost always yielded a significant positive GS in the WGCNA analysis, except one: MAL; (iii) Grade-salient genes displaying significant downregulation with Gleason progression almost always yielded a significant negative GS in the WGCNA analysis, except four: COL10A1, NOX4, FAP, and SFRP4. (iv) Regulation status of trait-specific key genes suggested by the sign of the GS was concordant with the inference from the statistical expression patterns with respect to controls, with one exception: LOC100128675. (v) All the grade-salient genes invariably reflected a strong and significant association with their WGCNA modules in the respective grades (MM > 0.4, P < 0.05). The results of the consensus analysis are detailed in Supplementary File S12, and encouraged the investigation of deeper concordance between the two orthogonal computational approaches. The grade-wise overlap between grade-salient genes from the statistical modeling and the trait-specific key genes from network modeling included two genes from Grade-1 (SLC43A1, PHGR1), 26 genes from Grade-4 (including C2orf88, ANGPT1, CAV2, TMLHE, IGSF1, PPARGC1A, LGR6, PPP1R3C, FRMD6, NECAB1; please see Supplementary File S12) and seven genes from Grade-5 (CBX2, FAM72B, SHCBP1, TMEM132A, TPX2, DPYS and UBE2C). The regulation status for these 35 consensus genes was concordant, underscoring their potential utility. A summary of the consensus genes is presented in Table 7. A gene-by-gene analysis of the trait-specific top ten key genes in each module is presented in Supplementary File S13. The main inference remained unaltered: the bulk of the trait-specific key genes were also 25 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint grade-salient, with sync in inferred regulation. It is striking that most of the consensus emerged from the aggressive forms of prostate cancer (i.e. Gleason grades 4 and 5). We used the 33 consensus genes from the aggressive grades to reconstruct a STRINGdb network, with 50 interactors in the first shell and 10 interactors in the second shell. This yielded a significantly enriched PPI with 542 edges (P < 1.0E-16; Figure 10). An analysis with KEGG revealed significant enrichment in oncogenic pathways like NF-kappa B signaling pathway (P < 1E-3), and p53 signaling pathway (P ~ 0.02). An analysis with Reactome showed significant enrichment in cell cycle processes. The detailed results of these analyses are presented in Supplementary File S14 table. The consistency in the results between the –omics analysis and WGCNA is complete, which substantively amplifies the significance of the findings and sets the stage for modeling the character (aggressive or not) of PRAD samples based on these results. Table 7: Grade-wise root biomarkers from consensus analysis of grade-salient genes from statistical analysis and trait-specific key genes from network analysis. In all cases, the Significant Module for the grade matches the module with which the gene has the largest MM. The inferred regulation is based on the sign of the GS, which denotes the correlation between gene expression and trait class (Gleason grade) of interest. Only the top ten genes from Grade-4 (ranked by GS) are shown. Consensus in regulation inferred from statistical analysis is also reported, and seen to be concordant throughout. Gene
Grade
Significant Module MM
MM P-value GS
GS P-value Inferred regulation Consensu s with –omics PHGR1
1
MEpink
0.8634
6.04E-28
0.7020
1.27E-14
UP
Yes SLC43A1
1
MEpink
0.8964
7.57E-33
0.6973
2.25E-14
UP
Yes 26 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint LOC100128 675 4
MEturquoise
0.9258
1.35E-46
0.8333
4.66E-29
UP
Yes NECAB1
4
MEturquoise
0.9218
1.95E-45
-0.7263
5.80E-19
DOWN
Yes UBXN10
4
MEturquoise
0.94228
3.49E-52
-0.7169
2.62E-18
DOWN
Yes SERPINA5
4
MEturquoise
0.9176
2.80E-44
-0.6927
9.95E-17
DOWN
Yes CLU
4
MEturquoise
0.9368
3.74E-50
-0.6868
2.29E-16
DOWN
Yes DGKG
4
MEturquoise
0.893
1.48E-38
-0.6664
3.50E-15
DOWN
Yes NCAM1
4
MEturquoise
0.89
6.16E-38
-0.6642
4.59E-15
DOWN
Yes ANGPTL1
4
MEturquoise
0.91043
1.96E-42
-0.6576
1.06E-14
DOWN
Yes SYNPO2
4
MEturquoise
0.92
6.66E-05
-0.6456
4.57E-14
DOWN
Yes EMX2OS
4
MEturquoise
0.8555
4.47E-32
-0.6446
5.10E-14
DOWN
Yes TMEM132A
5
MEyellow
0.8011
1.29E-38
0.6821
3.37E-24
Up
Yes CBX2
5
MEyellow
0.7818
1.11E-35
0.6497
2.11E-21
UP
Yes UBE2C
5
MEyellow
0.9448
6.98E-82
0.6402
1.21E-20
UP
Yes TPX2
5
MEyellow
0.9698
5.54E-10 3 0.6297
7.72E-20
UP
Yes FAM72B
5
MEyellow
0.9228
2.94E-70
0.6194
4.49E-19
UP
Yes SHCBP1
5
MEyellow
0.8797
3.93E-55
0.5684
1.12E-15
UP
Yes 27 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 10. Network reconstructed using the 33 consensus root biomarkers of the aggressive 28 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint grades of prostate cancer. A giant component with a clique-like core could be seen. It is remarkable that 25 of the consensus root biomarkers were isolated outliers, signifying their roles underpinning varied biological processes not immediately related to each other. The expression subset of the foregoing 35 consensus genes were the features, and the 538 samples were the instances for developing ML models for pre-screening PRAD and typing PRAD aggressiveness. As noted in Methods, the samples were annotated as benign, indolent or aggressive, and the mapping between this outcome and the feature space was modeled using various classifiers (Table 8). Hyperparameters were optimized using 10-fold cross-validation. Different kernels were tried for the SVM classifier (viz. linear, radial and polynomial), and results for only the kernel with the best performance are shown. The RandomForest model with optimal hyperparameters showed a balanced accuracy ~ 100.00% among the three classes of interest during cross-validation.
SVM with radial kernel and XGBoost were also effective models, with >90% cross-validated balanced accuracy. We investigated the RandomForest model with perfect discrimination for feature importance using R caret [60] (Figure 11). The top ten features based on mean decrease in Gini score [61] were SLC43A1, FAM72B, CBX2, UBE2C, SHCBP1, TMEM132A, LOC100128675, TPX2, PHGR1, DPYS. Both the consensus grade-I salient genes (SLC43A1 and PHGR1) were key features in the RandomForest solution, as were all
the
seven
consensus
grade-V salient genes (FAM72B, CBX2, UBE2C, SHCBP1, TMEM132A, TPX2, and DPYS), validating the significant contribution of differential Gleason grade-specific biomarkers to the aggressive character of PRAD cancers. The better-performing models, viz. RandomForest, SVM (radial kernel) and XGBoost, were re-built using the full dataset, and these are provided in Supplementary File S15 as RDS binaries for
academic
and
not-for-profit
use.
The
hyperparameter-optimized
best-performing 29 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint RandomForest
model,
PRADclass,
has
been
deployed
as
a
web-server (https://apalania.shinyapps.io/pradclass/) to facilitate the pre-screening of PRAD cancers and typing their aggressive character. Features from expression profiling have been earlier used to build predictive models of cancers, for e.g. breast cancer [62]. PRADclass may assist in risk stratification independent of deep learning systems, especially in settings where availability of Gleason grading expertise is constrained, and necessitates prospective clinical evaluation. It is a handy, documented, and readily available alternative decision support tool. The grade-wise consensus root biomarkers could indicate potential targets for therapy, suggesting experimental investigations. Future research may address the fine discrimination among Gleason grades. Table 8: Hyperparameters and performance measures of different models investigated for the
ternary classification problem of sample as normal (possibly benign hyperplasia), indolent
prostate cancer or aggressive prostate cancer, based on expression levels of root consensus genes.
Optimal values of hyperparameters are provided in the same order as the hyperparameters
considered. Performance was measured using 10-fold cross-validation balanced accuracy for the
multi-class problem. S.No
Classifier
Hyperparameters of interest
Optimal
hyperparam
eters CV balanced accuracy (%) 1
SVM (radial
kernel) cost, gamma
1, 0.1
93.87 2
RandomForest
ntree (# trees in the forest),
mtry (# candidate variables
randomly sampled for splitting) 500, 6
100.00 3
Neural Networks
(1-layer) size, decay
2, 1
77.94 4
Neural Networks
(2-layer) #units in hidden layer 1, #units
in hidden layer 2 2, 26
89.18 5
XGBoost
eta, max depth, colsample_by
tree 0.01,5,1
92.37 30 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Figure 11. Elucidation of feature importance for the best-performing RandomForest model. Shown are the top ten features enabling the detection of prostate cancers with aggressive character. SLC43A1, a Gleason grade-I root biomarker, has the greatest contribution to the classification problem, followed by many Gleason grade-V root biomarkers (FAM72B, CBX2, UBE2C, SHCBP1, TMEM132A). Also seen is LOC100128675, the lone Gleason grade-IV root biomarker among 26 such features. 31 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint CONCLUSION Gleason grading is the gold standard for risk stratification of prostate cancer patients, and molecular biomarkers specific to different Gleason grades could enable the diagnosis of high-risk prostate cancers requiring active management. In this study, we have applied a couple of orthogonal computational pipelines, based on statistical modeling and network analysis of expression data, to identify grade-wise consensus root biomarkers. Statistical modeling yielded ten grade-I salient genes (PHGR1, OR52R1, PCOTH, SLC43A1, ANXA8L2, RNF157, MUC15, SLC46A2, PTPRZ1, and SERPINB5), two grade-II salient genes (PPM1E and PPYR1), one grade-III salient gene including (PLA2G2A), 34 grade-IV salient genes (including UBXN10, ATCAY, ANGPTL1, DGKG, NECAB1, ASB2, KIAA1644, LOC2824276, PCYT1B, RNF175), and 30 grade-V salient genes (including CBX2, FAM72B, SLC7A4, ASPN, UBE2C, SHCBP1, TPX2, IL1F5, CTHRC1, CD38). Post WGCNA modeling, the consensus analysis yielded 35 root biomarkers, viz. two genes in Grade 1 (SLC43A1, PHGR1), 26 genes in Grade 4 (LOC100128675, PPP1R3C, NECAB1, UBXN10, SERPINA5, CLU, RASL12, DGKG, FHL1, NCAM1), and seven genes in Grade 5 (CBX2, DPYS, FAM72B, SHCBP1, TMEM132A, TPX2, UBE2C).
These
biomarkers
might
double
as
novel
chemotherapy
target
hypotheses, necessitating experimental corroboration. To explore the histoprognostic value of these molecular markers, we constructed various machine learning models for the ternary problem of classifying a sample as non-cancerous, indolent prostate cancer or aggressive prostate cancer. A RandomForest model, PRADclass, secured a cross-validated balanced accuracy ~ 100.00%, indicating potential uses for cancer pre-screening and character typing. PRADclass is available on the web for academic and non-commercial use at https://apalania.shinyapps.io/pradclass. 32 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint Further work is necessary to assess the clinical utility of PRADclass as a readily available decision support aid in the treatment-management of malignant prostate gland neoplasia. Abbreviations CGC: Cancer Gene Census; CV: cross-validation; DEG: Differentially expressed genes; DRE: Digital rectal examination; GO: Gene Ontology; GS: Gene significance; KEGG: Kyoto Encyclopaedia of Genes and Genomes; ISUP: International Society of Urological Pathology; lfc: log fold-change; ME: Module eigengene; MEG: Monotonically expressed genes; MM: Module membership; MS: Module significance; NCG: Network of Cancer Genes; PCA: Principal Components Analysis; PRAD: Prostate adenocarcinoma; PSA: prostate-specific antigen; RSEM: RNA-Seq by Expectation Maximization; TCGA: The Cancer Genome Atlas; TNM: Tumor, Node, Metastasis; TOM: Topological overlap matrix; SVM: Support Vector Machine; WGCNA: Weighted gene co-expression network analysis. Acknowledgements We would like to thank Amrutha Karthikeyan and R. Shivathmika for assistance with the WGCNA analysis. We are grateful to the School of Chemical and Biotechnology, SASTRA Deemed University, for infrastructure and computing support. Data availability: All supplementary materials are available at: https://doi.org/10.6084/m9.figshare.22549621 . 33 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288124
doi: 
medRxiv preprint",1
"Background Neurocognitive impairment linked to head impact exposure in otherwise healthy, non-concussed athletes may be associated with adverse long-term outcomes. The primary purpose of this study was to evaluate whether a dietary supplement, SynaquellTM, supports brain function and structure in male Junior A ice hockey players over the course of a season. Methods Players underwent pre-season testing, were randomized into a placebo or dietary supplement group, then were retested after the season. Objective tests included: NeuroCatch® portable evoked potential platform, King-Devick Test of rapid number naming, and blood biomarker assay for neurofilament light chain (NfL). Results Multivariate analysis revealed significant differences in neurocognitive changes between groups from pre to postseason after controlling for covariates related to head impact exposure. Post-hoc tests showed significant within-subject differences between groups from pre- to post-season in both N100 latency (p = 0.005) and King-Devick score (p = 0.043). Univariate tests of the NeuroCatch results replicated prior findings of a N400 amplitude decrease (p = 0.017) and N100 latency increase (p = 0.049) in the placebo group, but not in the dietary supplement group. Conclusions This prospective, randomized trial showed that, compared to the placebo group, a multi-ingredient dietary supplement significantly affected objective measures of brain function and structure in Junior A ice hockey players from pre- to post-season. Further investigation into the effects of dietary supplementation on the contact athlete’s brain is warranted. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Introduction Athletes sustaining repetitive body and head impacts may experience impaired neurocognitive function without observable symptoms (Bailes et al. 2013; Mainwaring et al. 2018). These impacts with resultant force transmission to the brain occur frequently over the course of a season in contact sports, such as ice hockey. Objective neurocognitive tests have shown pre- and post-season differences in athletes exposed to repetitive brain forces in the absence of a clinically diagnosed concussion (Abbas et al. 2015; Talavage et al. 2014; Tsushima et al. 2016). The emergence of detectable neurocognitive impairments is of critical concern due to recent evidence linking impact exposure to progressive neurodegenerative disease. The risk of chronic traumatic encephalopathy (CTE) has been shown to double with every 2.6 years of contact exposure (Mez et al., 2020). Proper medical evaluation is imperative to assessing the effects of repetitive head impacts (Pender et al. 2020). Advances have been made in point-of-care evaluation with objective neurophysiological measures, oculomotor tests, and neuro-axonal biomarkers. The NeuroCatch® is a medical device used to evaluate objective neurocognitive changes within a brain vital signs framework (Ghosh Hajra et al. 2016). Electroencephalography (EEG) data are processed to quantify event-related potential (ERP) responses for the N100 (auditory sensation; Davis 1939), the P300 (basic attention; Sutton et al. 1967), and the N400 (cognitive processing; Kutas et al. 1980). The ability of the NeuroCatch® to detect subtle but significant changes in cognitive brain All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint function has led to investigations on cumulative head impact exposure and “subconcussive” impairments (Fickling et al. 2021a; Fickling et al. 2021b). Fickling et al. (2019) first detected neurocognitive impairments tracked by the N400 ERP response for cognitive processing in ice hockey athletes without a diagnosed concussion throughout the season. In a subsequent ice hockey study (Fickling et al., 2021a), neurocognitive impairments in the N400 were replicated, with additional impairments detected in the N100 ERP response for auditory sensation. The pattern of brain vital sign ERP changes over the course of the hockey season was highly correlated and linearly predictive of the number of head Impacts measured by accelerometers. These findings were replicated in youth football showing a strong linear relationship with the number of games and practices representing a general measure of impact exposure (Fickling et al., 2021b). These studies demonstrate cognitive impairments related to impact exposure occur in different sports and age groups. Additional objective cognitive assessment tools include the King-Devick Test (K-D) and blood biomarkers. The K-D is a test of rapid number naming that relies upon saccadic function, attention, and language function. The K-D evaluates changes due to head impacts in both acute and long-term contact sports participation (Nowak et al. 2020; Munce et al. 2014; Joseph et al. 2018; Krause et al. 2021) and was shown to be reliable (Eddy et al. 2020). Caccese et al. (Caccese et al. 2019) reported a significant relationship between repetitive head impacts and change in K-D score over the course of a college football season, where greater head impact exposure was associated with poorer K-D performance. Blood biomarkers are used as an objective assay to quantify All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint neuro-axonal damage. Neurofilament light (NfL) is a biomarker that increases in individuals exposed to repetitive head impacts (Papa et al. 2022; Oliver et al. 2016b; Oliver et al. 2018b; Wirsching et al. 2019). Rubin et al. (Rubin et al. 2019) found an association between increased serum NfL levels and increased frequencies of head impact exposure. Collectively, these three objective neurological assessments (NeuroCatch®, K-D, and NfL) can monitor neurocognitive changes and benchmark the efficacy of potential interventions (Smith et al. 2017; Pender et al. 2020). A range of emerging interventions, including dietary supplements, may support brain function and structure (Walrand et al. 2021; Lucke-Wold et al. 2018; Oliver et al. 2018). Dietary supplements are non-pharmacological, easily accessible, and may support brain health (Mishra et al. 2022). A variety of supplements have been investigated in both animal models and human studies to determine their effects on the brain (Lucke-Wold et al. 2016), including magnesium (McIntosh et al. 1989; Standiford et al. 2020), resveratrol (Lopez et al. 2005, Lin et al. 2014), nicotinamide riboside (Cheng et al. 2022), curcumin (Wu et al. 2011), glutathione (Aoyama 2021), docosahexaenoic acid (DHA) (Raikes et al. 2022, Oliver et al. 2016a; Heileson et al. 2021), ketone bodies such as beta hydroxybutyrate (Lee et al. 2019), ubiquinone (Pierce et al. 2017; Pierce et al. 2018) and branched-chain amino acids such as valine, leucine and isoleucine (Aquilani et al. 2005). A combination of these ingredients and others are in the product SynaquellTM (manufactured by Thorne). This dietary supplement may influence the neurometabolic cascade of concussion (Giza and Hovda, 2014) by providing critical All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint nutrients and energy substrates to rebalance disrupted homeostatic processes (Walrand et al. 2021). The purpose of this blinded, randomized clinical trial was to investigate if daily oral ingestion of SynaquellTM supports brain function and structure over the duration of an ice hockey season. The primary hypothesis was that objective measures of brain function and structure (brain vital signs, King-Devick, NfL) would show significantly less decline as compared to a placebo control group when controlling for self-reported concussion history and number of games played (as a proxy for head-impact exposure). The secondary hypothesis was that the placebo control group would show significant impairments in brain vital sign changes from pre- to post-season, replicating analyses from prior studies evaluating contact sport athletes. Methods Participants The study was reviewed and approved by the Institutional Review Board. Fifty-four male athletes were recruited from the North American Tier 2 Hockey League (NAHL) and the North American Tier 3 Hockey League (NA3HL). Participants were between 18 and 20 years of age (18.73 ± 0.69) at the time of enrollment, spoke fluent English, and were medically cleared to play ice hockey by the team’s medical staff. All subjects provided written consent. Exclusion criteria included allergy to the SynaquellTM ingredients (Supplemental Table 1: SynaquellTM Ingredients) and contradictions for the NeuroCatch® Platform (clinically documented hearing issues, hearing device, aid or implant, or an All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint unhealthy scalp). Participants were asked to refrain from taking other dietary supplements and were excluded if they declared noncompliance. All players underwent a preseason medical evaluation and completed the Sideline Concussion Assessment Tool (SCAT5). Team rosters change during the season; therefore, participants who left their hockey team, lost medical clearance to play or did not comply with study protocols were withdrawn from the study. Athletes added to the roster during the hockey season were enrolled and tested (n=5). All participants were randomized into either the control (CON, n=28) or SynaquellTM (SQ, n=26) group. Experimental design This prospective, randomized clinical trial utilized repeated measures and a placebo group to determine the effects of a combination dietary supplement ,SynaquellTM, on brain function and structure. Pre-season testing included: brain vital signs recorded using the NeuroCatch® portable evoked potential platform, the K-D rapid number naming test, and venous blood analysis for NfL. Administered clinical neuropsychological questionnaires included: SCAT5, Neuro-Quality of Life Short Form Version 2.0 – Cognitive Function (nQoL), and Patient Health Questionnaire – 4 (PHQ- 4). These questionnaires were repeated after the hockey season, along with the Patient Global Impression of Change (PGIC). Post-season data collection was completed with 7 days of the last regular season game. Brain vital signs NeuroCatch® (Version 1.1) incorporates a portable 8-channel g. Nautilus EEG system (Gtec Medical Engineering, Austria). Testing involves a 6-min automated stimulus All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint sequence consisting of auditory tones (oddball stimuli with standard and deviant tones) interspersed with spoken prime-target word pairs (equally semantically congruent or incongruent). Participants were instructed to listen attentively, passively and remain still with their eyes fixed on a cross positioned at eye level 2 m away. Scans were conducted in a quiet, closed room away from distractions. Pre- and post-season scans were conducted in the same facility to maintain a consistent environment. The three scalp electrodes were referenced to an electrode clipped to the right earlobe. Disposable, Ag/AgCl, adhesive electrodes were used for electro-oculogram recording from the supra-orbital ridge and outer canthus of the left eye. g. GAMMAsys electrode gel was applied to each location to ensure conductivity. Skin-electrode impedances were maintained below the standard 30kΩ threshold at each site. Scan quality results were evaluated immediately, with all EEG data subsequently processed off-line in Python. Pre-processing was performed using a fourth-order Butterworth filter (0.1–20 Hz) and a notch filter (60Hz). Adaptive filtering (He et al. 2004) was used to correct for ocular artifacts, which were recorded from electrooculogram locations. ERPs were derived from Fz, Cz, and Pz channels and segmented (−100ms to 900ms), baseline corrected, and averaged by stimulus condition. Brain vital sign ERP responses were manually verified by a blinded reviewer by identifying the relevant local maxima/minima within expected polarities and temporal ranges for the N100, the P300 and the N400. Peaks were then evaluated for both latency (response time) and amplitude (synchronous processing), for a total of six measures. The N400 amplitude was recorded as the mean voltage from 350-450 ms. N400 latency was recorded as the response time of the selected peak. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint King-Devick K-D testing was completed in a quiet clinical setting both pre-season and post-season. Testing was conducted on an electronic tablet that was held at a comfortable reading distance. Players were instructed to read the numbers aloud as quickly and as accurately as possible from left to right and top to bottom. They completed an untimed demonstration card, followed by three timed test cards. The summed time of the three test cards was the player’s K-D score. At baseline, players completed the test twice and their best score was recorded. At post-season, the players completed the test once and their resultant K-D score was recorded. Blood Sampling Non-fasting venous blood samples were collected from the antecubital fossa by standard venipuncture procedures. Samples were collected in Serum Separator Tubes containing a clot activator and serum gel separator. Serum samples were assayed on the same instrument and by the same person in a blinded fashion. NfL concentrations in plasma were measured with the NF-Light digital immunoassay (Quanterix, Cat#103186) using the HD-X Analyzer per the manufacturer’s protocol. In brief, samples were thawed on ice, mixed thoroughly by low-speed vortex and centrifuged at 10,000 g for 5 minutes before transferring samples to 96-well plates. Samples were diluted 1:4 by the instrument and tested in duplicate. In addition to participant serum samples, 8 calibrators and 2 quality control samples provided with the kits were included in the assay. Concentrations were interpolated from the standard curve using a 4-parameter logistic curve fit (1/y2 weighted). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Supplementation The CON group received a placebo while the SQ group received the supplement, SynaquellTM. The ingredients for SynaquellTM and placebo supplements are listed in Supplemental Table 1. Athletes mixed one scoop (7.7 grams) of the provided powder with water and ingested twice daily. Supplements were administered by research team staff at team practices and home games. Supplements were prepared for players on road trips or days without organized team activities. Research staff tracked all doses administered during practices and home games. Players self-reported all doses taken outside the supervision of the research staff using an internet-based or paper survey. All subjects and research staff responsible for data collection were blinded to the group assignment. Both groups discontinued the intervention after the final regular season hockey game. The NAHL played 60 games and NA3HL played 47 games during this study. Both teams participated in two 60- minute ice hockey games, and 4 or 5 practices lasting 60 to 90 minutes each week. Statistical Analysis Statistical analyses were conducted using SPSS (IBM, New York, USA). Descriptive statistics were calculated for all dependent variables. T-tests were conducted to evaluate differences between groups across participant demographics for age, height, weight, number of games played, self-reported concussion history, and intervention All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint compliance (as % of doses taken). The data from this study are available upon request from the corresponding author. Multivariate Analysis – Objective function and structure assessments A repeated measures multivariate general linear analysis of variance (GLM) was used to investigate the effect of group allocation on dependent variables (6 brain vital sign measures, K-D, and NfL) as a factor of time (pre- to post- season). Prior concussion reporting and number of games played during the season were included as covariates. The number of prior concussions was derived from subjective participant recall as a standard aspect of the SCAT5 questionnaire, clinically diagnosed concussions during the season were not monitored as part of this study design. Post-hoc tests (both within and between subjects) were conducted to evaluate effects within individual dependent variables. Multivariate Analysis – Clinical Questionnaires The statistical model design was then repeated using only the four clinical questionnaires (SCAT5, nQoL, PHQ-4, PGIC) as the dependent variables. PGIC scores for all participants at pre-season was set to 0, given that it is a retrospective analysis of change. Brain vital signs univariate analysis To compare with prior evaluations of pre- to post-season changes of brain vital signs in contact athletes, grand average ERP waveforms (means +- 95% confidence intervals) were generated for deviant tone and incongruent word responses for each group. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Paired, two-tailed Welch’s t-tests were performed on regions of interest identified on the waveforms as areas of deviation between the confidence intervals. Standardized radar plots were also generated for brain vital signs to indicate relative changes in scores over time. Results Participants: Of the 28 players randomized into the CON group, five players withdrew, seven left their team, and one was injured reducing the sample size to N=15. Of the 26 players randomized into the SQ group, three withdrew, six left their team, one was injured, and one dataset was compromised reducing the sample size to N=15. Due to supply chain delays, the intervention began 105 days after pre-season testing of the NA3HL team and 90 days after pre-season testing of the NAHL team. By this time, 26 NAHL games and 25 NA3HL games had occurred. Players were supplemented for the remaining 84 days (22 games) of the NA3HL season and 112 days (35 games) of the NAHL season. Postseason testing occurred 189 days after pre-season testing for the NA3HL team and 202 days for the NAHL team. There were no significant differences between groups across all demographics (Table 1). Multivariate Analysis – Objective assessments The multivariate repeated-measures general linear model (Table 2) showed a significant overall within-subjects interaction effect of Group with Time (p = 0.036), which identified differential changes between the CON and SQ groups from pre- to post-season across all dependent variables. Post-hoc tests identified specific significant differences in both N100 latency (p = 0.005) and K-D score (p = 0.043). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint There was also a significant, multivariate within-subjects effect between Time and Concussion History (p = 0.001). Post-hoc tests showed that the number of self-reported concussions, independent of group allocation, was significantly related to changes in outcome measures over time for the N100 latency (p = 0.046), N400 latency (p = 0.007), and NfL score (p = 0.012). There were no significant multivariate effects between subjects. Multivariate Analysis – Clinical Questionnaires The multivariate repeated-measures general linear model for the clinical questionnaires (Table 3) showed no significant effects of group by time (p = 0.892). There were no significant effects or interaction effects for the covariates at any level of the model. Violin plots compare the objective measures and clinical questionnaires as a function of pre- versus post-difference for the CON and the SQ groups (Figure 1). The plot enables examination of the distributions of within-subject changes, separated by test type. Best- fit CON and SQ group differences are also line plotted with associated p-values for time effects from the respective multivariate GLM post-hoc tests. The violin plots show distribution differences between the CON and SQ groups in terms of pre- versus post- changes and highlight sensitivity differences between objective and subjective tests. Brain vital signs waveform analysis Grand average NeuroCatch ERP waveforms (+- 95CI of the mean) for CON (left) and SQ (right) groups were created for both deviant tone and congruent word stimuli (Figure All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint 2). Regions of interest were identified for both the N100 latency, and the N400 amplitude components.  Subsequent t-tests at each group identified significant delay in the N100 latency (p = 0.048) in the CON group, but not the SQ group (p = 0.137). Similarly, there were significant reductions in N400 amplitude (p = 0.017) in the CON group, which was not present in the SQ group (p = 0.506). Standardized brain vital sign radar plots compare CON (left) and SQ (right) with pre- versus post-season comparisons for both groups (Figure 3). Table 1: Player Demographics Both Groups (Mean ± SD) CON (Mean ± SD) SQ (Mean ± SD) Between Group T-test T-score (p value) Age 
18.73 ± 0.69 
18.87 ± 0.74 
18.60 ± 0.63 
1.058 (0.299) Height (cm) 
185.27 ± 7.24 
187.27 ± 8.59 
183.27 ± 5.13 
1.552 (0.134) Weight (kg) 
82.51 ± 9.26 
84.93 ± 10.17 
80.09 ± 7.86 
1.458 (0.157) Games Played 
39.03 ± 13.91 
40.20 ± 12.94 
37.87 ± 15.18 
0.453 (0.654) Concussion History 
1.00 ± 1.49 
1.27 ± 1.91 
0.73 ± 0.88 
0.983 (0.338) Compliance (% doses) 
84.42 ± 10.70 
83.19 ± 11.77 
85.64 ± 9.78 
-0.620 (0.541) All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Figure 1: Violin plots showing pre- versus post- season differences in CON (light blue) and SQ (dark blue) 
groups. Plots are separated by test type showing objective measures (left side) and subjective measures 
(right side). Lines connecting plots show best fit differences with statistical difference above (* P < 0.05, ** 
P < 0.01). Note the statistical differences are from post-hoc tests in the multivariate GLM, which do not 
show significant effects from other tests (e.g., N400 amplitude reduction in brain vital sign waveform 
analysis). Examination of the violin plots show distribution differences between CON and SQ, along with 
sensitivity differences between objective and subjective tests. SCAT-5: The Sport Concussion 
Assessment Tool 5, PHQ-4: Patient Health Questionnaire-4, nQoL: The Neuro-Quality of Life Short Form 
Version 2.0 – Cognitive Function, and PGIC: Patient Global Impression of Change questionnaire. e) ** m All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Table 2: Repeated Measures General Linear Model Results–- Objective Tests Multivariate Tests Effect 
Wil’'s Lambda 
F 
Hypothesis df 
Error df 
Sig Within Subjects 
Time 
0.503 
2.347 
8 
19 
0.061 Time * Concussion Hx 
0.304 
5.426 
8 
19 
0.001** Time * Games played 
0.501 
2.366 
8 
19 
0.059 
  
Time * Group 
0.467 
2.706 
8 
19 
0.036* Between 
subjects 
Intercept 
0.022 
104.576 
8 
19 
<.001*** Concussion history 
0.512 
2.263 
8 
19 
0.069 
 
Games played 
0.868 
0.361 
8 
19 
0.928 
 
Group 
0.729 
0.882 
8 
19 
0.549 Post-Hoc Tests of Within-Subject Effects Time 
Time * Concussion 
history 
Time * Games 
played 
Time * Group F 
Sig 
F 
Sig 
F 
Sig 
F 
Sig N100 amplitude 
0.015 
0.904 
0.868 
0.360 
0.020 
0.887 
0.289 
0.595 N100 latency 
0.045 
0.833 
4.394 
0.046* 
0.869 
0.360 
9.261 
0.005** P300 amplitude 
0.000 
0.999 
1.197 
0.284 
0.512 
0.481 
0.396 
0.535 P300 latency 
0.221 
0.642 
3.161 
0.087 
0.705 
0.409 
1.205 
0.282 N400 amplitude 
3.032 
0.093 
0.312 
0.581 
1.120 
0.300 
2.993 
0.095 N400 latency 
2.858 
0.103 
8.562 
0.007** 
5.875 
0.023* 
2.884 
0.101 King-Devick 
12.310 
0.002** 
3.122 
0.089 
2.486 
0.127 
4.523 
0.043* NfLScore 
0.963 
0.336 
7.204 
0.012* 
1.171 
0.289 
3.899 
0.059 Post-Hoc Tests of Between-Subject Effects Intercept 
Concussion history 
Games played 
Group F 
Sig 
F 
Sig 
F 
Sig 
F 
Sig N100 amplitude 
17.382 
<.001*** 
0.462 
0.503 
0.008 
0.929 
0.029 
0.867 N100 latency 
188.844 
<.001*** 
0.374 
0.546 
0.260 
0.614 
1.571 
0.221 P300 amplitude 
37.080 
<.001*** 
6.230 
0.019* 
0.234 
0.632 
1.906 
0.179 P300 latency 
250.560 
<.001*** 
5.333 
0.029* 
0.069 
0.795 
2.847 
0.103 N400 amplitude 
19.047 
<.001*** 
1.649 
0.210 
0.417 
0.524 
0.002 
0.961 N400 latency 
377.801 
<.001*** 
5.383 
0.028* 
1.182 
0.287 
1.464 
0.237 King-Devick 
78.686 
<.001*** 
0.122 
0.729 
1.443 
0.241 
0.521 
0.477 NfL Score 
37.279 
<.001*** 
2.171 
0.153 
0.011 
0.919 
0.126 
0.725 Group is modeled as the between-subjects factor and time point is the within-subjects factor. Games played and concussion history are included in the model as covariates. *p < 0.05, **p < 0.01, and ***p < 0.001. Bold values are where p < 0.05. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Table 3: Repeated Measures General Linear Model Results – Clinical Questionnaires Multivariate Tests Effect 
Wil’'s Lambda 
F 
Hypothesis df 
Error df 
Sig Within Subjects 
Time 
.675 
2.522 
4 
21 
.072 Time * Concussion Hx 
.939 
.344 
4 
21 
.845 Time * Games played 
.858 
.870 
4 
21 
.498 Time * Group 
.950 
.274 
4 
21 
.892 Between 
subjects Intercept 
.143 
31.342 
4 
21 
<.001** Concussion history 
.779 
1.487 
4 
21 
.242 Games Played 
.930 
.393b 
4 
21 
.811 Group 
.825 
1.115 
4 
21 
.376 Post-Hoc Tests of Within-Subject Effects Time 
Time * Concussion 
history 
Time * Games 
played 
Time * Group F 
Sig 
F 
Sig 
F 
Sig 
F 
Sig PGIC 
5.989 
.022 
.125 
.727 
.994 
.329 
.125 
.726 SCAT 5 
1.867 
.184 
.200 
.659 
1.810 
.191 
.313 
.581 PHQ 4 
.158 
.694 
.167 
.686 
.214 
.648 
.262 
.613 nQoL 
1.220 
.280 
1.242 
.276 
.155 
.698 
.464 
.502 Post-Hoc Tests of Between-Subject Effects Intercept 
Concussion history 
Games played 
Group F 
Sig 
F 
Sig 
F 
Sig 
F 
Sig PGIC 
5.989 
.022* 
.125 
.727 
.994 
.329 
.125 
.726 SCAT 5 
2.052 
.165 
.276 
.604 
.495 
.489 
.022 
.882 PHQ 4 
1.389 
.250 
2.233 
.148 
.330 
.571 
1.080 
.309 nQoL 
56.068 
<.001** 
2.771 
.109 
.112 
.741 
.000 
.987 Group is modeled as the between-subjects factor and time point is the within-subjects factor. Games played and concussion history are included in the model as covariates. *p < 0.05, **p < 0.01, and ***p < 0.001. Bold values are where p < 0.05. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Figure 2: Grand average NeuroCatch ERP waveforms (+- 95CI of the mean) for CON (left) and SQ (right) 
groups (*p <0.05). Pre-season is in blue and post-season is in orange. The N100 and P300 to deviant 
tone stimuli (top row) and the N400 to incongruent word stimuli (bottom row). Time is on the x-axis (ms) 
and voltage is on the y-axis (µVs). Note the N100 latency delay and the N400 amplitude reduction in the 
CON group that is not present in the SQ group. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Table 4: Univariate t-tests from pre-season to post-season on brain vital signs waveform data 95% Confidence Interval t 
df 
sig 
Low 
High N100 
CON 
-2.156 
14 
0.049* 
-21.81 
.0584 Latency 
SQ 
1.576 
14 
0.137 
-2.551 
16.685 N400  
CON 
2.715 
14 
0.017* 
0.184 
1.566 Amplitude 
SQ 
0.682 
14 
0.506 
-0.340 
0.658 Figure 3: Standardized NeuroCatch® radar plots showing brain vital sign results for each group at each 
time point (*p < 0.05). All 6 brain vital sign measures are plotted for both the CON (left) and SQ (right) 
groups. Pre-season results are in blue and post-season results are in orange. The dashed line denotes 
the median from prior normative data. Note the CON results replicate prior subconcussion impairment 
findings (Fickling et al., 2019, 2021a,b). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Discussion Main findings The aim of this study was to investigate the effects of the dietary supplement SynaquellTM on brain function and structure. Multivariate analysis, including brain vital signs, K-D, NfL, showed significant within-subject differences between groups in changes from pre-to post-season (p = 0.036) (Table 2, Figure 1; Primary Hypothesis). Post-hoc tests revealed significant differential changes between groups in functional variables, including N100 latency and King-Devick scores. Specifically, the N100 latency became slower in the CON group and faster in the SQ group from pre-season to post-season (Figure 1, Supplemental Table 2). While K-D scores in both groups improved from pre-season to post-season, the improvement in the CON group was faster (Figure 1, Supplemental Table 2). Additionally, a significant within-subject multivariate effect (p = 0.001) was observed for Concussion History * Time. Post-hoc testing identified significant changes in functional measures of N100 latency, N400 latency, and the structural NfL measure. This suggests that increased head impact exposure is linked to greater changes in both brain function and structure over the course of the season, regardless of group allocation. The secondary analysis replicated previous findings on neurocognitive changes in athletes with no intervention (i.e., the CON group), specifically the delay in the N100 latency and decrease in N400 amplitude (Figure 3, Secondary Hypothesis). In contrast, no significant change was observed in the SQ group from pre- to post-season (Figs. 2&3). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Differential changes in objective brain function measures between the CON and SQ subjects from pre- to post-season supports the use of the dietary supplement SynaquellTM, to support brain function and structure in healthy, male athletes. To our knowledge, this is the first human study in a population of healthy, non-concussed athletes in which a multi-ingredient dietary supplement was found to be effective in supporting neurocognitive function and brain structure (Feinberg et al. 2023). The most similarly advanced studies in such a population have investigated Omega-3 Fatty Acids in relation to brain structure (Oliver et al. 2016; Heileson et al 2021; Mullins et al. 2022). Our results coincide with the findings of Oliver et al. and Heileson et al., in which serum NfL was attenuated by the daily administration of DHA in athletes (Oliver et al. 2016; Heileson et al. 2021). Mullins et al. 2022 alternatively found that daily DHA supplementation did not reduce serum NfL levels over the course of the season (Mullins et al. 2022). Studies examining the effects of single-ingredient dietary supplementation on neurocognition and brain structure have been conducted in concussed populations post-injury (Feinberg et al. 2023). Multiple ingredients have the potential to impact a variety of pathways and processes in the neurometabolic cascade, which may improve effectiveness (Lucke-Wold et al. 2016). Brain vital signs The current study provides the latest evidence that head impact exposure affects cognitive processing (N400) and auditory sensory processing (N100). A pattern involving the N100 and N400 is emerging across different ages and sports. Specific responses vary slightly in terms of amplitude, latency and waveform shape across All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint studies, but the consistency of N100 and N400 impairment in athletes exposed to repetitive head impacts is notable (Fickling et al. 2019, 2021a,b). While the P300 component has shown little change to-date when examining group average data, the results of the multivariate analyses indicate greater complexity with respect to individual variability and factors affecting head impact exposure. For instance, the P300 amplitude and latency, as markers of basic attention, co-varied significantly with subjective concussion history (as did N400 latency). Similarly, significant covariance occurred between the N100 and the K-D test for pre- versus post- comparisons by group by games played (as did the N400 for pre- versus post by games played). Of interest, NfL results significantly co-varied in terms of pre- versus post- season and subjective concussion history. Taken together, the current findings suggest that while N100 and N400 changes may serve as an initial indicator of subconcussive impairment, additional brain vital sign features may also be informative as objective markers for underlying sub-concussive impairment profiles across a spectrum of differential head-impact exposure between players. Objective assessments of function and structure There were no significant differences between the groups in clinical neuropsychological questionnaires pre- to post-season despite significant differences in objective neurocognitive measures. This observation is not surprising, given that clinical questionnaires such as the SCAT5 are subjective in nature, rely heavily upon symptom reporting, and healthy, non-concussed athletes that are exposed to repetitive head All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint impacts typically do not show observable physical signs of neurological impairment (Lember et al. 2021). The SCAT5 has been shown to differentiate concussed and non- concussed athletes in an acute concussion diagnosis setting; however, it is less useful in monitoring recovery (Echmendia et al. 2017). The other neuropsychological questionnaires (PHQ-4, NQoL, and the PGIC) also did not show differences between groups that were detected by objective measures. Nonetheless, these questionnaires provide a subjective, yet insightful glimpse into the functional state of the individual's brain health. The current findings highlight the importance of expanding upon studies investigating the relationship between dietary supplementation, cognitive function and brain structure in athletes exposed to repetitive head impacts. Also important to note is the relationship between objective measures of concussion and subjective clinical symptoms. Batteries of objective tools such as the brain vital signs, K-D, and NfL may be able to detect neurocognitive impairments that are at or below current diagnostic thresholds for concussion. A critical issue involves the point at which cognitive impairments manifest as detectable clinical-behavioral symptoms. Further investigation is needed to investigate if dietary supplements alter or prevent the emergence of clinical-behavioral symptomatology. Limitations This interventional trial has several limitations. First, the cohort was exclusively male Junior-A hockey players; therefore, further studies are required to generalize the results. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint These investigations should also examine factors related to concussion diagnosis, including gender, age, and mechanism of injury (e.g., blast injuries). Second, variables related to the specific dietary supplement, SynaquellTM, need further analysis. SynaquellTM doses were not standardized by body weight, and individuals with higher body mass may require higher dosage to experience the same effects on neurocognition. Future studies should evaluate optimal dosages, ingredients, and the potential for individually tailored dietary supplementation. Third, supplementation did not begin at the beginning of the season due to supply chain challenges. The athletes participated in a portion of the season without receiving the supplement. Season long supplementation may have contributed to greater neurocognitive changes (or lack thereof) in the SQ group. Fourth, our study examined only pre- versus post- timelines, and further time points should be investigated in future studies, including whether dietary supplements support the brain during recovery from a clinically diagnosed concussion. Finally, the current study did not include quantified impact evaluation, so the relationship with impact exposure remains to be determined. Conclusion This prospective, randomized trial showed that a multi-ingredient dietary supplement significantly affected objective measures of brain function and structure from pre- to post-season in Junior A ice hockey players compared to a placebo control group. Further investigation into the effects of dietary supplementation on the contact athlete’s brain is warranted. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint Acknowledgments The current study is part of the clinical trial listed on ClinicalTrials.gov (ClinicalTrials.gov Identifier: NCT05498818). The study was funded by the USA Hockey Foundation (M. Stuart). The authors would like to thank Dr. Aynsley Smith for laying the foundation that made this research possible. We would like to thank Sydney Kalina, Chad Eickhoff, Jeff Lamb and Houston Hawkins for continued support with the implementation of data collection throughout this study. The authors would also like to thank Thorne for providing SynaquellTM along with on-going implementation support throughout the clinical trial. The NeuroCatch® Platform was provided by HealthTech Connex. Conflicts of Interest R. D’Arcy, S. Fickling, and T. Frizzell are with HealthTech Connex, who provided the NeuroCatch® Platform, with disclosed financial interests. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted May 25, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288079
doi: 
medRxiv preprint",1
"Cerebral palsy (CP) is the most common cause of physical disability during childhood. Early diagnosis is essential to improve functional outcomes of children with CP. The General Movements Assessment (GMA) is a strong predictor of CP, but access is limited by the need for trained GMA assessors. Using 503 infant movement videos acquired at 12-18 weeks’ term-corrected age, we developed a framework to automate the GMA using smartphone videos acquired at home. We trained a deep learning model to label and track 18 key body points, implemented a custom pipeline to adjust for camera movement and infant size and trained a convolutional neural network to predict GMA. Our model achieved an area under the curve (mean ± S.D.) of 0.80 ± 0.08 in unseen test data for predicting expert GMA classification. This work highlights the potential for automated GMA screening programs for infants. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 Introduction Cerebral palsy (CP) refers to a group of disorders that affect motor development, movement and posture and are attributed to non-progressive disturbances or injuries to the developing brain before 1 year of age1. Cerebral palsy is the most common cause of physical disability during childhood, occurring at a rate of  2.1 per 1000 live births worldwide2. While those born preterm or with low birthweight are at greater risk of having CP, almost 50% of infants with CP are born at term without overt risk factors3,4. Early diagnosis is essential to improve clinical and functional outcomes of children with CP5. Detecting abnormal motor development within the first 6 months after birth allows targeted interventions, coincident with periods of rapid neurodevelopmental plasticity and musculoskeletal development. It has been shown that early intervention improves children's motor and cognitive development as well parental wellbeing5,6. However, the average age of CP diagnosis is 19 months3, and only 21% of infants with CP are diagnosed before 6 months3,7, thus many infants miss a crucial window for early intervention. The General Movement Assessment (GMA) can accurately predict those at highest risk of CP within the first few months after birth8,9. General movements are spontaneous movements involving the whole body with a changing sequence of arm, legs neck, and trunk movements10. Between 9 and 20 weeks of age, spontaneous movements are characterised by continuous small movements with moderate speed and variable acceleration, termed ‘fidgety’ movements11. These ‘fidgety movements’ are typically recognised using a trained assessor's gestalt perception, while the infant is lying awake on their back with no direct handling or interaction11. This assessment is best completed from video recordings of the infant and has high predictive validity for neurodevelopmental outcomes and excellent inter-rater reliability9,12,13. The GMA when used during the ‘fidgety period’ has the potential to be an important screening tool in the diagnosis of CP. The specialized training required by GMA assessors means that many primary care services and hospitals do not offer routine GMA, which limits the widespread adoption of GMA as a screening tool16. The ability to perform the GMA using video recordings has raised the potential to improve equitable healthcare access for those living in remote regions or in low-resource settings. Recently, the development of smartphone apps that allow the standardised recording of video by an infant’s primary carers using a hand-held device, have been shown to improve access to the GMA and allow identification of high-risk infants outside of clinical settings12–15. Automated scoring of the GMA from video can provide a mechanism to overcome these bottlenecks and enable high- throughput assessment for screening programs. Recent advantages in computer vision and deep learning have led to the emergence of pose estimation techniques, a class of algorithms designed to estimate the spatial location of a set of . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 3 body points from videos or pictures, and track movements with high accuracy17–20. Pose estimation tools do not require any specialised equipment, movement sensors or markers, and can be readily applied to track movement in new videos once trained. Several open-source pose estimation tools, pre-trained on large databases of human movement, are available for direct application to new datasets17,21–23. However, the standard implementation of such algorithms has been found to perform poorly in videos of infants, likely due to significant differences in body segment size and scale24,25. Thus, fine-tuning or re-training of pose estimation models is required to accommodate the unique characteristics of infant movement data24. Further, videos acquired outside of controlled, clinical or research laboratory settings may vary significantly in terms of camera angle, length, resolution, and distance to subject, requiring additional processing steps before analysis24,26. Several recent studies have yielded promising results predicting motor outcomes in infants using movement tracking data from pose estimation tools24,26–30.  Using a semi-automated approach with manual key point annotation of clinical videos, Ihlen et al. demonstrated computer-based movement assessments can perform comparably with observation-based GMA in predicting CP (area-under-ROC-curve, (AUC) = 0.87)29. Using videos acquired in a specialised laboratory setting and an infant-adapted OpenPose model, Chambers et al. employed a Bayesian model of joint kinematics to identify infants at high-risk for motor impairment24. Recent applications of deep learning models to classify movement data have also reported good performance, with one example detecting the presence or absence of fidgety movements in 5-second video clips with 88% accuracy in a laboratory setting (n=45 infants)28. In a large, multisite study of high-risk infants (15% with CP) Groos et al. reported a positive predictive value of 68% (negative predictive value of 95%) for later CP diagnosis using an ensemble of Graph Convolutional Networks (GCN) applied to clinical videos27. Similarly, Nguyen-Thai et al. applied GCNs to OpenPose tracking data to create a spatiotemporal model of infant joint movement in 235 smartphone videos, reporting an average AUC of 0.81 for the prediction of abnormal fidgety movements26. Despite initial progress, significant challenges remain for the application and uptake of this technology. Many studies to-date have been limited by small sample size (typically < 100 infants) and few have been conducted outside of clinical or laboratory settings30. In this study, using a large cohort of infant movement videos (n=503) captured remotely via a dedicated smart phone app, we test the efficacy of automatic pose estimation and movement classification using deep learning methods to predict GMA classification. In addition, we design a custom processing pipeline to accommodate video capture from hand-held devices, identify factors that adversely affect automatic body point labelling accuracy and locate salient movement features that predict abnormal outcomes in individuals. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 4 Results Automated body point labelling with human-level accuracy We acquired 503 3-minute videos from 341 infants at 12 to 18 weeks’ term-corrected age using Baby Moves, a dedicated smartphone app12. To fine-tune a pose estimation model for infant videos, we created a training dataset using a random selection of n=500 frames from 100 videos (5 frames per video, see Methods). We manually annotated eighteen body points (crown, eyes, chin, shoulders, elbows, wrists, hips, knees, heels and toes; Supplemental Figure S1) and trained a fully-customisable pose estimation model, Deep Lab Cut (DLC)31 to automatically detect each body point (Figure 1 ;Supplemental Video 1). Body point labelling using the trained DLC model was highly accurate (Figure 1e) achieving an average root mean square difference (RMSD) between manual and automatic annotations of 6.78 pixels (SD: 6.60).  DLC performance was comparable to inter-rater reliability (IRR) of two annotators (average ± SD RMSD 6.90 ± 7.29 pixels; Figure 1e). Labelling accuracy varied moderately across body points, with the highest accuracy for the eyes (average ± SD RMSD: manual/auto Left 3.04 ± 2.24 pixels, right 3.61 ±  2.63 pixels; IRR Left  1.80 ± 0.86 pixels, right 2.32 ±  1.22 pixels) and lowest accuracy for the hips (average ± SD RMSD: manual/auto Left 10.37 ± 6.13 pixels, right 12.02 ±  8.41 pixels; IRR Left  10.05 ± 7.20 pixels, right 14.14 ±  8.85 pixels) (Supplemental Figure S3). There was no significant difference in RMSD between different video resolutions (Supplemental Table S2). During labelling, the DLC model assigned each point a measure of prediction confidence. After removing points labelled with low confidence (see Methods), we found that the percentage of frames labelled on average was 92% (SD: 16%). The percentage of frames in which each point was confidently labelled was lowest for the wrists (average ± SD Left: 81 ± 21%, Right 78 ± 24%) and heels (average ± SD: Left: 69 ± 24%, Right 77 ± 20%) (Supplemental Figure S4), due in part to these body points being occluded by other body parts at instances throughout the video and exhibiting a greater range of movement. We conducted a sensitivity analysis to determine potential factors that related to labelling failures (see Methods). We found that the amount of clothing worn by the infant moderately affected model performance with outfits that covered the hands and feet adversely affecting labelling of the extremities (F=5.180, p=0.006; Supplementary Table S4). As a quality control step, only videos where on average at least 70% of body points per frame were confidently labelled were included for further analysis. After quality control, our final cohort comprised n=484 videos from 327 unique infants. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 5 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 6 Figure 1: Data acquisition and analysis pipeline. a. Acquisition of 503 videos using the dedicated Baby Moves smartphone app12. b. 100 videos were selected for DLC training, stratified by age at video acquisition, sex and GMA classification. c. From each of the 100 training videos, five frames were selected for manual labelling using a k-means clustering algorithm (see Methods; total DLC training dataset: 500 frames). d. The trained DLC model was used to label all frames in all videos. This constitutes the full dataset with body point positional data used for GMA classification e. Labelling accuracy was evaluated in a subset of 50 frames not included in the training dataset. Root mean square difference (RMSD) in pixels was calculated between manual and automatic labelling (Manual/auto) and inter-rater reliability (IRR) of two annotators. Predicting GMA from video data As videos were not acquired in standardised clinical or experimental settings, positional data were pre-processed using a custom pipeline to account for different video acquisition parameters and potential camera movements relative to the subject, prior to classification. This consisted of outlier removal, gap filling, adjusting for camera movement, scaling to unit length based on infant size and framerate normalisation (see Methods). After pre-processing, each video was represented as a 46 × 4500 feature-by-frame matrix comprising standardised 𝑥 and 𝑦 coordinates of each body point and 2D joint angles of 10 joints in each frame. Abnormal movements may occur at any point during the video and occur with different frequencies, therefore we aimed to identify short periods where discriminant movements were present using a sliding window approach (Figure 2). We trained a convolutional neural network to predict GMA based on short instances of positional data over time (Figure 2c), calculating video-level predictions by integrating over all clips for a given video. Averaged over 25 cross-validation repeats (70% train/15% validation/15% test), the trained model achieved an AUC (mean ± S.D.) of 0.795 ± 0.080 in unseen test data (Figure 2d) and balanced accuracy of 0.703 ±  0.083 (Figure 2e). For abnormal/absent GMA, the positive predictive value (PPV) was 0.277 ± 0.077 and the negative predictive value (NPV) was 0.941 ± 0.035. Sensitivity and specificity were 0.755 ± 0.150 and 0.651 ± 0.078, respectively (Figure 2e). Performance was consistent over a range of model parameters, including batch size, learning rate and weight regularisation (Supplementary Figure S5). The inclusion of video metadata, age at video acquisition and birth cohort (extremely preterm or term-born infants) improved model performance significantly (Supplementary Figure S5), compared with classification using video data alone (AUC = 0.749 ± 0.077). Taking advantage of multiple model instances trained across different cross-validation repeats, we found that individual predictions were generally stable when videos were included in the held-out test set for a given repeat (Figure 2d-e, Figure 3Error! R eference source not found.). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 7 We compared performance with an alternative baseline model: an 𝑙2-regularised logistic regression applied to a set of timeseries features extracted from each video32. The baseline model achieved an AUC of 0.706 ± 0.098 (0.604 ± 0.106 without video meta-data). A nonlinear, kernelized logistic regression model achieved cross-validated AUC = 0.720 ± 0.100. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 8 Figure 2: GMA prediction from movement data. a. Framewise positional data from DLC labelled videos were preprocessed to derive a set of feature timeseries (46 features × 4500 frames) per video. b. The classification model was trained on 128-frame clips for the full timeseries (top). Data augmentation steps (magnitude scaling and time warping; bottom middle and right) were applied to each clip during training. For each augmentation method, dashed grey . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 9 lines indicate timeseries position prior to the augmentation step. c. Model architecture. 1D convolutional layers were combined with an attention module to classify normal and abnormal/absent GMA. Causal convolutions (inset) were applied to account for the temporal structure of the data. d. Receiver-operator curves (ROC) for each of the 25 cross- validation repeats. The mean curve is overlaid in teal. e. Model performance statistics for each of the cross-validation repeats. Mean and standard deviation across repeats are overlaid in black. AUC = area under the ROC; NPV = negative predictive value; PPV = positive predictive value. Figure 3: Variation in classifier prediction values. Classifier prediction values from 25-fold cross validation. Prediction values reported when infant movement included in held out test set, ordered by median prediction value. Top (orange) infant videos scored as abnormal/absent general movement assessment (GMA) by expert rater, n=73 infant videos. Bottom (blue) infant videos scored as normal GMA by expert rater, n=396 infant videos. Boxes with horizontal line represent interquartile range and median respectively, error bars represent 95% confidence interval and dots represent outliers. Dashed line at 0.5 represents cut-off value for classifier between abnormal/absent GMA prediction and normal GMA prediction. Examining spatial and temporal model attention during prediction To identify potential features that were important to model prediction, we computed spatiotemporal saliency maps for each video33, (Figure 4a). This value highlights features (for a given body point in a single video frame) where changes in input would elicit the largest change in model prediction and can be used as a measure of model sensitivity to input data33,34. An example saliency map is shown for a single subject in Figure 4a. Saliency varied across the length of the video, corresponding with variations in model attention (white line, Figure 4a bottom). Clips with high . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 10 saliency, relative to all subjects in the test set, are highlighted with yellow bars on the input feature timeseries, illustrating model attention to periods of different length spread throughout the video. Averaging total saliency across all clips for each body point reveals higher model sensitivity to position of the lower body points (Figure 4a middle), including movement of the knee and ankle joints. Similar patterns of model saliency were observed across all participants. A map of group average feature saliency (averaged across clips, participants and cross-validation repeats) is shown in Figure 4b. Model saliency was highest in the lower body. This pattern was consistent across cross-validation repeats (Supplemental Figure S7) and between normal and abnormal/absent GMA predictions (Supplemental Figure S6). To further characterise features to which the model prediction was sensitive, we compared timeseries data in clips with high (90th percentile) and low (10th percentile) total saliency (Figure 4c-d). The number of high saliency clips did not differ between normal (mean ± S.D. = 55.21 ± 33.93) and abnormal/absent (51.74 ± 35.56) GMA videos (Supplemental Figure S8). For each clip, we calculated the mean (absolute) displacement of body points from the average position, as well as the standard deviation of displacements over frames. We found that, high saliency clips were characterised by body point positions closer to the average body position (Figure 4c) and by a lower standard deviation of joint displacements over time compared to clips with low saliency (Figure 4d). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 11 Figure 4:  Sensitivity of model predictions to input features. a. Feature timeseries (top) and saliency map (bottom) for a single, correctly-classified video from an infant with normal GMA. Timeseries are shown for each feature (n=46), across the length of the video. Yellow bars indicate clips with high model attention (75th percentile across all subjects). Saliency was calculated for each feature in each frame and summed over frames within each clip (n=547 clips). The map has been upsampled and smoothed to match the length of the timeseries (frames=4500). Lighter colours indicate higher saliency (arbitrary unit). Clip attention derived from the attention module (upsampled and smoothed) is overlaid in white. Average saliency across the full video is shown for each body point (middle) and joint angle (right). The model prediction is shown top right, where 0 indicated normal GMA prediction. b. Body point saliency averaged across all participant videos. Lighter colours and larger size reflect higher saliency. c. Left, mean absolute distances between each body point and their respective average position during clips of high (solid line, filled) and low (dashed line) saliency. Density plots show the distribution of displacements for high and low clips over all videos and cross-validation repeats. Right, median difference between displacement in high and low clips. d. Left, standard deviation (std) of displacements from the average position in high and low saliency clips, over all videos and cross-validation repeats.  Right, median difference in standard deviation distributions. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 12 GMA prediction and development at 2 years We compared our model predictions with participant’s motor, cognitive and language outcomes at 2-years corrected age as assessed by the Bayley Scales of Infant and Toddler Development-3rd edition (Bayley-III) (Figure 5; Supplemental Figure S9, Figure S10). We found strong evidence for differences in 2-year motor composite scores between infants with different predicted GMA classifications (normal vs abnormal) when metadata (age at acquisition and birth cohort; extremely preterm or term-born infants) were included in the model, mean difference 11.35 (95%CI = [8.41, 14.30], t(439)=7.574, p<0.001). These differences were diminished when using movement data alone, mean difference 2.33 (95%CI = [-0.88, 5.54], t(439)=1.426, p=0.1555) (Figure 5a). There was also strong evidence for differences in 2-year cognition and language composite scores between predicted GMA classifications (Supplemental Figure S9, Table S4). As preterm birth is associated with both higher risk of abnormal GMA and poor neurodevelopmental outcomes, we conducted a secondary analysis to identify associations between birth cohort, GMA prediction and 2-year outcomes. We found a significant main effect of birth cohort (F=8.775, p=0.003) but not GMA prediction (F=1.762, p=0.185) on motor composite scores. The interaction between birth group and GMA prediction was not significant (F=1.181, p=0.278). Stratifying by birth cohort, there was weak evidence for differences in motor composite scores between GMA classification categories in term born infants, mean difference 9.52 (95%CI = [-1.16, 20.20], t(225)=1.756, p=0.080). In preterm infants, these differences were lower 2.09 (95%CI: = [-5.11, 9.29], t(212)=0.573, p=0.568) (Figure 5b). Similar results were observed in cognitive and language composite scores (Supplemental Figure S10, Table S6). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 13 Figure 5: GMA prediction and motor development at 2 years: a. Bayley-III motor outcome stratified by GMA prediction (n=441 infant videos) and model variants trained using video movement and metadata (both = age at acquisition and birth cohort) and movement data alone (none).  Blue indicates GMA prediction = 0 (normal) and orange indicates GMA prediction = 1 (abnormal/absent) for all graphs. * Indicates strong evidence for differences between GMA prediction groups from independent two-sample t-test. b. Top, density function of motor outcome by birth cohort and GMA prediction. Bottom, Peak of density function. Term infants are represented by dashed lines and preterm infants by solid lines. Discussion Using deep learning applied to smart phone videos, we tracked infant movements at 12-18 weeks of age, predicting GMA ratings outside of a controlled clinical setting. Our paper illustrates the potential for early automated detection of abnormal infant movements implemented through at- home video acquisition. Our best performing model for predicting expert GMA ratings, was a deep learning model, consisting of 1D convolutions and an attention module. Our model achieved an AUC 0.80 (SD: 0.08), comparable to results obtained from Ihlen et al. and Groos et al. in cohorts of high-risk infants using video recordings from stationary cameras in clinical settings27,29.  Our model outperformed alternative baseline models and was robust over various hyperparameter settings. We demonstrated that including participant metadata is crucial to improving model predictions, highlighting the increased risk for abnormal movements in preterm born individuals8. Of the 41 infants with abnormal GMA as scored by trained GMA assessors 35 (85%) were from preterm infants and 6 (15%) from term-born infants. We found that including metadata (birth cohort and . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 14 age) improved model performance from an AUC=0.70 based on movement data alone to AUC=0.80. Notably, classifying on birth cohort alone would result in an AUC of 0.69, quantifying the added value of the movement data. GMA model predictions were associated with poorer neurodevelopmental outcome at 2 years of age. We found that this effect was largely dependent on preterm birth, although infants with abnormal GMA predictions scored lower on average regardless of birth cohort. The association between preterm birth and poor neurodevelopmental outcomes is well established8,13 and this finding reflects the relatively lower predictive validity of GMA ratings, and therefore model predictions, for motor and cognitive outcomes at 2 years. GMA is a strong predictor of CP8,9. We used abnormal or absent GMA ratings as a surrogate measure for CP risk. Fidgety movements can be classified as abnormal or absent during this developmental window, both of which are associated with neurodevelopmental impairment8,13. Combining abnormal and absent groups, who may have different movement signatures, into a single cohort may have affected model performance but numbers were too small to further split the groups (n=40 and 36 respectively). Similarly, only 6 infants in the current cohort were diagnosed with CP by 2-year follow-up, precluding the use of our model framework to predict CP diagnosis in this group. To track infant movement, we used a pose-estimation algorithm, Deep Lab Cut31,35, which has the advantage of being customisable across species, age and features of interest using a minimal training dataset35. Our model achieved human-level labelling accuracy of body parts with a RMSD of 6.78 pixels (SD: 6.60). Due to the non-controlled settings in which videos were acquired, we performed a sensitivity analysis, identifying video features that could affect labelling accuracy in at- home video recordings.  Body point labelling was robust to background and video lighting but moderately affected by clothing worn by the infant, specifically clothing covering hands or feet. Use of at home video recordings introduced additional data processing challenges, including camera movement relative to the infant and different video formats (frame rate, resolution, distance from infant). To accommodate this, we developed a novel framework, that is versatile and supports videos taken outside a standardised clinical setting. Future work could focus on more detailed anatomical annotations for labelling, particularly relating to the hands and feet in recognition of the role those body parts play in identification of fidgety movement during the GMA11. We used 5 second clips to train our model, this allowed us to identify periods of movement that were informative to the model prediction. By analysing model saliency, we were able to extract information about which movement features were attended to by the model, discriminating between abnormal and normal movements. We identified that lower limb movements contributed more to the classifier’s output, with higher saliency attributed to video clips where infant position was closer to the average position, ignoring periods where the infant has moved significantly from the supine position (i.e.: out-of-frame movement, rolling). Other studies have used high-resolution . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 15 video annotations identifying periods of abnormal movement within videos29. While this approach is likely to improve automated movement identification it requires a significant amount manual annotation and labelling that would be difficult to achieve in larger cohorts. Several recent studies have yielded promising results predicting motor outcomes in infants. However, to date these studies have been limited by small sample size (typically < 100 infants), only including high-risk infants and few have been conducted outside of clinical or laboratory settings30. A strength of our study is the inclusion of both extremely preterm and term born infants within our dataset. Our study offers an automated approach to perform GMA ratings, that is capable of accommodating videos recording outside the clinical setting. Our work highlights the potential for automated approaches to screen for CP at a population level, which would enable increased access to early interventions for these children. Methods Participant data Videos were recorded using the Baby Moves smart phone app by the parent/caregiver on their personal device between April 2016 and May 201712. Videos were acquired from n=155 (77 female [50%]) extremely preterm infants (<28 weeks’ gestation) and 186 (91 female [49%]) term-born control infants, Supplementary Table S1. In total, 503 videos from n=341 infants aged between 12- and 18-weeks term corrected age were available. For a subset of n=160 (75 preterm, 85 term), two videos were collected per infant during this period. Full details of the study protocol can be found in Spittle et al., (2016)12. The study was approved by the Royal Children’s Hospital Ethics Committee (HREC35237). Video capture To facilitate video recording outside of clinical or laboratory settings, the Baby Moves app provides detailed instructions and a dotted outline overlay to improve positioning of the infant in the video frame12. Guidance was given to parents/caregivers to perform the video while the infant was lying quietly and not fussing with minimal clothing, consisting of singlet and nappy only. Subsequently, videos were securely uploaded to a REDCap database36,37 at the Murdoch Children’s Research Institute for remote review. The GMA was scored according to Prechtl’s GMA11 by two independent assessors that were unaware of participants’ neonatal history. General movements were classified as normal if fidgety GMs were intermittently or continuously present, absent if fidgety GMs were not observed or were sporadically present, or abnormal if fidgety GMs were exaggerated in speed and amplitude. If there was disagreement between the two assessors, then a third experienced GMA trainer and assessor made the final decision. Any videos rated as unscorable were not evaluated in this study. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 16 All videos were submitted in MP4 format. Due to differences in device model and settings, three video resolutions were present in the dataset: 480 x 360 (n=366 videos), 640 x 480 (13 videos) and 720 x 480 (126 videos) with a median frame rate of 30 frames per second (range: 15 to 31). Each video was 3 minutes in length resulting in mean (SD) 5100 (497) frames per video. Automated body point labelling We trained a deep learning model using Deep Lab Cut, version 2.1 (DLC)31 to label and track key body points. To train the DLC model, we formed a training dataset consisting of a subset of 100 videos from our dataset stratified for age, sex, birth cohort (preterm or term) and video resolution. Only one video per infant was allowed in the training set. For the training dataset, five frames from each video were manually labelled with 18 key body points: crown, chin, eyes, shoulders, elbows, wrists, hips, knees, heels and big toes (Figure 1; Supplemental Figure S1). Manual labelling was performed via the DLC graphical user interface. To ensure diversity of movements in the frames selected for labelling, we used a k-means clustering algorithm implemented in DLC to select five frames from different clusters within each video for labelling. We implemented a DLC model with a pre-trained ResNet-50 backbone and trained for 1 million iterations on a NVIDIA TITAN Xp using a training/validation fraction of 0.95/0.05 (see Supplemental Figure S2 for training performance). Once trained, the DLC model was used to automatically label the 18 body points for all videos in the dataset. For each frame, the DLC model returned the x- and y-coordinates in pixels of the body points relative to the corner of the video image and its prediction confidence. Body points with a prediction confidence below 0.2 were removed. Labelling accuracy of the DLC model was evaluated on an independent sample of 50 random frames not included in the training dataset using the root mean square difference (RMSD) between predicted labels and manual labels. To evaluate inter-rater reliability (IRR) for body point labelling a second human annotator repeated labelling on the same 50 frames. The RMSD between labels for the two annotators were calculated. Additional metrics of DLC model performance included the number of unlabelled body points per video. As videos were collected outside of a controlled clinical setting, we conducted a sensitivity analysis to determine whether variability in certain factors across the individual videos may influence model performance. Each video not included in the training dataset (n=403) was categorised by the following factors: Lighting (Dark/Okay/Bright), clothing (Bodysuit/Nappy & singlet/Nappy only), skin tone (Light/Fair/Medium/Dark), infant in frame entire video (Yes/No), background (Pattern/Solid Colour – Dark/Solid Colour – Light), extra items in view (No/Another Child/Recorder’s feet/Toys/Other). We tested if model performance was affected by the listed factors using a mixed model Analysis of Variance (ANOVA). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 17 Pre-processing pipeline Data pre-processing consisted of quality control, outlier removal, gap filling, adjustment for camera movement, scaling and feature extraction (Figure 6). Quality control To ensure high-quality movement data from each video was available for further analysis, we established a quality control measure based on body point labelling. Only videos in which more than 70% of body points were labelled on average across all frames were used. This resulted in the exclusion of 21 videos from further analysis, resulting in a final dataset of 484 videos from 327 infants. Outlier removal Body point outliers were removed in a two-step process. First, outlying labels were removed using an ellipse envelope centred in the centre of the torso (mid-point of hips and shoulders). The ellipse was scaled relative to infant size, with unit length set to distance between the infant’s crown and hip midpoint. The ellipse was scaled to 3 times unit length in the proximal to distal direction and two times unit length in the medial to lateral direction. Body point labels lying outside of the ellipse were removed. Following this, a similar process was applied to each body point using an ellipse envelope centred at the body point’s framewise median position. Each ellipse was again scaled by unit length with the proximal-distal and medial-lateral scaling set based on observed body point variance from the complete dataset. Body point labels lying outside of their respective ellipses were removed. Gap filling Where gaps in body point data existed due to missing, removed, or occluded body point labels, linear interpolation was used for gaps of five frames or less. For gaps greater than five frames we used an iterative multivariate imputation38, implemented in scikit-learn39 (v1.3.0). Adjusting for camera movement As videos were recorded on hand-held devices, camera movement relative to the infant was apparent during the three-minute video. To account for angular rotations, all points were rotated on a frame-by-frame basis so the mid-line of the body (mid-shoulder to mid-hip), was aligned to the vertical in each frame. In addition, body point position in each frame was normalised to infant unit length, measured as distance from crown to mid-hip. Framerate normalisation All pre-processed movements data were normalised to the same length. Due to variation in video frame rate, the number of frames in each 3-minute video varied. To account for this, all videos . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 18 were interpolated to 4500 timepoints in length or a framerate of 25 frames per second using cubic 1D interpolation as needed. Feature extraction For each frame, we extracted each body point’s 𝑥, 𝑦 position in addition to 10 joint angles (left and right shoulders, elbows, hips, knees and ankles; in radians, resulting in 𝑝= 46 features per frame (𝑘𝑒𝑦𝑝𝑜𝑖𝑛𝑡𝑠 × {𝑥, 𝑦} + 𝑗𝑜𝑖𝑛𝑡 𝑎𝑛𝑔𝑙𝑒𝑠). Figure 6: Preprocessing piepline. a. Quality control, videos with less than 70% of body points labelled on average were excluded from further analysis. b. Outlier removal, outlier body points were removed (denoted by x) when outside of ellipical envelope for the whole body or each body point indiviudally. c. Gap filling using linear interpolation for gaps less than 5 frames, or a multivariate imputer for gaps larger than 5 frames. d. Body points were rotated on a frame by frame basis ensuring midline of body is aligned to the vertical. e. Body point position was scaled to unit length based on infant size, unit length distance crown to mid hip. f. For each frame x- and y-coordiates and additional features consisting of joint angles from the left and right shoulder, elbow, hip, knee and ankle were extracted. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 19 Prediction of GMA from movement data As abnormal general movements can occur at any point during each video, may last for different lengths of time, and occur with different frequencies, we aimed to identify short periods of time where abnormal movements were present in each video and use a sliding window approach to generate subject-level predictions. During model training, each subject’s pre-processed timeseries data was split into short clips of 𝑡= 128 frames in length (approximately 5 sec.) with 𝑠𝑡𝑟𝑖𝑑𝑒= 8. In each training epoch, we randomly sampled 𝑠= 1 clip per video, selecting more than one clip per video per epoch did not offer an improvement in model performance and cost more memory and computation (Supplemental Figure S5). Model architecture The model architecture is shown in Figure 2c. Each subject’s data is represented as a tensor 𝑆∈ ℝ𝑠 × 𝑡 × 𝑝 where 𝑠 is the number of sampled clips per video, 𝑡 is the number of frames per clip and 𝑝 is the number of features per frame. We employ three 1D convolutions applied along the temporal dimension (𝑓𝑖𝑙𝑡𝑒𝑟𝑠 =  64; 𝑘𝑒𝑟𝑛𝑒𝑙 𝑠𝑖𝑧𝑒 =  3) with causal padding and 𝑅𝑒𝐿𝑈 activations. After each convolutional layer, we applied batch normalisation (Figure 2c). Each convolution was followed by max pooling along the temporal dimension with 𝑤𝑖𝑛𝑑𝑜𝑤 𝑠𝑖𝑧𝑒= 4 and 𝑠𝑡𝑟𝑖𝑑𝑒= 4. After the final convolution, features of each clip were concatenated across the remaining timesteps to form feature matrix 𝑀∈ℝ𝑠 × 128 (Figure 2c). Clip features are then passed through a single fully- connected layer (𝑢𝑛𝑖𝑡𝑠= 64, 𝑅𝑒𝐿𝑈). We applied dropout with a rate of 0.5 before and after the connected layer. To identify features that discriminate subjects with or without abnormal movements, we passed each clip through a sigmoid attention module40 (Figure 2c). In this context, clips with feature vectors that discriminate between classes are given a larger weight. A clip level context vector, 𝑢, is assigned to measure the importance of each clip to the final model output. First, each clip, 𝑚𝑐∈ ℝ1 × 64, is passed though a single fully connected layer with weights and bias, 𝑊 and 𝑏, and a 𝑡𝑎𝑛ℎ activation to create clip level representation, 𝑢𝑐: 𝑢𝑐= tanh(𝑊𝑚𝑐+ 𝑏) The similarity between each clip’s representation and that of a context vector, 𝑢 is calculated and scaled: 𝛼𝑐=
1 1 +  exp (−𝑢𝑐
𝑇𝑢) . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 20 Where 𝛼𝑐∈[0,1] and represents the importance of each clip to the final model output. A final representation is calculated though a weighted average of clip features: 𝑣= 1 𝑛∑𝑎𝑐𝑚𝑐 𝑛 𝑐=1 Where 𝑣 is a feature vector representing the sampled clips from each video. The context vector, 𝑢, and the layer weights and biases are randomly initialised and jointly learned with other model parameters during training. The context vector, 𝑢, can be considered a ‘signature’ that identifies a discriminative movement within a clip. The resulting weighted outputs form a final feature vector, 𝑣. We apply a final dropout (0.5) to this vector and pass to a fully connected layer with one unit and 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 activation to predict the class label of each subject. We used binary cross entropy (BCE) as the loss function with Stochastic Gradient Descent as the optimiser (Nesterov momentum = 0.9)41. As not all randomly sampled clips may contain abnormal movement patterns during each training epoch, we employed label smoothing of 0.1 to account for uncertainty in the assigned sample labels of each batch42. The learning rate was set to 0.005, batch size was set to 8 and we added 𝑙2-regularisation of 0.005 to all weight kernels.  We trained for a maximum of 10000 epochs, evaluating loss in the validation set and stopping training once validation loss had stopped improving for 100 epochs, retaining the model with minimum loss for testing. Data augmentation Data augmentation is a common processing step in various image recognition and classification tasks and provides additional protection against overfitting in small sample settings43,44. We employed data augmentation methods for timeseries data including random magnitude scaling and time warping43 (Figure 2b). We used cubic splines to generate a series of random, smooth sinusoidal curves (knots = 3 – 15; mean value = 1.0; sigma = 1.0). During training: i) the timeseries in each clip were multiplied with a randomly generated curve to smoothly scale magnitude across the clip’s length and ii) time warping was applied by smoothly distorting the time interval between points based on another randomly generated curve, shifting the temporal position of adjacent points closer or further apart43 (Figure 2b). Model calibration and class imbalance To account for the difference in class frequencies between normal and abnormal GMA (normal = 408 videos; abnormal/absent = 76 videos), we oversampled the minority class by a factor of 5 during training. For each video in the training sample with an abnormal GMA rating we sampled 5 . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 21 sets of clips during each training epoch, resulting in approximately equal number of training samples from each group. While resampling methods can improve model performance in imbalanced datasets, they can result in miscalibrated models due to the difference in class frequencies between the original sample population and the oversampled training set45,46. We employed Platt scaling47 as a post- training method to calibrate model predictions. Model calibration was performed by fitting a logistic regression over model predictions in the validation dataset, the parameters of which are used to transform model outputs to calibrated probabilities at inference. Metadata Age at video acquisition and birth cohort (extremely preterm or term-born) are both potential confounders that can affect GMA ratings13. To incorporate metadata into the model, we applied an additional 1D convolution (𝑓𝑖𝑙𝑡𝑒𝑟𝑠= 4, 𝑘𝑒𝑟𝑛𝑒𝑙 𝑠𝑖𝑧𝑒= 1) to a feature vector of age at video acquisition and categorical group membership (preterm or term-born). The outputs were concatenated with the video features prior to the final layer for classification (Figure 2c). Model evaluation At inference, each test subject’s timeseries data were split into 547 overlapping clips (𝑡 =  128, 𝑠𝑡𝑟𝑖𝑑𝑒 =  8) which were passed with associated metadata through the trained and calibrated model to generate the final model output from the attention-weighted sum of all clips. To evaluate model performance, we performed cross-validation by splitting the data into three subsets: train (70%), validate (15%) and test (15%), ensuring that the proportion of infants with abnormal movements were similar across subsets and, for infants with more than one video, that both videos were included in the same subset. Model performance was evaluated in the test set using the area under the receiver operating curve (AUC), balanced accuracy (BA), specificity, sensitivity and positive and negative predictive values (PPV; NPV). Cross-validation was repeated 25 times, each with random splits of the dataset. Performance metrics in the test set are reported as average values across the 25 cross-validation repeats. We explore the impact of different parameter choices on model performance in the Supplemental Material (Figure S5; Figure S6). To examine important model features, we calculated model saliency for each test output using vanilla gradient maps41. Baseline model We compared model performance to alternative models based on logistic regression. For each video, we extracted a set of dynamical features previously shown to perform well in timeseries classification tasks32, resulting in 𝑝= 24 features per timeseries. We concatenated timeseries features for each body point coordinate and joint angle along with associated meta data (age and birth cohort) into a single feature vector of length = 1014. Using this data, we trained an 𝑙2- . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 22 regularised logistic regression model to predict GMA. As with the convolutional model, we performed 25 cross-validation repeats, splitting the data into 85% training and 15% testing sets. Regularisation strength was set using a grid search (10-3 – 103) in a nested 5-fold cross-validation of the training data. To enable additional flexibility in the model, we also implemented a nonlinear kernelised logistic regression using Nystroem kernel approximation48. The baseline models were implemented in scikit-learn39 (1.0.2), timeseries features were extracted using pycatch2232 (0.4.2). GMA prediction and development at 2 years Participants were followed up at 2-years’ corrected age and their development assessed using the Bayley Scales of Infant and Toddler Development-3rd edition (Bayley-III) for motor, cognitive and language domains. Bayley-III scores were available for 292/327 infants (441 infant videos) for motor and cognitive domains and 262/327 infants (400 infant videos) for the language domain13. Each video was assigned a single GMA prediction label based on the GMA prediction label most frequently assigned during the 25-fold cross validation. This was done for each variant of model metadata inputs: movement data only (none), birth cohort, age at acquisition and combined birth and age (both). For each model variant we compared 2-year outcomes between GMA-prediction groups using an independent two sampled t-test (two-sided). To determine the association of birth cohort and GMA prediction with 2-year outcomes we performed a two-way ANOVA (factors: birth cohort and GMA prediction group, interaction birth cohort*GMA prediction group). As 2-year outcomes are likely confounded by birth cohort, we stratified by birth cohort and performed independent two-sample T-tests (two-sided) between GMA prediction groups. Data availability The data that supports the findings of this study are available from the corresponding author upon reasonable request. The data is not publicly available due to privacy and ethical restrictions. Code availability We used DeepLabCut to track body points in videos. Code for pre-processing body point data after DeepLabCut, training machine-learning models, analysis of the results are available in our GitHub repository https://github.com/epassmore/infant-movements. Competing interests Alicia Spittle is a tutor with the General Movements Trust. All other authors have no conflicts of interest to declare. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 23 Acknowledgements We would like to acknowledge the parents/families of infants who participated in the study. We would also like to acknowledge the extended Victorian Infant Collaborative Study team for their contribution in collecting infant and 2-year follow up data. We would like to acknowledge funding from the Rebecca L Cooper Medical Research Foundation (PG2019421 to G.B.), National Health and Medical Research Council Investigator Grant (1194497 to G.B.), NVIDIA Corporation Hardware Grant program, The Royal Children’s Hospital Foundation, Melbourne and the Murdoch Children’s Research Institute Clinician Scientist Fellowship. Author contributions Conceptualisation: E.P, G.B and A.S. Data collection: A.K, J.O, A.E. and general movements assessment. E.P and S.G annotated infant videos. Data curation: E.P. A.K and S.G. Data analysis: E.P, S.G and G.B. E.P and G.B wrote the code. J.C and A.S oversaw research study to collect infant videos. G.B oversaw the current study. E.P and G.B wrote the manuscript with input from all authors. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288092
doi: 
medRxiv preprint 24",1
"Objective We studied the rate dynamics of interictal events occurring over fast-ultradian time scales, as commonly examined in clinics to guide surgical planning in epilepsy. Methods Stereoelectroencephalography (SEEG) traces of 35 patients with good surgical outcome (Engel I) were analyzed. For this, we developed a general data mining method aimed at clustering the plethora of transient waveform shapes including interictal epileptiform discharges (IEDs), and assessed the temporal fluctuations in the capability to map the epileptogenic zone (EZ) of each type of event. Results We found that the fast-ultradian dynamics of the IEDs rate may effectively impair the precision of EZ identification,  and  appear to  occur spontaneously, that  is,  not  triggered  by  or exclusively associated with a particular cognitive task, wakefulness, sleep, seizure occurrence, post-ictal state or antiepileptic drug withdrawal. Propagation of IEDs from the EZ to the propagation zone (PZ) could explain the observed fast-ultradian fluctuations in a reduced fraction of the analyzed  patients, suggesting that other factors like the excitability of the epileptogenic tissue could play a more relevant role. A novel link was found between the fast-ultradian dynamics of the overall rate of polymorphic events and the rate of specific IEDs subtypes. We exploited this feature to estimate in each patient the 5 min interictal epoch for near-optimal EZ and resected zone (RZ) localization, which resulted at the population level better than those obtained using either 1) the whole time series available in each patient (P = 0.084 for EZ, P < 0.001 for RZ, Wilcoxon signed rank test) and 2) 5 min epochs randomly sampled from the interictal recordings of each patient (P < 0.05 for EZ, P < 0.001 for RZ, 105 random samplings). Significance Our results highlight the relevance of the fast-ultradian IEDs dynamics in mapping the EZ, and show how it can be prospectively estimated to inform surgical planning in epilepsy. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint Keywords Stereo  EEG  dynamics,  Epileptogenic  biomarkers,  Epileptiform  spike  subtypes,  Neural  tissue excitability, Nested outlier detection, Local false discovery rate. Key Points •
The rate of IED observed in SEEG recordings undergoes spontaneous fluctuations over fast- ultradian time scales. •
The fast-ultradian dynamics of the IED rate may impair the EZ identification and hence are clinically relevant for surgical planning. •
Propagation does not fully explain the fast-ultradian dynamics of the IED rate constraining the precision to localize the EZ. •
Interictal epochs, as commonly examined in clinics, producing near-optimal EZ mapping can be inferred based solely on the LFP dynamics. •
Fluctuations of the AUPREC based on epileptic and non-epileptic events are linked to scale- free and scale-rich processes, respectively. Introduction The success rate of the surgical treatment for drug-resistant focal epilepsies is in the order of 60%.1,2 This imperfect outcome can be partially explained by the fact that optimal delineation of the epileptogenic zone (EZ) remains unknown. In this context, significant efforts are being made to . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint quantify intracranial EEG3 and extract biomarkers that can best predict the location of the EZ. Epileptic spikes are the classic interictal biomarker of epilepsy, used by epileptologists as part of standard practice to inform surgical planing.4 The relationship of the EZ with the epileptiform spikes is complex. In a previous study it was shown a variable correlation of the spike rate with EZ, but particularly important in focal cortical dysplasia.5 Currently, there is no gold standard to define subtypes of epileptiform discharges and to differentiate them from non-epileptic paroxysmal events.6 However, some characteristics of spikes have been found to be better markers of EZ, such as frequency of occurrence, association with high frequency oscillations (HFO)7,8 or with gamma activity.9 An underestimated dimension is the time-varying factors underlying the genesis of different subtypes of interictal epileptiform discharges.10 In this regard, there are important open issues related to the way IEDs fluctuate in spatial extent,11 waveform shape (see Figure 2A in Tomlinson et al.12) and rate of occurrence. The temporal dynamics of IEDs expand over a wide range of time scales, from milliseconds associated with the propagation across macroscopic networks12,13 to circadian (day) and even multidien (week/month) fluctuations in relation to sleep and ictogenesis among other factors.14-18 Therefore, the integration of all available information over a broad range of spatial and temporal scales is crucial to improve the EZ localization based on interictal SEEG recordings. In this work we report on a quantitative analysis of the fluctuations of the IEDs rate over fast- ultradian time scales, as well as the impact of such temporal fluctuations on the capacity of IEDs to map the EZ. The fast-ultradian dynamics analyzed in our work refers to temporal fluctuations of the rate of interictal events, not necessarily periodic, expanding over time scales ranging from sub- minute up to half an hour (i.e. sub-hour temporal dynamics).19 In particular, we focus on the study of previously unexplored IEDs rate dynamics of spontaneous nature and occurring in SEEG recordings as commonly examined in clinics to inform surgical planning in epilepsy (in the range of 5 minutes to 2 hours of SEEG recordings). We developed a general data mining method aimed at clustering the plethora of transient waveform shapes emerging in interictal SEEG recordings (referred in this study as polymorphic interictal events), and assessed the fluctuation in predictive power of each type of event. Importantly, the proposed method for polymorphic events analysis paves the way to unveil a . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint novel and counter-intuitive link between the dynamics of the overall rate of polymorphic events and the rate of specific subtypes of epileptiform spikes which is exploited here to improve the EZ localization. Methods Patients and intracerebral recordings Patients with drug resistant focal epilepsy were selected from the database of the Epileptology Unit of La Timone Hospital based on the outcome of the resective surgery. The final study includes 35 patients (20 women and 15 men) with Engel I seizure outcome classification at least 12 months after the surgical procedure, identified in the period 2008 to 2019. A variety of pathologies and electrode coverages were represented (see Table 1). The median age at the time of the SEEG surgery was 27 years (range = 19.5 - 41 years). Written informed consent regarding the SEEG procedure was obtained from all individual participants included in the study. In all patients, indication for SEEG exploration was based on Phase I presurgical non-invasive assessments for pharmacoresistant focal epilepsy  including  examination  of  detailed  clinical  history,  neurological  evaluation, neuropsychological testing, long-term scalp video-EEG monitoring and high-resolution structural magnetic resonance imaging (MRI). SEEG exploration was performed using intracerebral electrodes (Dixi Medical or Alcis Neuro (France); 10 - 15 contacts, length: 2 mm, diameter: 0.8 mm, 1.5 mm apart), placed intracranially according to Talairach stereotactic method20. The anatomical targets for electrode placement were defined based on the hypotheses about EZ localization resulting from Phase I. A postoperative computed tomography (CT) scan and/or MRI was performed to verify the spatial accuracy of the implantation. CT/MRI data coregistration was  performed  to check the anatomical location of each contact along the electrode trajectory. Local field potentials (LFP) from the SEEG electrodes were recorded on a 128-channel system (Natus/Deltamed) sampled at 512 Hz or 1024 Hz (16 bits resolution), with a built-in hardware high-pass filter (cutoff frequency = 0.16 Hz) . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint and an antialiasing low-pass filter (cutoff frequency = 170 Hz for 512 Hz sampling rate, or 340 Hz for 1024 Hz sampling rate). Video-SEEG recordings were performed as long as necessary (1 - 3 weeks) to record  several  of the patient's habitual seizures. Long term video-SEEG recordings following withdrawal of antiepileptic drugs were judged to be necessary to delineate the localization of the epileptogenic zone (EZ) for surgical treatment. The SEEG recordings analyzed in this work were chosen at least 2 days after the electrode implantation surgical procedure, and when possible before medication tapering, to limit possible effects of general anesthesia and antiepileptic drug withdrawal. In addition, we selected interictal recordings that were temporally distant from the preceding and the following seizure by at least 2 hs. The SEEG traces were recorded during time intervals in which the patients were either awake at rest or during non-REM sleep. For the awake state at rest, the patients were instructed to remain awake and during the SEEG recordings they were laying in bed doing nothing (not engaging in any particular cognitive task). Some patients, in particular the ones corresponding to long SEEG recordings (> 30 min), could have undergo drowsy (not sleep) state during part of the recording time interval. In all the cases, the patient state (awake at rest or non-REM sleep) was confirmed by the dedicated staff who reviewed the SEEG traces and the video monitoring the patient during the intracerebral recordings. Unless otherwise indicated, the results presented in this work correspond to the 35 patients listed in Table 1 and S1 for the awake state at rest. The results corresponding  to the non-REM sleep are presented and discussed  in Appendix S1, Section 11. In order to compare the results between two SEEG recording sessions taken at different days and time of day for the same patient state (awake at rest), a second group of 12 patients are presented and discussed in Appendix S1, Section 12. Not all the patients included in this second group have Engel I seizure outcome (see Table S2). The epileptologists team performed a visual evaluation of the SEEG traces before the analysis to identify interictal periods with a limited amount of artifacts (see Table S1). The median time-length of the interictal SEEG traces among the 35 patients in awake state was 28.2 min, range = 23.5 - 31.6 min (see Table S1). The classification of the SEEG contacts in EZ, propagation zone (PZ) and non-involved zone (NIZ) was made according to the sites of seizure initiation (SOZ) based on visual analysis and on the Epileptogenicity Index . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint (EI).4,5,21 Further details about the classification of the SEEG contacts can be found in the Appendix S1, Section 1. Across the 35 patients included in the Table 1, a total of 428 brain regions were analyzed (median per patient = 13, range = 11 - 14). There were a total of 5154 SEEG contacts, 962 in epileptogenic zone (EZ), 904 in propagation zone (PZ), 3288 in non-involved zone (NIZ) and 1486 in the resected zone (RZ).  The SEEG macroelectrode contacts were converted to a bipolar referencing montage for subsequent analysis. Bipolar channels were obtained as the difference between signals recorded from spatially adjacent contacts pertaining to the same depth electrode array. For other clinical characteristics of the patients see Table 1. Data and statistical analysis To systematically study the variety of transient waveform shapes emerging in the interictal SEEG dynamics, we developed the Nested Outlier Detection (NODE) algorithm as a general data mining method. The NODE algorithm uses the Local False Discovery Rate (LFDR) method22,23 to define the interictal events by detecting anomalies (i.e. outliers) of amplitude across the frequency bands of interest. Figure 1A schematizes the main processing steps associated with the NODE algorithm. Briefly, the NODE algorithm assigns to each event a 4-digit label. Each label digit represents the proportion of the detected anomalies that can be expected to be true outliers in the frequency band associated with that digit. The four frequency bands range from High-Delta band corresponding to the first label digit from the left, to Ripple band corresponding to the last label digit from the left. For instance, the label 0_09_09_0 groups all the interictal events with a proportion of 90% of them that can be expected to have true outliers in the two medium frequency bands [8 Hz - 32 Hz] and [30 Hz - 155 Hz]. The label 0_09_09_05 groups sharper transient waveform shapes since these paroxysmal events have, in addition to the outliers in the two medium frequency bands, a proportion of 50% of the detected anomalies associated with the Ripple band [150 Hz - 255 Hz] of these events that can be expected  to  be  true  outliers  (see  Figure  1C).  The  label  09_09_09_05  groups  the  spike-wave complexes with a proportion of 90% of them that can also be expected to have true outliers in the . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint low frequency band [1 Hz - 10 Hz] corresponding to the first label digit from the left (see Figure 1B). Further details about the NODE algorithm and the semi-supervised constrained clustering method,24 can be found in the Appendix S1, Section 2. Time-frequency maps of the polymorphic events were computed as scalograms using Morlet wavelets (see Appendix S1, Section 3).25 To quantify the capability of the subtypes of events identified by the NODE algorithm in segregating the SEEG channels involved in the epileptogenic zone (EZ) from those not involved (NIZ), we implemented a precision and recall analysis which is a suitable tool for imbalanced classification problems (see Appendix S1, Section 4).  To compare distributions of paired samples we used a two-tailed non- parametric Wilcoxon signed rank test with α = 0.05. A non-parametric permutation test based on random sampling without replacement was used for non-paired group analysis.  Unless otherwise indicated, all the reported P values were Bonferroni-adjusted to correct for multiple comparisons. In all the violin plots, center gray boxes represent the 25th and 75th percentiles, whiskers (gray lines) extend to the most extreme data points not considered as outliers (1.5 times the interquartile range (IQR)), star markers represent outliers. The center white circle and white line indicate the median and mean, respectively. For further details about the methods used to characterize the fast-ultradian dynamics associated with the interictal events rate, the reader is referred to the Appendix S1, Sections 5 and 6. Results Detection and clustering of interictal events We used the NODE algorithm to detect and cluster interictal events by identifying amplitude outliers across two LFDR thresholds and four frequency bands, with a time discretization of 200 ms defining the time length of each event (see Figure 1A). The variety of the detected interictal events expanded over the maximum number of clusters corresponding to this particular set of NODE parameters (80 clusters, each one characterized by 4-digit label). In the detection and clustering stages, we followed . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint a epileptogenicity-agnostic approach, that is, no  a priori information about the epileptogenic or physiological nature of the events was introduced during the detection and labeling processes. Then, to assess the epileptogenicity of the detected events we implemented two quantitative strategies, 1) ordering the NODE clusters according to their power to segregate the EZ and NIZ channels across all the patients (see Figure S1), and 2) computing the fraction of epileptiform discharges as visually identified by an epileptologist (FBo) captured by each NODE cluster (see Figure S3). Importantly, consistency was found between these two approaches (see Figures S1 and S3). Figures 1B, 1C and 1D show examples of two subtypes of IEDs given by the clusters 0_09_09_05 and 09_09_09_05 which were associated by the epileptologists with interictal epileptiform spikes and spike-wave complexes respectively, and one type of non-epileptiform events associated with amplitude outliers occurring in the low frequency bands (1 Hz - 30 Hz, Cluster: 09_0_0_0). Interestingly, we found that the events corresponding to low frequency oscillations (e.g. Clusters: 09_0_0_0) were significantly less abundant in the EZ channels with respect to the NIZ channels across all the analyzed patients (see Figures S1C and S1D). Henceforth, we will refer to the events grouped in these clusters as non- epileptic  events.  Figure  S3  shows  that  only  a  small  fraction  of  the  visually  marked  spikes corresponds to NODE clusters which were found to be significantly more abundant in NIZ than in EZ across the 35 patients included in the Table 1 (e.g. cluster 0_09_0_0 in Figures S1 and S3). This observation is consistent with the notion that primary spikes associated with EZ and/or abnormal activity have sharper waveform shapes, meaning their spectral decomposition shows more power in the  high  frequency  bands  (e.g.  clusters  09_09_09_05,  0_09_09_05),  when  compared  to  other discharges having smoother waveform morphologies (e.g. cluster 0_09_0_0) which can be in part explained by a propagation mechanism.5,12,28-30 Figure 1E, 1F and 1G show the mean rate of events (average over 60 min recording) in each bipolar channel of the patient 7, together with the results of the precision and recall analysis quantifying the capacity of EZ localization associated with each type of event. Importantly, it was found that the subtypes of sharp paroxysmal events identified by the NODE algorithm present important differences in terms of their capacity for the localization of the epileptogenic zone (EZ). In particular, for the patient 7 the clusters 09_09_09_05, 0_09_09_05 and . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint 09_09_09_0 produce F1 max = 0.93, F1 max = 0.73 and F1 max = 0.64 for EZ localization, respectively (see Figures 1E, 1F and Figure S1A). The Figure S1C shows the quantification of the capacity for EZ localization of the 80 NODE clusters across all the analyzed patients. Spontaneous fast-ultradian dynamics of the rate of interictal events We then investigated  the temporal dynamics  of the interictal  events, not  necessarily  periodic, expanding over time scales ranging from sub-minute up to half an hour (i.e. sub-hour temporal dynamics). For this, we analyzed  SEEG recordings as commonly examined in clinics for the treatment of drug-resistant epilepsy (in the range of 5 minutes to 2 hours of SEEG recordings).5,8,9,26,27 Figures 2A and 2B show for two patients (7, 13) the cumulative count of all the detected events (CE) across the SEEG channels (each rectilinear segment corresponds to a single bipolar channel). In these plots, the slope of each rectilinear segment correspond to the mean rate of events in that particular bipolar channel (total number of events / whole time period shown in the figures). In order to analyze the fluctuations of the events rate around its mean value, we subtracted a fitted straight line from the CE to obtain the residuals of the CE for each bipolar channel. The resulting detrended count of events (DCE) are presented in Figures 2C and 2D showing a clear oscillatory dynamics of the DCE in particular for the bipolar channels pertaining to the EZ. In the Figures 2C and 2D, the time intervals of the DCE with positive or negative slope correspond to an increase or decrease of the instantaneous rate of events, respectively. Significantly, we found that the interictal dynamics of the rate of polymorphic events observed over fast-ultradian time scales was highly independent of the parameters used in the NODE algorithm (see Figures S4, S5 and S6) and also independent of the specificity of the events detector (compare the fluctuations disclosed by the clusters 0_09_09_05 and 09_09_09_05 with respect to the Spike-like group in the Figures 3A to 3D). These results suggest that the observed dynamics is an intrinsic feature of the interictal neural activity captured by the SEEG recordings, putatively elicited by physiological brain states and/or epileptogenic mechanisms. Then, we analyzed whether the temporal fluctuations of the DCE during the analyzed fast-ultradian . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint time scales entrain the rate of specific epileptogenic biomarkers or not, which could ultimately affect the localization of the EZ. To investigate this, we focused on the temporal evolution of the rate of epileptiform discharges that produced a good EZ localization when averaged over long time intervals (approx. 60 min recording, see Figure 1E and 1F). Figures 2E and 2F show the DCE including all the events as gray dots and highlighting in color the epileptiform discharges pertaining to the cluster 09_09_09_05. Figures 2E and 2F show a clear oscillatory dynamics and rhythmic burst of events pertaining to the cluster 09_09_09_05, respectively. Importantly, Figures 2E and 2F show that 1) the occurrence of the epileptiform discharges (CoI: 09_09_09_05) is specific to the EZ (resulting in more dark red dots than blue dots) and 2) the rate of the epileptiform discharges in the EZ (density of the dark red dots) increases and decreases following the temporal evolution of the DCE with positive slope and negative slope, respectively. These two features can also be identified in the Figures 2G and 2H showing the mean value and standard error of the rate of epileptiform discharges (ER) computed in a sliding epoch of 5 min in length and 90% overlap. The clinically relevant consequence of these observations is exemplified for two patients in the Figures 2I and 2J showing that the performance of EZ localization as quantified by the area under the precision and recall curve (AUPREC  for EZ) follows  the dynamics  of the mean  rate of epileptiform discharges.  Values fluctuate between close to perfect classification (which would correspond to an AUPREC = 1) and very small values close to chance level (gray filled circles in the Figures 2I and 2J). Importantly, we found that the observed temporal fluctuations of the IEDs rate entraining the precision to localize the EZ over fast-ultradian time scales occurs spontaneously during both the awake and the non-REM sleep  states  of  the  patients  (see  Appendix  S1,  Section  11).  Moreover,  the  magnitude  of  the fluctuations of the AUPREC for EZ based on the rate of different subtypes of interictal events disclosed no significant differences at the group level between both awake vs non-REM sleep states, and also between two SEEG recording sessions taken at different days and time of day for the same patient state (awake at rest). For a detailed description of these results the reader is referred to the Appendix S1, Sections 11 and 12. We also investigated the dependence of the observed fluctuations of the AUPREC for EZ on the type of events and on the specificity of the detector of interictal . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint discharges (see Appendix S1, Sections 6 and 7). Figures 2I, 2J, 3A and 3B show that the events pertaining to the clusters 09_09_09_05, 0_09_09_05 and the Spike-like group produce similar temporal fluctuations of the AUPREC for EZ. These results reveal that the temporal dynamics of the events rate during interictal periods is present across different subtypes of epileptiform discharges and effectively entrains the precision to localize the EZ. Figures 3A and 3B show that, as expected, non-epileptic events (blue dots) produce very low values of AUPREC for EZ when compared to the epileptiform discharges (red dots), however, fluctuations of the AUPREC for EZ values are observed in both types of events. Figures 3E and 3F show the absolute difference (AD) between the extreme values (max - min) of the AUPREC for EZ time series based on the rate of IEDs, as a function of the sliding epoch length. Importantly, we found that the magnitude of the fast-ultradian fluctuations of the AUPREC for EZ based on the rate of IEDs decay exponentially as a function of the epoch length with a characteristic time scale Tao (i.e. scale-rich process. See Figures 3E and 3F). This scale-rich behavior is essentially different from the temporal dynamics of the AD of AUPREC for EZ based on the non-epileptic interictal events showing a scale-free trend (see Figures 3G and 3H and Appendix S1, Section 8). Besides, the non-epileptic interictal events were found to be significantly more abundant in the NIZ (see Figures S1C and S1D). Of note, the fluctuations of the rate of IEDs in EZ present a scale-free like dependence as a function of the epoch length (linear trend in a log-log plots shown in Figures S2I and S2J), which is different from the exponential trend with a characteristic time scale Tao disclosed by the fluctuations of the corresponding AUPREC for EZ (see the linear trend in the log-linear plots shown in Figures S2A, 3E and S2B, 3F). The latter, suggests that the observed fast-ultradian fluctuations of the AUPREC for EZ can not be completely explained by considering it simply as a function of the IEDs rate fluctuations within the EZ (see the caption of the Figure S2). Taken together, these results suggest that overlooking the spontaneous fast-ultradian dynamics of IEDs can produce incomplete and/or misleading information, leading to a suboptimal delineation of epileptogenic targets, and this holds true regardless of the specificity of the IEDs detector included in the processing pipeline (compare the fluctuations disclosed by the clusters 0_09_09_05 and 09_09_09_05 with respect to the Spike-like group in the Figures 3A to 3D). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint Putative mechanisms linked to the fast-ultradian dynamics of the rate of interictal events The temporal fluctuation of the rate of IEDs in a given brain region can originate from variations in excitability within this region. However, it could also be explained - in a complementary way - by fluctuations in propagation from a primary irritative zone to a secondary zone. 28-30 In order to test the propagation hypothesis, we correlated the event rates between the EZ and the PZ (see Appendix S1, Section 5). It is important to note that the propagation of IEDs across macroscopic networks occurs within  sub-second  time  scales  (approx.  100  ms  from  temporal  to  frontal  regions).12,13 As  a consequence, this fast propagation should not influence the measure of zero-lag correlation between event rates averaged over sliding epochs of several minutes in length (see Appendix S1, Section 5). Following this reasoning, we defined two criteria associated with the propagation mechanism in order to quantitatively test this hypothesis over the fast-ultradian time scales: 1) the dynamics of the mean rate of IEDs in EZ should positively correlate with the dynamics of the mean rate of the same subtype of IEDs in PZ and 2) the AUPREC for EZ time series should negatively correlate with the dynamics of mean rate of IEDs in PZ (i.e., the occurrence of IEDs in PZ effectively impairs the EZ localization). Figure 4 shows the quantification of the two criteria associated with the propagation mechanism for three type of events and across three sliding epoch lengths. The percentage of patients satisfying the two criteria associated with the propagation mechanism is up to approx. 23% (8/35), 14% (5/35) and 11% (4/35) for epoch lengths of 1 min, 5 min and 10 min, respectively. These results strongly suggest that the propagation mechanism could have a dominant role in explaining the interictal fluctuations of the AUPREC for EZ time series only in a limited fraction of the analyzed patients. This suggests that the excitability of the epileptogenic tissues, along with other factors such as the brain state and the probed brain region, could play a more relevant role in explaining the spontaneous dynamics of the IEDs rate constraining the precision to localize the EZ over fast- ultradian time scales. Further details related to the propagation mechanism are discussed in the Appendix S1, Section 9. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint Predicting the spontaneous fast-ultradian dynamics to improve the epileptogenic zone localization In this section we propose a strategy to prospectively estimate, i.e. without knowing the ground truth classification of EZ, the interictal epoch for near-optimal EZ localization based on IEDs rate. For this, we quantitatively define the optimal EZ localization for each patient as that corresponding to the maximum value of the AUPREC for EZ time series which is characterized by a temporal fluctuations over fast-ultradian time scales (see Figures 2I, 2J, 3A and 3B). Accordingly, the proposed method aims at estimating the time position of the sliding epoch corresponding to the maximum value of the AUPREC for EZ time series computed based on the rate of a given subtype of IEDs. The violin plot corresponding to the case ""Best 5 min epoch (Max AUPREC)"" shown in Figure 5D, summarizes the optimal EZ localization in each patient based on an epoch length of 5 min and IEDs pertaining to the cluster 09_09_09_05. We started by considering the fact that the interictal temporal dynamics of the AUPREC for EZ highly correlates with that associated with the mean rate of IEDs in EZ. Figures 2E to 2J illustrate this phenomenon for two particular patients (see also Figures S13 to S16 and S19 to S22). Moreover, Figures 5A and 5B show that the correlation between the two time series: AUPREC for EZ and ""ER of IEDs (CoI: 09_09_09_05) in EZ"", is positive and statistically significant in 77% (27/35) of the patients. Then, we investigated possible measures correlating with the AUPREC for EZ time series and suitable to be applied in a prospective manner. This requires the measures to be independent of the EZ, PZ, NIZ ground truth classification of the SEEG channels. We found that the time series corresponding to the mean rate of events including all the clusters and averaged across all the channels (ER of all Clust in all Chan) negatively correlates with the AUPREC for EZ time series computed using these IEDs subtypes (e.g. clusters 0_09_09_05 and 09_09_09_05 associated by the epileptologists with interictal epileptiform spikes and spike-wave complexes respectively). Note that the overall rate of interictal events ""ER of all Clust in all Chan"" across all the SEEG channels is a suitable  measures  for  prospective  analysis  since  they  do  not  depend  on  the  SEEG  channels classification in EZ, PZ, NIZ. Figures 5B and 5C show that the correlation between the time series . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint AUPREC for EZ (CoI: 09_09_09_05) and ""ER of all Clust in all Chan"", is negative and statistically significant  in  63%  (22/35)  of  the  patients.  A discussion  regarding  the  putative  mechanisms underlying this correlations is given in the Appendix S1, Section 10. Based on these findings we estimated the epoch for near-optimal EZ localization as the interictal epoch corresponding to the minimum value of the  ""ER of all Clust in all Chan"". For the IEDs corresponding to the cluster 09_09_09_05, Figures 5D, 5G and 5H show that the 5 min-epoch estimated  in each patient using the proposed method produces  at  the population level  an EZ localization slightly better than that obtained using the whole time series available in each patient (Bonferroni-adjusted P = 0.084, non-parameteric Wilcoxon signed rank test). Besides, these two cases (estimated best 5 min-epoch and whole time series) perform significantly worst and better than the cases for the actual best 5 min-epoch (i.e. maximum value of the AUPREC for EZ time series in each patient) and the worst 5 min-epoch (i.e, minimum value of the AUPREC for EZ time series in each patient), respectively (see Figures 5D and 5F to 5I). Figures 5E shows that the 5 min-epoch estimated in each patient using the proposed method produces an EZ localization at the population level significantly better with respect to that obtained with a 5 min epoch randomly sampled from the interictal recordings of each patient (P = 0.016, 105 random samplings). Notably, we found that for the EZ and RZ localization using the cluster 09_09_09_05, the proposed method performs at the population level significantly better with respect to the approach based on a randomly sampled epoch, for epoch lengths in the range 5 - 10 min (in all the cases we obtained P < 0.05, 105 random samplings,  see Figures  5  and  S12).  The  predictive  performance  for near-optimal  EZ and  RZ localization of all the NODE clusters is discussed in the Appendix S1, Section 10 in connection with the Figures S8 to S11. Discussion Spontaneous fast-ultradian dynamics of the rate of interictal events . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint In this study, we show that the spontaneous dynamics of different subtypes of IEDs observed over fast-ultradian time scales emerge as an intrinsic feature of these EZ biomarkers, defined by the occurrence of amplitude outliers across frequency bands of interest (see Figure 1). This approach allowed us to investigate the temporal dynamics of a massive number of interictal events (see Figures 2A to 2D), including several subtypes of epileptiform discharges predominantly occurring in EZ as well as non-epileptic events associated with amplitude outliers in low frequency bands (1 Hz - 30Hz). These latter were found to be significantly less abundant in EZ with respect to NIZ (see Figure 1, S1 and S3). The analysis of the rate of these interictal events revealed underlying temporal dynamics characterized by sub-hour time scales which includes, but is not exclusive of, the IEDs observed in the EZ (compare Figures 2C vs 2D and Figures 2E vs 2F). Crucially, we found that the observed dynamics of the rate of events is an intrinsic feature of the interictal brain activity captured by the SEEG traces. That is, the interictal dynamics of the rate of polymorphic events is not, or at least can not be explained solely by, an epiphenomenon associated with a particular parameters configuration of the NODE algorithm (see Figures S4, S5 and S6). We found that the NODE algorithm segregates the interictal epileptiform spikes visually marked by an epileptologist (FBo) into different sub-clusters (see Figure S3). Whereas most of visually marked spikes correspond to NODE clusters found to be significantly more abundant in EZ than in NIZ (e.g. 09_09_09_05, 0_09_09_05),  leading  to  a  good  EZ  localization  (see  Figure  S1),  they  showed  different characteristics regarding their temporal dynamics. Firstly, we found that the rate of different types of interictal epileptogenic discharges undergo temporal fluctuations over fast-ultradian time scales as commonly examined in clinics. Previous studies have reported that the rate of interictal epileptic spikes  increase during  wakefulness  and sleep,10,14,15,17,18 and  is suppressed  during  attention  and memory  tasks.31-34 In contrast, the fast-ultradian temporal dynamics reported  in this work  was observed and characterized preoperatively in the interictal SEEG traces of 35 patients with Engel I seizure  outcome and  appears  to  occur  spontaneously, that  is,  not  triggered  by  or  exclusively associated with a particular cognitive task, wakefulness, sleep, seizure occurrence, post-ictal state or antiepileptic drug withdrawal (see subsection ""Patients and intracerebral recordings"" in Methods and . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint Sections 11 and 12 of the Appendix S1). Secondly, we found that the minimization of the rate of interictal events (epileptic and non-epileptic) across all the SEEG channels is a good predictor of the interictal epoch (in the range 5 - 10 min in length) for near-optimal EZ localization based on specific subtypes of IEDs.  Notably, we found that for the RZ localization based on the IEDs subtype 09_09_09_05, the proposed method performs at the population level significantly better than both 1) using the whole time series available in each patient and 2) using a short epoch (length ≈ 5 - 10 min) randomly sampled from the interictal recordings of each patient. Of note, this novel and counter- intuitive link between the dynamics of the overall rate of polymorphic events and the rate of specific subtypes epileptiform spikes which was exploited here to improve the EZ localization would be very difficult, if not impossible, to unveil by solely considering a limited subtype of events (e.g. visually marked  epileptic  spikes).  Regarding  the  mechanisms  underlying  the  observed  fast-ultradian dynamics, our results suggest that the propagation mechanism could have a dominant effect only in a limited fraction of the analyzed patients (see Figure 4), suggesting that the excitability of the epileptogenic tissue along with other factors such as the brain state and the probed brain region could play a more relevant  role in explaining the spontaneous temporal  dynamics  of the IEDs rate entraining the precision to localize the EZ over sub-hour time scales. Taken together, our results show that the rate of different subtypes of IEDs spontaneously undergoes temporal fluctuations which effectively constraint the precision to localize the EZ over fast-ultradian time scales as commonly analyzed in clinics,5,8 hence, representing a clinically relevant factor for surgical planning in drug-resistant epilepsy. Limitations Whereas all the patients included in the study where seizure-free after surgery, indicating that the analyzed SEEG traces effectively capture the brain activity associated with EZ, further investigation is warranted to disentangle the observed dynamics of interictal events from the heterogeneity and variable  number  of  explored  brain  regions.  Besides,  given  the  heterogeneous  exposure  to . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint antiepileptic drugs of the analyzed patients, further investigation is required to assess the effect of this factor on the fast-ultradian dynamics of the rate of interictal events. Importantly, the possibility that the fast-ultradian fluctuations of the rate of interictal events analyzed in this work could vary with or be hierarchically coupled to another rhythms with longer time periods (e.g. circadian rhythms), deserve further investigation. However, this does not change the conclusions of our study regarding the existence of the fast-ultradian fluctuations of the rate of interictal events entraining the presicion to localize the EZ, and its importance for surgical planning in drug-resistant epilepsy. Conclusion A major limitation of previous works assessing the performance of epileptogenic biomarkers like interictal spike and HFOs, refers to the underestimation (or total neglect) of the time-varying factors underlying the genesis of these biomarkers.5,8,9,26,27 This limitation can effectively produce incomplete and/or misleading information, leading to a suboptimal delineation of epileptogenic targets. In this study, we provided for the first time a method to investigate and predict the spontaneous fast- ultradian  dynamics  of  interictal  transient  event-like  biomarkers.  Based  on  our  results,  the recommendation for clinical applications is that an interictal time interval of at least 30 min in length is required to stabilize the findings, i.e. in order to produce a relative attenuation of 90% of the AUPREC for EZ fluctuations with respect to that observed in the case of a 5 min interictal time window (see  the calculations in  Appendix S1, Section 8). The proposed method for polymorphic events analysis paves the way to prospectively estimate the interictal time interval for near-optimal EZ localization in a substantial fraction of the analyzed patients based solely on the dynamics of the intracranial recordings (see Figure 5B). This fact reveals a novel and counter-intuitive link between the fast-ultradian dynamics of the overall rate of polymorphic events (epileptic and non-epileptic) and the rate of specific subtypes of epileptiform spikes as an intrinsic feature of the interictal brain activity (see Figures 5 and S7 to S12). This result is clinically relevant as we showed how it can be . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288085
doi: 
medRxiv preprint exploited to improve the EZ localization and also offer a novel approach to investigate the time- varying factors underlying the genesis of different subtypes of IEDs.",1
"Background: Stroke is a life-threatening medical condition that can result in lifelong brain 
impairment, 
complications, 
and demise. Stroke is the world's second biggest cause of mortality and could soon overtake as the biggest cause of death globally. It has 2 major pathological types’ i.e. ischemic stroke 
and 
hemorrhagic 
stroke. Hypertension, diabetes mellitus, cardiac diseases, smoking, physical inactivity and age are the risk factors that contribute in the occurrence of a stroke. Objective: To find out the epidemiological status of stroke types among patients admitted in the public tertiary care hospitals of Peshawar. Methodology:  A descriptive cross-sectional study was carried out to determine the prevalence of stroke types in tertiary care hospitals of Peshawar. The sample size calculated for the research study was 109. Convenience sampling technique was used in this study. Results: This study was performed among 109 research participants. The most affected were males as 51.13% and females were 45.87%.And the rate of ischemic stroke were 71% while that of hemorrhagic stroke were 28%. Conclusion: the current research study concluded that majority of the patients had ischemic stroke as compared to the hemorrhagic stroke. Key words: prevalence, Stroke, ischemic stroke, hemorrhagic stroke . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287970
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. INTRODUCTION Stroke 
is 
a 
life-threatening 
medical condition that can result in lifelong brain impairment, complications, and demise. Stroke is the world's second biggest cause of mortality and could soon overtake as the biggest cause of death globally. A stroke is the loss of brain functions that occurs rapidly as a result of a disruption in the blood vessels providing blood to the brain. This can be produce by ischemia from a thrombosis or embolism, or a hemorrhage. As a result, the brain's damaged area is unable to operate, causing one or more limbs on one side of the body to become immobile, inability to recognize or produce speech, or seeing only one side of the visual field(Marwat, Usman, & Hussain, 2009). According to Mumtaz Ali Marwat, et al. an observational study was conducted in Pakistan under the title Stroke and its relationship to risk factors. Total of 88 people were included in analysis. The mortality 
rate 
in 
stroke 
was 
27.2% (Marwat, Usman, & Hussain, 2009) According to Tapas Kumar Banerjee, et al. a population-based survey was conducted in different parts of India under the title Epidemiology of stroke in India that reviewed data from last decade of about 100,000 patients and the prevalence rate for stroke was 1.2%(Banerjee & Das, 2006). Ischemic stroke happens when a blockage in the blood vessel of the neck or brain occurs, and is the most common type of stroke. This blockage 
can 
be 
produced 
by, 
“the development of a clot in a brain or neck blood artery, known as thrombosis; the migration of a blood clot from one section of the body to another such as the heart to the brain, known as embolism; or a serious narrowing of a cerebral artery, known as “stenosis” (Kummer et al., 2019). In hemorrhagic stroke a blood vessel on surface of the brain ruptures which allow the blood to enter the gap between the skull and brain or when a faulty artery in the brain ruptures, causing blood to flood the surrounding tissue (Shah et al., 2013). METHODS The study design was a descriptive cross- sectional study (from February 2022 to July 2022). After approval of Advance Studies and Research Board (ASRB), graduate committee and Ethical board of Sarhad University of Science and Information Technology the data was collected. The sample size calculated for this study was 109 
through 
Raosoft. 
Non-probability . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287970
doi: 
medRxiv preprint Convenience sampling technique was used and data was analyzed by using software SPSS 
25. Inclusion Criteria: • 
Those patients who are admitted in 
hospitals. 
• 
Both male and female patients are 
included. 
• 
Individuals age 65 and above are 
included. 
• 
Patients 
suffering 
from 
both 
hemorrhagic and ischemic stroke are 
included. 
• 
Acute, sub-acute and chronic, all 
stages of stroke are included. Exclusion Criteria: • 
Those patients who do not want to give 
data by will are excluded. 
• 
Stroke due to any accident or whiplash 
injury is excluded DATA COLLECTION PROCEDURE AND PERMISSION TAKEN FROM SUIT REASEARCH AND ETHICAL COMITTE:  A relevant questionnaire was distributed among the attendants of patients and it was filled by attendants of the patients. Data was collected through a self- modified 
QUESTIONNAIRE 
FOR VERIFYING STROKE FREE STATUS (QVSFS). After obtaining approval from the research 
committee, 
permission 
from director SIAHS SUIT Peshawar and from the head of  the respective hospitals were taken, data were collected from the stroke, patients admitted in the public tertiary care hospitals  of Peshawar who were fulfilling the eligibility criteria of study. Data were collected 
through 
self-modified QUESTIONNAIRE 
FOR 
VERIFYING STROKE FREE STATUS (QVSFS). The questionnaires were filled from the patients themselves or from/by their attendants. RESULTS The main objective of this study was to find out the epidemiological status of ischemic & hemorrhagic stroke of geriatric population stoke, patients admitted   in the public tertiary care hospitals of Peshawar. For this purpose, we took the data from geriatric population of age between 60 to 99 (n=109) in which the greatest percentage of stroke were in age group of 60-70 (55.0 %), L as nts he - R S he m m re e, re ng re ed G he nts nd & on lic his ric 9) ke ), . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287970
doi: 
medRxiv preprint followed by the age group of 71-80 (31.19 %), the age group of 81-90 (7.34 %), and the age group of 91-99(6.42 %). Age of Participants Table 2 shows the gender of research participants. A total of 109 individuals were included in the study, with 59(51.13%) of the males and 50(45.87%) of females. Gender of research participants Gender 
Frequency 
Percent Male 
59 
54.1 Female 
50 
45.9 Total 
109 
100.0 Table 3 shows the weights of research participants in kg. In the sample size of 109 individuals, the greatest percentage 31(28.44%) is found in the weight range of 55-65kg. Followed by 54(49.54%) is observed in the weight range of 66-75kg. Another proportion is 21(19.27%) in the 76-85kg weight range. Lastly, only 3(2.75%)  in the 86-99kg weight range. Age 
Frequency 
Percent 60-70 
60 
55.0 71-80 
34 
31.2 81-90 
8 
7.3 91-99 
7 
6.4 Total 
109 
100.0 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287970
doi: 
medRxiv preprint Weight of research participants Weight 
Frequency 
Percent 55-65 kg 
31 
28.4 66-75 kg 
54 
49.5 76-85 kg 
21 
19.3 86-99 kg 
3 
2.8 Total 
109 
100.0 Tables 3 shows the status of TIA/mini stroke among research participants of the 109 patients, 49 (45%) had TIA/mini stroke, whereas 60 (55%) had no history of TIA/mini stroke. TIA/mini stroke in research participants Frequency 
Percent Yes 
49 
45.0 No 
60 
55.0 Total 
109 
100.0 Type of Stoke Frequenc y Percent 
Valid Percent Cumulative Percent ischemic stoke 
78 
71.6 
71.6 
71.6 hemorrhagic stroke 
31 
28.4 
28.4 
100.0 Total 
109 
100.0 
100.0 Table 4 shows the type of stroke among 109 research participants, with 78 (71.56 %) having ischemic stroke and 31(28.44 %) having hemorrhagic stroke. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287970
doi: 
medRxiv preprint DISCUSSION According to the study conducted in tertiary care hospital in Karachi by Fawad Taj, et.al a total of 159 patients were included in this audit with 104 males (65.4%) and 55 females (34.6%). Ischemic stroke was seen in 108 patients (67.9%), while 34 patients (21.4%) 
with 
hemorrhagic 
stroke. Hypertension was the most prevalent risk factor present in 78% (n=124) of the population, Followed by the diabetes in 40.3% (n=64) and smoking 21% (n=33). (36). Another study conducted by Nighat Musa, et.al in Peshawar in the year of august 2018 reported that Study results showed that 62% patients were males and 38% were females. Males age distribution less than 40 years were 24% and more than 41 years were 76%. Similarly, females were 21% and 79% respectively. Major medical risk factors found were hypertension, diabetes and cardiovascular diseases. The environmental risk 
factors 
were 
physical 
inactivity, smoking and obesity. The less common risk factors were alcohol and oral contraceptive use. (8) As our study shows the prevalence of stroke is more in males than females, a similar result were also found in the study conducted by Payam Sariaslani, et.al in Iran in the year of 2019 stating that the present study found the prevalence of stroke to be higher in men than in the women Similar results to our study regarding association of hypertension to ischemic stroke from the above study conducted by Payam Sariaslani, et.al in Iran in year 2019 they stated that The present study investigated 122 patients the most 
important 
clinical 
risk 
factors associated with ischemic stroke in the study subjects 
comprised 
a 
history 
of hypertension, heart diseases, angina and internal and rheumatic diseases, smoking, taking 
high-risk 
medications, 
diabetes mellitus and hyperlipidemia.(37) CONCLUSION This study was performed among 109 research participants to find out the frequency of stroke in geriatric population in tertiary care hospitals of Peshawar. In current study we have concluded a high prevalence of ischemic stroke among adults aged 60 and above in tertiary care hospitals of Peshawar. The most affected were males as 51.13% and females were 45.87%.",1
"Intranasal administration of oxytocin is increasingly explored as a new approach to facilitate social development and reduce disability associated with a diagnosis of autism spectrum disorder (ASD). In light of the growing number of trials, it is crucial to gain deeper insights into the neuroplastic changes that are induced from multiple-dose, chronic use of oxytocin, over a course of weeks. To date however, oxytocin’s chronic neuromodulatory impact in the pediatric brain remains unknown. Here, we present a double-blind, randomized, placebo-controlled pharmaco- neuroimaging trial examining the neural effects of a four-week intranasal oxytocin administration regime (12 IU, twice daily) in pre-pubertal school-aged children with ASD (8-12 years, 45 boys, 12 girls). Resting-state fMRI scanning and simultaneous, in-scanner heart rate measurements were assessed before, immediately after and four weeks after the nasal spray administration period. Four weeks of chronic oxytocin administration in children with ASD induced significant reductions in intrinsic functional connectivity between amygdala and orbitofrontal cortex, particularly at the four-week follow-up session, thereby replicating prior observations of neuromodulatory changes in the adult brain. Notably, the observed reductions in amygdala- orbitofrontal connectivity were associated with improved autonomic stress-regulation, indexed by increased high-frequency heart rate variability. Further, oxytocin’s neural and cardiac autonomic effects were significantly modulated by epigenetic modifications of the oxytocin receptor gene, indicating that oxytocin-induced stress-regulatory effects were more pronounced in children with reduced epigenetic methylation, and thus higher oxytocin receptor expression. Finally, whole-brain exploratory functional connectivity analyses also revealed an overall oxytocin-induced enhancing effect on amygdala coupling to regions of the salience network (insula, anterior cingulate cortex), likely reflective of oxytocin’s (social) salience effects. Together, these observations provide initial insights into the stress-regulatory neural and cardiac effects induced by chronic oxytocin administration in children with ASD, and point toward important epigenetic modulators that may explain inter-individual variations in oxytocin- induced responses. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint Introduction Intranasal administration of the neuropeptide oxytocin is increasingly considered as a novel approach for alleviating disabilities associated with an autism spectrum disorder (ASD) diagnosis, including difficulties in social communication and interaction, and restricted and repetitive behaviors and interests 1. In relation to ASD, initial single-dose oxytocin administration studies consistently demonstrated behavioral improvements on various social tasks, including tasks assessing affective speech comprehension (emotional intonations) 2, 3, emotion recognition 4, 5, repetitive behavior 6 and social decision making (cyberball computer game) 7 (for review, see e.g., 8). Recent multiple-dose, chronic administration studies have yielded a more mixed pattern of effects, with some studies showing beneficial clinical effects 9-15, while others identified no benefit of oxytocin over placebo nasal spray administration 16-19. In light of this mixed pattern of effects, it is crucial to gain a deeper understanding of the neural substrates that underlie behavioral effects, allowing to delineate possible mechanisms of inter-individual variation in clinical treatment responses. To date, insights into the neural effects of oxytocin have predominantly emerged from pharmaco-neuroimaging studies examining the acute effects of single-dose administrations in adults, predominantly highlighting a key role for the amygdala in exerting oxytocin’s neuromodulatory action 20-22. Specifically, prior single-dose administration studies in neurotypical and clinical populations (including ASD) generally demonstrated an attenuating effect of acute oxytocin administration on task-evoked amygdala reactivity, presumed to reflect oxytocin’s arousal-dampening and anxiolytic effects 20, 23. In particular, a seminal study by Kirsch et al. (2005) showed that a single-dose of oxytocin elicits an acute attenuating effect on amygdala reactivity and amygdala–brainstem connectivity 24, suggestive of oxytocin’s impact on regulating autonomic arousal, sympathetic-parasympathetic tone and fear behavior. However, a more complex pattern of results emerged from subsequent resting-state fMRI studies, with some showing an acute increase in amygdala functional connectivity to prefrontal regions, while others showed reductions in amygdala coupling or no effects after a single-dose administration (for a recent review see 21). Insights in how repeated, daily administrations of oxytocin affect neural circuits are still highly sparse, with only two pharmaco-neuroimaging studies addressing this topic, both in adults with ASD. In a first study, Watanabe et al. (2015) explored the neural effects of a six- week oxytocin administration regime in 17 adult men with ASD and reported an increase in connectivity between the anterior cingulate and prefrontal cortex 10. Note however, that the reported multiple-dose, chronic effect may - at least in part - reflect an acute effect of exogenously administered oxytocin, considering that participants were scanned approximately . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint 15 or 40 min after the last nasal spray administration (of the six-week regime), which corresponds to the optimal time frame for assessing acute, single-dose effects. In another study, 38 adult men with ASD received oxytocin or placebo nasal spray, daily for a period of four weeks and chronic effects of the repeated administration were examined at least 24 hours after the last nasal administration. In the latter study, chronic oxytocin administration elicited an attenuation of amygdala reactivity, both intrinsically during resting-state scanning 25 and task-evoked 26, further highlighting a predominant anxiolytic impact also after chronic oxytocin administration. Chronic oxytocin-induced changes in intrinsic functional connectivity of amygdala regions were also observed, predominantly indicating reduced coupling of the amygdala to prefrontal and orbitofrontal cortices, which was interpreted to reflect a reduced need to downregulate amygdala activity via frontal top-down control 27. Importantly, neural adaptations in amygdala-orbitofrontal connectivity were shown to outlast the period of actual administration until one month and even one year after the nasal spray administration period, and were associated with behavioral improvements in repetitive behavior and attachment style 26, 27. These observations thereby provided important evidence that in the adult brain, chronic oxytocin administration can induce long-lasting neuroplastic changes in amygdala circuitry that associate to retained clinical improvements. However, 
a 
more 
comprehensive 
and 
integrative 
understanding 
of 
the neuromodulatory impact of chronic oxytocin administration is needed, especially for pediatric populations. This is of particular importance, considering that ASD is a neurodevelopmental condition for which therapeutic approaches can be preferably administered in early life developmental windows 28. To fill this gap, here, we present a first pharmaco-neuroimaging study with school-aged boys and girls with ASD, examining the neural effects of a four-week course of chronic oxytocin administration on amygdala functional connectivity as assessed using resting-state fMRI scanning. To examine whether chronic oxytocin administration could also induce long-lasting neuroplastic changes in the pediatric brain, we also included a follow- up resting-state fMRI assessment four weeks after cessation of the daily administrations. Primary analyses examined whether oxytocin administration would induce similar attenuating effects on amygdala-orbitofrontal connectivity in school-aged boys and girls with ASD, as previously identified in adult men with ASD 27. Additionally, exploratory analyses were performed, to obtain a complete picture of how chronic oxytocin impacts whole-brain connectivity of amygdala regions. To obtain a more integrative assessment of oxytocin’s purported anxiolytic role in modulating cardiac autonomic arousal and sympathetic-parasympathetic homeostatic balance, we additionally performed in-scanner assessments of heart rate variability (HRV), an established marker of cardiac autonomic nervous system (ANS) activity 29. The high-frequency component of HRV uniquely represents the contribution of the parasympathetic system via the . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint nervus vagus and is generally regarded to constitute an important marker of psychophysiological homeostasis and well-being 29. While insights into the effect of oxytocin on HRV are still relatively sparse, existing single-dose administration studies demonstrated acute effects on facilitating an increase in resting-state high-frequency HRV 30-34. In this study, we examined, for the first time, whether chronic oxytocin administrations can elicit similar cardiac autonomic effects, and perhaps a (long-lasting) retention of heightened intrinsic cardiac parasympathetic tone. Finally, there is a growing awareness that oxytocin nasal spray administration effects may not be uniform and that distinct context- and/or person-dependent factors may hamper or facilitate treatment responses 35. Since oxytocin’s effects on brain function and behavior are known to be mediated by the prevalence of oxytocin receptors as determined by the oxytocin receptor gene (OXTR), it is increasingly put forward that (epigenetic) variations of OXTR may form an important factor for explaining variable treatment responses 36-38, but empirical evidence is largely lacking. To investigate the impact of OXTR epigenetics, we examined whether variability in neural effects is mediated by variability in baseline (pre-nasal spray) DNA methylation of the OXTR gene. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint Methods General study design This double-blind, randomized placebo-controlled trial with between-subject design investigated the neural effects of chronic oxytocin administration on intrinsic functional connectivity of the amygdala in school-aged children with ASD. Resting-state fMRI scanning and HRV assessments were performed at baseline (T0); immediately after four consecutive weeks of daily nasal spray administrations (24 hours after the last nasal spray administration) (T1); and at a follow-up session, four weeks after cessation of the nasal spray administration period (T2). Please see Figure 1 for the CONSORT Flow diagram visualizing the number of participants randomized and analyzed. Please also see Supplementary Methods outlining the impact of COVID-19 related health restrictions on the recruitment and flow of participants in the trial. Written informed consent from the parents and assent from the child were obtained prior to the study. Consent forms and study design were approved by the local Ethics Committee for Biomedical Research at the University of Leuven, KU Leuven (S61358) in accordance with The Code of Ethics of the World Medical Association (Declaration of Helsinki). The neural assessments were conducted at the Leuven University Hospital (registered at the EU Clinical Trials register: EudraCT 2018-000769-35 and the Belgian Federal Agency for Medicines and Health products). As indicated in the EudraCT registration, fMRI data collections were part of a broader assessment including behavioral-clinical characterizations 19, as well as other (neuro)physiological (electroencephalographic) and biological assessments (reports in preparation). The trial was monitored by the Clinical Trial Center at the University hospital of Leuven, and all trial staff had Good Clinical Practice certification and was trained in the study protocol. Participants Children with a formal diagnosis of ASD were recruited through the Autism Expertise Centre at the Leuven University Hospital between July 2019 and January 2021 (Figure 1, Table 1). The diagnosis was established by a multidisciplinary neuropediatric team based on the strict criteria of the DSM-5 (Diagnostic and Statistical Manual of Mental Disorders) 1. Prior to randomization, the Autism Diagnostic Observation Schedule (ADOS-2) 39 and estimates of intelligence (four subtests of the Wechsler Intelligence Scale for Children, Fifth Edition, Dutch version) 40 were acquired (Table 1). Principal inclusion criteria comprised a clinical diagnosis of ASD, age (8-12 years old), intelligence quotient (IQ) above 70, native Dutch speaker, a stable background treatment for at least four weeks prior to the screening and no anticipated changes during the trial. Only . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint premenstrual girls were included. Principal criteria for exclusion comprised any neurological (e.g., stroke, epilepsy, concussion), significant physical disorder (liver, renal, cardiac pathology), prior use of oxytocin, or any contraindication for magnetic resonance imaging (MRI). The presence of comorbid psychiatric disorders and current psychoactive medication use was screened for and logged (see Table 1 and 19). Since this is the first pharmaco-neuroimaging study examining the neural effects of chronic oxytocin administration in children with ASD, formal power calculation based on existing literature was difficult. In a prior study from our lab with a similar parallel clinical trial design in adults with ASD 27, four weeks of chronic oxytocin versus placebo nasal spray administration yielded a reduction in amygdala-prefrontal functional connectivity with an overall effect size of d = 1.057. Sample sizes for the functional connectivity analyses in the current study were therefore set at 50 participants (25 oxytocin, 25 placebo) allowing to detect a similar effect size with α = 0.05 and 90% power. Thus far, no prior studies investigated the effect of chronic oxytocin administration on resting-state HRV. Study medication and dosing Participants were randomized (permuted-block randomization) to receive oxytocin (Syntocinon®, Sigma-tau) or placebo nasal sprays, administered in identical blinded amber 10 ml glass bottles with metered pump (preparation, packaging, blinding by Heidelberg University Hospital, Germany). The placebo spray consisted of all the ingredients used in the active solution except the oxytocin compound. Participants were randomly assigned in a 1:1 ratio, with stratification according to age, IQ and biological sex. All research staff conducting the trial, participants and their parents were blinded to nasal spray allocation. Children (assisted by their parents) were asked to self-administer a daily dose of 2 x 12 international units (IU) nasal spray or placebo equivalent (3 puffs of 2 IU in each nostril), 12 IU in the morning and 12 IU in the afternoon, during 28 consecutive days. Participants received clear instructions about use of the nasal sprays 41 through a demonstration together with the experimenter. More information regarding nasal spray adherence and side effects screening is provided in Supplementary Methods. MRI data acquisition and functional connectivity analysis MRI data acquisition. Anatomical and resting-state fMRI images (7 min, eyes open) were acquired on a 3.0 Tesla Philips MR scanner (Best, The Netherlands) with a 32-channel phased-array head coil. Detailed information on the scanning parameters, MRI data pre- processing and in-scanner head motion analyses are provided in Supplementary Methods. Amygdala-orbitofrontal connectivity. Region-of-interest (ROI) connectivity analysis was performed assessing nasal spray-induced changes in connectivity between bilateral amygdala . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint and orbitofrontal cortex (OFC) (see Figure 2A). The ROIs were identical to those adopted in a prior chronic oxytocin fMRI pharmaco-neuroimaging study in adult men with ASD 27, as defined by the FSL Harvard-Oxford subcortical and cortical atlas. As implemented in the CONN functional connectivity toolbox version 17.f 42, mean time-series were extracted by averaging across all voxels in each ROI and bivariate correlation coefficients were computed between the time-course of the amygdala seeds (left and right) and the time-courses of the left and right orbitofrontal ROIs. Correlation values were Fisher z-transformed. In addition to the hypothesis- driven ROI analysis, exploratory whole-brain analyses were performed by assessing nasal spray-induced changes in functional connectivity between bilateral amygdala (seeds) and all other regions of the cortical (n = 91) and subcortical (n = 13) Harvard-Oxford atlas. Heart rate variability recordings and data handling Photoplethysmography (PPG) recordings were performed simultaneous to the resting- state fMRI recording by means of a wireless Peripheral Pulse Unit MRI Sensor (Philips) (sampling rate: 500 Hz). The PPG sensor was placed over the index finger of the non-dominant hand to monitor blood volume changes in the microvascular bed of the underlying tissue. The time intervals between blood volume pulse waves were assessed using Kubios HRV Premium software (version 3.5.0) 43 to derive continuous inter-beat-intervals (IBI) for assessing heart rate variability (HRV). All IBI time series were manually inspected prior to analysis and automatic artifact removal, as implemented in Kubios, was performed. At T1, PPG recordings were available for 43 participants (20 oxytocin/23 placebo) and for 40 participants (18 oxytocin/22 placebo) at T2. Upon artefact removal, 4 additional participants (3 oxytocin/1 placebo) were removed from the final analyses (availability of less than 5 min of noise-free data and/or >5% ectopic beats 44). For each of the remaining subjects, HRV frequency domain analyses were performed using Fast Fourier Transformation based on Welch’s periodogram. Percentage power values were computed for the high-frequency component of the HRV signal (HF-HRV%), which in children is defined between 0.24 – 1.04 Hz 45. Assessment of baseline DNA methylation of the oxytocin receptor gene Salivary samples were collected from each child at the end of their baseline (T0) study visit using the Oragene DNA sample collection kit (DNA Genotek Inc., Canada), to explore epigenetic variations of the oxytocin receptor gene (OXTR). An important epigenetic mechanism is the methylation of a cytosine from a CpG dinucleotide (i.e. a cytosine followed by a guanine, connected by a phosphate) within the DNA, which is generally associated with a silencing of the gene and thus a decrease in gene transcription 46. Here, DNA methylation was assessed at three CpG sites of OXTR that have been shown to be impacted in autism . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint (i.e., -934, -924 and -914; see 46). More detailed information regarding the DNA collection procedures and analyses are provided in Supplementary methods. Statistical analysis To assess oxytocin-induced effects, amygdala-orbitofrontal functional connectivity z- transformed r-values were subjected to mixed-effects analyses of variances. The factor ‘subject’ was inserted as a random effect and the factors ‘nasal spray’ (oxytocin, placebo), ‘session’ (T1, T2), ‘amygdala seed’ (left, right) and ‘OFC ROI’ (left, right), as well as all interactions with the factor ‘nasal spray’, were inserted as fixed effects. To correct for variance in the individuals’ baseline T0 scores, baseline values prior to nasal administration were included as a covariate in the model. Oxytocin-induced effects in high-frequency HRV were analyzed using similar mixed models, i.e., with the factors ‘nasal spray’ (oxytocin, placebo) and ‘session’ (T1, T2), and baseline (T0) scores modeled as covariate. Spearman correlation analyses were performed to examine possible associations between neural and cardiac autonomic changes observed in the oxytocin group. Spearman correlations were also performed between (baseline) OXTR DNA methylation (average across CpG sites) and oxytocin-induced changes in amygdala-orbitofrontal connectivity and high- frequency HRV to examine possible moderations of neural and cardiac oxytocin-induced effects by variations in epigenetic modifications of the oxytocin receptor gene. Additionally, exploratory, oxytocin-induced changes in whole-brain amygdala connectivity analyses were performed separately for each assessment session (T1, T2) by subjecting change from baseline connectivity scores to independent t-tests with the between- group factor ‘nasal spray’ (oxytocin, placebo). Considering the exploratory nature of the whole- brain analysis, amygdala connections displaying significant oxytocin-induced effects are reported at an uncorrected p < .05 threshold (connection-level, CONN-toolbox). Data availability The data that support the findings of this study are available on request from the corresponding author, KA. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint Results Oxytocin-induced changes in amygdala-orbitofrontal connectivity Mixed-effects analyses revealed a main effect of ‘nasal spray’, indicating an attenuation in amygdala-orbitofrontal connectivity in the oxytocin group, compared to the placebo group (F(1, 52.97) = 4.26; p = .044; ŋ2 = .07) (Figure 2B). Also a ‘treatment x session’ interaction effect was evident (F(1, 305) = 7.26; p = .007; ŋ2 = .02), indicating that the oxytocin-induced attenuation in amygdala-orbitofrontal connectivity was evident at the four-week follow-up session (T2, Bonferroni post-hoc, p < .001), not at the T1 post-session (p > .100) (Figure 2B). None of the other main or interaction effects reached significance (all, p > .05). Oxytocin-induced changes in high-frequency HRV Mixed-effect analysis of high-frequency HRV yielded a main effect of ‘treatment’ (F(1, 76) = 4.67; p = .034; ŋ2 = .057), indicating overall higher parasympathetic high-frequency HRV in the oxytocin, compared to the placebo group (Figure 2C). The main effect of ‘session’ or the ‘treatment x session’ interaction were not significant (both, p > .05). Association between neural and cardiac autonomic oxytocin-induced changes Spearman correlation analyses revealed a significant association between neural changes in amygdala-orbitofrontal connectivity and cardiac autonomic changes in high- frequency HRV, indicating that children who display a stronger oxytocin-induced attenuation in amygdala-orbitofrontal connectivity at the T1 assessment session, displayed a stronger oxytocin-induced increase in high-frequency HRV (Spearman ρ = -.61; p = .009) (Figure 2D). No significant associations were evident at the T2 follow-up session (p > .05). Modulation of oxytocin-induced responses by OXTR epigenetic variations Baseline variations in OXTR DNA methylation averaged across CpG sites -934, -924 and -914 were predictive of oxytocin-induced changes in amygdala-orbitofrontal connectivity, indicating that children with low OXTR DNA methylation (associated with higher OXTR expression) displayed a stronger effect of chronic oxytocin administration on attenuating amygdala-orbitofrontal connectivity, immediately after the nasal spray administration period, at T1 (Spearman ρ = .50; p = .011) (Figure 3A). The retention of oxytocin-induced neural changes at T2 was not significantly modulated by variations in OXTR DNA methylation (p > .05) (Figure 3B). Modulations of oxytocin-induced changes in high-frequency HRV on the other hand, were significant for the T2 retention session (not for the T1 session), indicating that children . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint with low OXTR DNA methylation frequencies displayed a stronger retention of increased high- frequency HRV (Spearman ρ = -.52; p = .046) (Figure 3C, D). Exploratory whole-brain amygdala functional connectivity analyses In addition to the hypothesis-driven exploration of oxytocin’s effects on amygdala- orbitofrontal cortex connectivity, we also performed exploratory whole-brain analyses examining oxytocin-induced changes in functional connectivity between bilateral amygdala (seeds) and all other regions of the cortical (n = 91) and subcortical (n = 13) Harvard-Oxford Atlas. At the T1 post session, an overall pattern of enhanced amygdala connectivity was identified (Figure 4, Supplementary Table 1), particularly to regions of the salience network, including bilateral insular cortex (including posterior insula in planum polare regions), the anterior cingulate cortex and right supplementary motor area. Increased amygdala connectivity was also evident to cortical regions in frontal, parietal and temporal regions overlapping with the action observation network, i.e., including bilateral inferior frontal gyrus, pars triangularis and pars opercularis, and right supramarginal gyrus (encompassing inferior parietal lobule) and middle temporal gyrus (overlapping with the superior temporal sulcus). Increased subcortical amygdala connectivity was also evident with the globus pallidum. At the T2 follow-up session, four weeks after cessation of the nasal spray administrations, a remarkably different pattern of oxytocin-induced changes in amygdala connectivity was evident, indicating an emergence of reduced amygdala connectivity to distinct cortical and subcortical regions (Figure 4, Supplementary Table 1). Particularly, in addition to reductions in amygdala-orbitofrontal connectivity, reductions in amygdala connectivity were identified with bilateral temporal fusiform cortex, right transverse temporal gyrus (Heschle’s gyrus), opercular cortex and parietal regions including the supramarginal gyrus and superior parietal lobule. Reductions in amygdala-subcortical connectivity were mainly identified for the brainstem, the globus pallidum and parahippocampal structures. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint Discussion In this pharmaco-neuroimaging study examining the neural effect of chronic oxytocin administration in children with ASD, we identified significant changes in intrinsic functional connectivity of the amygdala up to four weeks after cessation of the actual nasal spray administration period. Prior observations of neuroplastic changes in intrinsic functional connectivity between amygdala and orbitofrontal cortex, as seen in the autistic male adult brain 27, were replicated in the current pediatric population of boys and girls with autism, highlighting the importance of this amygdala-centered circuit in oxytocin’s neuromodulatory effects. In children, however, the pattern of reduced amygdala-orbitofrontal connectivity only emerged significantly at the follow- up session, four weeks after cessation of the daily administrations, whereas in the adult sample, the effect already emerged significantly immediately after the nasal spray administration period and further enlarged at follow-up. Generally, adaptations in amygdala-prefrontal/orbitofrontal interactions are interpreted to reflect changes in top-down inhibitory control of prefrontal regions over amygdala reactivity 47. However, considering that the repeated oxytocin administrations may have halted excessive amygdala-driven arousal 25, 26, it is anticipated that the observed pattern of reduced amygdala- orbitofrontal coupling primarily reflects a reduced need for top-down modulation. This interpretation is corroborated by the identified relationship between amygdala-orbitofrontal adaptations and changes in cardiac autonomic activity, predominantly in terms of enhanced parasympathetic high-frequency HRV. Importantly, this relationship indicates that children who display more pronounced attenuations in amygdala-orbitofrontal coupling, also display stronger oxytocin-induced increases in cardiac parasympathetic drive, i.e., reflective of a facilitation of the stress-regulatory ‘rest & digest’ autonomic state 29. Previous acute effect studies yielded a pattern of oxytocin-induced increases in resting- state high-frequency HRV in healthy adults 31, 32, 34, patients with obstructive sleep apnea 30, individuals at risk for psychosis 33 and pregnant women 48 (although see 49). Note however, that studies examining oxytocin-induced changes in task-related HRV predominantly demonstrated an opposite pattern, indicating reduced cardio-parasympathetic tone, for instance during a mental arithmetic task in patients with chronic back pain 50 as well as during a mildly stressful social challenge in patients with Fragile-X-syndrome 51 (although see 52). This provides support to the notion that the impact of oxytocin may not be uniform and that the direction of effects may depend on the context in which oxytocin is administered. Accordingly, and as outlined above, it appears that particularly in ‘task-free’ conditions, oxytocin may further facilitate the body’s parasympathetic tone 30-34, 48. Upon mildly stressful tasks, on the other hand, oxytocin may primarily induce an increase in cardiac-sympathetic tone, likely reflective . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint of enhanced attention and vigilance to the presented task 50, 51, as proposed in the Social Salience Hypothesis of oxytocin 53. Also with regard to chronic oxytocin administration studies, consensus is growing that variable treatment responses may at least be partly attributed to the fact that, in contrast to acute dosing studies, there is often a lack of standardization of the context in which the daily oxytocin nasal spray is administered. In a recent combinatory trial, significant clinical improvements in social functioning were observed in young children with ASD receiving oxytocin in combination with standardized parent-child social interactions, held within the two hours after the daily nasal spray administrations 15. These and other observations 19, 54 therefore prompt future trials to administer the daily doses of chronic oxytocin administrations within more standardized contexts, e.g. by pairing them with psychosocial trainings. Aside context, also distinct person-dependent factors are put forward to contribute to the observed variability in treatment responses within and across studies. In line with this notion, we here showed that oxytocin’s neural and cardiac autonomic effects were significantly modulated by epigenetic variations of the OXTR, such that oxytocin-induced changes were more pronounced in children with lower levels of OXTR methylation (associated with higher oxytocin receptor expression). Albeit interpretative, these observations provide indications that chronic oxytocin administration may be predominantly beneficial for children who display at least a minimal level/availability of OXTR expression. While, to our knowledge, this is the first oxytocin trial to examine OXTR methylation as a potential treatment modulator, prior work in patients with obsessive compulsive disorder (OCD) has proposed OXTR DNA methylation to constitute an important biomarker of cognitive behavioral therapy (CBT) treatment responses 55, 56. Particularly, two recent trials consistently demonstrated OXTR hypermethylation (lower receptor expression) to be associated with impaired CBT treatment responses in OCD patients 55, 56. With regard to diagnosis-related alterations in OXTR DNA methylation in ASD, evidence is currently sparse and inconclusive. A recent review pointed towards differential ASD-related alterations in OXTR DNA methylation depending on developmental stage suggesting a predominant pattern of hypomethylation in children, but hypermethylation in adults with ASD 46. The number of existing studies is limited, however, and overall sample sizes were small to modest. Also considerable design-related variations can be noted (e.g. in terms of included CpG sites) rendering it difficult to draw conclusive interpretations regarding diagnosis-specific alterations in OXTR DNA methylation in ASD 46. Despite these difficulties in delineating consistent categorical diagnosis-related alterations, the current research provides important indications that dimensional variations among ASD individuals may form an informative biomarker for delineating subgroups of patients that may benefit the most from chronic oxytocin administration. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint Exploratory analyses of oxytocin-induced effects on whole-brain amygdala connectivity showed remarkable similarities between those reported in adults with ASD 27 and those seen in the current study in children with ASD. Particularly, both in adults and in the current pediatric population, an overall pattern of attenuated amygdala coupling was evident, indicating reduced functional connectivity to distinct regions including orbitofrontal cortex, as well as fusiform cortex, 
brainstem, 
opercular 
cortex, 
globus 
pallidus, 
supramarginal 
gyrus, 
and (para)hippocampal regions. Notably however, while in adults with ASD, this broader pattern of attenuated amygdala connectivity already emerged immediately post-treatment 27, it appeared that in children with ASD, a different time course of oxytocin’s amygdala effects was evident, with attenuating effects only significantly emerging at the T2 follow-up session, four weeks after cessation of the last nasal spray administration. Immediately after the nasal spray administration period (T1) on the other hand, children with ASD displayed a predominant pattern of enhanced amygdala coupling to distinct brain regions, particularly to regions of the salience network, including bilateral insular cortex, the anterior cingulate cortex and supplementary motor area. While speculative, the divergent pattern of neural amygdala effects in children, compared to adults immediately post-treatment may reflect a differential mechanistic recruitment of neural circuits. Particularly, in children the enhanced amygdala connectivity to the salience network may indicate a stronger reliance on oxytocin’s (social) salience enhancing effects 53, likely facilitating increased attention attribution to and awareness of sensory, emotional and cognitive information 57. In adults with ASD, on the other hand, there appeared to be a stronger immediate reliance on oxytocin’s stress-regulatory, anxiolytic role as reflected by the overall pattern of attenuated amygdala connectivity e.g. to orbitofrontal cortex and brainstem 24, 27. In children with ASD, these anxiolytic, amygdala attenuating effects appear to emerge only at a later stage, rendering oxytocin’s long-lasting neuromodulatory changes in children to be similar to those observed in adults. Future research will be needed to gain deeper insights into these divergent results patterns, and to discern whether potential differences in dosing schema (one daily dose of 24 IU in the adults, versus two daily doses of 12 IU in the pediatric sample) and/or differences in the duration of acute dosing effects may constitute variability inducing factors. For example, in adult populations, heightened levels of circulating oxytocin were shown up to 7 hours after acute administration 58. It is however unclear whether the duration of acute dosing effects is similar in pediatric populations or whether they can endure longer, potentially rendering the reported T1 post effect (assessed 24h after the last nasal spray) to be susceptible of reflecting – at least in part – an acute oxytocin dosing effect, rather than solely reflecting oxytocin’s recursive, chronic action on amygdala-centered circuits. While the current study provides important new insights into the neural and cardiac autonomic effects of chronic oxytocin administration in children with ASD, the following . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint limitations and recommendations are noted. First, a rather homogenous group of high- functioning children within a tight pre-pubertal age range were included, rendering generalizability of the identified oxytocin-induced effects to more heterogeneous or younger/older children and adolescents uncertain. Also, while the included number of boys and girls in our sample reflected the well-documented four-to-one male bias in ASD prevalence 59, future research is warranted to examine the observed oxytocin-induced effects also in larger samples of girls with ASD, especially given prior reports of sex-related differences in neural and behavioral responses to oxytocin 60. Finally, in the current study, participants administered 12 IU of the oxytocin nasal spray twice a day in the morning and afternoon, similar to 13, although considerable variation in dosing schemes can be noted among prior pediatric trials, e.g. one daily dose of 24 IU 14, 17 or intermittent 15 and flexible daily dosings ranging from 8 IU to 80 IU 18. Future trials should therefore be directed at identifying optimal daily dosing schemes, administration length and intervals. To conclude, in this pharmaco-neuroimaging study in a pediatric population of children with ASD, we demonstrated important stress-regulatory neural and cardiac effects of chronic oxytocin administration, indicative of a facilitation of the ‘rest and digest’ parasympathetic autonomic state, even up to four weeks after cessation of the nasal spray administration period. Further, epigenetic profiling showed that inter-individual variation in OXTR DNA methylation may form an important biomarker for delineating subgroups of children with ASD who may benefit the most from a chronic regime of oxytocin administrations. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint Acknowledgements We would like to thank all the participants of the study and our colleagues of the Leuven Autism Research Consortium (LAuRes). This research was supported by an internal C1 and Small Research Equipment fund of the KU Leuven (ELG-D2857-C14/17/102; KA/20/080), a Doctor Gustave Delport fund of the King Baudouin Foundation, the Branco Weiss fellowship of the Society in Science - ETH Zurich, an Excellence of Science EOS grant (G0E8718N) granted to KA or BB. JP is supported by the Marguerite-Marie Delacroix foundation and a postdoctoral fellowship of the Flanders Fund for Scientific Research (FWO; 1257621N). SVDD is supported by a FWO postdoctoral fellowship (12C9723N). MM is supported by a KUL postdoctoral mandate. ME is supported by an FWO aspirant fundamental fellowship (11N1222N). The funding sources had no further role in study design; in the collection, analysis and interpretation of data; in the writing of the report; and in the decision to submit the paper for publication. Statement of Ethics This study protocol was reviewed and approved by the Ethics Committee for Biomedical Research at the University of Leuven, approval number [S61358]. Written informed consent from the parents and assent from the child were obtained prior to the study. Conflict of Interest Statement The Authors declare no conflicts of interest. . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288109
doi: 
medRxiv preprint",1
"Background: Clinical Decision Support Systems (CDSS) are rapidly altering the face of healthcare and their potential to improve patient outcomes has been exploited, in some countries. This study aims to explore the current landscape of the Indian healthcare sector to identify the favourability of current practises, organisational and infrastructural readiness, attitudes and concerns of the stakeholders concerning the implementation of CDSS. Methods: The methodology that this study used was carrying out structured interviews comprising of 16 close-ended questions, split into three sub-categories. There was a total of 61 interviews were conducted with medical and administrative staff in public and privately run facilities, present in Tier 1 and Tier 2 cities in India. The study will focus on hospitals in Tier 1 cities as these are in a position to bring technological transformation. Results: The results identified various trends and patterns that would likely govern the incorporation of CDSS. A large proportion of the experts answered positively about the current level of digitalisation of their workplace and the availability of funds for future innovation, indicating high favourability for CDSS. Various roadblocks were isolated with respect to stakeholder attitudes, standardisation of care and general knowledge about CDSS and that in two cities, privately owned facilities were better equipped than state-run facilities. Conclusions: There have been many recent initiatives in India to promote digital health. Performing a CDSS cost-effectiveness study will demonstrate the benefits of using CDSS in the country to overcome any adoption hesitancies. Keywords: CDSS, India, Healthcare, Artificial intelligence, Tier 1, Tier 2 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint Declarations Ethics approval and consent to participate: At the outset, the research methodology was approved by YouDiagnose Ethical Approval Committee. This research work was commenced after this approval and this study did not involve any human data, tissues, samples, or materials. Consent to participate was obtained from each participant at the beginning of the study. Using the NHS Health Research Authority and United Kingdom Medical Research Council decision-making tool, it was determined that this study would produce generalisable or transferable findings. Therefore, informed consent was obtained from each participant before the structured interview. It was also determined that the study would anonymize the participants to mitigate any risk. All the methodologies were in line with the core practices of the Committee on Publication Ethics (COPE) and the Declaration of Helsinki. Consent for publication: Agree Availability of data and materials: The research data is shared with the readers and with the editorial board for the peer-review process. The datasets generated and/or analysed during the current study are not publicly available due to the databases being held anonymously but are available from the corresponding author at reasonable request. As per the terms of engagement, the participants held positions in various public and private institutes, and they have disallowed public disclosure of the institutes, participants’ names and positions. Many times, such disclosures might have repercussions in the office’s political circle. Many of the opinions may be misconstrued as tarnishing the reputation of institutions which might have consequences e.g., career progression etc. Competing interests: The company does not have any financial non-financial professional or personal competing interests or conflict of interest that might interfere with the full and objective presentation of the subject matter of this article. We have taken into account all the financial and non- financial competing interests while making this statement. Funding: This study was funded by YouDiagnose Limited as it is part of its market research activities Authors' contributions: N.M. and S.T. helped in designing the concept of the study. S.G. helped in arranging, conducting and collecting data for the interviews. A.M.1 and P.W. compiled and analysed the data. P.W., H.G. and N.K. were major contributors to manuscript writing. A.M. was a major contributor to each step of the current work. All authors read and approved the final manuscript. 1.0 Background With growing inequalities, an enormous population and a vast talent pool, the Indian healthcare sector is prime for the successful implementation of sustainable and innovative technologies like Clinical Decision Support Systems (CDSS). CDSS is a component of Health Information Technology (Health IT) which tries to incorporate Artificial Intelligence (AI) to improve patient outcomes. It is based on the principle of developing and applying algorithms to clinical data and formulating advanced diagnostic assessments, risk predictions, prognostications and treatment strategies, aimed at ultimately improving overall healthcare. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint The Indian healthcare system has been ranked 112 out of 190 countries in the World Health Report published by WHO1 in 2000 (1). However, India has a competitive edge over some of the better- ranked economies due to the high percentage of skilled professionals, the size of the economy, and the cost-efficient medical and surgical treatments offered. The Indian healthcare sector is an integral contributor to the nation’s revenue and employment. The public and private healthcare delivery systems are the two major categories of the Indian healthcare delivery systems. While the public healthcare system is streamlined to address the basic healthcare needs specifically of the rural population, the private care system caters to the more advanced medical needs with several secondary, tertiary, and quaternary private institutions concentrated in top-tier cities (2). Prehistorically, several forms of traditional medicine were popular in India, some of which still exist. The advent of modern medicine in the subcontinent can be traced back to the 17th century (3). Currently, the doctor-to-patient ratio in India is 1: 1456 against the WHO recommendation of 1:1000 (4). Despite rapid growths and wide expansions in the healthcare arena in recent years, most of the nation’s healthcare is still predigital, written case sheets and radiological films are still more common than their electronic counterparts. Health IT has revolutionised the way medicine is practised, over the globe. In the last decade, the country has seen unprecedented booms in technology. The push for digitalisation has seen more institutions incorporate basic technologies in their healthcare systems. Currently, EHRs2, online appointment scheduling, and e-commerce for home delivery of medicines are gaining popularity. With digitalisation, the urge for standardisation of care through the development of standard treatment guidelines has garnered due attention (5). The main role of CDSS is to assist clinicians rather than make decisions for clinicians, with a better analysis of individual 'patients' needs (6). The healthcare industry which is deemed to be one of the most complex and dynamic sectors in India is expected to reach a staggering $372 billion by 2022. The hospital industry which comprises 80% of the sector is growing at a compound annual growth rate of 16-17% and is predicted to hit $132 billion by 2023 (7). According to the Bank for International Settlements (BIS)3 research report published in 2018, the Indian CDSS market was valued at $43.8 million. The global and Indian CDSS markets were projected to reach $10.83 billion and $206.1 million, respectively, by 2025. The annual cost savings due to the incorporation of AI in healthcare is predicted to be $150 billion, by 2026 (8). Given the positive trend and outlook of the CDSS market, we studied the Indian market readiness for adopting such an innovative digital product in its highly regulated healthcare environment. The research question to be addressed in this study is; “Are the Indian healthcare institutions ready to incorporate a CDSS into clinical practice?”’. The findings obtained from the study will contribute to answering this question. 1 The World Health Organisation 
2 Electronic Health Record 
3 Market intelligence company . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint A few studies have been conducted on the applicability of CDSS in rural India, particularly for use by public non-physician healthcare workers, and physicians working in resource-poor settings in the domain of cardiovascular health and psychiatry. There have been many positive outcomes in regard to the use of CDSS. However, there is not enough quality data available regarding its adoption in any setting other than the primary care environment. The current study is, therefore, focused on the application of CDSS in secondary and tertiary care settings, especially in the domain of cancer care (9-11). 2.0 Methods 2.1 Aim The aim of the study is to critically analyse clinical care at secondary care and tertiary care hospitals in India, to gain an insight into the concerns and attitudes towards the incorporation of CDSS into patient practises, which are crucial determinants of the future scope of CDSS in the country (12). 2.2 Study Design This study followed a semi-structured interview design where the participants were asked a set of closed-ended pre-determined questions (e.g., “Are you ready to take the initiative and leadership to bring the change in your organisation if CDSS is given free of charge?”) followed by an open-ended exploratory question (e.g., “How do you view the present and future of CDSS in your personal or institutional clinical practice?”) that would reflect their knowledge, awareness and attitudes towards CDSS, while also provide useful information about the current healthcare practises. All the interviews followed the same prescribed structure. All the interviews were conducted face-to-face in the participants’ offices, over a three-month period. The language used for the face-to-face interviews was English. None of the interviews was repeated. The interviewer was provided with the sequence of the interview e.g. structured questionnaire to start with followed by an open-ended question to explore the circumstances further. The participants were asked a total of 16 questions which were grouped into 3 categories; current healthcare practises, organisational & infrastructural limitations, and personal attitudes towards CDSS. An interview guide was developed with the subcategories which were used in this study. The purpose of the semi-structured interview is to give structure to the entire interview while generating a sufficient amount of quality data that allows comparison across the hospitals and regions. The provision of subjective opinion promotes flexibility and inclusivity to capture the motive, and emotions of the interviewee. Some example questions from each category are, ‘Do you have electronic health records for the clinical services?’, ‘Do you know or have heard of the Clinical Decision Support System?’ and ‘Do you think CDSS can add value to your existing clinical practices?’. The complete set of questions is provided in Appendix 1. The participants could answer either yes, no or cannot say. They were also encouraged to explain their answers and the responses were recorded. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint 2.3 Settings and Study Participants To maximise the generalisability of the results and to include the perspectives of diverse potential users, the study took the following characteristics into consideration - ownership of the facility, current professional positions held by the participants and the location of the medical institutions that employed the study participants. 25 hospitals were visited in Thiruvananthapuram, Chandigarh and other Tier 1 and 2 cities in India including New Delhi and Mumbai, between the months of August and October 2019. These included corporate, state and central government-run facilities. In India, Tier-1 is defined as being highly developed with good facilities and a population of over 100,000, whereas Tier- 2 are in the process of being developed and has a population of between 50,000 and 99,999. The participants of the interviews included medical and admin staff. The recruitment of participants was carried out using a mixed methodology, e.g., social media (LinkedIn and WhatsApp messaging services) and email. We contacted the administrators of 2 surgical societies in order to recruit the participants necessary for this study. The leaflet containing our survey was posted on two WhatsApp groups of senior surgeons and hospital administrators. Out of the 37 people who responded, 22 were eligible to take part in this survey. The eligibility criteria were that they are a senior consultant (e.g., clinical practice in a cancer care speciality for at least 15 years), senior hospital administrator, or head of nursing or technology. 120 eligible participants were emailed about the survey. Out of those, 18 responded. We also found 22 more participants through social media (e.g., LinkedIn). The study included 14 private institutions and 11 government hospitals from Tier 1 and Tier 2 cities of India. Among those, 13 hospitals were in Tier 1 cities and a further 12 hospitals were from Tier 2 cities. 61 out of 62 experts requested the interview and participated in the study. There was a high participation rate of 98.4 % in this study. This is because 61 out of 62 experts that were requested for the interview took part in this study with one declining. Table 1: Service Sectors of Study Participants Service Sector 
Number of Participants Oncology 
18 Medical Services 
6 Senior Medical or Surgical Oncology Consultants 
20 Senior Hospital Administrators 
12 Heads of Nursing 
3 Chiefs of Healthcare Technology 
3 Table 1 shows that 32 participants recruited for the study were employed in facilities in Tier 1 cities while 29 participants worked in hospitals in Tier 2 cities. 30 and 31 participants from the private and public healthcare delivery systems participated in the study, respectively. User data was collected and analysed using Hall’s 7 Stages of Concern model (13), with 0 being Unconcerned and every subsequent stage of increasing concern e.g. 1 = Informational (learning and . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint weighing risks and benefits), 2 = Personal (perceived potential personal concerns and risks), 3 = Management (concerns related to the management of system), 4 = Consequence (impact on other users and environment), 5 = Collaborative (e.g. concerns on the integrated multi-disciplinary workflow), 6 = Refocusing (e.g. ideas for improvements). Stage-0 concern was for uninformed users, while stage-1 was for users who were uninformed but found it intriguing. Stage-2 concerns were related to those subjects who had personal concerns about the impact on their personal circumstances, such as loss of autonomy and external imposition. The stage-3 concerns included logistics issues related to the integration of CDSS into the clinical environment. The level-4 concerns were related to value generated, return on investment (ROI), risk and threat perception (collective). 2.4 Data Collection and Analysis Data from the 61 interviews was collected from the semi-structured interviews conducted in the hospital settings.  Following completion of the pre-determined closed questions of the interviews, the interviewee asked to provide their view on their current working environment and feasibility of CDSS. While the responses of the pre-determined structured questions were recorded on a paper-based note, the entire interview was recorded using microphone with the user’s consent and they were later transcribed for the purpose of future referencing. The data collected was analysed and recurrent themes were identified. The responses were grouped into the 3 broad categories that was pre- determined. Comparisons were then made among the sub-groups, with respect to the demographic differences among the study participants. However, the qualifications and the experiences of the individual participants were not considered for analysis. 3.0 Results Out of 61 interviewees, there were 18 oncology professors, 6 directors of medical services, 10 senior consultants in medical oncology, 9 senior consultants in surgical oncology, 4 medical superintendent and 8 senior hospital administrators. The mean age of the participants was 52.2 years and the age distribution is shown in Fig. 1. There are 50 male and 11 female participants in total. While the responses were directed to the adoption of CDSS, they also reflected the infrastructural, people, and process readiness needed for its adoption. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint Fig. 1: The age distribution of the interviewed participants Out of total of 976 responses from 61 participants (e.g., 16 sets of responses from 61 participants), 227 were favourable to the adoption of CDSS into clinical practice while 571 were unfavourable. There was a total of 128 “don’t know” responses. Out of the favourable responses, 61.2% were from Tier-1 cities while 38.8% were from Tier-2 cities. In the user adoption issues category, an overwhelming 97% of the favourable responses were from Tier-1 cities while 3% were from Tier-2 cities. 3.1 Analysis of the favourability of current practises for CDSS integration On questions posed to analyse the favourability of current practises for CDSS integration, the majority of answers were not in favour of AI implementation, see Fig. 2. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint Fig. 2. Percentage of participant answers on the favourability of current healthcare practises. Summary of Fig. 2: EHRs; 18% no, 82% yes, 0% can’t say. Wide variation in practice standards; 10% no, 90% yes, and 0% can’t say. The practice of up-to-date multi-disciplinary EBM in decision-making in general patient care; 63% no, 36% yes, 0% can't say. The departmental consensus of standardised decision-making; 90% no, 10% yes, 0% can't say. Multi-disciplinary decision-making concept in cancer care; 87% no, 13% yes, 0% can't say. 82% of the study participants used EHRs in their clinical practises, which is the only positive attribute that emerged in this subsection, for the further digitalisation of healthcare with the incorporation of CDSS. However, the participants claimed to use basic versions of EHRs in their hospital settings. Explaining the complexity of EHRs, one of the participants from a Tier 1 government hospital said, “the EHRs we use consist of scanned records of the written case sheets. Though it is theoretically an electronic record, it doesn’t offer the advantage of a conventional EHR by linking patient details to the results of the investigations or interlinking the results of diverse investigations like blood tests and CT4 scans. Hence, gathering patient information is sometimes easier on a paper-based system rather than electronic records.” Another physician from a Tier 1 private institution explained that digitalisation is not the one-solution for everyone, “EHRs reflect well on the hospital but I serve 4 hospitals. With the need to attend to patients spread across 4 hospitals in a day, I can’t lose valuable time sitting at a computer desk. Therefore, I have set my practice to the paper-based system which is readily available and quite efficient. I use EHRs only as a backup.” Time constraints and the need to spend more time with the patients were also highlighted by another physician, “I need to ensure that I spend a good amount of time with my patients amongst my tight schedules. When there are 1-2 computers with people queuing up to access them, I cannot stand and wait. That is the time I should be spending with my patients.” The lack of standardisation of care is evidenced by 90% of the study participants claiming wide variation of treatment, 90% acknowledging the absence of a diagnostic and research consensus, 87% denying the usage of SOPs5 in cancer care and a further 63% reporting the absence of SOPs in the decision-making process of current practises, is a notable roadblock. One of the senior consultants explained how multidisciplinary decision processes are not formalised in India, “when I trained in the UK, multidisciplinary decision-making used to be a regular practice standard. However, the outpatient units here are always crammed up and we are required to work in a very hectic schedule with time constraints. So, we don’t have the luxury of dedicating a time slot for a multi-disciplinary meeting. Now, when I attend to a pre-diagnosed cancer patient, I call up the 4 Computed Tomography scan used for diagnostic purposes in radiology 
 
5 Standard Operating Procedure . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint oncologist and share a snapshot of the patient record on WhatsApp and when he/she replies, I record it on the patient’s record. This is how the MDT6 system works here. While I am fully aware of CDSS through my experience in the Western countries, I don’t see any scope of its implementation in corporate practice here.” Another senior consultant rejected the requirement of multidisciplinary decision-making by voicing out a personal belief, “I believe that the multi-disciplinary concept is ingrained in most clinicians and every doctor should develop that skill. In my practice, I’m responsible for all the therapeutic decisions for my patients. I decide about the requirement and the relevance of a diagnostic investigation, procedures, surgical interventions, chemotherapy, or endocrine therapy. I am the one-stop solution for all the requirements my patients have. Having said that, I cannot perform radiotherapy as I am not trained in that. I don’t see a place for CDSS in my practice as I’m personally equipped with these algorithms. For me, multi-disciplinary decision making is more of an academic exercise for teaching and training the trainees rather than a patient requirement.” Addressing the lack of consensus and diverse practice standards, one physician said, “all the surgeons and physicians in our local area have an unwritten consensus on how cancer patients are to be treated. We are aware that the consultants have different preferences, so I see no point in holding an MDT meeting. As colleagues, we respect each other’s preferences and acknowledge that there are multiple ways of treating the same condition. At the end of the day, we are unified by our individual responsibilities to the patients and take decisions in their best regard. We offer our services and refer the patients to other specialists, if and when the need arises. In this climate, CDSS can’t add any value to the system unless we standardise our whole practice.” Another clinician further explained how a system that focuses on standardisation of care will not be beneficial in a sector where a wide variation of practice is the norm, “there are several right ways of treating the same condition. For my patients, I decide the treatment course and ultimately, it is down to the patient to accept or reject my suggestions. It is very likely that treatment practices vary among consultants. I’ll provide a suggestion to a colleague, only when I’m asked for one. CDSS will not work with the widely varying treatment options and therefore, for some, it might work as a wonderful system while for others it might be dismal. Hence, I think it can be widely adopted only if we can modify the system to cater to individual preferences.” A consultant who recently joined a corporate facility reflected the same thought while questioned about the lack of institutional SOPs, “I am not aware of standard departmental practice or standard operating procedures. There is no such thing called departmental induction and I don’t see why that is necessary. The only department meeting we attend is a monthly morbidity and mortality meeting, where we discuss serious patient care issues like patient death. We have been practising medicine for a long time and therefore, we know the best practices. We are answerable only to the patient and to the court of law for practice standards. On CDSS, I would love to use it in my practice but I’m uncertain about how my colleagues will perceive it.” 
                                                          
 
6 Multi-Disciplinary Team . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint 3.2 Organisational and Infrastructural Concerns While answering questions concerning the organisational and infrastructural readiness for newer innovative healthcare technology, the participants had more negative than positive responses, see Fig. 3. Fig. 3. Percentage of participant answers on organisational and infrastructural issues regarding CDSS implementation Summary of Fig. 3: Availability of internet, or PC or electronic literacy about use of digital products; 34% no, 66% yes, 0% can't say. Funds for innovation; 2% no, 98% yes, 0% can’t say. Leadership for transition to CDSS-led system; 95% no, 5% yes, 0% can’t say. Aware of the concept of CDSS; 77% no, 23% yes, 0% can't say. Prior experience of CDSS; 97% no, 3% yes, 0% can't say. 98% of the participants confirming the organisational availability of funds dedicated to newer innovation and a further 66% claimed that their institutions had the necessary technical systems in place is a promising reflection of the infrastructural readiness. However, that is hampered by the manpower needed to efficiently work the systems. Only a mere 5% of the study population expressed enthusiasm to take charge and hold leadership positions necessary for CDSS incorporation if it was available free of charge. Around 3% of the experts had prior experience with CDSS systems, and a staggering 77% were unaware of the concept of CDSS. Elaborating on the availability of funds for innovation, a C-suite level executive of Tier 1 corporate hospital said, “at our organisation, we have funds allocated for innovation. Usually, we provide these funds to start-ups with the potential for innovation. Recently we had given Indian rupee 5 lakhs for innovations. The start-ups produced certain results, but we are not sure how to promote it further. Though we go to start-up events and support innovation, any funds that we use for business problem solving will involve due diligence to ensure there is a good return on investment.” A hospital administrator shed light on the lack of efficient manpower by discussing the techno- competence of senior physicians, “I’ve observed that the consultants aged 40 and above tend to 0    10    20    30    40    50    60    70    80    90    100 Lack of internet, or PC or electronic literacy about 
use of digital products Funds for innovation Leadership for transition to CDSS-led system Aware of the concept of CDSS Prior experience of CDSS Can't Say
Yes
No . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint possess poor computer literacy. Though they can work efficiently with their smartphones, their typing speed is generally not very good, so they require a computer-literate person (e.g. smart intern) to accompany them all the time. From the managerial perspective, it is difficult to find doctors who will work as transcribers, it is not the rewarding or challenging career they are after. Hence, the feasibility of the use of a system like CDSS becomes difficult, as that would require people to enter the data and generate results.” 3.3 Attitudes towards CDSS Besides the aforementioned factors, attitudes towards a system will have a massive role in determining its success. Unfortunately, the attitudes towards CDSS were found to be not very favourable, see Fig 4. Fig. 4. Percentage of participant answers on the user-adoption and attitudes towards CDSS Summary of Fig.4:  Doctors see CDSS as an imposition on their free will; 15% no, 85% yes, 0% can't say. Concerns on return on investment on CDSS; 5% no, 7% yes, 88% can't say. The value generated by CDSS; is 93% no, 7% yes, and 0% can’t say. Concerns on the integration of CDSS; 0% no, 0% yes, 100% can’t say. Concerns on CDSS conforming to local practises; 0% no, 0% yes, 100% can’t say. While 85% of the population studied see CDSS as an imposition of their free will, a mere 7% were convinced of the value of integrating CDSS into the existing systems. A physician who saw CDSS as an imposition of decision-making freedom elaborated, “I would rather not use such a system in my practice as it would curb the freedom to make the best decisions for my patients. Clinical decisions should be taken jointly between the patient and the treating doctor. CDSS is not a party to this decision, even remotely. If we are to make it a standard practice, then I would be forced to make certain decisions without being convinced that is the right one.” Another doctor expressed fears over the system’s influence on clinical expertise, “why would I develop an unwanted dependence on a CDSS system? I would like to train my skills and decision-making skills. That will be the best investment as I am paid to make decisions. CDSS is not only unnecessary, but I’m also worried I might lose my skills if I use it in my practice.” 0    
10    
20    
30    
40    
50    
60    
70    
80    
90    100 Doctors see CDSS as imposion on their freewill Concerns on return on investment on CDSS Value generted by CDSS - convinced CDSS integration into workflow CDSS conforming to local practices Can't Say
Yes
No . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint Talking about the value of CDSS in clinical practice, a consultant with experience with IBM Watson said, “I am one of the first few consultants to try IBM Watson in a multi-disciplinary decision-making process, but I figured out that it is no match to an expert. When I used the system in my practice, I found it to be largely time-consuming, and rather an academic exercise. Of course, if we have CDSS integrated into the EHRs, it might be useful in day-to-day practice but in the absence of widespread integration, I don’t think it can add any value to the practice.” 88% of the participants expressed uncertainty over the returns of introducing this healthcare innovation, while 5% were confident that it would not yield any return. When prompted to identify the issues surrounding the integration of CDSS and the conformation of CDSS to local practises, all of the participants failed to produce a response, which testifies to their uncertainty about CDSS. Talking of the monetary concerns, a manager of a corporate hospital said, “One of the main issues we are facing is improving the flow of our patients and improving the flow of revenue to the hospital. I do not see how CDSS will be helpful to us. Our main income is from patient referrals, investigations, surgeries, and hospital stays. Anything which increases these will be considered as a good return on investment.” A hospital administrator expressed concerns regarding the integration of CDSS into clinical workflow, “in our hospital, there are 2 groups of physicians and surgeons; one is the senior, digitally illiterate, experienced group while the other is the new generation of doctors who are digitally oriented and agile in technology adoption, but they are often side-lined by the powerful seniors. The senior group can be quite inflexible when it comes to technology and change in the model. In an institution where hierarchical medicine has a stronghold, it is difficult for me to imagine a smooth transition to digital health technologies such as CDSS.” CDSS conformity to local practices was one of the integral issues identified, as one of the consultants said, “All the successful outcomes of IBM Watson are from Western studies. They are not applicable to the Indian subcontinent. For instance, we know that the breast cancer behaviour in the Afro- Caribbean population is significantly different to the Caucasian population and therefore, in the context of India, using an AI model framed in the Western set-up will not only be useless but can have unintended consequences too.” 3.4 Demographic Analysis The results were further analysed to identify any patterns that emerged with regard to the demographic differences of the participants involved in the study; Tier 1 versus Tier 2 cities and state- run facility versus a corporate-run facility. Comparing the responses between the participants from public and private organisations, a significant difference was observed with respect to the organisational and infrastructural concerns. Health practitioners in the private sector displayed 40% more awareness about CDSS when compared to their counterparts in the public sector. Furthermore, when compared to those in the public healthcare sector, the participants in the private setting were found to be 10% more enthusiastic to take initiative . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint and lead teams for the successful incorporation of CDSS in their organisations. There were not any significant differences in the other aspects. Wide variations were observed when the responses of participants from Tier 1 cities were compared with those of Tier 2 participants. 56% more participants from Tier 1 cities gave an account of the usage of institutional SOP in their current practises to facilitate decision-making. A similar pattern was observed with respect to the digitalisation of their workplaces – 49% of participants from Tier 1 institutions were more likely to have a digital workplace when compared to their Tier 2 counterparts, and 34% more clinicians in the Tier 1 cities had access to EHRs. The awareness of CDSS was more among participants in Tier 1 cities (44%). 28% more medical practitioners in Tier 2 cities compared to those in Tier 1 cities considered CDSS as an imposition on their will to make clinical decisions. 4.0 Discussion The paradigm shift in today’s healthcare and complex clinical decision-making brought about by clinical decision support systems has resulted in widespread adoption (8,14,15,16,17,18). The electronic health record has shown a massive amount of workload automation and smart capabilities due to the integration of these advanced analytics systems. The enhanced safety, cost-efficiency, patient, and physician satisfaction have forced regulators, governments, and hospital owners to consider these smart systems. The governments of the US, Canada, Denmark, Estonia, Australia and the United Kingdom endorse or incentivise the integration of clinical decision support systems into electronic health records (8,14). It has resulted in 41% of the US hospitals (15) and 62% of Canadian practitioners (16) integrating this technology into their workflow. CDSS-powered workflow has improved resource utilisation and has resulted in a cost savings of $717,538 per year without increasing the length of stay and mortality (21). In one study, CDSS was able to reduce 91.6% of medication consultations without introducing any errors (22). The impact of CDSS has been reported to make a positive impact on laboratory resource utilisation (17), medication error prevention (22), accurate disease coding (23), improving the quality of medical records (20), improving the outcome following surgery (21,22), patient guidance (26), implementing best practice in patient services (23) and appropriate utilisation of radiological imaging (21). The diseases in which CDSS has shown evidence of clinical impact include but are limited to inflammatory bowel diseases, heart failure, hypertension, elderly care, sleep apnoea, diabetes, flu, electrocardiogram (ECG) analysis, arterial blood gas analysis, protein electrophoresis, blood cell counting, brain tumour classification and grading, bladder cancer grading and recurrence, Hepatitis-B and C testing, cardiac arrhythmia detection, tumour detection, medical imaging interpretation, diabetic retinopathy 
diagnosis, 
Alzheimer’s 
diagnosis 
and 
peripheral 
neuropathy (24,26,27,28,29,30,31,32,37,38,39,40). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint Artificial Intelligence (AI) in healthcare is gaining momentum and popularity due to its immense potential to enhance the quality of care. However, the advances are largely limited to the western economies with the US and Europe holding 72% and 15% of the global CDSS market share, respectively (37). The incorporation of CDSS has huge potential to enable affordable health care delivery at a large scale and bridge the gaps in resource distribution in developing countries such as India (38,39,40). Developing economies deal with their unique array of challenges to achieve the seamless introduction of newer technology in their healthcare systems, due to its sheer population, complex sociocultural norms and vast demographic differences (41). Due to the expanding population, vast talent pool, lack of standardisation of medical care and diverse population demographics, the Indian healthcare system poses a unique set of challenges and potentials (42). While there are various types of CDSS produced in the market, the usage will depend on several factors. Nevertheless, in India where 1 million cases of cancer are diagnosed annually (43), CDSS has the tremendous potential to improve the quality of cancer care with early detection and screening. Technology-enabled tuberculosis medication adherence systems can reduce the burden of Multidrug-Resistant Tuberculosis (MDR-TB) (44). Diabetic screening is another domain where CDSS can play a massive role, with the country predicted to emerge as the diabetic capital of the world (45). With machine learning algorithms, it is possible to bridge the gap and distribute scarce knowledge to the masses, in a bid to establish egalitarian healthcare. The current study was designed to assess the favourability of the Indian healthcare sector for the introduction of this digital healthcare technology. Applying Hall’s Concerns-Based Adoption Model (13) to this study, 77% (n=47) of users had stage 0 or awareness concern at the outset where the survey participant was not aware if this was a concern as he/she has no idea about it while only 23% (n=14) had a higher level of concerns. When the potential users were educated about the CDSS, the number of people with level 1(information) & level 2 (personal) concerns increased to 85% (n=52). Here the users were more concerned about the threat arising out of the use of the system in their clinical practice e.g., the imposition of CDSS on their decision-making freewill. 93% (n=57) had raised a stage 3 or 4 concern e.g., CDSS might not cope with the existing logistics and systems in place, “largely time-consuming”, “the absence of widespread integration”, “an AI-model framed in the Western set-up” “can have unintended consequences too” etc. Most of the stage-4 concerns were related to the widespread scepticism surrounding the ROI and value generated by the integration of the CDSS. The collected data did not have any concerns about the collaborative, multidisciplinary workflow created by CDSS (e.g., level-5 concerns) or suggestions on ways and means to improve current models of CDSS available in the market (level-6). Earlier in this article, we observed the lack of standardised practice in India, evidenced by wide variation, lack of departmental consensus, multi-disciplinary decision-making, and standard operating procedures in cancer and general patient care. The vagaries of clinical care standards have endowed clinicians with a significantly higher degree of autonomy in decision-making. It may prove to be a roadblock in the introduction of CDSS. The collation of all the interview data indicated that the perceived usefulness and perceived ease of use were big barriers to user adoption . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint of this automation technology. This is a relative disadvantage as per Roger’s Diffusion of Innovations theory (46). Moreover, the organisation’s readiness to embrace the innovation was marred by various infrastructural issues e.g., lack of reliability of internet, adequate number of computers and electronic literacy about the use of digital products. These infrastructural issues created a sense of misfit of digital technology in the eco-system, reducing the compatibility and increasing the complexity as per Roger’s Diffusion of Innovations (47). Even though in an academic setting good results can be demonstrated, integration issues reduced the real-world triability of the technology (47). One of the most important factors for the successful adoption of new technology is observability (Roger’s Diffusion of Innovation) e.g., an easily noticeable result upon the use of the technology was lacking. Most hospital decisions on technology investments were based on return on investment such as saving of manhours, or generation of revenue. The technology can easily show its impact in settings where there are a high patient load compared to resources. However, in Tier 1 and 2 cities in India, there is a higher (almost double) density of doctors to patients and private practice is highly competitive. Average socioeconomic status, poor digital literacy, less than average digital infrastructure and lack of technology leadership force the country to approach digital technologies such as CDSS with a high degree of scepticism, concerns and poor risk-taking. These factors are likely to leave the country as late majority or laggards. It is also possible that this technology may fail to diffuse due to a lack of awareness. This is in sharp contrast with the Western economies who are more open to the use of CDSS technologies. A study by French Breast oncologists showed that physician compliance with CDSS exceeded 90% when the technology provided the correct proposition (48). On the other hand, a study involving general practitioners in West Ireland showed that 94% of physicians were open to the use of CDSS despite 74% being unfamiliar with the technology (49). In recent years, India has made substantial efforts to digitalise the existing healthcare system and make quality healthcare accessible to the marginalised sections of society. The Integrated Health Information Program (IHIP) is a testimony to the bold commitment shown by the nation to digitalise healthcare as it aims to provide an EHR for every citizen, integrate the existing EHRs to enable interoperability and establish uniform standards for EHRs across the country (50) In a bid to promote digitalisation of healthcare, the Ministry of Health and Family Welfare established National e-Health Authority (NeHA) in 2015 which aims to provide transparency in the maintenance and regulation of electronic transmissions of health information (51). A draft bill was also passed to ensure confidentiality of the digital clinical information, known as the Digital Information Security in Healthcare Act (DISHA) (42,53). The world’s largest government-funded health initiative, Ayushman Bharat introduced by the Government of India in 2018 caters to 50 crore people and the services are integrated with both private and public hospitals (53). These governmental initiatives are definite indicators of the country’s willingness and readiness to embrace newer innovations in the multi- layered healthcare sector. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint The lack of product awareness and conviction appears to be two major barriers to the implementation of CDSS in Indian healthcare, where the market is in the stage of innovation and overcoming such barriers may be crucial to the adoption of the product into the market as other studies have shown (56,57). Studying the regional differences between Tier 1 and 2 cities, the hospitals in the former group may hold the key to bringing the technological transformation by being favourably placed to become agents of change by championing the ideas and values. Such a model use case will change the public attitude and increase user confidence (51). The high degree of usage of EHRs and availability of funds for innovation in hospitals was an encouraging trend as introducing advanced technology such as CDSS into a digitalised healthcare system is considerably easier. 5.0 Conclusions India spends about 1.28% of its Gross Domestic Product (GDP) on its public healthcare (56). In comparison, many of the Organisation for Economic Co-operation and Development (OECD) countries spend between 9-16% of their GDP on the public healthcare system (58). Given the shocking level of patient safety issues amidst resource constraints and rising demand, Artificial Intelligence is the way forward. CDSS can have the greatest impact on the healthcare of developing economies by providing the highest level of healthcare standards while minimising healthcare expenditures. At the same time, it can also reduce medical errors and enhance patient safety by design and standardisation as discussed earlier. Average socioeconomic status, poor digital literacy, less than average digital infrastructure and lack of technology leadership have forced India to approach digital technologies such as CDSS with a high degree of scepticism, concerns and poor risk-taking. These factors are likely to leave the country as late majority or laggards. Despite all the impediments discussed earlier, India appears to be striving well to embrace digital health as seen by the many recent efforts e.g., National Digital Health IDs, Ayushman Bharat, the world’s largest insurance scheme and EHR for every citizen (59-62). Therefore, a high-quality CDSS cost-effectiveness study to showcase the benefit, safety, ease, and cost-saving potential of the use of CDSS will help the country to overcome the hesitancy associated with CDSS adoptions. The hospitals in Tier 1 cities appear to hold the key to bringing the technological transformation being favourably placed to become agents of change. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint List of Abbreviations AI: Artificial Intelligence BIS: Bank for International Settlements CDSS: Clinical Decision Support Systems CT: Computed Tomography DISHA: Information Security in Healthcare Act ECG: Electrocardiogram EHR: Electronic Health Record GDP: Gross Domestic Product Health IT: Health Information Technology ID: Identity IHIP: Integrated Health Information Program MDR-TB: Multidrug-Resistant Tuberculosis NeHA: National e-Health Authority OECD: Organisation for Economic Co-operation and Development WHO: World Health Organization SOP: Standard Operating Procedure MDT: Multi Disciplinary Team . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 7, 2023. 
; 
https://doi.org/10.1101/2023.04.02.23288046
doi: 
medRxiv preprint",1
"Background: Multivariable Mendelian randomization (MVMR) is a statistical
approach using genetic variants as instrumental variables to estimate direct causal
effects of multiple exposures on an outcome simultaneously. In univariable MR findings
are typically illustrated through plots created using summary data from genome-wide
association studies (GWAS), yet analogous plots for MVMR have so far been
unavailable due to the multidimensional nature of the analysis.
Methods: We propose a radial formulation of MVMR, and an adapted Galbraith radial
plot, which allows for the direct effect of each exposure within an MVMR analysis to be
visualised. Radial MVMR plots facilitate the detection of outlier variants, indicating
violations of one or more assumptions of MVMR. In addition, the RMVMR R package
is presented as accompanying software for implementing the methods described.
Results: We demonstrate the effectiveness of the radial MVMR approach through
simulations and applied analyses, estimating the effect of lipid fractions on coronary
heart disease (CHD). We find evidence of a protective effect of high-density lipoprotein
(HDL) and a positive effect of low-density lipoprotein (LDL) on CHD, however, the
protective effect of HDL appeared to be smaller in magnitude when removing outlying
variants. In combination with simulated examples, we highlight how important features
of MVMR analyses can be explored using a range of tools incorporated within the
RMVMR R package.
Conclusions: Radial MVMR effectively visualises causal effect estimates, and provides
valuable diagnostic information with respect to the underlying assumptions of MVMR. 4, 2023
1/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Introduction
1 Mendelian randomization (MR) is a methodological framework in which genetic
2
variants- often single nucleotide polymorphisms (SNPs)- are used as instrumental
3
variables to estimate causal relationships in the presence of unmeasured confounding [1].
4
MR analyses are often performed using summary data from publicly available
5
genome-wide association studies (GWAS), reflecting the ease with which such data can
6
be accessed in contrast with individual-level data [2,3].
7
A typical summary MR study begins with identifying a set of SNPs associated with
8
the exposure of interest, after which SNP-exposure and SNP-outcome association
9
estimates for each SNP are obtained [2,3]. Individually, dividing the SNP-outcome
10
association by the SNP-exposure association yields a Wald ratio estimate for the effect
11
of the exposure on the outcome corresponding to each SNP [2,4]. When multiple SNPs
12
are used ratio estimates are typically combined using inverse variance weighting (IVW),
13
producing an average causal effect estimate. SNPs are often weighted by the inverse of
14
the variance of their SNP-outcome association, though a range of weighting
15
specifications can be applied [2,5,6].
16
The extent to which MR causal effect estimates are unbiased is largely determined
17
by three key assumptions. SNPs serving as instruments must be robustly associated
18
with the exposure of interest (IV1), independent of confounders of the exposure and
19
outcome (IV2), and independent of the outcome when conditioning on the exposure
20
(IV3) [7]. Assumption IV1 requires the denominator (the SNP-exposure association) to
21
be non-zero, ensuring the ratio estimate is defined. Assumptions IV2-3 require the
22
SNP-outcome association to be the product of the SNP-exposure association and causal
23
effect of interest, such that the association between the SNP and outcome is entirely
24
mediated by the exposure of interest (see supplementary material). As ratio estimates
25
using valid (IV1-3 satisfying) SNPs would asymptotically converge towards the causal
26
effect of interest, observed heterogeneity in effect estimates using many SNPs can
27
potentially serve as an indicator of IV2-3 violation.
28
The value of estimated heterogeneity as an indicator of IV2-3 violation serves as the
29
motivation for conducting summary MR analyses within a radial framework, previously
30
described in Bowden et al (2018) [6]. Radial MR adapts the original summary MR
31
regression model such that causal estimates are a function of the ratio estimate and
32
weighting corresponding to each SNP. Radial IVW, for example, regresses the product
33
of the ratio estimate and square root weighting for each SNP upon the set of square
34
root weightings, omitting an intercept. This produces an IVW causal effect estimate
35
identical to the standard IVW approach, while allowing effects to be visualised using an
36
adapted Galbraith radial plot [10,11]. Importantly, such plots show the weighting
37
attributed to each SNP on the x-axis, while the contribution to global heterogeneity is
38
proportional to the distance of each data point from the superimposed regression line,
39
or its estimated residual. This facilitates outlier detection, highlighting SNPs which may
40
violate the underlying MR assumptions. Finally, as the weighting applied to each SNP
41
is always positive, and the ratio estimates themselves are independent of allele coding,
42
there is no need to reorient SNP-exposure associations [6].
43 4, 2023
2/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint SNP-outcome associations which violate assumptions IV2-3 can potentially be
44
mediated through additional phenotypes for which measures are available, and in such
45
cases multivariable Mendelian randomization (MVMR) approaches can be used to
46
estimate the direct effect of multiple exposures on an outcome simultaneously through a
47
generalisation of the univariable IVW model [3,12–14]. Such analyses are particularly
48
valuable where SNPs selected as instruments in univariable analyses violate assumptions
49
IV2 or IV3 through a measured phenotypic pathway, as such associations can be
50
accounted for when estimating causal effects. In the summary data setting, Burgess et
51
al (2015) demonstrate how MVMR estimates can be obtained using a generalisation of
52
the IVW model, specifically by regressing SNP-outcome associations upon
53
SNP-exposure associations obtained for each included exposure [3,12]. Sanderson et al
54
(2019) further develop these methods, proposing a range of sensitivity analyses specific
55
to MVMR. This includes methods to assess conditional instrument strength (an
56
extension of IV1 necessary for MVMR analyses) and horizontal pleiotropy (IV3) [13,14].
57
However, while several pleiotropy robust methods for MVMR have been proposed, there
58
is an absence of approaches to effectively visualise MVMR analyses [15,16].
59
In this paper we present a radial MVMR approach which allows for important
60
features of conventional MVMR analyses to be highlighted, in particular SNPs which
61
violate the underlying assumptions of MVMR. Radial MVMR addresses two key
62
limitations of existing approaches, providing a means with which MVMR analyses can
63
be visualised and a process through which outliers can be detected after conditioning on
64
additional exposures. Initially, we demonstrate how univariable radial MR can be
65
extended to incorporate multiple exposures, creating a radial analogue of the IVW
66
MVMR model. With this complete we describe how MVMR estimates can be visualised
67
using radial plots, and crucially, how including an adjustment to ratio estimates to
68
account for additional exposures included in the MVMR model facilitates the detection
69
of pleiotropic SNPs. Specifically, outliers in a radial MVMR analysis can be formally
70
and visually identified through an evaluation of their contribution to global
71
heterogeneity, indicating likely violations of assumption IV3. Through simulated
72
analyses we also highlight the extent to which pruning for such outliers can greatly
73
improve causal effect estimation, both in terms of reducing observed bias and increasing
74
the precision of MVMR estimates.
75
To demonstrate the application of radial MVMR we present an applied example
76
evaluating the effects of low-density lipoprotein (LDL), high-density lipoprotein (HDL),
77
and triglycerides on coronary heart disease (CHD). Using publicly available summary
78
data from the Global Lipids Genetics Consortium (GLGC) and
79
CARDIoGRAMplusC4D Consortium we find evidence of a protective effect of HDL,
80
and a positive causal effect of LDL in relation to CHD [17,18]. When pruning for
81
identified outliers the effect of HDL decreases in magnitude, and we find evidence of a
82
positive association with both LDL and triglycerides. We also illustrate how pleiotropic
83
bias appears likely when conducting univariable analyses, and how such bias is
84
potentially mitigated when including additional exposures. Throughout we perform all
85
analyses using the RMVMR R package for the R software environment, which has been
86
developed to facilitate the application of radial MVMR analyses. The RMVMR R
87
package is freely available from https://github.com/WSpiller/RMVMR.
88 4, 2023
3/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Methods
89 Univariable summary Mendelian randomization
90 In univariable MR analyses one or more SNPs are used as instruments to estimate the
91
causal effect of a single exposure X upon an outcome Y . Let Gj represent the jth
92
independent SNP from a set of j ∈{1, 2, . . . , J}, and let U denote one or more
93
unmeasured confounders. A SNP is considered valid provided it satisfies assumptions
94
IV1-3, with assumed relationships depicted in Fig 1. Letting i ∈{1, 2, . . . , N} index
95
subjects:
96 Xi = γ0 + J
X j=1
γjGj + γJ+1Ui + ϵXi
(1) Yi = β0 + β1Xi + J
X j=1
αjGj + β2Ui + ϵY i
(2) Fig 1. Directed acyclic graph (DAG) illustrating the assumptions of MR.
Associations required to be zero for the MR assumptions to be satisfied are shown as
dashed lines. Specifically, IV2 is satisfied when θj is zero, and IV3 is satisfied when αj
is zero. If all confounders of the exposure and outcome were measured, an unbiased estimate
97
for the effect of X upon Y (β1) could be estimated by performing a multivariable
98
regression of Y upon X including all confounders in the set U. As information on U is
99
unavailable by definition, summary MR uses SNP-exposure and SNP-outcome
100
associations obtained for each SNP to estimate the effect of X on Y in a manner robust
101
to confounding bias. SNP-exposure and SNP-outcome associations are estimated using
102
the following simple regression models (excluding variables commonly included in
103
GWAS such as principal components or age).
104 Xi = γ0 + γjGji + ϵXji
(3) Yi = Γ0 + ΓjGji + ηY ji
(4) 4, 2023
4/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Using the estimated total effect of Gj on X (ˆ
γj) and the estimated total effect of Gj
105
on Y (Γj), the ratio estimate for a given SNP (ˆ
β1j) using estimated parameters ˆ
γj and
106
ˆ
Γj can be written as:
107 ˆ
β1j =
ˆ
β1ˆ
γj + αj ˆ
γj
=
ˆ
Γj ˆ
γj
(5) Assumptions IV1-3 imply γj ̸= 0 and αj = 0, and consequently the ratio can be
108
shown to be a consistent estimator of the effect β1 provided assumptions IV1-3 hold
109
(see supplementary material). Using wj to represent the weight applied to each SNP j,
110
the IVW estimate using multiple uncorrelated SNPs is given by
111 ˆ
βIV W = PJ
j=1 wj ˆ
β1j
PJ
j=1 wj
, wj = ˆ
γ2
j ˆ
σ2
Y j
(6) Note that in Eq 6 inverse variance weights are used, though a wide-range of
112
weightings are available. As described in Bowden et al (2018), an equivalent IVW
113
estimate in Eq 6 can be obtained by fitting a radial regression model, regressing the
114
product of the ratio estimate and square root weight attributed to each SNP against the
115
set of square root weights across all SNPs [6].
116 ˆ
β1j√wj = βIV W √wj + ϵj
(7) Constructing a scatter plot with √wj and ˆ
β1j√wj on the y-axis, and superimposing
117
the regression line from Eq 7, the distance of each observation from the regression line is
118
equal to its square-root contribution to Cochran’s heterogeneity statistic,
p Qj, where
119 Q =
X
Qj = J
X j=1
wj (β1j −βIV W )2
(8) As previously described in Bowden et al (2018), the global Q-statistic follows a
120
chi-squared distribution with J −1 degrees of freedom, and individual estimates Qj
121
have a chi-squared distribution with 1 degree of freedom [6]. This allows p-values to be
122
used as a threshold for identifying outlying SNPs. Fig 2 shows an example radial plot
123
constructed using previously published GWAS summary data from Do et al, included
124
within the RadialMR R package [17]. This data contains information on LDL from the
125
Global Lipids Genetics Consortium (GLGC), and CHD data from the CARDIoGRAM
126
study [19,20]. Considering the effect of LDL upon CHD, variants identified as outliers
127
are highlighted in yellow and the IVW estimate is represented by a black regression line
128
through the origin.
129 4, 2023
5/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Fig 2. A scatter plot showing a radial IVW estimate using data from Do et
al. Here the x-axis represents the square root of the weight applied to each SNP, while
the y-axis shows the product of the ratio estimate and square root of the weight given
to each SNP. Outliers are identified using Cochran’s Q-statistic and a p-value threshold
of 0.05/number of SNPs has been used to correct for multiple testing. Multivariable Mendelian randomization
130 MVMR extends the univariable MR framework to include multiple potentially
131
correlated exposures, leveraging the entire set of SNPs associated with at least one
132
included exposure [13,14]. This allows for the direct effect of each exposure to be
133
consistently estimated, (that is, the effect of an exposure holding the others fixed),
134
provided the total set of SNPs G is:
135 • Strongly associated with each exposure when conditioning on remaining exposures
136
(MVMR1)
137 • Independent of all confounders of any individual exposure and the outcome
138
(MVMR2)
139 • Independent of the outcome when conditioning on all included exposures and all
140
confounders (MVMR3)
141 The previous data generating model can readily be generalised to include an
142
arbitrary number of SNPs and exposures, though there needs to be at least as many
143 4, 2023
6/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint SNPs as exposures for the MVMR model to be identified [13]. For i ∈{1, 2, . . . , N}
144
observations including j ∈{1, 2, . . . , J} SNPs, a set of SNPs l ∈{1, 2, . . . L} where j ̸∈l,
145
a set of exposures k ∈{1, 2, . . . , K}, and a set of exposures m ∈{1, 2, . . . , M} for which
146
k ̸∈m:
147 Xki = γk0 + J
X j=1
γkjGj + γk(J+1)Ui + M
X m=1
δkmXm + ϵXki
(9) Yi = β0 + K
X k=1
βkXk + J
X j=1
αjGj + β(K+1)Ui + ϵY i
(10) The set of relationships between variables is illustrated in Fig 3.
148 Fig 3. A DAG illustrating associations described in equations 1-2 and 9-10
for an arbitrary number of SNPs and exposures. Dashed lines represent
associations which would violate assumptions MVMR2-3. Using equations (9) and (10) the univariable estimand can be derived. For clarity,
149
we denote the total effect of an instrument Gj on an exposure Xk as γ∗
kj, while γkj
150
represents the direct effect of Gj on Xk conditioning on all relevant exposures on the
151
pathway from Gj to Xk. This allows us to define the univariable ratio estimand as:
152 γ∗
kj = γkj + M
X m=1
δkmγmj
(11) Γj = K
X k=1
βkγ∗
kj + αj
(12) 4, 2023
7/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Γj γ∗
kj
= βk + PM
m=1 βmγ∗
mj + αj γkj + PM
m=1 δkmγ∗
mj ! (13) A derivation of this result is provided in the supplementary material. If the MVMR
153
assumptions are satisfied, and independent SNPs are used, Eq 13 simplifies to
154 Γj γ∗
kj
= βk + PM
m=1 βmγ∗
mj γ∗
kj ! (14) Note that the term in parentheses in Eq 14 represents the effect of the additional
155
exposures Xm, and including ratio estimates for all additional exposures Xm within a
156
multivariable regression will yield marginal effects of each exposure, adjusting for this
157
term in each case. In univariable MR, pleiotropic effects violating IV3 would likely be
158
present when βmˆ
γmj ̸= 0. The direct effect of each exposure can be estimated by
159
regressing instrument-outcome associations upon instrument-exposure associations for
160
each exposure simultaneously [2], such that:
161 ˆ
Γj = βIV W 1ˆ
γ1j + βIV W 2ˆ
γ2j + . . . + βIV W kˆ
γkj + ϵj
(15) MVMR relies upon a sufficient proportion of instruments being strongly associated
162
with each exposure, conditional on remaining included exposures. This can be evaluated
163
by calculating the conditional F-statistic for each exposure using a conventional
164
threshold of 10 [14]. In terms of ratio estimates the conditional instrument strength of
165
instrument Gj can be thought of as the extent to which ˆ
γkj ̸= 0, and the conditional
166
independence of the genetic instruments with respect to the outcome (MVMR3) can be
167
evaluated by estimating observed heterogeneity across the set of ratio estimates, as
168
described in Sanderson et al (2021) [14].
169 Radial MVMR
170 The univariable radial MR model can be readily extended to include multiple exposures,
171
creating an analogue of the MVMR regression model shown in Eq 16:
172 ˆ
β1j(√w1j) = βIV W 1(√w1jsgn(γkj)) + βIV W 2√w2j + . . . + βIV W K√wKj + ϵj
(16) where wkj represents the weighting for each SNP with respect to exposure k. For
173 example, w1j is equal to ˆ
γ2
11 σ2
Y when first order weights are used. It is important to note
174
that the SNP-exposure associations must be re-oriented so as to match the direction of
175
the SNP-exposure associations contributing to the regressand, in contrast to univariable
176
radial MVMR which does not require SNP-exposure associations to be reoriented. This
177
does not alter effect estimates obtained, and they remain equal to those obtained using
178
conventional MVMR.
179
An immediate benefit of conducting MVMR within a radial framework is plotting
180
ˆ
βkj√wkj against √wkj can be accomplished using generalised axes scales, allowing ratio
181
estimates for each exposure to be projected onto the same scatter plot simultaneously.
182
The RMVMR plot has the same x-axis scale as the univariable Radial MR plot, plotting
183
√wj values. The y-axis of the RMVMR plot, however, represents the product of the
184
ratio estimate and weighting for the reference exposure. As all instruments associated
185
with at least one exposure are used to estimate causal effects, an RMVMR plot will
186
have K × J observations, including a set of weightings for each included exposure k.
187
For clarity, while all weightings are used to estimate causal effects it is often
188
appropriate to limit the number of observations represented on an RMVMR plot to
189 4, 2023
8/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint instruments exceeding a given weighting threshold. For example, points corresponding
190
to an exposure X1 would be shown on the RMVMR plot provided they have an
191
F-statistic greater than 10. This omits clusters of instruments with negligible weightings
192
which are unlikely to be of interest, while improving the readability of the plot.
193
RMVMR plots are particularly useful as a tool for highlighting the extent to which
194
individual SNPs contribute towards global heterogeneity with respect to each included
195
exposure. Following the the data generating model given in equations Eq 9-10
196
performing a univariable MR analyses will result in biased estimates where SNPs are
197
associated with multiple causally relevant phenotypes. This is primarily because the
198
sum of associations PM
m=1 βmˆ
γmj represent pleiotropic pathways through the omitted
199
exposures, resulting in increased heterogeneity provided such effects are not identically
200
distributed across the set of SNPs (see Eq 14). If this is the case, then it follows that
201
adjusting for such associations would result in a decrease in effect estimate heterogeneity,
202
with estimates converging towards the MVMR estimate once the univariable pleiotropic
203
bias is corrected. The radial analogue for Eq 14 can be written as:
204 Γj
√wkj
= βk + PM
m=1 βm√wmj
√wkj ! (17) This result has important implications in terms of visualising heterogeneity in
205
RMVMR analyses. In an RMVMR plot we plot the product of the ratio estimate and
206
corresponding square root weighting against each set of weights on a generalised x-axis
207
(√wj). However, as the univariable ratio estimate for each instrument is used,
208
superimposed regression lines representing the RMVMR estimate for each exposure will
209
not represent the best fit through the plotted observations. This is because the ratio
210
estimates do not account for the adjustment from other exposures. We can write the
211
position of each data point on the y-axis as:
212 βkj√wkj = βk√wkj + PM
m=1 βm√wmj
√wkj !
√wkj = βk√wkj + M
X m=1
βm√wmj
(18) Eq 18 highlights how, by subtracting PM
m=1 βm√wmj from the y-axis value of each
213
data point, an adjustment can be performed to account for exposures included in the
214
RMVMR model. Crucially, though the true value of βm is unknown in Eq 18, an
215
adjustment can be made using the estimate of βm obtained from the RMVMR model,
216
that is, βIV W m in Eq 16.
217
When the MVMR assumptions are satisfied MVMR ratio estimates will be unbiased.
218
This means that adjusted ratio estimates should converge towards their corresponding
219
estimated effect, for example, βk in equation (17). Consequently, observed heterogeneity
220
in MVMR ratio estimates can be indicative of violations of MVMR3. This can be
221
formally evaluated through an adapted form of Cochran’s Q-statistic calculated for each
222
exposure:
223 Qk = J
X j=1
Qkj = J
X j=1
wkj βkj − PM
m=1 βm√wmj
√wkj ! −βIV W k !2 (19) 4, 2023
9/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint As in the univariable Radial MR setting, the square root contribution of each
224
instrument to global heterogeneity with respect to an exposure k is equal to:
225 p Qkj = βkj√wkj − M
X m=1
βm√wmj −βIV W k√wkj
(20) Equation (20) describes how the square root contribution to heterogeneity with
226
respect to exposure k is represented by the distance from each adjusted point to the
227
superimposed regression line for βIV W k, evaluated at √wkj.
228
To visualise the extent to which the addition of an exposure minimises effect
229
estimate heterogeneity, a pair of RMVMR plots can be constructed. Initially an
230
RMVMR plot is created including regression lines showing the MVMR estimate for each
231
exposure. For the first plot, each data point shows the the square root weighting for
232
each instrument (√wkj), and the product of the square root weighting and unadjusted
233
ratio estimate for each exposure. The second RMVMR plot includes the adjustment to
234
each univariable ratio estimate, described in equation (18). An example of such plots
235
using simulated data is presented in Figure 4.
236 Fig 4. A pair of RMVMR plots using two exposures presented for
illustration. In this case, both exposures have a non-zero effect on the outcome, and
10 instruments are associated with both exposures simultaneously. Fig 4a uses ratio
estimates prior to adjusting for the additional exposure, resulting in substantial
observed heterogeneity. Fig 4b shows a substantial reduction in observed heterogeneity
when adjusting for the additional exposure. In this simulated example a total of 30 instruments are used, of which 10 are
237
associated with exposure X1, 10 are associated with exposure X2, and a final 10
238
instruments are associated with X1 and X2 simultaneously. In univariable analyses,
239
provided both exposures have an effect on the given outcome 10 instruments will violate
240
IV3 through the omitted exposure. This is shown by the degree of observed
241
heterogeneity in Figure 4a prior to using adjusted ratio estimates. As shown in Figure
242
4b, where the inclusion of an additional exposure accounts for remaining pleiotropic
243
effects, the resulting adjusted ratio estimates will converge to the MVMR estimate, that
244
is, the direct effect of each exposure on the outcome of interest.
245 4, 2023
10/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint A number of important features of can be discerned from Figure 4 which warrant
246
consideration. Initially, the degree to which the position of data points changes can serve
247
to indicate whether the inclusion of an additional exposure within an RMVMR model is
248
appropriate. Specifically, if omitting an exposure induces bias when calculating ratio
249
estimates, we would expect the vertical position of data points to change when applying
250
an adjustment. In cases where data points do not appreciably change in position, this
251
can imply either that the additional exposure has no effect on the outcome (βm = 0),
252
that instruments are exposure specific such that wmj = 0, or an unlikely scenario where
253
the adjustments across all exposures are balanced such that PM
m=1 βm√wmj = 0.
254
A second benefit of RMVMR plots is the ability to visually identify the relative
255
exposure-specific weighting of each instrument. In Figure 4 the position of each
256
instrument on the x-axis reflects its weighting (√wj) with respect to each individual
257
exposure. It should be noted, however, that this does not reflect the conditional
258
instrument strength of each instrument.
259
Finally, as highlighted in equation (20) the vertical distance of each data point in
260
Figure 4b from their corresponding superimposed regression line is equal to their square
261
root contribution to heterogeneity with respect to the given exposure. This can be
262
indicative of invalid instruments, and would warrant further follow-up using external
263
data. It is, however, critical to note that the adjustments made to each exposure are
264
reliant upon initial estimates for the direct effect of each exposure ˆ
βIV W k. In cases
265
where these estimates are initially biased an iterative process can be applied, identifying
266
and removing outliers and repeating effect estimation until no outliers exceeding a given
267
Q-statistic threshold are present. However, as in univariable MR, this is reliant upon
268
such outliers being invalid. In cases where the majority of instruments are pleiotropic
269
with a similar distribution of pleiotropic effects, it is possible that valid instruments will
270
be identified as outliers. In these cases, the removal of outliers can lead to estimates
271
converging towards biased estimates of ˆ
βIV W k.
272 The RMVMR R package
273 The RMVMR R package is a tool designed to facilitate the implementation and
274
visualisation of RMVMR analyses. RMVMR analyses should ideally be performed in
275
five stages. First, summary GWAS data need to be obtained for a set of instruments,
276
including instrument-exposure associations for all included exposures,
277
instrument-outcome associations, and corresponding standard errors. With this
278
complete, the data are formatted for downstream analyses using the format rmvmr()
279
function, and conditional instrument strength is evaluated using the strength rmvmr()
280
function. Causal effect estimates and tests for pleiotropic instruments can then be
281
performed using the ivw rmvmr() and pleiotropy rmvmr() functions. Finally, the
282
plot rmvmr() function can be used to construct RMVMR plots. A flow chart showing
283
each step for applying RMVMR using the RMVMR software package is provided in the
284
supplementary material.
285
Outliers are detected based on their contribution to heterogeneity after adjustment,
286
and are calculated with respect to each individual exposure. The significance level for
287
identifying outliers can be defined by the user, and a data frame containing the
288
Q-statistics for each individual variant is provided as an output from the
289
pleiotropy rmvmr() function. Identified outliers can then be followed up using external
290
sources such as PhenoScanner or the MR Base online software platform [21,22]. The
291
RMVMR package builds upon the RadialMR and MVMR R packages, and can be used
292
in conjunction with data obtained using the MR Base platform. Further details for the
293
RMVMR software and installation instructions are available at
294
https://github.com/WSpiller/RMVMR.
295 4, 2023
11/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Results
296 Demonstrating the implementation of RMVMR through
297
simulation
298 To demonstrate the implementation and advantages of RMVMR a simulation study is
299
presented comprised of two components (simulations 1 and 2). In simulation 1 a single
300
data frame is generated and analysed. This serves to illustrate how RMVMR analyses
301
are implemented and interpreted in an individual case, including outlier detection and
302
plot construction. Simulation 2 considers estimates of causal effect obtained from 1,000
303
data frames, highlighting broader features of RMVMR. Code for replicating the
304
analyses is provided in the supplementary material, and the data set used in simulation
305
1 is the last of 1,000 generated for simulation 2. All RMVMR analyses are performed
306
using the RMVMR R package.
307
Each data frame is simulated so as to include N = 200, 000 observations of J = 240
308
instruments Gj, three exposures X1−3, a single unmeasured confounder U and an
309
outcome Y . The data were generated using a data generating model conforming to
310
equations (9 and 10). The set of instruments were generated so as to represent eight
311
equal groups based on their association with one or more exposures, and were assigned
312
arbitrary identification (rsid) numbers. Simulated groups of instruments include:
313 • Instruments associated with X1 only (group 1: rs1-30)
314 • Instruments associated with X2 only (group 2: rs31-60)
315 • Instruments associated with X3 only (group 3: rs61-90)
316 • Instruments associated with X1 and X2 (group 4: rs91-120)
317 • Instruments associated with X1 and X3 (group 5: rs121-150)
318 • Instruments associated with X2 and X3 (group 6: rs151-180)
319 • Instruments associated with X1, X2 and X3 (group 7: rs181-210)
320 • Instruments associated with X1, X2 and X3 with a direct effect on Y (group 8:
321
rs211-240)
322 Non-zero associations between instruments and exposures were randomly sampled
323
from a normal distribution with mean 0 and standard deviation 10. To ensure strong
324
instruments were used in the analysis, values with an absolute value less than 2 were
325
resampled. The effects of X1, X2 and X3 upon Y were defined as β1 = 1, β2 = 0.2, and
326
β3 = −0.5 respectively. Exposures were also simulated so as to be correlated, with a
327
correlation coefficient ranging from −0.5 to 0.5.
328
Instrument group 8 was subdivided into three equal groups, wherein instruments
329
with a direct effect on Y are associated with one of the three exposures X1−3. The
330
direct effects of instruments in group 8 were sampled from a normal distribution with
331
mean 10 and standard deviation 5, resampling to ensure parameters had an absolute
332
value greater than 2. Finally, each set of instrument-exposure estimates, as well as
333
instrument-outcome associations, were obtained from separate non-overlapping samples.
334 4, 2023
12/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Simulation 1: Demonstrating the application of RMVMR using a single data frame
335 Initially, univariable radial IVW models are applied using only instruments robustly
336
associated with each exposure (F-statistic > 10). Radial MR estimates for each
337
individual exposure are presented in Table 1, while univariable radial plots are provided
338
in the supplementary material.
339 Table 1. Causal effect estimates1 obtained using radial MR and RMVMR models
with differing exposure combinations2. Model
Estimate (se)
p-value
Q-statistic (p-value) Univariable Radial IVW X1
1.051 (0.041)
< 0.001
9242.04 (< 0.001) X2
0.697 (0.069)
< 0.001
24012.2 (< 0.001) X3
0.342 (0.076)
< 0.001
29652.94 (< 0.001) RMVMR (X1,X2) X1
0.971 (0.045)
< 0.001
623.81 (< 0.001) X2
0.142 (0.046)
0.002
621.41 (< 0.001) RMVMR (X1,X2,X3) X1
1.098 (0.039)
< 0.001
544.08 (< 0.001) X2
0.311 (0.041)
< 0.001
554.90 (< 0.001) X3
-0.422 (0.040)
< 0.001
557.61 (< 0.001) Pruned RMVMR (X1,X2,X3) X1
1.001 (0.010)
< 0.001
23.66 (> 0.999) X2
0.201 (0.011)
< 0.001
26.46 (> 0.999) X3
-0.497 (0.011)
< 0.001
22.90 (> 0.999) 1 True effects of each exposure: β1 = 1, β2 = 0.2, β3 = −0.5.
2 Sample size N = 200, 000 In Table 1 we can see that effect estimates exhibit substantial bias when estimated
340
using univariable radial IVW. The high Q-statistics estimated for each exposure provide
341
evidence of heterogeneity in estimates obtained using each instrument individually,
342
suggesting potential violations of assumption IV3. When using a two exposure RMVMR
343
model including exposures X1 and X2, there continues to be evidence of bias. In this
344
case the observed bias is smaller in magnitude, reflecting how adjustment for both X1
345
and X2 accounts for a proportion of the pleiotropic bias observed in univariable
346
analyses. This is expected given instrument groups 4, 7, and 8 are simultaneously
347
associated with exposures X1 and X2. This interpretation is further supported by a
348
notable decrease in observed heterogeneity for each exposure. For reference, the
349
estimated conditional F-statistics for X1 and X2 were 111.32 and 112.22 respectively.
350
The inclusion of exposure X3 adjusts for associations between instruments and the
351
outcome through X3 which are not mediated downstream by either X1 or X2. This
352
again results in a substantial decrease in heterogeneity, although the continued presence
353
of instruments from group 8 has the effect of inducing pleiotropic bias. The conditional
354
F-statistics for each exposure were 105.94, 98.21, and 103.46 respectively.
355
By removing instruments identified as outliers on the basis of their contribution to
356
global heterogeneity, it is possible to perform a pruned analysis using the iterative
357
approach previously described. Calculating the individual Q-statistic for each
358
instrument with respect to each exposure, a total of 16 SNPs are identified as outliers
359
using a p-value threshold of 0.05, shown in Figure 5. From Figure 5 it can be seen that
360
all identified outliers correspond to group 8 (rs211-240); instruments generated so as to
361
violate assumption MVMR3 by having a direct effect on the outcome Y . Consequently,
362
removing these instruments will have the effect of reducing pleiotropic bias in RMVMR
363
analyses. In Table 1 estimates obtained using the pruned RMVMR approach show no
364 4, 2023
13/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint evidence of bias or substantial heterogeneity. The conditional F-statistics for the pruned
365
analysis were 115.56, 107.42, and 114.11 for exposures X1, X2, and X3.
366 Fig 5. Scatter plot showing instruments which are identified as outliers
using the p-value for their contribution to observed heterogeneity. A dotted
line is shown representing the p-value threshold for identifying outliers (p < 0.05). All
instruments correspond to Group 8 (rs211-240) for which a directional pleiotropic effect
is present. Fig 6 shows RMVMR plots corresponding to the models adopted in simulation 1. In
367
Fig 6s observations do not appear to converge towards their respective effect estimates,
368
instead forming two widely dispersed clusters. The extent to which observations diverge
369
from their corresponding direct effect estimate is representative to their contribution to
370
global heterogeneity, and consequently serves as an indicator of MVMR3 violation.
371
In Fig 6b the inclusion of exposure X3 has the effect of substantially reducing global
372
heterogeneity. In this case, a pattern emerges where it is possible to visually identify
373
valid instruments defined in the simulation. The inclusion of instruments from group 8,
374
however, induces pleiotropic bias in causal effects. The continued presence of pleiotropic
375
instruments is indicated by the continued presence of substantial heterogeneity.
376
Following the systematic pruning of outliers, Fig 6c shows how remaining
377
instruments converge towards unbiased estimates of direct effect for each exposure.
378
Notably, the change of scale on the y-axis reflects the reduction in relative distance from
379
each observation to their respective superimposed regression line, and the absence of
380
substantial heterogeneity suggests an absence of pleiotropic bias. It is, however,
381
important to once again emphasise that such an interpretation is predicated on
382
MVMR3 violating instruments being identified as outliers, and care should be taken
383
when considering outlier removal.
384 4, 2023
14/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Fig 6. Panel showing radial MVMR plots corresponding to each of the
simulated analyses presented in Table 1. Fig 6a represents the two-exposure
model, while Fig 6b includes all measured exposures. Fig 6c, shows a plot generated
after pruning pleiotropic SNPs identified in Fig 5. Simulation 2: Evaluating the performance of RMVMR over multiple iterations
385 In simulation 1 attention was given to the implementation and interpretation of
386
RMVMR using a single data set. To provide a more concrete demonstration of how
387
RMVMR can lead to an overall reduction in pleiotropic bias, we repeat the previous
388
analyses using a total of 1,000 independent data sets. Mean effect estimates, standard
389
errors, and F-statistics are presented in Table 2, and illustrated in Fig 7.
390
When evaluating the performance of RMVMR across multiple iterations, we can see
391
that identifying and pruning outliers in this case consistently leads to a reduction in
392
pleiotropic bias. It is also interesting to highlight that conditional instrument strength
393
for each exposure is greater after removing outlying instruments. This is a consequence
394
of removing noise from instruments in group 8, such that the reduction in heterogeneity
395
affords more accurate prediction of each exposure using the remaining instruments.
396 4, 2023
15/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Table 2. Mean causal effect estimates1 obtained using RMVMR models across 1,000
independent data sets2) from simulation 2. Model
Mean estimate
Mean std.error
Mean F-statistic3 RMVMR (X1,X2,X3) X1
1.010
0.038
107.30 X2
0.311
0.041
98.12 X3
-0.418
0.040
103.50 Pruned RMVMR (X1,X2,X3) X1
0.999
0.010
117.01 X2
0.206
0.011
107.77 X3
-0.486
0.010
115.24 1 True effects of each exposure: β1 = 1, β2 = 0.2, β3 = −0.5.
2 Sample size N = 200, 000
3 Conditional F-statistic Fig 7. Forest plot showing mean effect estimates and 95% confidence
intervals for the effect of exposures X1, X2, and X3 following simulation 2.
Vertical dashed lines depict defined true effects, coloured by exposure. Initial estimates
indicate RMVMR estimates obtained prior to outlier pruning. When evaluating the performance of RMVMR across multiple iterations, we can see
397
that identifying and pruning outliers in this case consistently leads to a reduction in
398
pleiotropic bias, as well as a substantial increase in estimate precision. It is also
399
interesting to highlight that conditional instrument strength for each exposure is greater
400
after removing outlying instruments. This is a consequence of removing noise induced
401
by instruments in group 8, such that the reduction in heterogeneity affords more
402
accurate prediction of each exposure using the remaining instruments.
403 4, 2023
16/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Applied analysis: Lipid fractions and coronary heart disease
404 To demonstrate the RMVMR approach in an applied setting we consider the effects
405
of lipid fractions, specifically HDL, LDL, and triglycerides, upon CHD. SNP-exposure
406
estimates were obtained from previously published GWAS summary data, using data
407
from the Global Lipids Genetics Consortium presented in Willer et al [19]. Each lipid
408
fraction was recorded in mg/Dl, and standardised before GWAS were performed.
409
SNP-outcome associations were obtained from the CARDIoGRAMplusC4D Consortium
410
as presented in Nikpay et al [18]. CHD associations are presented on a log-odds scale,
411
and were obtained using logistic regression. All SNP associations were obtained using
412
the MRBase online platform [21]. For each exposure, univariable radial MR analyses are
413
performed, after which radial MVMR models including all exposures are fit to the data.
414
Univariable radial MR analyses were performed for HDL, LDL, and triglycerides,
415
using only SNPs identified as robustly associated with each exposure (p < 5 × 10−8).
416
Independent SNPs were selecting using a linkage disequilibrium clumping threshold of
417
R2 < 0.001, and palindromic SNPs were also removed prior to performing analyses.
418
This resulted in a total of 87 SNPs for HDL, 67 SNPs for LDL, and 40 SNPs for
419
triglycerides being selected for subsequent analyses. Effect estimates in Table 3 have
420
been transformed so as to be interpreted on an odds ratio scale. Mean F-statistics for
421
SNPs used in univariable analyses are presented in Table 3, and corresponding plots for
422
univariable analyses are provided in the supplementary material.
423 Table 3. Causal effect estimates obtained using radial MR and radial MVMR models,
estimating the effect of lipid fractions (HDL, LDL, and triglycerides) on CHD. Model
Estimate (se)
p-value
Q-statistic (p-value)
F-Statistic Univariable Radial IVW HDL1
0.829 (0.058)
0.001
444.90 (< 0.001)
121.6 LDL1
1.499 (0.055)
< 0.001
229.35 (< 0.001)
100.1 Triglycerides1
1.255 (0.062)
< 0.001
162.81 (< 0.001)
170.3 RMVMR HDL
0.899 (0.053)
0.046
144.24 (0.004)2
42.9 LDL
1.408 (0.057)
< 0.001
109.36 (0.020)2
39.2 Triglycerides
1.123 (0.064)
0.074
84.13 (0.055)2
29.1 Pruned RMVMR HDL
0.939 (0.042)
0.135
74.39 (0.910)2
43.6 LDL
1.399 (0.050)
< 0.001
40.13 (0.997)2
32.2 Triglycerides
1.146 (0.055)
0.014
46.16 (0.869)2
24.9 1 Estimates obtained using univariable radial MR.
2 Calculated using corrected ratio estimates Considering the univariable radial IVW analyses there appears to be evidence of a
424
positive association of LDL and triglycerides with CHD, in contrast to HDL which
425
shows evidence of a protective effect. However, evaluating heterogeneity across the
426
range of individual ratio estimates for each exposure indicates that one or more SNPs
427
may be pleiotropic, potentially violating assumption IV3. This is indicated by high
428
Q-statistics for each exposure, as seen in Table 3.
429
Previous research has highlighted how individual SNPs simultaneously associated
430
with multiple lipid fractions, coupled with an observed strong correlation between
431
phenotypes, can result in pleiotropic bias. To account for such relationships, it is
432
possible to fit a radial MVMR model incorporating each exposure simultaneously.
433
Radial MVMR estimates for each exposure are shown in Table 3, showing the effect of
434
each lipid fraction to be directionally consistent, though smaller in magnitude,
435
compared to univariable estimates. Assuming that the observed heterogeneity in
436 4, 2023
17/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint univariable analyses is the result of omitting one or more lipid fractions, performing a
437
correction for each observation and re-evaluating heterogeneity using radial MVMR
438
should show evidence of a substantial Q-statistic decrease. Table 3 shows a substantial
439
decrease in observed heterogeneity when fitting the radial MVMR model, though HDL
440
and LDL still show evidence of significant global heterogeneity using a Q-statistic
441
p-value threshold of p < 0.05.
442
As in the previous simulation analyses, it is possible to identify and remove SNPs
443
which contribute a substantial degree of heterogeneity within analyses. For this analysis,
444
SNPs with a significantly high Q-statistic for either HDL, LDL, or triglycerides were
445
omitted, subsequently estimating causal effects through an iterative process until no
446
outliers were detected. These results are shown in Table 3, where effect estimates
447
remain directionally consistent with the initial radial MVMR analysis. Performing the
448
pruned analysis results in effects of smaller magnitude for HDL and LDL, and the
449
effects of all exposures are estimated with greater precision. Importantly, the
450
conditional F-statistic for each exposure remains at a similar level to the initial radial
451
MVMR analysis, limiting the extent to which differences in estimation are the result of
452
bias due to weak instruments being used after pruning.
453
Removing SNPs which exhibit heterogeneity does not necessarily imply that
454
estimates will be less biased. If the majority of SNPs exhibit pleiotropic effects in a
455
similar direction and magnitude, it is possible that SNPs satisfying the MVMR
456
assumptions will be removed. To consider this possibility it is important to follow-up
457
identified outliers using external data, focusing on associations with phenotypes for
458
which a pleiotropic association is plausible. In the pruned analysis, a total of 17 SNPs
459
were identified and removed as outliers (see supplementary material). Using the
460
PhenoScanner online platform to evaluate potential pleiotropic pathways, there did not
461
appear to be a consistent pattern across the set of removed SNPs, though phenotypes
462
such as diastolic blood pressure are present [22].
463
The radial MVMR estimates are visualised in Figure 8, and adjusted Q-statistics are
464
presented in Table 3. The reduction in observed heterogeneity suggests that univariable
465
analyses exhibit bias when failing to account for pleiotropic associations through other
466
lipid fractions. A multivariable model would therefore appear to be a more effective
467
approach in this instance. The plots shown in Figure 8a and 8b show the estimates
468
obtained without pruning SNPs based on their heterogeneity contribution, while plots
469
8c and 8d show the plots constructed after removing observed outliers. In this case, the
470
reduction in global heterogeneity is clear and primarily reflected by the change of scale
471
on the y-axis, in combination with the data points for each exposure being substantially
472
closer to their corresponding superimposed regression lines.
473
When pruning for outliers the effect of HDL is greatly attenuated, showing no
474
evidence of an effect on CHD. LDL and triglycerides continue to show evidence of a
475
positive association, with LDL having the most substantial impact on CHD risk.
476
Provided a majority of valid SNPs with respect to their weighting have been used, this
477
would suggest that LDL and triglycerides represent promising targets for CHD
478
prevention.
479 4, 2023
18/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Fig 8. Panel showing RMVMR plots for applied analysis using HDL, LDL,
and triglycerides. Observations correspond to ratio estimates and weightings with
respect to each exposure. Regression lines represent MVMR causal effect estimates
coloured by exposure. Fig 8a-8b correspond to the radial MVMR estimates prior to
performing heterogeneity pruning, while Fig 8c-8d are constructed using heterogeneity
pruned summary data. Discussion
480 Radial MR and MVMR approaches facilitate the assessment of pleiotropic associations
481
between genetic variants and phenotypes using GWAS summary data. Conducting
482
analyses within a radial framework allows for outliers to be effectively visualised, and
483
MVMR analyses allow for the direct effects of multiple exposures to be estimated
484
simultaneously. Radial MVMR builds upon both these existing methods, providing a
485
means for visualising MVMR approaches absent until this point, and justifying the use
486
of MVMR where relevant exposure data are available. We propose that the radial
487
MVMR approach be used to assist in communicating key findings as a visual aid, and
488
also as a sensitivity analysis for identifying pleiotropic bias using adjusted heterogeneity
489
statistics.
490
Radial MVMR builds of existing work which leverages publicly available genetic
491
data to correct for pleiotropic bias. The work in this paper differs in providing further
492
diagnostic tools, as well as a means of visualising MVMR analyses. In this way, it is
493
potentially easier to identify specific SNPs which may serve as outliers in an analyses,
494 4, 2023
19/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint warranting subsequent follow up.
495
When implementing the radial MVMR approach it is crucial to consider the
496
underlying assumptions of MVMR. Instruments selected should be sufficiently strong so
497
as to overcome substantial weak instrument bias, estimated using the conditional
498
F-statistic. Specific to radial MVMR, the correction of individual ratio estimates is
499
reliant upon unbiased estimates of the direct effect of each exposure. As demonstrated
500
in the simulation study, in cases where direct effects exhibit biases the subsequent
501
correction for each SNP will be incorrect. However, this is unlikely to result in a
502
substantial reduction in heterogeneity unless the distribution of pleiotropic associations
503
is similar, and will likely still indicate that pleiotropic bias may be present. This is due
504
to the differing contribution of each SNP towards heterogeneity, as a consequence of
505
differences in their relationship within one or more pleiotropic pathways.
506
As previously emphasised, care should be taken to consider identified outliers and
507
phenotypic associations which could plausibly form horizontal pleiotropic pathways. If a
508
majority of instruments have direct effects upon an outcome, and the distribution of
509
such direct effects is similar, it is likely that SNPs satisfying the MVMR assumptions
510
will be identified as outliers. As a consequence, the removal of such SNPs would result
511
in estimates converging toward the biased estimate produced by such pleiotropic SNPs.
512
Decisions to down weight or remove outliers during an analysis should be made with
513
consideration of the biological mechanisms underlying observed SNP-phenotype
514
associations, and adequate justification. It is with this in mind that a general
515
heterogeneity pruning function has not been incorporated within the RMVMR R
516
package, though code for performing such analyses is provided in the supplementary
517
material and is available at https://github.com/WSpiller/RMVMR Analyses.
518
A further issue related to the applied analysis is the use of binary outcomes in
519
summary MR analyses. When using SNP-outcome associations estimated on a log-odds
520
scale, it is possible that causal estimates will be correlated with their precision,
521
introducing heterogeneity which is not a consequence of pleiotropic associations [6,23].
522
This issue, which is a wider issue within the summary MR literature, warrants careful
523
consideration prior to performing analyses, and care should be taken in evaluating as an
524
indicator of pleiotropy when a binary outcome is used [23].
525
Finally, it should be noted that while first-order weights have been used throughout
526
this paper, radial approaches allow for a wide-range of weighting options to be used. As
527
arbitrary weights can be used, should be possible for modified second order weights to
528
be incorporated with a radial MVMR model. Such weights may prove more effective
529
than first-order weights, as they incorporate the precision of SNP-exposure estimates
530
mitigating violations of the NO-Measurement Error (NOME) assumption in summary
531
MR [5]. Future work will explore how differing weight specifications can improve
532
estimation using radial MVMR.
533 4, 2023
20/23 . 
CC-BY 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288134
doi: 
medRxiv preprint Supporting information
534 Data availability
535 Scripts for reproducing the simulated examples in this paper, in addition to summary
536
GWAS data for applied analyses are available at
537
https://github.com/WSpiller/RMVMR Analyses. Summary GWAS data for applied
538
analyses is publicly available, and was obtained from the OpenGWAS catalogue
539
(https://gwas.mrcieu.ac.uk/).
540 Contributions
541 WS, JB, and ES contributed to the idea and development of the RMVMR method. WS
542
performed the analyses and created the RMVMR R package. WS drafted the
543
manuscript, and JB and ES provided critical input throughout the drafting process.
544 Competing interests
545 None of the authors have a competing interest to declare.
546 Acknowledgments
547 The authors would like to thank Dr Tom Palmer for his help in improving and
548
maintaining the RMVMR R package.
549",1
"Observational studies are rarely representative of their target population, be-
cause there are known and unknown factors that affect an individual’s choice
to participate (known as the selection mechanism). Selection can cause bias
in a given analysis, if the outcome is related to selection (conditional on the
other variables in the model).
However, the selection mechanism usually
cannot be detected from the observed data if we have no data on the non-
selected sample - for example, when the selected sample is participants in a
research study. Here, we develop methods to examine the selection mech-
anism by comparing correlations among variables in the selected sample to
those expected under no selection. We examine the use of four hypothesis
tests to identify induced associations between genetic variants in the selected
sample. We evaluate these approaches with Monte Carlo simulations. Fi-
nally, these approaches are demonstrated with an applied example, using
data from UK Biobank (UKBB), with alcohol intake as exposure to test the
presence of selection bias. The proposed tests have identified selection due
to alcohol intake into UKBB, and the subsample of individuals with weekly
alcohol intake. Analyses in UKBB with alcohol consumption as exposure or
outcome may be biased by this selection. Keywords: Selection bias; correlation; covariance. 1 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 1
Introduction In statistical inference, we usually assume the sample under analysis is repre-
sentative (i.e. a random subsample) of the target population. This assump-
tion could be violated by participants being non-randomly selected into the
study sample. This can occur when measured and unmeasured factors affect
initial study enrollment or loss to follow up, or when the analysis is limited to
a selected group (e.g. only those with no disease at baseline). The distortion
of the parameter estimate between the analysis sample and the true value in
the target population, is known as “selection bias” (Rothman et al., 2008).
In complete case analysis, i.e. analysis only using the individuals who
have complete data, selection causes bias in the estimated exposure regres-
sion coefficient if selection is related to both exposure and outcome (or just
outcome for linear regression) (J. W. Bartlett et al., 2015; Hughes, Heron,
et al., 2019). In Mendelian randomisation (MR), a popular method for esti-
mating causal effects using genetic variants that yields estimates unaffected
by unmeasured confounding of exposure and outcome, selection bias may
occur if selection is related to exposure or outcome (Hughes, Davies, et al.,
2019). Therefore in order to examine the likelihood of selection bias in a given
analysis, we need to examine whether exposure or outcome could cause se-
lection. It is possible to examine the association of measured variables with
selection, if these variables have also been measured in non-selected individ-
uals. If the selection is of participants into a study, then information on
non-participants is usually non-existent or minimal, and thus it is hard to
examine which variables are related to selection. However, causes of expo-
sure (or outcome) that are independent in the target population would be
dependent if exposure (or outcome) causes selection. Thus correlations be-
tween these causes of exposure/outcome observed within the sample would
indicate that it was plausible that exposure/outcome caused selection, and
that therefore the exposure-outcome analysis may be biased.
Most studies are prone to non-random selection, for example one of the
largest cohort studies, UK Biobank (UKBB), has been shown to differ from
the UK population in various characteristics (Fry et al., 2017). UKBB is
often used for genetic analyses, and in particular MR, and some analyses
have been found to be biased by selection (Munaf`
o et al., 2018). For ex-
ample, the association between alcohol intake and cardiovascular disease has
been found to be under-estimated in UKBB (Stamatakis et al., 2021). One
aspect of genetic studies that can help with the detection of selection bias is
that different unlinked genetic variants (i.e. those on different chromosomes)
should (after quality control) be independent (Pirastu et al., 2021). In other
words, genetic variants should be independent in the target population. If 2 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint U X S Y
G2 G1 Gk Figure 1: Causal diagrams representing the hypothesized relationship be-
tween p genetic instruments, Gj (j = 1, ..., p), exposure (X), outcome (Y ),
selection variable (S), and all unmeasured variables (U) which confound X
and Y . they are found to be correlated in a study sample, one reason for this would
be selection bias. More specifically, shown by Figure 1, suppose we have a
population of individuals, i = 1, .., N. Let the variable X be caused by p
independent genetic variants Gj (j = 1, ..., p). We define S = 1 if a partici-
pant is selected into the study and S = 0 if they are not. Conditioning on S
induces associations between all causes of S, hence the genetic variants Gj
will be correlated in the selected sample.
Therefore, to detect selection bias we can examine associations between
variables that should be independent in the target population (e.g. genetic
variants causing a given phenotype). One approach would be to examine
every pairwise correlation coefficient, but this would require many tests to
be conducted, and thus correction for multiple testing (Larzelere & Mulaik,
1977). Therefore, we have focused on approaches that test homogeneity in
correlation/covariance matrices directly. M. S. Bartlett, 1951, Jennrich, 1970
and Steiger, 1980 proposed test statistics for comparing correlation matrices.
(Box, 1949) compares covariance matrices. Our aim is to examine the use
of these four hypothesis tests to examine induced associations between ge-
netic variants in a selected sample. We consider two scenarios for identifying
selection; ‘one sample’ or ‘two samples’. The former examines the evidence
for a single sample not being a random subsample of the target population,
by comparing the observed correlation matrix to that expected in the target
population (the identity matrix). The latter examines the evidence for two 3 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint different samples not being from the same population. This is particularly
relevant in the case of two-sample MR, which derives causal effect estimates
with summary statistics obtained from two separate samples - one provides
the SNP-exposure associations and the other provides the SNP-outcome as-
sociations.
A key assumption is that both samples come from the same
underlying population (Bowden et al., 2017). With full individual patient
data on both samples one could assess which factors were associated with
selection into each sample - but with only summary data, as are usually
available, this cannot be done. Comparison of correlations between genetic
factors could instead be used to assess the plausibility of this assumption.
We begin with a motivating example using a MR study from UK Biobank
(UKBB). Section 2 will introduce the different hypothesis testing approaches.
Section 3 will describe the method of simulation using the ADEMP frame-
work (Morris et al., 2019) to evaluate these approaches. Section 4 will give
the results of Monte Carlo simulations.
Finally these approaches will be
applied to the motivating example to test for the presence of selection bias. 1.1
Motivating Example: alcohol consumption in UK
Biobank A recent MR study showed that alcohol consumption has a causal effect
on risk of having stroke (Larsson et al., 2020). The instruments used were
single nucleotide polymorphisms (SNP) hits from a Genome-wide association
study (GWAS) on alcohol use (Liu et al., 2019), in which the authors defined
“alcohol use” as weekly alcohol intake. However in UKBB, an individual’s
weekly alcohol intake is only measured if the participant indicated that their
alcohol consumption was “more often than once or twice a week”. This meant
that the complete case sample excluded the 35% of total sample of UKBB
(total N=501,532) who did not have weekly alcohol intake measured. We
hypothesise that in the selected sample (those with alcohol intake measured),
the SNPs associated with alcohol frequency will be correlated with each other.
This means that in UKBB an analysis using alcohol use as the outcome
(including the first stage of an MR analysis with alcohol use as the exposure)
would be prone to selection bias. 2
Methods We first describe the assumed model and define the correlation and covariance
matrices for the one sample case, then for two sample case. 4 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint In the one sample case, suppose that we have p variables , V = (V1, . . . , Vp)T following some distribution with mean µ = (µ1, . . . , µp) and covariance ma-
trix Σ = (σi,j)p×p.
For a sample of n individuals, the sample mean is ¯
v = 1 n
Pn
k=1 Vk and sample covariance matrix ˆ
Σ = (ˆ
σi,j)p×p is defined as ˆ
Σ = 1 n n
X k=1
(Vk −¯
v)(Vk −¯
v)T.
(1) The sample correlation matrix is defined as ˆ
R = (ˆ
ri,j), and is estimated
from the sample covariance matrix; ˆ
ri,j =
ˆ
σi,j
p ˆ
σi,iˆ
σj,j
,
1 ≤i ≤j ≤p
(2) In the two samples case, we have two independent random samples,
V1, . . . , Vn1 and D1, . . . , Dn2, both are i.i.d. from a p-variate distribution
with µ1 = (µ1,1, . . . , µp,1)T and covariance matrix Σ1 = (σi,j,1)p×p, and with
µ2 = (µ1,2, . . . , µp,2)T and Σ2 = (σi,j,2)p×p respectively. Then we define the sample means by ¯
v =
1 n1 Pn1
k=1 Vk and ¯
d =
1 n2 Pn2
k=1 Dk and the sample covariance matrices ˆ
Σ1 = (ˆ
σi,j,1)p×p and ˆ
Σ2 = (ˆ
σi,j,2)p×p by ˆ
Σ1 = 1 n1 n1
X k=1
(Vk −¯
v)(Vk −¯
v)T
and
ˆ
Σ2 = 1 n2 n2
X k=1
(Yk −¯
d)(dk −¯
d)T. (3) Let Rl = (ri,j,l) be the correlation matrices of l sample, where l = 1, 2.
Then the sample correlation matrices are ˆ
Rl = (ˆ
ri,j,l)p×p with ˆ
ri,j,l =
ˆ
σi,j,l
p ˆ
σi,i,lˆ
σj,j,l
,
1 ≤i ≤j ≤p,
l = 1, 2.
(4) If we have external information that the correlation matrix in the target
population would be the identity matrix (e.g. if the V = (V1, . . . , Vp)T are
genetic variants, which are expected to be independent after quality control)
then the null hypothesis for testing for correlation of the V = (V1, . . . , Vp)T in the one sample case; I. H0 : R = I For a comparison of the associations between two samples, the null
hypothesis is; II. H0 : R1 = R2 or H0 : Σ1 = Σ2 5 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint where I is the identity matrix. H0 : R = I and H0 : Σ = σ2I are es-
sentially equivalent as they both test whether the off-diagonal of the matrix
is zero. This one sample equivalence of the correlation matrix to the iden-
tity matrix is formally known as the “Identity Hypothesis”. For (II), we
assume that there are no overlapping individuals between the two samples.
As hypothesis testing (I) requires one sample and (II) requires two, we here-
after refer (I) and (II) as hypothesis testing for one-sample and two-sample
respectively.
All of the following tests are χ2 tests (or likelihood ratio test) and com-
monly used for low-dimensional settings i.e. p < n (Cai, 2017). 2.1
Testing the identity hypothesis for one sample 2.1.1
Bartlett test (M. S. Bartlett, 1951) has proposed the following test statistic; χ2
B = −

n −1 6(2p + 5)

log| ˆ
R| which is assumed to be distributed as a χ2 with p(p−1)/2 degrees of freedom. 2.1.2
Jennrich test The Jennrich, 1970 test defines ¯
R = ( ˆ
R + I)/2, S = I + ( ¯
R ¯
R−1), c = n/2
and Z = √c ¯
R−1( ˆ
R −I). Then χ2
J = 1 2tr(Z2) −diag(Z)TS−1diag(Z) with p(p −1)/2 degrees of freedom. The first term on the right is testing the
equality of the covariance matrices, and second term is a correction term for
testing correlation matrices. 2.1.3
Steiger test Steiger, 1980 described the test statistic for one sample as, χ2
S = (n −3)
X j<i
z2
ij, where zij = 1 2log
(1 + ˆ
rij) (1 −ˆ
rij)  with p(p −1)/2 degrees of freedom. For small samples, this test statistic
performs better than Jennrich’s, as the Fisher’s r-to-z transformation ensures
the correlation coefficients are normally distributed (Neill & Dunn, 1975). 6 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint 2.2
Testing the equality of correlation/covariance ma-
trices from two samples 2.2.1
Box’s M test Box, 1949 proposed the M test; χ2
M =(1 −q)M with degrees of freedom p(p + 1)/2. M is the test statistic and q is the scale
factor that ensures the test statistic is asymptotically distributed as χ2 even with small samples. M and q are derived from vl = nl −1, ¯
s =
P2
l=1 vl ˆ
Σl
P2
l=1 vl
, resulting in; M = 2
X l=1
vllog|¯
s| − 2
X l=1
(vllog|ˆ
Σl|), q =2p2 + 3p −1 6(p + 1) 2
X l=1 1 vl
−
1 n1 + n2 −2 ! . Note that Box, 1949’s M test can test the equivalence of multiple covariance
matrices (defined by l), however we have simplified the test statistics to test
the equivalence of two covariance matrices. 2.2.2
Jennrich test Jennrich, 1970 described testing the off-diagonal of the difference between
two correlation matrices (R1 −R2) against the zero matrix. Let ¯
R = (n1 ˆ
R1 +
n2 ˆ
R2)/(n1 + n2), S = I + ( ¯
R ¯
R−1), c = n1n2/(n1 + n2) and Z = √c ¯
R−1( ˆ
R1 −
ˆ
R2). Then χ2
J = 1 2tr(Z2) −diag(Z)TS−1diag(Z) with p(p −1)/2 degrees of freedom. As with testing the identity hypothe-
sis for one sample, the first term on the right is testing the equality of the
covariance matrices, and second term is a correction term for testing corre-
lation matrices. If the two samples are independently drawn from the same
underlying population, then as n1, n2 →∞, ¯
R will tend to the population
correlation matrix R and the difference between R1 and R2 will tend to zero
(a matrix with all its elements equal to zero). 7 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint 2.2.3
Steiger test For testing the difference between two sample correlation matrices, the Steiger,
1980 test simply becomes χ2
S = (N −3)
X j<i
X2
ij with p(p−1)/2 degrees of freedom, where Xij = zij1−zij2, zij1 = 1 2log
(1 + ˆ
rij1) (1 −ˆ
rij1) 
, zij2 = 1 2log
(1 + ˆ
rij2) (1 −ˆ
rij2) 
and N =
n1n2 n1 + n2
. 3
Simulation study plan This section follows the ADEMP framework (Morris et al., 2019). 3.1
Aims The simulation study aims to compare the performance (in terms of type-I
and type-II error) of Bartlett, Jennrich, Steiger and Box’s M test for testing
the correlation/covariance matrix of a set of genetic variants (SNPs), varying:
sample size (N); amount of variance in exposure (X) explained by these
SNPs (R2
GX), and number of SNPs (p) with various minor allele frequencies
(MAF). The simulations use ranges for R2
GX and N that are typically seen
in Mendelian randomisation studies. 3.2
Data-generating mechanisms The genotypes for SNP k with specified MAF, fk, are created first by sim-
ulating a latent variable ZSNPk ∼N(0, 1), then coding the genotype as 0 if
Φ(ZSNPk) < (1 −fk)2, 1 if (1 −fk)2 < Φ(ZSNPk) < 1 −f 2
k, and 2 otherwise,
where Φ() is the standard normal integral. The MAFs from all the SNPs are
simulated from uniform distribution of Unif(0.1, 0.5).
Each simulated dataset has information on p SNPs (Gk) and phenotype
(X) on N individuals. X is normally distributed and R2
GX is the proportion
of variance of X explained by the p SNPs. The following describes their
relationship; Xi = α0 + p
X k=1
αGkGki + ϵxi 8 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint where ϵxi is independent random errors of X distributed as N(0, 1), i =
1, . . . , N. As Gs explain R2
GX of the variance in X, the remaining 1−R2
GX is
explained by ϵxi and their regression coefficients are calculated accordingly.
Two types of selection (i.e. selection into the study sample, by having X
observed) will be simulated: • selection completely at random (SCAR); • selection at random, conditional on X (SAR). Within each simulated dataset, 60% of the N individuals are selected (i.e.
have exposure data observed). For SCAR, 60% of individuals were randomly
selected. For SAR, the following gives the probability of participants being
selected (Hughes, Davies, et al., 2019): P(partcipant i selected) = expit{η0 + ηxXi}, where expit{w} =
exp{w} 1 + exp{w}. The parameters η0 and ηx are chosen to give mean probability of selection of
0.6 and standard deviation of 0.2, which reflects 60% of N individual having
exposure data.
Table 1 gives the summary of varied and default parameters for different
scenarios. Each scenario will be simulated 1,000 times. Table 1: Summary of simulation scenarios. p, number of Gs; N, sample size;
R2
GX, total variance explained by Gs on X. Scenario
p
R2
GX
N One sample testing
1
50
0.45
2000-10000
2
10-90
0.45
8000
3
50
0.05-0.45
8000
4
50
0.05
10000-400000
5
10-90
0.05
8000
Two sample testing
6
50
0.45
5000-20000
7
10-90
0.45
10000
8
50
0.05-0.45
10000
9
50
0.05
10000-400000
10
10-90
0.05
10000 9 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint 3.3
Estimands Our estimand is the p-value from testing the null hypotheses described in
Section 2. 3.4
Methods In the one-sample case the correlation matrix from selected individuals (those
with exposure observed) was compared to the identity matrix using the fol-
lowing methods (as described in Section 2): 1. Bartlett; 2. Jennrich; 3. Steiger. For comparing correlation/covariances from two samples, the correla-
tion/covariance matrix for the unselected sample (individuals without expo-
sure observed) is compared to the correlation/covariance matrix from the se-
lected sample (individuals with exposure observed) using the following meth-
ods (detailed in Section 2): 1. Box’s M; 2. Jennrich; 3. Steiger. 3.5
Performance measures The performance is measured by the proportion of the simulated datasets
for which the p-value for the specified test was < 0.05. Where there is no
selection (SCAR), this should be 0.05, i.e. the nominal type-I error rate.
Where there is selection based on exposure (SAR), an ideal test would have
a high proportion of tests giving a low p-value. 4
Results 4.1
Testing the identity hypothesis using one sample When testing the null hypothesis that a correlation/covariance matrix is
equal to the identity matrix, Steiger, Jennrich and Bartlett tests had the 10 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint correct nominal 5% Type I error rate (T1E) under SCAR for all scenarios
(Table 2). Under SAR, for all three methods, the proportion of tests cor-
rectly rejecting the null hypothesis increases with sample size and variance
explained, and decreases with number of Gs. All methods correctly reject
the null hypothesis in all 1,000 simulated datasets when the sample size and
variance explained are large, and number of Gs are small.
We further investigated whether the proportion of tests which correctly
reject the null hypothesis when the data are truly SAR improves with greater
sample size and fewer Gs when the variance explained is kept at 5% (Scenario
4). Increasing sample size increases the proportion of tests correctly rejecting
the null hypothesis for each of the three methods, however the sample does
have to be greater than 400,000 to gain nominal power of 0.8 in our scenario.
Reducing the number of Gs from 90 to 10 only increased the proportion
of tests correctly rejecting the null hypothesis in SAR by 0.04 for all three
methods (Supplementary Table S1). 4.2
Testing the equality of two sample correlation/ co-
variance matrices When testing equality of two sample correlation/covariance matrices under
SCAR , all three methods (Steiger, Jennrich and Box’s M) had approximately
the nominal 5% T1E (Table 3) in all scenarios. Under SAR, the proportion
of the Steiger and Jennrich tests correctly identifying a difference between
the two matrices, i.e. with p-value< 0.05 was low. Even with a large number
of SNPs explaining a high proportion of the variance, the proportion of tests
with p-value< 0.05 was only slightly above the nominal 0.05 level. Box’s M
test correctly rejects the null hypothesis in all 1,000 simulated datasets in all
scenarios, except when variance explained by the genetic variants is small.
The simulated datasets in Scenario 9 have the low variance explained
and large sample size that is typically seen in MR study. The number of
simulated datasets in which the test correctly rejects the null hypothesis did
not increase with sample size for either Steiger or Jennrich’s methods. In all
scenarios, Box’s M test has 4% T1E and correctly rejects the null hypothesis
in all 1,000 simulated datasets under SAR , except when sample size is small.
Fewer Gs did not improve the number of simulated datasets for which the
test correctly rejects the null hypothesis for Steiger or Jennrich’s methods,
but did for Box’s M test (Supplementary Table S2).
Both Steiger and Jennrich methods performed poorly for all scenarios
when testing equality of the correlation/covariance matrices from two sam-
ples. To investigate the reasons for this, we simulated a further three scenar- 11 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint Table 2: Proportion of tests with p-value < 0.05 in 1,000 simulated datasets
when testing the identity hypothesis using one sample. Miss., Missingness;
SCAR, selection completely at random; SAR, selection at random, condi-
tional on X; p, number of Gs; N, sample size; R2
GX, total variance explained
by SNPs on X. Test Stats.
Bartlett
Jennrich
Steiger
Miss.
SCAR
SAR
SCAR
SAR
SCAR
SAR Scenario 1: N (p = 50 and R2
GX = 0.45)
2000
0.04
0.24
0.04
0.19
0.04
0.19
4000
0.06
0.61
0.06
0.50
0.06
0.49
6000
0.05
0.90
0.04
0.79
0.04
0.79
8000
0.05
0.99
0.05
0.95
0.05
0.95
10000
0.05
1.00
0.06
0.99
0.05
0.99
Scenario 2: p (R2
GX = 0.45 and N = 8000)
10
0.06
1.00
0.06
1.00
0.06
1.00
30
0.06
1.00
0.05
1.00
0.05
1.00
50
0.06
0.98
0.05
0.94
0.05
0.94
70
0.05
0.88
0.05
0.76
0.05
0.76
90
0.04
0.70
0.04
0.54
0.04
0.54
Scenario 3: R2
GX (p = 50 and N = 8000)
0.05
0.06
0.06
0.06
0.06
0.06
0.05
0.25
0.05
0.25
0.06
0.25
0.06
0.29
0.45
0.05
0.95
0.05
0.95
0.06
0.99
Scenario 4: N with low R2
GX (p = 50 and R2
GX = 0.05)
10000
0.05
0.06
0.06
0.06
0.06
0.06
100000
0.05
0.14
0.05
0.14
0.05
0.14
200000
0.05
0.28
0.06
0.28
0.06
0.28
400000
0.05
0.64
0.05
0.63
0.05
0.63 ios with larger variance explained (>0.45) and lower number of instruments
(<20), as shown in Supplementary Table S3.
Both Jennrich and Steiger
test correctly rejected the null hypothesis in 81% and 62% of the simulated
datasets once sample size is greater than 10,000, 5 instruments and when
variance explained by the genetic variants is 0.9 (Supplementary Table S4). 12 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint Table 3: Proportion of tests with p-value < 0.05 in 1,000 simulated datasets
when testing the equality of correlation/covariance matrices from two sam-
ples. Miss., Missingness; SCAR, selection completely at random; SAR, se-
lection at random, conditional on X; p, number of Gs; N, sample size; R2
GX,
total variance explained by SNPs on X. Test Stats.
Box’s M
Jennrich
Steiger
Miss.
SCAR
SAR
SCAR
SAR
SCAR
SAR Scenario 6: N (p = 50 and R2
GX = 0.45)
5000
0.04
1.00
0.08
0.07
0.05
0.04
10000
0.04
1.00
0.06
0.06
0.04
0.04
15000
0.04
1.00
0.06
0.05
0.05
0.04
20000
0.03
1.00
0.06
0.05
0.05
0.04
Scenario 7: p (R2
GX = 0.45 and N = 10000)
10
0.04
1.00
0.05
0.06
0.05
0.05
30
0.03
1.00
0.04
0.05
0.04
0.04
50
0.05
1.00
0.07
0.08
0.06
0.06
70
0.03
1.00
0.08
0.08
0.05
0.04
90
0.05
1.00
0.10
0.11
0.05
0.05
Scenario 8: R2
GX (p = 50 and N = 10000)
0.05
0.03
0.70
0.05
0.06
0.04
0.04
0.25
0.04
1.00
0.07
0.07
0.04
0.05
0.45
0.04
1.00
0.07
0.05
0.05
0.04
Scenario 9: N with low R2
GX (p = 50 and R2
GX = 0.05)
10000
0.03
0.70
0.05
0.06
0.04
0.04
100000
0.04
1.00
0.06
0.06
0.06
0.06
200000
0.04
1.00
0.05
0.05
0.05
0.05
400000
0.04
1.00
0.05
0.04
0.04
0.04 5
Applied data example:
alcohol consump-
tion in UK Biobank Our motivating example of alcohol consumption in UKBB was described in
the Introduction. Here we will detail the methods of selection of instruments
and quality control excluding SNPs with low MAF, in Hardy-Weinberg dis-
equilibrium and linkage disequilibrium. We then describe how we use the
genetic instruments associated with alcohol consumption, BMI, and a ran-
dom set of genetic variants to explore: 1) selection bias in UK Biobank (by
using the one sample methods to compare the correlation matrix between
variants within UK Biobank to the identity matrix and 2) selection bias in 13 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint the analytical sample of those with alcohol consumption measured, by com-
paring the correlation matrices between those with and without the exposure
measured. 5.1
Method A large meta-analysis of GWAS (N = 941, 280) identified 99 SNPs associ-
ated with weekly alcohol intake (Liu et al., 2019). As negative controls, we
randomly selected 99 independent SNPs from across the genome with UKBB
(after quality control). As a positive control we selected 82 previously identi-
fied body mass index (BMI) SNPs (Locke et al., 2015). The former (random
SNPs) are unlikely to be related to selection either into UKBB or by having
weekly alcohol intake measured. For the latter (BMI SNPs), there is evidence
that obesity levels in UKBB are lower than in the general population (Fry
et al., 2017) and thus we expect these SNPs to be correlated in the UKBB
sample. We do not expect selection into the sample with alcohol measured
to depend on BMI, therefore the correlation matrics between BMI SNPs in
those with and without alcohol consumption measured should not differ.
There are in total 391,872 individuals with genotype data in UKBB after
restricting the sample to European, unrelated individuals and imputation ac-
curacy greater than 0.8 (Mitchell et al., 2019). 28, 30 and 27 SNPs remained
for weekly alcohol intake, random and BMI respectively, when restricted to
SNPs that have MAF between 0.1 and 0.5, in Hardy-Weinberg equilibrium
and not in linkage disequilibrium (defined as r2 =0.01, r2 is estimated from
1000 Genomes European population). In total, the alcohol intake SNPs ex-
plain 1% of the variance of alcohol intake, and BMI SNPs explain 2% of
the variance of BMI. 45.4% of individuals with genotype data have weekly
alcohol intake observed and 99.7% have BMI measured. See Supplementary
Table S5, S6 and S7 for phenotypic associations with the three sets of SNPs
used. Calculations for linkage disequilibrium structure from 1000 Genomes
Project and GWAS associations are from TwoSampleMR R package (version
0.5.6).
For testing the identity hypothesis using one sample, the correlation/
covariance from all individuals in UKBB is tested against the identity matrix.
Here we are testing for the presence of selection bias in UKBB as a whole.
For hypothesis testing for two samples (i.e. equality of the two correlation/
covariance matrices), the matrix derived from the selected sample (those with
alcohol intake measured) is tested against the matrix from the non-selected
sample (those without alcohol intake measured).
Here we are testing for
the association of alcohol consumption with having weekly alcohol intake
observed.
We know from the methodology for UKBB that this selection 14 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint mechanism is operating, so the aim of this analysis is to see how well the
correlation methods detect this known selection mechanism. We have only
used Box’s M test for two samples; as demonstrated by the simulations when
variance explained is small (Scenario 9) both Jennrich and Steiger test cannot
differentiate between missing completely at random and missing at random
condition on X. 5.2
Results For testing the identity hypothesis in one-sample, shown in Table 4, all the
tests demonstrate no correlation between randomly selected SNPs within
the full sample. All the tests demonstrate evidence for correlation between
alcohol intake and BMI SNPs in the full sample, which suggests that there
is selection on alcohol intake and BMI into UKBB related to each of the two
sets of SNPs. Table 4: p-value from testing the identity hypothesis in one sample with
entire sample of UKBB. Alco.
Alcohol intake SNPs and Rand.
random
SNPs. Test/SNPs
Alco.
Rand.
BMI Bartlett
<0.001
0.1930
<0.001
Jennrich
<0.001
0.1951
<0.001
Steiger
<0.001
0.1951
<0.001 When testing equality of correlation/covariance matrices from two sam-
ples, Box’s M test found a difference (p-value < 0.001) in covariance matrices
between individuals that have weekly alcohol intake measured and unmea-
sured (i.e. between those with low alcohol frequency and those with higher
alcohol frequency). Box’s M test gave no evidence against equality of cor-
relation/covariance matrices between those with and without weekly alcohol
data for randomly selected and BMI SNPs (p-value is 0.6487 and 0.1739
respectively).
In conclusion, all the one-sample tests and the two-sample test demon-
strated that there is evidence for selection bias due to alcohol intake in UKBB
when analysing either the full dataset, or only individuals that had weekly
alcohol intake measured. 15 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint 6
Discussion The Monte Carlo simulation demonstrated that for testing the identity hy-
pothesis using one sample, Steiger, Jennrich and Bartlett tests are able to
reject the identity hypothesis in the presence of selection and have nominal
type I error rates when selection is completely at random, with large sample
size, low number of SNPs and higher proportion of variance explained by the
SNPs. When testing equality of correlation/covariance matrices from two
samples, Box’s M test is the only approach that is able to detect a differ-
ence when selection is conditional on X and has nominal type I error rates
when selection is completely at random. We would not recommend Jennrich
or Steiger for testing the equality of correlation/covariance matrices from
two samples, because as shown by the simulations even with large sample
size both test statistics could not distinguish selection completely at random
from selection at random. Furthermore, we recommend checking the vari-
ance explained by the SNPs, as none of the one-sample tests perform well at
detecting selection mechanisms with low variance explained, except with a
large sample size.
One previous Monto Carlo simulation study also showed that the propor-
tion of simulations in which the Steiger or Bartlett tests correctly reject the
null hypothesis increases with sample size (Brown & Forsythe, 1974). This
study also found that Bartlett’s test is sensitive to non-normality (Brown &
Forsythe, 1974; Layard, 1973), with an inflated type I error if the variables
were from a heavy-tailed error distribution. However, Box’s M test is robust
to non-normality (Box, 1953; Layard, 1973). Yang and DeGruttola (2012)
have developed a bootstrap version of Bartlett’s test statistic in attempt to
reduce inflated type I error and heavy tailed error distribution. Their Monte
Carlo simulation used small sample sizes, however, and in GWAS the sample
sizes are usually in thousands which will substantially increase computa-
tional time to run their approach. Within a GWAS setting, Jennrich’s test
statistics have been used for detecting whether shared risk variants of two
traits are the result of “subgroup heterogeneity” or “whole-group pleiotropy”
(Han et al., 2016). (Han et al., 2016) have also found Jennrich’s test to have
low power and modified it by including allele frequency and effect sizes as
weights. The test statistics reviewed within our study are designed for low-
dimensional setting i.e. the number of variables is less than the sample size.
For an extensive review of test statistics for the high-dimensional setting see
Zheng et al., 2019.
In our applied example, we identified selection into both UKBB, and the
subsample with alcohol intake measured. The design of the questions for
UKBB means that only those with higher alcohol consumption had alcohol 16 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint intake measured. The correlation test used correctly identified that the SNPs
for alcohol consumption were correlated differently in the sample with data
on alcohol intake than in the sample without. This has implications for any
analyses using the alcohol intake measure as either an exposure or an out-
come. In this example, the selection mechanism was known (by the design
of the survey), and we were using the correlation analysis as a proof of prin-
ciple. In cases where the selection mechanism is not known, then comparing
correlation matrices for genetic variants related to exposure and outcome
between those selected and not selected would help to identify whether ex-
posure or outcome cause selection, and therefore the likelihood of selection
bias. In the analysis of potential selection into UKBB, the correlation test
implied that the SNPs for both alcohol consumption and BMI were corre-
lated within the UKBB sample. Both alcohol consumption and obesity have
been shown to be differently distributed in UKBB participants than in the
general population (Fry et al., 2017), suggesting that the correlation test
is indeed identifying selection mechanisms. Analyses in UKBB with either
BMI or alcohol consumption as exposure or outcome may be biased by this
selection - for a specific analysis, further examination of the other factors
involved in the selection mechanism would be required.
These test statistics require complete-case analysis, as only individuals
with an observation for every variable in the correlation matrix will be in-
cluded in the analysis. Missing SNP data is usually due to poor DNA quality
or quantity or technical fault (Pompanon et al., 2005) so the missingness is
likely to be completely at random and therefore complete case analysis will
not cause bias. However, a complete case analysis would have reduced sam-
ple size and thus lower power. Alternatively, if a few SNPs are missing for
multiple individuals, then these SNPs could be removed from the analysis.
Or, as a third option, the values of the missing SNPs could be imputed, based
on the values of SNPs in linkage disequilibrium.
A key advantage of these methods is that they can be used to detect
selection (and thus the likelihood for selection bias) even without data on the
unselected sample (e.g. by using a correlation matrix from 1000 Genomes
Project). In order to identify whether a given analysis is likely to be biased
by selection, the selection mechanism must be known (Hughes, Heron, et
al., 2019). Usually this is impossible without data on the unselected group.
However, if genetic data (or other variables that are known to be independent
in the target population) are available, then our proposed use of correlations
allows identification of selection using just the observed data.
A researcher with a given analysis question should use knowledge about
missing data/selection to identify which selection mechanisms would bias
their results. For example, in an MR of alcohol intake on BMI, if selection 17 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint was related to alcohol intake, this would bias the results Hughes, Davies,
et al., 2019. This could be investigated further by taking SNPs identified as
related to the exposure and outcome (here, SNPs related to alcohol intake and
to BMI) and examining their correlation in the analysis sample. Correlation
between these SNPs would indicate that the selection mechanism was caused
by those SNPs, and thus by implication caused by the exposure/outcome,
and thus that the complete case analysis may be biased.
Another use of these correlation tests is in two-sample MR. The two-
sample test for equality of correlation/covariances is one way to examine
whether the two samples are from the same underlying population. Even
for correlated genetic instruments, two-sample tests can be used to test this
assumption.
Selection is not the only possible cause of correlation between genetic
variants in a given sample. Assortative mating is another potential expla-
nation. Previous work used genetic correlation to detect assortative mating
for genetically predictive traits within the population (Yengo et al., 2018),
where under the null hypothesis of random mating, the correlation is zero
between alleles on different chromosomes. Detecting assortative mating in
MR can be realised through family data, however GWAS family data are
sparse and within-family analysis usually lacks power as it requires many
mother-father-offspring trios (Howe et al., 2022). For the two-sample test, if
assortative mating is the same in two samples being compared (e.g. if they do
come from the same population), then this would not cause the correlation
matrices from the two samples to differ. In the one-sample test, assortative
mating and other forms of selection will not be separable as the one-sample
test assumes independence between the SNPs within the underlying popula-
tion.
In conclusion, we recommend Steiger, Jennrich and Bartlett’s test for one
sample and Box’s M test for two sample as sensitivity analyses to identify
selection bias in studies where data on genetic variables are available, and
to examine the assumption that two samples come from the same underly-
ing population in two-sample MR studies.These hypothesis tests should be
examined along side other evidence for selection or assortative mating.These
tests will be particularly useful where there are no data on the unselected
sample (e.g. for UK Biobank participation). Acknowledgments We thank the reviewers and associate editor for providing comments and
suggestions to improve this paper. All authors works in a unit that receives 18 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint support from the University of Bristol and the UK Medical Research Council
(MC UU 00011/1 MC UU 00011/3). This study was conducted under UK
Biobank application 66074. Conflict of Interest: None declared. 19 All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.04.23288120
doi: 
medRxiv preprint",1
"Introduction As health reforms move Ireland from a mixed public-private system toward universal healthcare, it is important to understand variations in prescribing practice for patients with differing health cover and socioeconomic status. This study aims to determine how prescribing patterns for patients aged ≥65 years in primary care in Ireland differ between patients with public and private health cover. Methods This was an observational study using anonymised data collected as part of a larger study from 44 general practices in Ireland (2011-2018). Data were extracted from electronic records relating to demographics and prescribing for patients aged ≥65 years. The cohort was divided between those with public health cover (via the General Medical Services (GMS) scheme) and those without. Standardised rates of prescribing were calculated for pre-specified drug classes. We also analysed the number of medications, polypharmacy, and trends over time between groups, using multilevel linear regression adjusting for age and sex. Results Overall, 42,456 individuals were included (56% female). Most were covered by the GMS scheme (62%, n=26,490). The rate of prescribing in all medication classes was higher for GMS patients compared to non-GMS patients, with the greatest difference in benzodiazepine anxiolytics. The mean number of unique medications prescribed to GMS patients was 10.9 (SD 5.9), and 8.1 (SD 5.8) for non-GMS patients. The number of unique medications prescribed to both GMS and non-GMS cohorts increased over time. The increase was steeper in the GMS group where the mean number of medications . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 3 prescribed increased by 0.67 medications/year. The rate of increase was 0.13 (95%CI 0.13, 0.14) medications/year lower for non-GMS patients, a statistically significant difference. Conclusion Our study found a significantly larger number of medicines were prescribed to patients with public health cover, compared to those without. Increasing medication burden and polypharmacy among older adults may be accelerated for those of lower socioeconomic status. These findings may inform planning for moves towards universal health care, and this would provide an opportunity to evaluate the effect of expanding entitlement on prescribing and medicines use. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 4 Introduction With changing population profiles and increasingly costly medical interventions, high and middle-income countries are facing challenges in providing affordable healthcare to their populations.[1] By 2041, citizens over 65 years of age will make up 22% the Irish population; a doubling of the 2006 figure.[2]  Delivering functional and affordable systems of universal healthcare requires identifying the optimal healthcare system which balances patients’ needs with services and costs covered.[3] In Ireland, political discussions surrounding healthcare reform culminated in the Sláintecare report in 2017, which provided a roadmap to a future single-payer system of universal healthcare, based on need and not on ability to pay.[4] At present, the Irish healthcare system is two-tiered and incorporates a mix of both public and private elements.[5] Notably, access to prescription medicines varies considerably for individuals based on income and age. Some patients with full public health cover pay only a small prescription charge for each medicine. Alternatively, individuals who do not meet income and age criteria pay out-of-pocket for the cost of their prescription medicines, up to a monthly household cap. Differences in prescription medicine use between these groups may arise due to differing individual characteristics (i.e. socioeconomic status), but also the effect of differing healthcare. Existing literature has identified variation in medication prescribing for individuals with public and private health cover and access. Previous studies in countries in Africa and Sweden found physicians working in the private sector are less likely to adhere to guidelines, while also being less likely to prescribe rationally for certain conditions.[6, 7] In Ireland, polypharmacy and potentially inappropriate prescribing have increased in recent years; however, the evidence for prescribing variation between the public and private sector is mixed.[8] A 2008 study found evidence of no difference in prescribing rates, but higher inappropriate prescribing of antipsychotics to individuals in private residential care settings . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 5 compare to public.[9] A more recent study identified that public patients in Ireland had a 21-38% greater risk of polypharmacy compared to patients with private healthcare coverage. The study authors concluded that publicly funded healthcare in Ireland led to greater medication use in people aged 50-69 years.[10] International evidence has also examined prescription practices, and in several Swedish studies, private providers were found to prescribe a higher number of medicines, though less cost-effectively, than public GPs.[11, 12] The majority of the studies comparing prescription in the public and private sectors have been carried out with regard to low- or middle-income countries, where a series of comprehensive meta-analyses support the idea that there is measurable variability in prescribing practice between sectors.[13, 14] The Irish system presents a unique opportunity to evaluate prescribing differences among patients with differing healthcare entitlements, cared for by the same providers. An understanding of differences in prescribing patterns between public and private patients in Irish general practice is important if future health reform extends coverage of prescription medicines entitlement. Aim and objectives This study aims to determine how prescribing practices for patients aged 65 years and over in primary care in Ireland differ between patients with public and private health cover. The objectives are to assess differences in the: - 
Rate of prescribing of common drug classes. - 
Prevalence of individual drugs within common drug. - 
Number of medications prescribed. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 6 Methods Study design, population, and setting This was an observational study reported in line with the STrengthening the Reporting of OBservational studies in Epidemiology (STROBE) statement.[15] Anonymised data were collected as part of a larger study from 44 general practices in the Republic of Ireland using the patient management software Socrates (www.socrates.ie) between January 2011 and April 2018. Ethical approval was obtained from the Irish College of General Practitioners. Participating practices from the catchment areas of Dublin (n=30), Galway (n=11), and Cork (n=3) hospitals represented 91% of those contacted. Ireland has a mixed public-private health system, and a proportion of the population are entitled to public health cover, with eligibility based on household income and age. The General Medical Service (GMS) scheme covers the most socioeconomically deprived people, approximately one third of the population, and entitles them to GP visits and a range of health services free at the point of access, and prescription medications (with a small co-payment of €2.50).[16] The Doctor Visit Card (DVC) scheme covers people with higher, but still limited, means, who are entitled to free GP visits but pay for other health services and their medications. All other individuals pay for healthcare and prescription medications (with a household cap of €144 per month applying during the study period). Data were extracted from the patient management system relating to demographics, consultations, prescribing and hospitalisations for patients aged 65 years and older. Patients were included in the present analysis if they had prescriptions issued on at least two dates during the study period, and had demographics (age and sex) and date of prescribing data recorded. Observations with a date of prescription outside of the study period were removed from the analysis. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 7 Study variables Prescription records in the dataset are at the medication level and included date of prescription, number of issues (i.e., how many times a prescription could be dispensed), product name, and generic name. Medications were coded using the Anatomical Therapeutic Chemical (ATC) classification, a system developed by the World Health Organisation for drug utilisation research and monitoring. ATC codes are organised by physiological system and are hierarchical, with the full seven-character ATC code identifying the active substance, and the five-character ATC code identifying the chemical subgroup level (usually equivalent to the drug class). Age and sex were extracted as demographic variables from the GP records, as was the type of health cover a patient had: GMS scheme (considered “public”), DVC scheme, or neither of these (considered to be “private”). We grouped DVC scheme cohort with the private cohort as a “non-GMS” category, as although GP visits are covered by the state, medications are not in this instance. We also created a time-varying variable, counting the number of hospitalisations each individual had during the study period. We calculated the rate of prescribing for drug classes at the five-character ATC code (ATC5) level, both overall and separately for GMS and non-GMS patients. We pre-specified 12 drug classes of interest before commencing the study (Table 1), based on their high prevalence of use, their inclusion in Ireland’s Preferred Drugs Initiative (Health Service Executive Medicines Management Programme),[17] or potential for sub-optimal prescribing. We calculated the number of unique drug classes (at the ATC5 level) each patient had been prescribed over the previous 12 months on a rolling basis across the study period, which was used as the number of medicines each patient was prescribed. The number of medicines prescribed was also converted into a categorical variable with prescription of 5-9 medications being classed as ‘polypharmacy’ and 10 or more medications being classed as ‘major polypharmacy’. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 8 Table 1. Pre-specified drug classes of interest and corresponding ATC codes. ATC code 
Drug class C10AA 
Statins A02BC 
Proton pump inhibitors C07AB 
Beta blocking agents, selective B01AA, B01AE, 
B01AF 
Direct oral anticoagulants C09AA and C09B 
ACE inhibitors (both single agent products and combinations) C08CA 
Dihydropyridine calcium channel blockers N05CF 
Z-drug hypnotics N06AB 
Selective serotonin reuptake inhibitors N05BA 
Benzodiazepine anxiolytics C09CA and C09D 
Angiotensin receptor blockers (both single agent products and combinations) R03AC and 
R03AK 
Adrenergics in combination with corticosteroids or other drugs, and/or 
anticholinergics J01 
Antibacterials for systemic use Statistical analysis First, we described patient characteristics, both overall and separately for GMS and non-GMS patients. We then directly standardised rates of prescribing (based on number of prescriptions and number of repeats/issues per prescription) for drug classes among GMS patients to the non-GMS population, using age group (65-69, 70-74, 75-79, 80-84, 85-89 and 90 years and over) sex, and calendar year, generating 95% confidence intervals (95% CIs) for the rates in both groups. Including year as a standardisation variable accounted for the amount of time patients were present in the dataset. The ratio of the prescribing rate for each drug class among the GMS versus non-GMS patients was plotted as a bubble graph. The same analysis was carried out comparing the GMS group to the DVC group alone, and the private group alone. We determined the prevalence of individual medications (seven-character ATC codes) within each drug class of interest, and assessed any difference between health cover groups in the distribution of prescribing within drug classes using a chi-squared test. A single practice, which was missing number of repeats/issues data, was excluded from this drug class analysis. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 9 We used monthly values for the number of unique drug classes (at the ATC5 level) each patient had been prescribed over the previous 12 months to plot the mean number over time for GMS and non- GMS patients.  We also plotted the proportion of GMS and non-GMS patients with polypharmacy over time in categories of 1-4, 5-9, 10-14 and 15+ medications. We also summarised the mean number of medicines prescribed per person over the full study period for the GMS and non-GMS groups, taking an average of the number of medicines each time a prescription was issued (excluding observations in the 12 months after the first date of prescription for an individual, where a full 12-month period for calculating number of medicines was not yet available). We used a multilevel linear regression analyses to assess whether the number of medications differed by health cover and over time. Data was hierarchical with monthly time points, nested within individual patients, nested within GP practices. The fixed covariates included date of prescription (scaled to 1 unit per year and continuous), health cover type (categorical, GMS and non-GMS), age (continuous in years) and sex (categorical, male and female). Random intercepts were included for the patient and practice level, and variance and variance partition coefficients were estimated for each level. A second model was also fitted to include an interaction between date of prescription and health cover, assessing whether any change in number of medicines prescribed over time differed according to health cover. A third model included a hospitalisations variable, to examine how this may explain differences in the number of medications between health cover groups. When modelling, the average number of unique medications prescribed to individuals over time, observations occurring less than 12 months after the first for an individual were removed as incomplete 12-month periods. Analyses were conducted using the lme4 package in R,[18, 19] and statistical significance was assumed at p<0.05. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 10 Results The analyses included data on 42,456 individuals, of which 44% (n=18,695) were male and 56% (n=23,761) were female. The majority (62%, n=26,490) of individuals were covered by the GMS scheme, while the remaining 15,966 were non-GMS (70% Private and 30% DVC). The mean age of the GMS cohort was 78.9 years (SD 8.1) and the mean age of the non-GMS cohort was 79.4 (SD 9.2). There was a higher proportion of females in the GMS group (58%) compared to the non-GMS group (52.7%). Demographics and health cover status for participants are included in Table 2. Table 2. Descriptive characteristics of included participants Characteristic 
Total (n=42,456) 
GMS (n=26,490) 
Non-GMS (n=15,966) Age (years), mean (SD) 
79.0 (8.3) 
78.9 (8.1) 
79.4 (9.2) Age group, n (%) 65-69 years 
7,965 (18.8%) 
3,591 (8.5%) 
4,374 (10.3%) 70-74 years 
9,070 (21.4%) 
5,232 (12.3%) 
3,838 (9.0%) 75-80 years 
7,729 (18.2%) 
5,328 (12.6%) 
2,401 (5.7%) 80-84 years 
6,919 (16.3%) 
5,057 (11.9%) 
1,862 (4.4%) 85-89 years 
5,480 (12.9%) 
3,916 (9.2%) 
1,564 (3.7%) 90+ years 
5,294 (12.5%) 
3,366 (7.9%) 
1,928 (4.5%) Female, n (%) 
23,761 (56.0%) 
15,353 (36.2%) 
8,408 (19.8%) Male, n (%) 
18,695 (44.0%) 
11,137 (26.2%) 
7,558 (17.8%) Health cover, n (%) General Medical 
Services scheme 
26,490 (62.4%) 
26,490 (100.0%) 
0 Doctor Visit Card 
4,743 (11.2%) 
0 
4,743 (29.7%) Private 
11,223 (26.4%) 
0 
11,223 (70.3%) Drug class prescribing The rate of prescribing in all pre-specified drug classes was higher for GMS patients compared to non-GMS patients. Figure 1 shows the ratios of GMS to non-GMS prescribing rates for these classes. In all cases, the rate of prescribing was at least 1.3 times higher in the GMS group, with the smallest difference in systemic antibacterials. We saw the greatest disparity in benzodiazepine anxiolytics where . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 11 the rate of GMS prescribing was 1.78 times higher; a rate of 996 per 1000 person-years in the GMS group versus a rate of 559 per 1000 person-years in the non-GMS group. The next largest difference was inhaled adrenergic medication combined with corticosteroids and/or anticholinergics, with a rate 1.58 times higher in the GMS group. Crude and standardised rates for each medication class in each group are reported in supplementary table 1. In sensitivity analysis, ratios of GMS to DVC rates were higher than the corresponding ratio of GMS to private rates in most cases, with the exception of statins, angiotensin receptor blockers, and dihydropyridine calcium channel blockers (Supplementary figure 1). A further sensitivity analysis considering prevalence (i.e. number of people prescribed the medication class, rather than the rate of prescribing per 1,000 person-years) again showed higher prevalence in the GMS group versus non-GMS across all drug classes. The difference were more modest, ranging from 1.04 to 1.30, the largest difference being in inhaled adrenergic combinations (Supplementary figure 2 and Supplementary table 2). . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 12 Figure 1: Ratio of GMS to non-GMS prescribing rates for pre-specified medication classes, with bubble size indicating the rate of prescribing of each class among GMS patients As examples, the mosaic plots below (figure 2) show the relative proportions of medications (at the ATC7 level) that make up four of the pre-specified drug classes (benzodiazepine anxiolytics, statins, inhaled adrenergic combinations, and calcium channel blockers). For benzodiazepine anxiolytics, diazepam makes up a significantly greater proportion of prescribing in the GMS group compared to the non-GMS group (a difference of 5 percentage points), whereas the reverse is true of alprazolam (which is 2 percentage points higher among non-GMS patients). Within calcium channel blockers, amlodipine makes up a significantly greater proportion of prescribing within the non-GMS cohort (a difference of 4 percentage points). For inhaled adrenergic combinations, salmeterol/fluticasone made up significantly more prescribing in the GMS group (4.5 percentage points higher), whereas formoterol/budesonide . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 13 made up significantly more of non-GMS group prescribing for this drug class (5 percentage points higher). For statins, the largest difference was rosuvastatin accounting for 3 percentage points more of statin prescribing in the non-GMS group. Mosaic plots for the other drug classes are included as supplementary figure 3, and frequency tables for medications within each drug class by health cover are included as supplementary table 3. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 14 Figure 2: The relative proportions of individual medication prescribing (indicated by ATC7 codes) for (clockwise from top left) benzodiazepine anxiolytics, statins, dihydropyridine calcium channel blockers, and inhaled adrenergic combinations in the GMS and non-GMS groups Number of medications The number of unique medications prescribed to both the GMS and non-GMS cohorts increased over time, as depicted by the time trend below. The increase was more pronounced and more sustained in the GMS group, rising from a mean of 7.3 (SD 5.8) medications in January 2011 to a level of 14.2 (SD 7.1) in April 2018 compared to the non-GMS group rising from 5.8 (SD 4.8) to 9.2 (SD 6.6). Figure 3, shows the fitted line for the number of medicines over time for each group. Figure 3: Time trend comparing the changes in number of unique medications prescribed to both GMS and non- GMS groups over time, with grey shading indicated 95% confidence intervals. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 15 The rates of polypharmacy (≥5 medications), and major polypharmacy (≥10 medications) over time are shown in the stacked area chart below. The GMS group began the study period with higher rates of major polypharmacy and this became more pronounced over time. The rate of major polypharmacy in the GMS group increased from 33.2% in January 2011 to 76.5% in April 2018. Figure 4: Proportion of patients with levels of polypharmacy over the study period for GMS (top) and non-GMS groups (bottom) The mean number of unique drug classes prescribed to GMS patients over the full study period was 10.9 (SD 5.9), compared to a mean of 8.1 (SD 5.8) among non-GMS patients. Similarly, the median number of unique drug classes prescribed (Figure 5) was higher among GMS patients at 10.1 (IQR 6.5 to 14.3) compared to non-GMS patients (median 6.6, IQR 3.7 to 11.1). . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 16 Figure 5: Violin plot showing the number of unique medications prescribed to GMS and non-GMS patients. The results of the multilevel regression model are shown in Table 3. Based on variance partition coefficients in Model 1, 4% of variation was between practices, 75% was between patients within practices, and 21% was within patients over time. There was a statistically significant increase in number of unique medications over time (0.65 additional medicines per year, 95% CI 0.64, 0.65), with non-GMS patients (compared to GMS patients) being prescribed 1.93 (95% CI 2.00, 1.87) fewer medications. Being female was associated with a higher number of medicines (0.91 additional medicines, 95% CI 0.85, 0.96) compared to males. In model 2, including an interaction term between time and health cover, the VPC were similar to model 1. In this model, mean number of medications prescribed increased by 0.67 medications/year for GMS patients. The rate of increase was 0.13 (95%CI 0.13, 0.14) medications/year lower for non-GMS patients, a statistically significant difference. In model 3, including a variable counting the number of hospitalisations, the increase in medications over time and the difference in the rate of increase by health cover were both attenuated. Table 3: Characteristics associated with number of unique medications over time in multilevel linear regression Change in number of medications (95% confidence interval) . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 17 Covariates 
Model 1 
Model 2 
Model 3 Time, per year 
0.65 (0.64, 0.65) 
0.67 (0.67, 0.67) 
0.54 (0.54, 0.54) Non-GMS 
-1.93 (-2.00, -1.87) 
-1.58 (-1.64, -1.51) 
-1.58 (-1.65, -1.52) Age  
0.22 (0.22, 0.23) 
0.22 (0.22, 0.22) 
0.21 (0.21, 0.22) Sex (female) 
0.91 (0.85, 0.96) 
0.90 (0.85, 0.95) 
0.97 (0.92, 1.03) Non-GMS:Time, per 
year, interaction 
- 
-0.13 (-0.14, -0.13) 
-0.10 (-0.11, -0.10) Hospitalisations 
- 
- 
0.48 (0.47, 0.48) Variance (VPC) Practice 
1.5 (3.6%) 
1.5 (3.5%) 
1.4 (3.6%) Patient 
31.2 (75.4%) 
31.1 (75.4%) 
29.6 (74.9%) Residual 
8.7 (20.9%) 
8.7 (21.0%) 
8.5 (21.5%) GMS, General Medical Services scheme; VPC, Variance Partition Coefficient. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 18 Discussion In this study, we found higher numbers of medications prescribed to older adults with public health cover (GMS) compared to those without. We also identified steeper growth in the number of medications over time within the GMS cohort. This is reflected in higher rates of prescribing of all of the pre-specified medication classes we examined, with the greatest difference in rates for inhaled adrenergic combination medications. Within drug classes, there were some differences in the percentage share of individual medications between health cover groups, however these did not consistently align with national preferred drug guidance. The steeper growth in medications over time for the GMS group was partly explained by the higher rate of hospitalisation. Direct comparison with other research is challenging, as most examine prescribing differences between patients attending public versus private providers in other healthcare systems, rather than the same providers prescribing to those with differing healthcare entitlements, as in Ireland’s health system. Studies by Granlund (2009) and Hakansson et al. (2001) found a significantly larger number of unique medicines were prescribed to public, rather than private patients. In the Irish setting, Mohan et al. (2021) also reported this disparity in number of medications and the faster growth over time in the over 50s public cohort in Ireland.[11, 12, 20] One reason for the disparity we identified may be that socioeconomic status is known to correlate negatively with several measures of health, in Ireland.[21] As a result, the publicly covered population is likely to have a higher illness burden, requiring greater pharmaceutical intervention. This association is robust in the literature as made clear by Pathirana and Jackson, (2018), who performed a systemic review encompassing of 24 cross-sectional studies, primarily in high-income settings, showing level of educational attainment and deprivation (as measures of socioeconomic status) were both associated with increased risk of multimorbidity.[22] Guthrie et al. (2015) identified an association . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 19 between living in a deprived area and increasing polypharmacy among adults of all ages in a region of Scotland.[23] Also in Scotland, a study by Barnett et al. (2012) showed that the accumulation of chronic conditions was more substantial, and occurred earlier (by up to 15 years), in those of a lower socioeconomic status.[24] Given the overrepresentation of socioeconomic deprivation among those with public health cover in Ireland, this may partly explain higher rate of growth in medication burden among GMS patients. Inhaled adrenergic combination medications showed the second largest or largest difference in prescribing (across prescribing rates or prevalence) of our chosen drug classes, which is striking, as there is a particularly strong negative correlation between socioeconomic status and respiratory diseases.[25] Previous evidence in Ireland has shown this relationship, and respiratory diseases as a whole are more common in Ireland than in many comparable developed nations in Europe (O’Shea, 1997).[26] By way of partial explanation, rates of smoking in Ireland have historically been shown to be significantly higher in those of lower socioeconomic status (Layte and Whelan, 2009).[27] The smallest difference in prescribing rates was for systemic antibacterials, being 1.3-fold higher in GMS patients. Unlike most of the other drug classes examined, these are often short-term prescriptions (thus the impact of deprivation on illness burden may be amplified/propagated less). Further evidence from Scotland found an association between deprivation and rates of antimicrobial prescribing.[28] In contrast, a previous study in Ireland including individuals of all ages found private patients were more likely to receive an antibiotic prescription than GMS patients, however this was reversed among patients aged 65 years and over, consistent with our findings.[29] The less pronounced difference in prescribing rates may also be partly explained by the existence of primary care antimicrobial prescribing guidelines in Ireland since 2012.[30] An increase in medication burden post hospitalisation is a common occurrence.[31, 32] However, whether the increased medication burden is maintained after discharge is often not . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 20 examined.[33, 34] We addressed this issue with a multilevel regression model that accounted for the association of hospitalisation with number of medications over time, and found a sustained positive effect. The appropriateness of the increased medication burden is unclear. Viktil et al., (2012), cite a similar number of medication changes upon discharge, commenting on a delay in receipt of discharge notes and speculate that failure to communicate between primary and secondary care contributes to potentially inappropriate prescribing. This finding is built upon by Coll et al., (2021), who show that the inclusion of instructions upon discharge accelerates the discontinuation of benzodiazepines and Z-drugs in older adults.[35] Further work by Perez et al., (2018), suggest that the risk of potentially inappropriate prescribing increases with rates of hospitalisation and degree of multimorbidity.[36] Patients were found to be 72% more likely to have been prescribed a potentially inappropriate medication after a single hospitalisation. However, Corsonello et al. (2007) suggest that due to their finding that the new drugs tended to relate to chronic conditions, they may largely represent a ‘true and stable’ increase. This may be reflected in our study, as the cohort that accrues chronic conditions earlier and to a greater degree, show the largest increase in polypharmacy. Our study did not account for changes to medication regimens that produced no overall change in medication burden, though this has been put forward as an indicator for identifying patients at risk of potentially inappropriate prescribing.[37] Our study provides a longitudinal analysis of polypharmacy, a view which is under reported in the literature. Falster et al., highlight that although the medications that make up patients’ polypharmacy change regularly over time, once reached, chronic polypharmacy is often permanent among older patients.[38] A limitation of our study is that we were unable to examine which factors relating to public healthcare entitlement (i.e. increased access to healthcare and medications, or the underlying differences in socioeconomic status) have the greatest relationship with prescribing differences. Therefore, it is not possible to conclude what prescribing rates would be if healthcare . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 21 entitlement was widened. Although we found higher prescribing rates across our pre-specified drug classes, other classes could potentially show different patterns. However our overall findings for number of medications and polypharmacy support a widespread relationship. Our analysis was also limited to those aged 65 years and over, and therefore cannot be generalised to younger patients. However, the older age group account for the majority of medication utilisation. Conclusion Our study found a significantly larger number of unique medicines were prescribed to patients with public health cover, compared to those without. This disparity increased over time and was consistent within all drug classes analysed. This may be driven by socioeconomic deprivation rather than health cover. We provide new evidence that the growth in medication burden and polypharmacy among older adults is accelerated for those of lower socioeconomic status, and evidence to support decisions about extending medications entitlement further in Ireland in the future. Such an expansion would provide a further opportunity to assess the impact of extended entitlement on prescribing and medicines use. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 22 Declarations Ethics approval and consent to participate: Approval was granted by the Irish College of General Practitioners Research Ethics Committee. This is a secondary analysis of data that was anonymised before automatic extraction from electronic records, and therefore individual consent was not required. Availability of data and materials: The dataset analysed during the current study is not publicly available as provisions for data sharing were not included in the initial ethical approval. Competing interests: The authors declare that they have no competing interests Funding: This research was funded by the Health Research Board in Ireland (HRB) through an Investigator Led Projects grant (grant number ILP-HSR-2019-006). BC is supported by an Emerging Investigator Award grant from the HRB (grant number EIA-2019-09). Authors' contributions: MF, BC, TF, and FM conceived the study. CP, MF, TF and FM designed the study. TF provided the data. CP and FM cleaned and analysed the data. All authors interpreted the data. CP and FM drafted the manuscript, and MF, LTM, BC and TF critically revised the manuscript. Acknowledgements: We wish to acknowledge the general practitioners and patients whose data was contributed to this study. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287967
doi: 
medRxiv preprint 23",1
"Background : Complex regional pain syndrome (CRPS) is a chronic pain condition characterised by peripheral and central sensory and motor dysfunction. Implicit motor imagery is known to be impaired in these patients, but evidence is still lacking for explicit motor imagery. Using a self-rated questionnaire, this study aims to compare explicit motor imagery abilities between individuals with CRPS, with chronic limb pain (CLP) and healthy controls and also examine differences between affected and unaffected limbs. Methods: . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. Motor Imagery in CRPS, COHEN-AKNINE, 2023  
2 In this single-centre observational study, 123 participants were recruited (CRPS = 40, chronic limb pain, CLP = 40, and healthy individuals = 43). Participants completed the Movement Imagery Questionnaire - Revised Second (MIQ-RS) once for each body side. The total MIQ-RS score, and the kinesthetic and visual subscores were compared between groups and between the affected and unaffected sides. Results: The MIQ-RS revealed no significant differences in explicit motor imagery abilities, neither between groups nor between the affected and unaffected side. Null Hypothesis Bayesian Testing on kinesthetic motor imagery abilities indicated a sevenfold likelihood of no differences between groups and a more than fivefold likelihood of no differences between sides. Conclusion: CRPS and chronic limb pain individuals showed preserved explicit motor imagery abilities, notably on the pain side. The preservation of these abilities supports the recommendation of mental imagery therapy to improve motor function and relieve pain in chronic pain patients. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
3 INTRODUCTION Complex regional pain syndrome (CRPS) is a rare, debilitating condition that primarily affects a single extremity and is characterised by pain that is disproportionate to the original injury (M. C. Ferraro et al., 2024; Goebel et al., 2019). The International Association for the Study of Pain (IASP) classifies CRPS as chronic primary pain in the ICD-11 and suggests that it may also meet the criteria for non-disabling pain (Goebel et al., 2021; Harden et al., 2010a; Kosek et al., 2021). The pathophysiology of CRPS involves complex interactions between immune- mediated inflammatory responses, vasomotor changes, genetic factors, psychological components and changes in the nervous system. These changes cause sensory, motor, autonomic and trophic dysfunctions (Birklein et al., 2018; M. C. Ferraro et al., 2024) such as allodynia or hyperalgesia, with 63% of patients experiencing a reduction in active movement (Ott & Maihöfner, 2018). Sensory and motor dysfunction are associated with cortical changes, referred to as maladaptive plasticity (Ma et al., 2022; Shokouhi et al., 2018; Zangrandi et al., 2021). Engaging patients in active approaches improves motor recovery and promotes brain plasticity (M. Ferraro et al., 2023; Harden et al., 2022; Smart et al., 2022). This has motivated the use of motor imagery therapy, alone or in combination with other therapies, to activate motor neural networks with reduced pain associated with physical movement (Lotze & Moseley, 2022a). Motor imagery (MI) is a dynamic state in which individuals mentally simulate specific actions or gestures without actually performing the movement (Decety, 1996; Moran et al., 2012). Motor imagery training has shown promise in improving function and reducing pain by activating neural pathways similar to those used during actual movement execution (M. Ferraro et al., 2023; Hardwick et al., 2018; Ríos-León et al., 2024). . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
4 Despite various theoretical models proposing explanations for the motor imagery phenomenon, such as the motor simulation theory and the effect imagery model, there is no consensus on a definitive explanation (Hurst & Boe, 2022; Solomon et al., 2022) These therapies require patients to imagine their affected side in different modalities, and assessment methods are heterogeneous, ranging from implicit tasks (lateral judgement) to explicit tasks (self-report questionnaires, mental chronometry) detailing visual and kinesthetic perspectives (Chepurova et al., 2022; Guillot & Collet, 2005). Previous studies have shown that individuals with CRPS have significant deficits in explicit MI compared to healthy controls and their unaffected side (Breckenridge et al., 2019; Ravat et al., 2020). Furthermore, a study by La Touche et al. (La Touche et al., 2019) showed that explicit MI difficulties are more pronounced in individuals with chronic low back pain than in asymptomatic controls. However, explicit MI abilities in people with CRPS are poorly understood. Consistent with the observed impairments in implicit motor imagery in patients with chronic limb pain compared to healthy individuals and their unaffected limbs, we hypothesize that CRPS patients will show specific deficits in explicit motor imagery abilities. This study compares explicit motor imagery between CRPS patients, chronic limb pain patients, healthy controls, and between affected and unaffected limbs. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
5 MATERIAL AND METHODS Design This was a prospective, single-centre, cross-sectional study conducted at the University Hospital of Nîmes (France). The study procedures complied with the ethical standards of the competent committee for human experimentation (local ethics committee 2020-A02281-38 designated ""Comité de Protection des Personnes, Sud Méditérrannée IV"" on 8/12/2020) and the Helsinki Declaration of 2013. The study protocol was registered on clinicaltrials.org on 11/01/2021 (NCT04703348). All individuals received an information letter and informed consent was obtained from all individuals. Participants and setting Individuals with CRPS were recruited at the Department of Pain Medicine (CHU Nîmes, France) between January 2021 and October 2022. Pain specialists recruited patients based on the verification of the presence of the Budapest and eligibility criteria. Patients with CRPS were affected in either the upper or lower limb and on the dominant or non-dominant side. Healthy individuals were recruited through a poster campaign among hospital staff. Patients with chronic limb pain (CLP) were recruited and diagnosed at the Department of Physical Medicine and Rehabilitation by specialists in physical medicine and rehabilitation. They were included in the study if they had experienced limb pain for more than three months due to conditions such as musculoskeletal disorders, chronic post-traumatic pain, or post-operative pain, regardless of the underlying cause. Inclusion criteria were: Age over 18 years, less than 150 minutes of moderate-to-vigorous physical activity per week, and education up to high school graduation or equivalent. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
6 Patients with CRPS had to meet the diagnostic criteria set out in the Budapest criteria (Harden et al., 2010b; Mesaroli et al., 2021). Both CRPS patients and those with chronic limb pain needed to have experienced pain for at least three months. In addition, individuals were excluded from the study if they met any of the following criteria CRPS secondary to stroke, stellate block performed within three weeks prior to the interview, presence of a central neurological disorder, diagnosis of chronic fibromyalgia or low back pain, pregnancy, postpartum or lactation, visual impairment that interfered with the use of the MIQ-RS questionnaire, history of limb amputation, previous experience with motor imagery practice, or psychiatric illness. To characterise the population age, sex, body mass index (BMI), upper and lower dominant limb, education level, pain duration and physical activity level were recorded. Protocol Prior to assessing the eligibility criteria and obtaining informed consent from the physician, individuals were asked to complete the Movement Imagery Questionnaire - Revised, Second Edition (MIQ-RS) during the consultation. Age, weight, height, pain duration and limb dominance (upper and lower) were self-reported. Patients were also asked to report their level of physical activity on a three-point scale (less than 1 hour, between 1 and 1.5 hours, and more than 1.5 hours of moderate to vigorous activity per week) and their level of education on a six-point scale (from high school to PhD and beyond). The Movement Imagery Questionnaire-Revised, Second Edition (MIQ-RS) was chosen for several reasons: its suitability for patients with motor limitations (Gregg et al., 2010), its ability to measure lateralised imagery scores (comparing left and right sides), its validation in French (Loison et . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
7 al., 2013), and its acceptable reliability and reproducibility (Butler et al., 2012). Furthermore, this questionnaire has been shown to correlate visual and kinesthetic scores with fMRI signals in stroke patients (Confalonieri et al., 2012), although it has not yet been used in patients with CRPS. However, due to the Covid-19 pandemic, some individuals (60%) completed the questionnaire via videoconference with an investigator. The questionnaires were audio- recorded on REDCap© (online questionnaire) (Floridou et al., 2022). Pain individuals completed a self-rated questionnaire twice, starting with the right side and then answering the left side, pausing if necessary. The session was administered in a single session, with no follow-up. The expected heterogeneity in dominance, laterality and upper or lower limb affected did not allow for randomisation. Outcome measures The MIQ-RS is a validated self-rated questionnaire for the assessment of explicit motor imagery (Butler et al., 2012; Gregg et al., 2010). It is a 14-item therapist-administered questionnaire in which patients first perform a movement, e.g. raising the knee, followed by visual and then kinesthetic motor imagery. Patients rate their abilities on a 7-point Likert scale, ranging from ""very easy to see/feel"" (1 point) to ""very hard to see/feel"" (7 points). The MIQ-RS offers two methods for scoring, as documented in the literature: the first method involves the calculation of a total score and two subscores for Kinesthetic Motor Imagery (KMI) and Visual Motor Imagery (VMI). The total possible score is 98, with each of the subscores (KMI and VMI) having a maximum of 49 points (Rimbert et al., 2019). Alternatively, the score can be derived by taking the mean of the responses for the total . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
8 score and the two subscores on a 7-point Likert scale. There is no official cutoff point for assessing explicit motor imagery abilities. However, based on the systematic review by McInnes et al. (2016), explicit motor imagery abilities were categorized into three levels: unable (scoring less than 48 out of 98), impaired (scoring between 49 and 73 out of 98), and normal (scoring more than 74 out of 98). The primary outcome of the study was the categorisation of motor imagery (MI) abilities for the three groups of participants. Secondary outcomes included differences in total and subscores of the MIQ-RS between the groups and differences in scores on the unaffected and affected sides within the CRPS and CLP groups. No published results for this population were available for sample size calculation. Based on previous research using the MIQ-RS in patients with chronic conditions such as chronic low back pain, where La Touche et al.  (La Touche et al., 2019) found an effect size of 0.57 between groups and aiming for 95% power with a beta risk of 0.80 to compare three groups, GPower 3.1.9.7 was used to calculate the required sample size. The initial power calculation suggested 33 individuals per group. However, to account for a potential 20% exclusion due to eligibility criteria, the number was adjusted to 40 individuals per group. This adjustment results in a total of 120 participants required for the study. Data analysis The softwares JASP © and R Studio © were used to perform the statistical analyses. First, a frequentist statistical approach was used with significance set for a two-tailed α level of 0.05. As data were not distributed normally, we used non-parametric signed-rank tests and report median and interquartile range (IQR) with a 95% confidence interval. To assess . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
9 the effect of group (CRPS vs. CLP vs. healthy) on motor imagery scores, we performed a Kruskal-Wallis test. To assess the effect of pain on motor imagery scores across limbs (unaffected vs. affected), we performed a Wilcoxon signed-rank test. Finally, we assessed the dispersion of scores between groups using a coefficient of variation. Secondly, we used a Null Hypothesis Bayesian Testing (NHBT) approach for the assessment of evidence of a lack of difference (Kruschke, 2021; van Doorn et al., 2021). Specifically, we calculated the Bayes factor (BF01), which quantifies the likelihood of the null hypothesis versus the alternative hypothesis (for example, BF01 = 7.13 indicates that, given the data, no difference is 7.13 times more likely than a difference). A Bayes factor between 5 and 10 indicates moderate evidence in favour of the null hypothesis (Quintana & Williams, 2018). Levene's method was used to measure the equality of variances between the groups and sides. This analysis was performed using JASP © software with a prior in favour of differences between groups according to our hypothesis (van Doorn et al., 2021). RESULTS We screened 129 participants for inclusion, and 123 were included after exclusion and age matching (40 participants in the CRPS and CLP group and 43 participants in the healthy group). The main characteristics of the participants are described in Table 1 (further demographic details are provided in Appendix S1). Descriptive Statistics 
CLP Group (n=40) CRPS Group (n=40) Healthy Group (n=43) F Test (Kruskal- Wallis Test) P value . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
10 Age, years median [IQR] (1st – 3rd Quartile) 49 [18.75] (40.75-59.5) 54 [18.5] (43-61.5) 47 [16] (37-53) 
2.72 
0.082 BMI, kg/m2, median [IQR ] (1st – 3rd Quartile) 25 [8] (22-30) 25.5 [6] (23-29) 23 [6.5] (21-27.5) 
2.25 
0.108 Pain Duration, month [IQR] (1st – 3rd Quartile) 12 [34] (5.75-39.75) 9.5 [9] (6-15) 
NA 
4.22 
< 0.01 CLP Group 
CRPS Group 
Healthy Group 
X² Test 
P value Sex 
Men 
17 (42.5%)  
9 (22.5%)  
16 (37.2%)  
3.83 
0.147 Women 
23 (57.5%) 
31 (77.5%) 
27 (62.8%) Upper dominant Right 
39 (99.5%) 
35 (87.5%) 
41 (95.3%) 
3.62 
0.160 Left 
1 (0.5%) 
5 (12.5%) 
2 (4.7%) Lower dominant Right 
26 (65%) 
31 (77.5%) 
29 (67.5%) 
1.67 
0.432 Left 
14 (35%) 
9 (22.5%) 
14 (32.5%) Affected side Right upper limb 
13 (32.5%) 
7 (17.5%) 
NA 4.09 
0.252 Left upper limb 
8 (20%) 
6 (15%) 
NA Right lower limb 
10 (25%) 
11 (27,5) 
NA Left lower limb 
9 (22.5%) 
16 (40%) 
NA Test duration (in minutes) 
13 
14 
14 
2.17 
0.338 Sub-groups of physical activity levels per week < 1h 
17 (42.5%) 
32 (80%) 
16 (37.2.%) 23.05 
< 0.01 >1h - <1h30 
7 (17.5%) 
4 (10%) 
16 (37.2%) >1h30 - <2h30 
16 (40%) 
4 (10%) 
11 (25.6%) Sub-groups of education levels A level 
13 (32.5%) 
24 (60%) 
2 (5%) 46.56 
< 0.01 1 years of study after A level 
5 (12.5%) 
11 (27.5%) 
5 (11.5%) 2 years of study after A level 
5 (12.5%) 
3 (7.5%) 
10 (23.1%) Bachelor degree 
7 (17.5%) 
2 (5%) 
9 (20.9%) Master’s Degree 
7 (17.5%) 
0 (0%) 
11 (25.5%) Ph.D and higher 
3 (7.5%) 
0 (0%) 
6 (14%) . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
11 Table 1: Participant demographic characteristics. Data are presented as medians for 
continuous variables and as numbers and percentages for categorical variables in each 
group. Identification of Motor Imagery abilities The MIQ-RS total scores for participant with complex regional pain syndrome, chronic limb pain and matched healthy participants showed high heterogeneity. This variability meant that groups of participants could not be categorised as having no, impaired or normal motor imagery ability based on the MIQ-RS total scores for both the healthy and painful side (Fig. 1). Indeed, if we apply the classification proposed by McInnes et al. (McInnes et al., 2016), which was developed based on participants with brain lesions, it appears that all groups, including those with complex regional pain syndrome, chronic limb pain and healthy participants, show impairments (between 49 and 73) in explicit motor imagery even in unaffected side (Appendix S1, Table S2). The median values of the MIQ-RS Total scores for the affected side and the unaffected side are shown in Appendix S1 (Table S2). . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
12 Figure 1 : Violin plot of categorisation of total MIQ-RS scores between complex regional pain 
syndrome, chronic limb pain and healthy groups Between-Individuals coefficient of variation The coefficient of variation between individuals for the CRPS group is 24.53% for the affected side and 22.52% for the unaffected side. For the CLP group, the coefficient of variation is 18.13% for the affected side and 19.05% for the unaffected side. For the healthy group, the coefficient of variation is 15.84% for the right side and 17.85% for the left side. These results show a high dispersion of results, especially for the CRPS group, but with consistency between the affected and unaffected side for the pain groups. Comparison of MI abilities between groups There were no statistical differences between the three groups for the MIQ-RS total mean score (H(2) = 1.795, p = 0.408, n² = -0.002) (Figure 2), the kinesthetic mean subscore (H(2) = . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
13 0.936, p = 0.626, n² = -0.009), or the visual mean subscore (H(2) = 4.175, p = 0.124, n² = 0.018) (Appendix S1, Table S3). Figure 2: Violin plots of MIQ-RS mean scores between complex regional pain syndrome, 
chronic limb pain and healthy groups with Bayesian null hypothesis tests (BF01) between 
groups. Comparison of MI abilities between the affected and unaffected sides Complex regional pain syndrome Group analysis There was no statistical difference between the affected side and the unaffected side for the MIQ-RS total mean score (W = 347.500, p = 0.826, r = 0.148), kinesthetic mean subscore (W = 377.000, p = 0.706, r = 0.086) and visual mean subscore (W = 272.500, p = 0.0881, r = 0.0187).The results are summarised in Appendix S1 (Table S4). . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
14 Chronic limb pain group analysis There was also no statistical difference between the affected side and the unaffected side for the MIQ-RS total mean score (W = 343.500, p = 0.910, r = 0.148), kinesthetic mean subscore (W = 372.500, p = 0.983, r = 0.334) and visual mean subscores (W = 317.500, p = 0.514, r = 0.006) in the CLP group (Appendix S1, Table S4). The complete inferential statistical analysis can be found in the Supplementary Information document (Appendix S2 in html). Bayesian Null Hypothesis Testing Frequentist analysis showed no significant differences between the groups or between the affected and unaffected sides. Therefore, a Bayesian null hypothesis testing approach was used to draw conclusions about the null hypothesis and group similarities (Kruschke, 2021). Thus, we confirmed the absence of between-group differences in explicit motor imagery abilities with moderate evidence for the null hypothesis (BF01 > 5), specifically in Kinesthetic Motor Imagery (KMI) with a Bayesian factor of 7.28, as detailed in Appendix S1 (Table S5). We also confirmed the absence of differences in explicit motor imagery abilities between the affected and unaffected side in both the CRPS and CLP groups, with moderate evidence for the null hypothesis (BF01 > 5). Details are provided in Appendix S1 (Table S6). This indicates that chronic pain does not affect explicit motor imagery abilities as assessed by the MIQ-RS (Fig. 2). . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
15 In simpler terms, our analysis suggests that there is a 7-fold probability that there are no differences in kinesthetic motor imagery abilities between the groups. Similarly, when comparing motor imagery (both kinesthetic and visual) between the unaffected side and the affected side in individuals with complex regional pain syndrome and chronic limb pain, there is also a five-fold probability that no differences exist. However, we could not confirm the similarity  between groups for the total and visual mean subscores. DISCUSSION In this study we observed high inter-individual variability, with dispersion around the group medians ranging from 15% to 25% for total explicit motor imagery scores. There were no significant differences between groups or between the affected and unaffected side. A secondary analysis showed similar explicit motor imagery abilities between groups and between the affected and unaffected side. The significance of these findings will be discussed below. First, our study revealed significant variability in explicit mental imagery abilities between participants, consistent with the broader spectrum of explicit imagery vividness identified in the literature, ranging from aphantasia to hyperphantasia, as reported by Zeman (Zeman, 2024). The MIQ-RS was unable to discriminate between healthy and painful individuals, suggesting that it may not effectively capture explicit motor imagery at the group level. The observed heterogeneity between individuals in explicit motor imagery abilities does not . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
16 reflect changes in these abilities over time, nor does it allow us to understand the dynamics, i.e. whether individuals suffering from chronic pain had the ability to engage in explicit mental imagery prior to the onset of chronic pain. However, our results show that patients with chronic pain exhibit greater variability in ability compared to healthy individuals, a pattern consistent with the theory of pain-sensorimotor interactions (Murray & Sessle, 2024). This theory highlights the influence of biological, psychological and social factors on changes in sensorimotor behaviour in patients with chronic pain. Similarly, our findings suggest that the heterogeneity observed in sensorimotor changes may also extend to explicit motor imagery abilities. Secondly, the absence of differences between groups for total mean scores and visual mean subscores was not confirmed by our Bayesian Null Hypothesis Testing. This may suggest that factors such as age may differentially affect explicit motor imagery abilities. Indeed, performance on implicit motor imagery tasks has been shown to be influenced by age (Muto et al., 2022). However, for explicit motor imagery tasks, age appears to differentially affect performance, with younger individuals showing greater visual dominance while older individuals showing stronger kinesthetic abilities, as differences are observed across different task types (Saimpont et al., 2015; Subirats et al., 2018). Furthermore, the brain areas involved in the modalities of motor imagery tasks differ (Lotze & Moseley, 2022b). Implicit and explicit motor imagery tasks activate similar areas, particularly in the beta band frequency of electroencephalography, but implicit tasks are less spatially specific and more intense than explicit tasks, suggesting different ways of mobilising sensorimotor areas (Osuagwu & Vuckovic, 2014). However, individuals can perform hand laterality judgment tasks (explicit motor imagery) without using a motor imagery-based strategy (Mibu et al., 2020). Furthermore, kinesthetic tasks have shown greater brain activation and correlation . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
17 with brain areas in fMRI than visual modalities (Confalonieri et al., 2012; Lee et al., 2019). These findings provide additional confidence in our results. Third, our findings contrast with previous studies that have identified effects of chronic pain on explicit motor imagery abilities (La Touche et al., 2019). In such studies, participants with chronic LBP exhibited more catastrophizing than healthy controls, with previous research showing a significant interaction between motor imagery and levels of catastrophizing (Moseley et al., 2008). Furthermore, a pilot study showed that stress conditions affect implicit but not explicit motor imagery abilities in healthy individuals (Schlatter et al., 2020), and it is well documented that participants with chronic pain report higher levels of stress (Mills et al., 2019). Furthermore, cognitive factors have been shown to influence implicit motor imagery tasks (Pelletier et al., 2018). These results could explain our discrepancy by highlighting the importance of the interaction between psychological factors and motor imagery abilities. Motor imagery tasks target the same brain networks as voluntary motor movements and motor imagery therapy has been shown to be effective in improving neuronal excitability and synaptic conductance in both healthy and pathological individuals  (Bowering et al., 2013; Decety, 1996; Lotze & Moseley, 2022a; Ríos-León et al., 2024; Ruffino et al., 2017). Interestingly, CRPS participants show no change in motor planning when engaged in object affordance tasks (Ten Brink et al., 2024), suggesting that motor imagery may represent a more conscious experience of motor planning, as described by the perceptual-cognitive model (Hurst & Boe, 2022). This finding highlights the potential of explicit motor imagery as an entry point for rehabilitation aimed at improving motor performance and reducing pain. However, engaging in explicit motor imagery tasks can induce pain and sudomotor . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
18 symptoms in patients with complex regional pain syndrome (CRPS), suggesting the need for a graduated approach to exposure even during motor imagery therapy sessions (Moseley et al., 2008). Explicit imagery training is the second stage of Graded Motor Imagery (GMI) therapy, which consists of three phases: starting with implicit motor imagery tasks, progressing to explicit motor imagery tasks, and ending with mirror therapy. The sequence of these phases is thought to be important for the benefits of the therapy (Lotze & Moseley, 2022b; Moseley, 2005). Despite its effectiveness, GMI shows inconsistent results, which could be explained by the interindividual variability observed in our study and previously described by others (Méndez-Rebolledo et al., 2017; Smart et al., 2022). Individuals with lower explicit motor imagery abilities could exhibit hypoactivation, which could explain a form of motor disuse or dysfunction (Kantak et al., 2022; Punt et al., 2013). Consequently, assessment of motor imagery vividness using more inclusive tools than the MIQ-RS could allow tailoring of rehabilitation programmes to individual needs prior to mirror therapy. This personalised approach could effectively address the different subtypes of CRPS and improve recovery outcomes (Knudsen et al., 2023; Mangnus et al., 2023). All in all, our results suggest that chronic pain affects cortical function and structure (Yang & Chang, 2019) differently depending on the processes involved in motor behaviour, the type of pain, the presence of psychological factors or individual behavioral strategies related to motor imagery. Our findings are consistent with the notion of reciprocal changes in motor behaviour and plasticity induced by chronic pain, as well as clinical recovery and plasticity induced by exercise (Hodges & Smeets, 2015; Kourosh-Arami & Komaki, 2023; Merkle et al., 2020). . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
19 Limitations include the non-random order of questionnaire completion and the aggregation of upper, lower and spinal motor imagery scores in the calculation of the MIQ-RS score, which may have masked specific limb scores. Furthermore, the lack of homogeneity of the subgroups in terms of physical activity level, education level, and pain duration may have biased our results, although all participants were inactive (physical activity less than 2.5 hours per week) and all pain groups were chronic (pain duration more than three months). Educational level appears to be a predictor of pain chronicity (Prego-Domínguez et al., 2021), and despite the lower educational level in the pain groups, there were no differences in motor imagery abilities, which may mitigate the recruitment bias in our study.  Our study did not assess pain intensity, which could have revealed potential sources of bias in different subtypes of chronic pain patients (Knudsen et al., 2023). Future research should pursue a deeper understanding of the efficacy mechanisms underlying different motor imagery training tasks (explicit, implicit, external, internal) in patients with complex regional pain syndrome (Diers, 2019), using more objective measures such as brain imaging in longitudinal research designs. In addition, exploring patient identification methods for personalised interventions, similar to those explored in chronic low back pain research (Simula et al., 2020), may help to identify individuals suitable for specific interventions (Mangnus et al., 2023). CONCLUSION Individuals with CRPS and chronic limb pain showed high inter-individual variability in explicit motor imagery tasks, similar to that observed in healthy people, with preserved abilities . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
20 between groups and sides. This supports the recommendation of mental imagery therapy to improve motor function and reduce pain in chronic pain patients. ACKNOWLEDGMENTS We thank the following people for their support and help: Marine Ourmet, Brigitte Laffont for regulatory documents, Willy Fagart, Dr. Anaïs Pages, Julie Bourdier, Romain Dolin, Shuan Banh, Kevin Jezequel and Sarah Kabani for editing the manuscript. The authors declare no conflicts of interest. The study data are available upon reasonable request to the corresponding author. AUTHOR CONTRIBUTIONS G. Cohen-Aknine: Conceptualization, Methodology, Formal Analysis, Investigation, Writing- Original Draft A Homs: Resources, Writing-Review & Editing, Visualization D. Mottet: Methodology, Validation, Data Curation, Writing-Review & Editing, Visualization, Supervision, T. Mura: Methodology, Writing-Review & Editing F. Jedryka: Investigation, Resources, Writing-Review & Editing A. Dupeyron: Validation, Resources, Writing-Review & Editing, Visualization, Project Administration DATA AVAIBILITY The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request. . 
CC-BY-NC 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted June 25, 2024. 
; 
https://doi.org/10.1101/2023.04.02.23288051
doi: 
medRxiv preprint Motor Imagery in CRPS, COHEN-AKNINE, 2023  
21",1
"Objective: Understand the potential for pre-operative biomarkers of cellular senescence, a primary aging mechanism, to predict risk of cardiac surgery-associated adverse events. Methods: Biomarkers of senescence were assessed in blood samples collected prior to surgery in 331 patients undergoing CABG +/- valve repair or replacement. Patients were followed throughout the hospital stay and at a 30-day follow-up visit. Logistic regression models for pre-operative risk prediction were built for age-related clinical outcomes with high incidence including KDIGO-defined acute kidney injury (AKI), decline in eGFR ≥25% between pre-op and 30 days, and MACKE30, a composite endpoint of major adverse cardiac and kidney events at 30d. Results: AKI occurred in 19.9% of patients, persistent decline in kidney function at 30d occurred in 11.0%, and MACKE30 occurred in 13.4%. A network of six biomarkers of senescence (p16, p14, LAG3, CD244, CD28 and suPAR) were able to identify patients at risk for AKI (AUC 0.76), kidney decline at 30d (AUC 0.73), and MACKE30 (AUC 0.71). Comparing the top and bottom tertiles of senescence-based risk models, patients in the top tertile had 7.8 (3.3-8.4) higher odds of developing AKI, 4.5 (1.6-12.6) higher odds of developing renal decline at 30d, and 5.7 (2.1-15.6) higher odds of developing MACKE30. All models remained significant when adjusted for clinical variables. Patients with kidney function decline at 30d were largely non-overlapping and clinically distinct from those who experienced AKI, suggesting a different etiology. Typical clinical factors that predispose to AKI (e.g., age, CKD, surgery type) associated with AKI but not the 30d decline endpoint which was instead associated with new-onset atrial fibrillation. Conclusions: A six-member network of biomarkers of senescence, a fundamental mechanism of aging, can identify patients for risk of adverse kidney and cardiac events when measured pre-operatively. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint INTRODUCTION Aging confers risk for most medical conditions. Cellular senescence is a well-known aging mechanism that links deleterious subcellular changes with multi-system loss of organ function and physiologic decline. Senescent cells are permanently growth arrested but metabolically active, secreting pro-inflammatory and pro- fibrotic cytokines that contribute to chronic inflammation and impaired tissue regeneration 1,2. These kinds of aging-related vulnerabilities can manifest as adverse events after major medical interventions. Such risks may be discordant with expectations based on chronological age and multi-morbidities. The latter are routinely considered in medical decision making and form the foundation of pre-surgical risk assessment. In the surgical setting, measuring aging-related risk pre-operatively through molecular biomarkers could allow for optimization of care pathways across the peri-operative period. Cardiac surgery has been at the forefront of risk mitigation and outcomes reporting, including quality improvement initiatives such as ERAS Cardiac3. However, post-operative morbidity and mortality after coronary artery bypass grafting (CABG), the most common form of cardiac surgery, remain high. Older, more co-morbid patients, increasing procedural costs, and value-based payments are driving strong interest in accurate, pre-operative patient stratification and targeted risk mitigation for cardiac patients. A better understanding of aging-related patient risk remains an unexplored area, ripe for discovery and integration into clinical practice. In this study we report the use of network of cellular senescence biomarkers to identify patients at-risk of cardiac surgery-associated adverse events. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint METHODS Study design and participants In the pilot study, adult patients >50 years and older undergoing primary elective or urgent, on-pump, CABG (isolated or combined with valve surgery) were prospectively enrolled at the Johns Hopkins University Hospital between September 2010 and March 2013, or Duke University Hospital between June 2015 and July 2017. Patients were excluded if they required emergency or salvage CABG, aortic aneurysm or congenital heart disease repair, primary ventricular assist device implantation, severe heart failure (LVEF <25%), hemodynamic instability requiring preoperative vasopressors or IABP, pre-existing end stage kidney disease (eGFR <15 mL/min/1.73m2) or renal transplantation, chronic liver disease or cirrhosis, or presence of major active infection (chronic or acute e.g., sepsis, HIV, pneumonia). A total of 46 and 42 participants were enrolled from Duke University and Johns Hopkins Hospitals, respectively. In the GUARD-AKI study, adult patients (>40 years) undergoing non-emergency (urgent or scheduled) cardiac surgery using cardiopulmonary bypass (CABG or combined CABG/valve) were prospectively enrolled at WakeMed Health and Hospitals, Johns Hopkins University Hospital, and Hoag Memorial Hospital Presbyterian between October 2020 and July 2022. Patients were excluded if they required emergency or salvage CABG, off-pump coronary bypass grafting, aortic aneurysm or congenital heart disease repair, primary ventricular assist device implantation, severe heart failure (LVEF <25%), hemodynamic instability requiring preoperative vasopressors or IABP, pre-existing end stage kidney disease (eGFR <30 mL/min/1.73m2) or renal transplantation, chronic liver disease or cirrhosis, or presence of major active infection (chronic or acute e.g., sepsis, HIV, pneumonia). All participants were tested for COVID infection per standard of care in preparation for surgery. This clinical study was registered with clinicaltrials.gov (NCT03635606). IRB of Johns Hopkins University, Duke University, and Central IRB (WIRB/WCG) overseeing WakeMed Health and Hospitals and Hoag Memorial Hospital Presbyterian gave ethical approval for this work. Power calculations for the GUARD-AKI study were based on the pilot study for a primary endpoint of AKI. The number is predetermined to achieve a 95% confidence interval width of 0.175% on the area under the curve (AUC) of the receiver operating curve (ROC) given that events to non-events occur in a 1:4 ratio such that: a random sample of 40 subjects from the AKI positive population and 158 subjects from the AKI negative All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint population produce a two-sided 95.0% confidence interval with a width of 0.175 when the sample AUC is 0.86. As such the lower limit of the 95% CI is 0.776 and the upper limit is 0.947.  Upon study conduct, it was noted that the event to non-event ratio was 1:5 and hence enrollment was increased to achieve the minimum number of required events. Outcome definitions Patients were followed for the duration of their hospital stay and at 30-day follow-up with the surgeon. In all cohorts, the primary endpoint was development of stage 1 or higher postoperative AKI as defined by KDIGO (sCr increase of ≥0.3 mg/dL in the first 48h or a relative increase of ≥50% in peak sCr from baseline within 7 days post-surgery). GUARD-AKI had additional outcome measures: development of worsening renal function (a ≥25% reduction from baseline eGFR at the 30-day follow-up visit); development of 30-day major adverse kidney events (MAKE30): a composite of persistently impaired renal function (a ≥25% reduction from baseline eGFR at the 30-day follow-up visit, new dialysis, and death); development of 30-day major adverse cardiac events (MACE30): a composite of myocardial infarction (MI), stroke, heart failure, and death; and development of the combination of MAKE30 and MACE30 (major adverse cardiac and kidney events [MACKE30]). A creatinine-based eGFR equation that does not incorporate race, developed by CKD-EPI 4, was used to calculate eGFR for all participants. Sample collection and biomarker measurements Peripheral blood samples were collected prior to surgery, stabilized according to Sapere Bio protocol and processed either on site (Duke University or Johns Hopkins University) or at Sapere Bio. 7.5ml of stabilized blood was used to isolate T cells as previously reported 5 and the remainder of the sample was used to isolate plasma. Samples were stored at -80oC until the analysis. Gene expression of p14, p16, LAG3, CD28, and CD244 was analyzed by real-time qPCR. Expression for each gene was normalized to expression of a housekeeping gene. Positive and negative controls were included in each run, and Cts over 37 were considered below the limit of detection. Expression of each senescence gene is reported in log2 units and as arbitrary units, as is standard for qPCR reporting of gene expression levels. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint Expression of suPAR, sTNF-R1, and Activin A was measured by ELISA in plasma using commercial kits (R&D Systems) per manufacturer’s instructions. In all analyses, lab personnel were blinded to clinical information and outcome measures. Statistical analyses Pilot Study—To build a predictive model for AKI, expression of p16, p14 (and their second- degree interaction), pre-operative sCr, as well as demographic and clinical variables such as gender, diabetes, and surgery type were tested in a logistic model. Factors for the logistic model for each outcome were chosen by backwards elimination. Akaike information criterion with correction for sample size (AICc) was used to find the model with both good fit to the truth and few parameters. GUARD-AKI – Descriptive statistics were reported as mean (sd) for continuous variables, and as frequency (percentage) for categorical variables. For analyses of all endpoints, missing data for any reason was not imputed.  If an outcome was missing, the subject was excluded from summary statistics and statistical analyses. If one of the biomarker measurements was missing, the subject was still considered for analysis if the particular predictive model did not include that biomarker as a variable. To build a predictive model for each outcome, a subset of pre-selected potential factors was tested in a logistic model.  These pre-identified factors were evaluated for inclusion using a randomly selected balanced dataset (50%) from GUARD-AKI. Expression of p14, p16, CD28, CD244, LAG3, suPAR (and their 2nd degree interaction), and sCr were evaluated. Using random sampling with replacement (>=200 iterations), logistic models were created using forward selection with model inclusion criteria of p-value<=0.25 after forced inclusion of p14 and p16.  The percentages of inclusion for each of the factors was calculated. Factors with percentage approximately 4% were considered for further model retention and final model parameter estimates were derived using the entire sample. The performance of each risk model was assessed by performing ROC analysis and measuring the total area under curve (AUC). A composite factor was created using each risk model and a distribution of values was generated across the complete sample. Probabilities of predicting each event were also modeled categorically in tertiles, with the lowest tertile serving as a reference group to derive an odds ratio for each outcome. The models were also adjusted for age, diabetes, CKD and CHF as potential effect modifiers. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint Optimal thresholds for each model were identified by examining a distribution of fitted probabilities vs classification of the outcome in question. Sensitivity, specificity, PPV, NPV and accuracy were calculated at a threshold identified for each outcome model. Confidence intervals for sensitivity, specificity and accuracy are “exact” Clopper-Pearson confidence intervals and confidence intervals for PPV and NPV are standard logit confidence intervals as escribed by Mercaldo et al 6. Two-tailed p values of less than 0.5 were considered statistically significant. Statistical analyses were performed in SAS version 9.4 and JMP 12.2.0 (SAS Institute, Cary, NC). All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint RESULTS Proof of concept of clinical relevance of biomarkers of senescence for risk prediction To determine if measurements of senescence can identify patients at risk for adverse events after cardiac surgery, we measured expression of p16INK4a (p16) mRNA in peripheral blood T cells, a master regulator and well-established biomarker of senescence1,7, as well as the related transcript, p14ARF (p14).  Expression of p16 and p14 was measured pre-operatively in a cohort of 60 patients who underwent elective cardiac surgery at two centers.  Study flow diagram and patient characteristics are shown in Figure S1 and Table S1. Given the small sample size, we focused on the most common adverse event post cardiac surgery, AKI. In this cohort, 30% (18/60) of patients developed in-hospital AKI. A regression model that included p16, p14, the p16*p14 interaction, and pre-operative sCr could identify patients at risk for AKI with AUC of 0.76, 80% accuracy, and 86% NPV (Figure S2). Thus, we found that cellular senescence biomarkers measured prior to surgery have the capacity to predict the most common cardiac surgery-associated adverse event, AKI. Biomarker network used to characterize cellular senescence While expression of p16 is a gold-standard measure of senescent cell load, and is a marker of established senescent cells 7,8, accumulation of senescent cells depends on both formation of senescent cells and their clearance by the immune system 9–11 (Figure 1A). Broadly, with aging there is an increase in the rate of formation of senescent cells, and a decline in immune surveillance capacity, leading to a progressive accumulation of senescent cells9,10,12. Thus, measuring biomarkers of immune function in addition to p16 would allow us to better capture potential age-related vulnerability to adverse events, challenges associated with managing inflammatory responses induced by cardiac surgery, and overall capacity to recover. The biomarkers shown in Figure 1 were selected for analysis based on studies in donors and patients in multiple clinical settings (manuscript in preparation), in order to capture established cellular senescence as well as age-dependent components of the adaptive and innate immune system. Briefly, CD28 and LAG3 are established markers of T cell exhaustion13,14 . CD244 was first described as an exhaustion marker and has also been shown to correlate with age-dependent impairment of T cells15 ; however, more recently it was shown to regulate autophagy16, an important process that may also involve p1417. Finally, suPAR has been All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint shown to be secreted by senescent cells18,19; it is an immune-derived pathogenic factor of kidney disease and potentially cardiovascular disease20,21. As expected, there is a large degree of association between these markers (Figure 1C) and a general, although weak, positive association with chronological age. CD28 expression declines with age consistent with a negative association with age and senescence markers. suPAR has the weakest association with senescent biomarkers consistent with the idea that total plasma levels reflect various sources of suPAR. GUARD-AKI study A total of 331 participants who underwent cardiac surgery at three different centers were enrolled into the study and used in the analyses in this manuscript (Figure 2). Baseline characteristics of the patients in the entire cohort and at each site are shown in Table 1. The average age was 67 ± 10, 79% of the participants were male, and most participants (79%) were white. Mean BMI was 28.9, eGFR 82.9, LVEF 53; 81.0% had hypertension, 42.3% had diabetes, 28.7% had congestive heart failure, and 17.5% had chronic kidney disease. Surgery was elective for 52.3% patients and urgent for 47.7% (emergency surgery was an exclusion criterion). 87.0% of procedures were isolated CABG and 77.6% involved three or more vessels. Expression of biomarkers of senescence was measured pre-operatively and is shown in Table 2. Preoperative biomarkers of cellular senescence predict AKI AKI occurred in 20% of patients postoperatively. To determine if pre-operative biomarkers of cellular senescence can predict the incidence of AKI, we performed regression analyses that included biomarkers of cellular senescence, their pairwise interactions, and serum creatinine (Figure 3). The resulting model (see Methods for detailed description) had an AUC of 0.76. The cut-off for determining patients at risk for AKI was chosen to balance false positives and false negatives. At a cut-off of 30% probability, our model could identify patients at risk for AKI with 78.6% accuracy, 86.7% specificity, and 86.6% NPV. Preoperative biomarkers of senescence predict decline in renal function at 30 days Patient characteristics at a 30-day post-operative follow-up vist are shown in Table 3. Eleven percent of patients had a decline in kidney function compared to pre-operative status (decline in eGFR ≥25%). In addition, 13.4% had at least one major adverse cardiac and kidney event at the 30-day post-operative follow-up. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint Post-discharge decline in eGFR is an important endpoint because there is substantial evidence that some patients who experience AKI will never recover their renal function and will experience an AKI-to-CKD progression. To determine if biomarkers of cellular senescence could predict the incidence of decline in eGFR at 30 days, we performed regression analyses as described for the AKI model, thereby generating another risk-prediction model with an AUC of 0.73 (Figure 4). For this model, the cut-off for designating patients at risk of renal decline was established to minimize false negatives. At a cut-off of 18% probability, our model could identify patients at risk for decline in renal function with 85.2% accuracy, 89.6% specificity, and 93.5% NPV. Incidence of AKI is largely non-overlapping with incidence of eGFR decline at 30 days While these models demonstrated that biomarkers of senescence can predict patients at risk for both AKI and persisting eGFR decline, we noticed a difference in the components between the two models, including elimination of sCr as a risk factor in the eGFR decline model. When examined closely, we found that only 21% of patients who had in-hospital AKI also had a decline in eGFR at 30 days. Surprisingly, the majority (63%) of patients who had decline in eGFR did not have AKI during their hospital stay. To better understand this phenomenon, we compared patient characteristics, multi-morbidities, and surgical factors across the AKI and eGFR decline endpoints. As expected, the incidence of AKI was strongly associated with known AKI risk factors such as age, CKD, hypertension, PVD, need for blood products post- operatively, and complex surgery (CABG+valve) as well as biomarkers of kidney inflammation (Activin A, sTNF-R1, and suPAR) (Table 4). Surprisingly, none of these risk factors for AKI was significantly associated with the incidence of eGFR decline. Moreover, while pre-operative CKD was strongly associated with AKI, it was not associated with post-operative eGFR decline and 80% of patients showing this eGFR decline did not have pre-existing CKD. This finding may explain why sCr was retained in the AKI prediction model but did not contribute to the eGFR decline prediction model. Thus, for most patients with eGFR decline at 30 days after surgery, this adverse event represents a new loss of kidney function, and not an AKI-to-CKD progression. Interestingly, incidence of new onset post-surgical atrial fibrillation (NOAF) was highly associated with the incidence of eGFR decline at 30 days, but only weakly associated with incidence of AKI. And while not statistically significant, patients with congestive heart failure were enriched in the group with eGFR decline as All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint compared to the group with no eGFR decline (37% vs. 28%), but equally distributed in patients with or without AKI (30% vs. 29%). Together, these results suggest different etiologies for these adverse events. Preoperative biomarkers of senescence predict a composite of cardiac and kidney events at 30 days Given the interdependent nature of heart and kidney function and disease, a composite of cardiac and kidney adverse events at 30 days post-surgery (MACKE30) was tested as an outcome. 13.4% patients had a MACKE30 event. Using the same markers and methods from the previous two models, we built a third regression analysis model with an AUC of 0.71. Similar to the model for eGFR decline prediction, the cut point was established to minimize false negatives (Fig. 5). At the cut-off of 19% probability, our model could identify patients at risk for MACKE30 with 79.9% accuracy, 84.8% specificity, and 91.4% NPV. Models based on senescence biomarkers are not improved by patient characteristics To assess the value of using cellular senescence biomarkers relative to demographics and multi-morbidities that are commonly used in patient care, we determined both the odds ratio of having an event based on a biomarker model, and the adjusted odds ratio when clinical variables were added to the model. Predicted probability of an event was examined in tertiles and the probabilities in the highest tertile was compared to those in the lowest tertile. Patients in the highest tertile of the cellular senescence biomarker-based model of AKI had 7.8 (95%CI 3.3-18.4, p=0.0001) higher odds of developing AKI than patients in the lowest tertile (Figure 6). Adjustment for clinical variables decreased the average odds to 5.5 (95%CI 2.2-13.7), but AKI prediction remained highly statistically significant (p=0.0003). A similar approach was used to estimate odds of decline in eGFR and MACKE30 using non-adjusted and adjusted senescence biomarker-based models. Again, senescence-biomarker-based models remained highly statistically significant but largely unchanged after the adjustment.  These data suggest that biomarkers of cellular senescence are identifying patients at risk for cardiac surgery-associated adverse events independently of clinical factors. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint DISCUSSION This is the first report using pre-operative, aging-related biomarkers of cellular senescence and immune system function to predict risk of common and serious cardiac surgery-related adverse events including in- hospital AKI, a decline in eGFR by ≥25% at 30 days, and a composite of cardiac and kidney adverse events at 30 days. Biomarkers of aging represent a significant conceptual departure from other measures of patient risk that, historically, have been disease or organ-specific, and thereby fail to capture the multi-system vulnerability associated with aging. As a primary mechanism of aging, senescent cells increase in abundance over time. However, senescent cell load varies dramatically between individuals, including among those of the same chronological age1,7. The rate of accumulation of senescent cells is a balance between senescence induction and clearance which is mediated by the immune system9,10,12. Induction of cellular senescence in immune cells alone was recently shown to induce senescence in tissues and organs throughout the body22, and there is increasing evidence that immune system impairment inhibits clearance of senescent cells and contributes to age-related conditions11,23. The data presented here support the assertion that senescent cell load is a key, multi-system risk factor for adverse events after cardiac surgery. Senescence-based risk alone appears sufficient to stratify patients by risk of AKI, eGFR decline at 30 days and MACKE30. Patients in the highest tertile had between 4.5-7.8 higher odds of developing an outcome independently of chronological age, CKD, diabetes, and CHF. Given the interdependent and perpetuating nature of kidney and cardiac function, there has been considerable effort in the past to predict risk of AKI. The only available molecular diagnostic, NephroCheck, uses kidney- specific biomarkers to identify injury after it occurs, eliminating the opportunity for prevention and the earliest intervention24. Predictive algorithms based on clinical variables alone have focused on only the most severe outcomes, such as the Thakar score25, or they have utilized intra-operative and post-operative variables that preclude use in everyday practice. In contrast, the biomarkers used in this study are measured pre-operatively and can be integrated into clinical decision making throughout the peri-operative period. Further, cellular senescence biomarkers are not kidney-specific, but rather capture overall organismal aging as well as immune system status. The multi-system nature of aging vulnerability captured here underlies the success of this study. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint Measuring the senescence network in the immune system has the potential to define the physiologic impact of senescent cell load on age-related dysfunction, low grade chronic inflammation, and reduced capacity to heal. We also observed that there is only 21% overlap between the patients who experienced AKI and those with a ≥25% decline in eGFR at 30 days, suggesting different etiologies. New post-operative eGFR decline, in the absence of prior renal decline (AKI or CKD), may instead associate more closely with cardiac dysfunction, since the population is enriched with those experiencing new onset atrial fibrillation, and potentially a history of congestive heart failure. Given that the vast majority of patients (80%) experiencing eGFR decline at 30 days did not have CKD (pre-op eGFR <60 ml/min/1.73m2), they may not currently be seen as high risk for kidney injury. Early, pre-operative identification of risk through senescence biomarker testing both reveals unexpected risk and allows for careful targeting of peri-operative interventions, such as the recently proposed order set for preventing AKI26. These interventions include intensive fluid management and hemodynamic monitoring, limited nephrotoxin exposure, and strict metabolic management. In addition, patients with low-risk biomarker status could be identified as candidates for an accelerated recovery plan, with potential for earlier mobility, line removal, transfer to less intensive care, NSAID vs opioid pain management, and early discharge. The predictive models in this study were designed with high specificity and negative predictive values to prioritize true negatives, i.e., higher confidence in “fast track” recovery status, at the cost of more false positives, i.e., utilization of a prevention focused order set in some patients who are not at-risk. This approach is well-aligned with the rapidly expanding ERAS Cardiac movement which has demonstrated that multi-modal, peri-operative interventions can improve patient outcomes, meet quality goals, and improve patient and staff satisfaction3,27. Beyond the discovery nature of this study, other limitations include the limited number of sites and enrollment during the pandemic. The characteristics of patients undergoing surgery may have been affected, whether through changing hospital policies or by changing preferences among both providers and patients regarding selection and timing of surgery. Future studies will be conducted to validate these predictive models, demonstrate clinical utility, and develop a molecular prognostic that can be utilized in everyday surgical practice. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint In summary, this is the first report utilizing senescence biomarkers for risk prediction to pre-operatively identify cardiac surgery patients at-risk of adverse events. These findings lay the foundation for future studies that can bring molecular aging to pre-surgical risk assessment, improving both clinical outcomes and resource utilization in cardiac surgery. All rights reserved. No reuse allowed without permission. 
(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
The copyright holder for this preprint
this version posted April 11, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288081
doi: 
medRxiv preprint",1
"Background In low- and middle-income countries (LMICs), including Bangladesh, modern contraception use remains lower than desired, resulting in a higher unmet need. A potential factor contributing to lower contraceptive use is reduced access to and use of lower tiers of government healthcare facilities, including home visits by family welfare assistants (FWAs), as well as women’s visits to community and satellite clinics. These relationships, however, are still unexplored in Bangladesh and LMICs more broadly. The aim of this study was to explore effects of lower tiers of government healthcare facilities on unmet need for contraception and contraception use in Bangladesh. Methods Data from 17,585 sexually active married women were analyzed from the 2017 Bangladesh Demographic and Health Survey. The outcome variables were any contraceptive use, modern contraceptive use, unmet need for contraception, and unmet need for modern contraception. The explanatory variables considered were respondent’s home visits by FWAs, respondent’s visits to a community clinic, and respondent’s visits to a satellite clinic. Multilevel mixed-effect Poisson regression with robust variance was used to determine the association between the outcome and explanatory variables, adjusted for individual-, household-, and community-level factors. Results Approximately 18% of respondents were visited by FWAs in the three months prior to the survey date and only 3.4% and 3.1% of women attended community and satellite clinics, respectively. Women who reported being visited by FWAs in the three months prior to the . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint survey were approximately 36% less likely to report an unmet need for modern contraception and 42% more likely to report using modern contraception than women who did not report such a visit. A higher likelihood of unmet need for contraception and a lower likelihood of contraception use was found among women who did not visit these community or satellite clinics or visited these clinics for other reasons than collecting contraception as compared to women who visited these clinics to collect contraception. Conclusion FWAs’ visits to respondents’ homes to provide contraception, as well as respondent’s visits to satellite and community clinics play a major role in Bangladesh to ensure contraception use and reduce the unmet need for contraception. However, their coverage is quite low in Bangladesh. The findings suggest an urgent need for greater government initiatives to increase the number of FWAs and engage in proper monitoring them at the field level. Keywords: Family welfare assistant visits, community clinic, satellite clinic, modern contraception, unmet need, Bangladesh. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint Introduction Every year, an estimated 121 million unintended pregnancies occur worldwide, accounting for 48% of the total 250 million pregnancies [1]. Around 92% (111.9 million) of these pregnancies occur in low- and middle-income countries (LMICs) - a percentage that is now increasing with the increasing number of reproductive aged (15-49 years) women in the total population [1, 2]. Nearly 61% of these pregnancies are ended by induced abortion. Access to safe abortion services however is restricted in LMICs with traditional or non-expert providers often the main source of abortion care despite limited knowledge of performing such care [1, 3-5]. This practice is responsible for approximately 192,000 maternal deaths every year in LMICs, and over 7 million women are admitted to hospital because of abortion related complications [6, 7]. Women who continue with unintended conceptions have also been shown to use maternal healthcare services less and engage in adverse health behaviours, which further increases adverse maternal and child health outcomes, including maternal and child mortality [2, 8-10]. Consequently, unintended conceptions are considered as ongoing public health threat in LMICs and a significant challenge to achieving the Sustainable Development Goal 3, which aims to ensure health and well-being for all by 2030 [2]. Key reasons for the higher prevalence of unintended pregnancy in LMICs is the non-use of contraception and an unmet need for modern contraception. This has been attributed to lower exposure to family planning messages, poor contraception knowledge and availability of emergency contraception [1, 2]. However, improving access to, and uptake of contraception in LMICs remains challenging, particularly for socio-economically disadvantaged women and those living in rural areas, despite the fact that the successful implementation of the Millennium Development Goals resulted in a significant increase in modern contraception use (from 52% to 62% in 2015) [11, 12]. Consequently, it has been found that around half of women of reproductive age who are in-union and living in LMICs like Bangladesh do not have proper . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint access to modern contraception [13]. In Bangladesh, only 48% of women use modern contraception [14] and 15.5% have an unmet need for contraception, with these figures significantly higher in rural areas. Together, these behaviours, lead to a higher occurrence of unintended conception (48%) in the country [12]. Family planning services in most LMICs, including Bangladesh, play a critical role in providing access to, and use of, contraception [14, 15]. However, evidence suggests a significant decline in reliance on family planning services in Bangladesh in recent years, possibly due to the government's focus on maternal and child health issues over family planning services [14]. As a result, there has been an increase in births resulting from contraception failure in Bangladesh in recent years [15]. Bangladesh has strong family planning services network countrywide through (i) Family Welfare Assistants’ (FWAs) visits to eligible couples' (sexually active married women) homes every 14 days, (ii) providing access to contraception at the community level through 18,000 community clinics, and (iii) ensuring availability of contraception at satellite clinics [2]. Additionally, several non-governmental organizations (NGOs) provide family planning and contraception services, primarily in disadvantaged areas. Family planning and contraception services are also available in specialized private clinics and pharmacies; however, accessing these services requires a service charge. Therefore, their contributions in providing contraception to eligible couples is low. As a result, the government network continues to be the dominant source of contraception provision to eligible couples [2, 15]. However, little is known about the contribution of government sources of contraception provision, particularly with respect to the unmet need for contraception. It is critical for policymakers to identify which dimensions of family planning services are most effective now and where more emphasis should be placed to ensure universal access to contraception. However, this understanding is lacking in LMICs, including Bangladesh, and available studies have mainly examined socio-demographic factors associated with . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint contraception use and the unmet need for contraception [16-21]. We therefore aimed to determine the effects of respondent’s home visits by FWAs, respondent’s visits to community clinics, and respondent’s visits to satellite clinics on contraception use and unmet need for contraception adjusted for potential confounders at the individual, household, and community- level. Methods We analysed data from the 2017/18 Bangladesh Demographic and Health Survey, a nationally representative survey conducted every three years. The survey was administered by the National Institute of Population Research and Training, a subsidiary of the Ministry of Health and Family Welfare of Bangladesh. Financial and technical support was provided by several development partners, including USAID and UNFPA. The survey selected nationally representative households through a two-stage stratified random sampling method. At the first stage of sampling, the survey selected 675 enumeration areas (EAs, clusters) from the list of 293,579 EAs used by the Bangladesh Bureau of Statistics in the 2011 National Population Census. Of these EAs, data collection was undertaken in 672 EAs. Prior to data collection, a household listing operation was conducted. This was then used to select a fixed number of 30 households from each selected EA through probability proportional to the unit size. A total of 20,160 households were selected, of which data collection was undertaken in 19,457 households, resulting in a 96% inclusion rate. There were 20,376 eligible women aged 15-49 years who were usual residents of or lived in those households the night before the date of the survey. Data were collected from 20,127 women, resulting in a response rate of 98.8%. Details about this survey have been published elsewhere [14]. Analytic sample Data from 17,585 women who met the inclusion criteria were analysed in this study. The inclusion criteria were: (i) reproductive aged (15-49) married women with capacity to conceive, . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint (ii) sexually active (at least one reported episode of sexual intercourse within one month preceding the survey), (iii) not currently pregnant and not in the post-partum amenorrhea period, (iv) not wanting to have a baby within 24 months of the interview date and (iv) responded to the questions related to contraception use and the unmet need for contraception. Outcome variables We considered four outcome variables: (i) use of any contraception methods, (ii) use of modern contraception methods, (iii) unmet need for contraception, (iv) unmet need for modern contraception. These variables were based on the women's responses to questions related to contraception use or non-use and their intention for future conception. At first, eligible women were asked, ""Are you or your husband currently doing something or using any method to delay or avoid getting pregnant?"" Responses were recorded dichotomously as Yes or No. Women who reported positively were then asked, ""Which method are you using?"" Where women reported multiple contraceptive methods, the most frequent contraceptive method was used. This response was then reclassified as any contraception use (Yes vs No) and modern contraception use (Yes vs No), using the World Health Organization's recommended list of contraception [22]. Furthermore, women's responses on contraception methods used were considered along with their intention for pregnancy and birth in the future to calculate the unmet need for contraception and unmet need for modern contraception. Data on future pregnancy intentions were collected by asking women, ""Would you like to have (a/another) child, or would you prefer not to have any (more) children?"" If the respondent wanted to have another child in the future, then they were asked, ""How long would you like to wait from now before the birth of (a/another) child?"" The variables unmet need for contraception (No vs Yes) and unmet need for modern contraception (No vs Yes) were generated if the respondent reported they did not want to have a baby in the future or near future (<24 months), but they were not using contraception or modern contraception, respectively. Study factors . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint The study variable was the accessibility of contraception methods. Women who reported using contraception were asked about the source of their last obtained contraception method. Given the majority of contraceptive services are provided by the government at the field level, only these sources were ascertained. Specifically, respondents were asked whether they had received the following services to collect contraception in the three months prior to the survey i) a community clinic (yes, no, respondent’s visited community clinic but did not collect contraception), ii) a satellite (yes, no, respondent’s visited satellite clinic but did not collect contraception), ii) a visit by FWAs (yes, no). Confounding variables Potential confounding variables were identified by a comprehensive literature search [16-21]. The identified variables were women's age (treated as a continuous variable), women's educational status (no formal education, primary, secondary, and higher), women's working status (yes, no), parity (no children, 1-2 children, >2 children), and intervals between the two most recent live births (≤2 years, 3-4 years, >4 years). Partner's education (no formal education, primary, secondary, and higher), partner's occupation (agriculture worker, physical worker, services, business, other), type of family (joint, nuclear), and wealth quintiles (poorest, poorer, middle, richer, and richest) were also considered. Other variables included were women's place of residence (urban, rural), and region (Barisal, Chattogram, Dhaka, Khulna, Rajshahi, Mymensingh, and Sylhet). Statistical analysis We explored the weighted distribution of the respondents' socio-demographic characteristics through descriptive statistics, including frequency (percentage) and median (Q1-Q3). The effects of service accessibility on any contraception use, and modern contraception use, unmet need for contraception, unmet need for modern contraception were examined using a multilevel Poisson regression model. Separate models were run for each of the three outcome variables, with each . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint model containing one explanatory variable: respondent’s home visits by FWAs, respondent’s visits to community clinics, and respondent’s visits to satellite clinics, along with other individual, household, and community-level factors. We used a multilevel Poisson regression model because the survey data we analyzed were clustered. This is important to obtain accurate results particularly when prevalence of the outcome variables is higher than 10%. Previous studies have shown that the logistic regression model overestimates the true likelihood if the prevalence of the outcome variable is higher than 10%. The multilevel Poisson regression model can consider both the clustering structure of the data and address the issue of overestimation. We checked for multicollinearity before including variables in the models. If evidence of multicollinearity was found, we deleted the relevant variable and ran the model again. Results were reported as Prevalence Ratios (PR) along with their 95% Confidence Intervals (95% CI). All statistical analyses were conducted using Stata software version 15.1 (Stata Corp, College Station, Texas, USA). Results Background characteristics of the respondents The weighted percentage of the respondents’ socio-demographic characteristics are represented in Table 1. The median age of the respondents was 32 years. Around 38% of the total respondents had secondary level education. Almost 6% of respondents had not given birth at the time of the survey and among the women, who had children, around 11% had a birth interval of less than 2 years. Nearly 20% of the interviewed sample were from poorest quintile and 72% resided in rural areas. Table 1: Background characteristics of the respondents Characteristics  
n (%) Women’s age, median (Q1-Q3)  
32 (25-40) Women’s education status No formal education 
3157 (17.9) Primary  
5712 (32.5) Secondary  
6760 (38.4) Higher  
1956 (11.2) . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint Women’s employment status Yes 
8986 (51.1) No  
8599 (48.9) Partner’s education status No formal education 
3772 (22.9) Primary  
5322 (32.4) Secondary  
4800 (29.2) Higher  
2510 (15.3) Partners’ occupation Agriculture worker 
4424 (27.6) Physical worker 
7380 (46.0) Services  
837 (5.2) Business  
3361 (20.9) Other  
38 (0.28) Parity No children   
1091 (6.2) 1-2 children 
8610 (49.0) >2 children 
7884 (44.8) Intervals between the two most recent live births ≤2 years 
1974 (11.2) 3-4 years  
4219 (24.0) >4 years  
11392 (64.8) Family type Nuclear family  
7614 (43.3) Joint family  
9971 (56.7) Household wealth status Poorest  
3356 (19.1) Poorer  
3537 (20.1) Middle  
3560 (20.3) Richer  
3621 (20.6) Richest  
3511 (20.0) Place of residence Urban  
4908 (27.9) Rural  
12677 (72.1) Region (administrative division) Barishal  
1005 (5.7) Chattogram  
3164 (18.0) Dhaka  
4381 (24.9) Khulna  
2074 (11.8) Mymensingh 
1323 (7.5) Rajshahi 
2472 (14.1) Rangpur  
2129 (12.1) Sylhet  
1036 (5.9) Prevalence of contraception use and unmet need for modern contraception The percentage of individuals using any form of contraception was 63.1%, while the percentage of those using modern contraception was 52.9% (Table 2). Additionally, the reported percentage . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint of unmet need for contraception was 13.5%, and the unmet need for modern contraception was 24.4%. Table 2: Distribution of contraception use and unmet need for contraception, Bangladesh, 2017/18. Contraception use/unmet need for contraception 
Prevalence, 95% CI Any contraception use  
63.1 (62.06-64.13) Modern contraception use  
52.9 (51.8-54.0) Unmet need for contraception 
13.5 (12.7-14.3) Unmet need for modern contraception 
24.4 (23.5-25.3) Figure 1: Source of collecting contraception across respondents’ wealth quintile Access to lower tiers of government healthcare facilities for obtaining contraception The distribution of the respondents based on the sources of obtaining last form of modern contraception are presented in Table 3. FWAs provided last form of modern contraception to 18.1% of the total respondents. Around 3.4% and 3.1% of respondents reported they obtained their last form of modern contraception from satellite and community clinics, respectively. Together, around 23% of respondents reported that they collected their last form of modern contraception from one of the three sources in the three months prior to the survey. Table 3: Distribution of specific sources for obtaining contraception (Family welfare assistants, community clinic, and satellite clinic) in Bangladesh in the three months prior to the survey completion, BDHS, 2017/2018. Specific sources for obtaining contraception 
n (%) Home visits by FWAs No 
14404 (81.9) . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint Yes 
3181 (18.1) Visits to community clinic Yes 
540 (3.1) Respondent’s visited community clinic but did not collect contraception 
10106 (57.5) No 
6932 (39.4) Satellite clinic Yes 
591 (3.4) Respondent’s visited satellite but did not collect contraception 
15725 (89.5) No 
1250 (7.1) Respondent either visited by FWAs or they visited community clinic or satellite clinic No    
13542 (77.0) Yes  
4042 (23.0) Distribution of contraception use and unmet need for contraception across respondents’ contraception accessing options The distribution of use of any contraception use, and modern contraception use, unmet need for contraception, and unmet need for modern contraception across respondents’ source of contraception are presented in Figure 1. The use of any contraception (84.47% vs 56.72%) and modern contraception (78.03% vs 45.42%) were found significantly higher among women who either visited community or satellite clinics or FWAs visited their homes to provide contraception. We found unmet need for contraception (6.9% vs 15.63%) and unmet need for modern contraception (13.38% vs 27.94%) were around 50% lower among the respondents collected contraception from community clinic or satellite clinic or visited by FWAs to provide contraception. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint Figure 1: Contraception use and unmet need for contraception use status across respondents’ contraception accessing options Lower tiers of government healthcare facilities and its association with contraception use and unmet need for contraception The effects of contraception accessing options on contraception use and unmet need for contraception are presented in Table 4. A 36% lower prevalence of unmet need for modern contraception (aOR, 0.64, 95% CI, 0.58-0.70) and 42% lower prevalence of unmet need for contraception (aOR, 0.58, 95% CI, 0.51-0.67) among women who were visited by FWAs than those who were not was found. In addition, around 3 to 11 times higher likelihoods of unmet need for contraception and unmet need for modern contraception were found among women who did not obtain their contraception from either a community clinic or satellite clinic or visited community clinic or satellite clinic but did not collect contraception as compared to the women who collected contraception from the community clinic or satellite clinic. Having at least one visit by FWAs in the three months prior to the survey date to provide contraception was associated with a 1.34 (95% CI, 1.31-1.38) times higher likelihood of using any contraception 15.63 27.94 56.72 45.42 6.9
13.38 84.47
78.03 0 10 20 30 40 50 60 70 80 90 100 Unmet need for contraception 
Unmet need for modern 
contraception 
Any contraception use
Modern contraception use Respondents were not exposed to any sources of family planning Respondents were exposed to at least one family planning provider source . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint method and a 1.46 (95% CI, 1.41-1.51) times higher likelihood of modern contraception use. Around 31%-46% lower likelihoods of any contraception use and modern contraception use was found among women who reported either visits to community or satellite clinics, but not to collect contraception, as compared to the respondents who reported they collected contraception from any of these sources. Exposure to at least one of the three sources resulted in 51-54% lower likelihoods of unmet need for contraception and 1.46-1.66 times higher likelihoods of any contraception use or modern contraception use. We also conducted separate analyses for rural and urban samples to check whether lower tiers of government healthcare facilities operate differently in rural and urban areas. However, we did not report any significant differences  (results not shown in the table). . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint Table 4: Government sources of obtaining contraception in the three months prior to the survey and its association with unmet need for contraception and contraception use Service  
Any contraception use, 
PR (95% CI) 
Modern contraception 
use, PR (95% CI) 
Unmet need for 
contraception, PR 
(95% CI) Unmet need for 
modern 
contraception, PR 
(95% CI) Respondent’s home visits by FWAs No 
1.00 
1.00 
1.00 
1.00 Yes 
1.34 (1.31-1.38) *** 
1.46 (1.41-1.51) *** 
0.58 (0.51-0.67)*** 
0.64 (0.58-0.70) *** Community clinic Respondent’s visits to community clinic 
1.00 
1.00 
1.00 
1.00 Yes 
0.64 (0.62-0.66) *** 
0.54 (0.52-0.56) *** 
10.62 (5.09-22.15) ** 
12.15 (6.81-21.67) ** Respondent’s visited community clinic but did not collect contraception 
0.66 (0.64-0.69) *** 
0.57 (0.55-0.60) *** 
9.12 (4.38-18.99) ** 
10.82 (6.09-19.24) ** Satellite clinic Yes 
1.00 
1.00 
1.00 
1.00 Respondent’s visited satellite but did not collect contraception 
0.67 (0.65-0.69) *** 
0.57 (0.55-0.59) *** 
4.27 (2.67-6.83) *** 
5.59 (3.69-8.48) ** No 
0.69 (0.65-0.73) *** 
0.59 (0.55-0.64) *** 
3.55 (2.17-5.81) *** 
4.95 (3.22-7.60) *** Respondent either visited by FWAs or they visited community clinic or 
satellite clinic No    
 
 
1 Yes  
1.46 (1.42-1.50) *** 
1.66 (1.61-1.72) *** 
0.46 (0.40-0.54) *** 
0.49 (0.45-0.55) *** Note: Models are adjusted for women’s age, women’ education, women’s working status, partner’s education, partner’s occupation, parity, interval between two most recent life birth, family types, wealth quintiles, place of residence and place of region. ***p<0.01, **p<0.05 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint Discussion In this study, we investigated the effectiveness of lower tiers of government healthcare facilities, such as, respondent’s home visits by FWAs, respondent’s visits to community clinic, and respondent’s visits to satellite clinic on the use of contraception and unmet needs for contraception, while adjusting for individual, household, and community-level factors. We found that approximately 23% of all reproductive-aged women in Bangladesh obtained contraception from one of these three sources in the three months prior to the survey, with the majority receiving it from FWAs. After accounting for various factors, our fully adjusted model revealed that access to each of the three sources was a significant predictor of contraception use and non- access increased the likelihood of unmet needs for contraception. These findings underscore the importance of strengthening the government's family planning options at the field level to promote the use of modern contraception and decrease unmet needs for contraception in Bangladesh. This could aid Bangladesh in achieving its SDGs targets by ensuring universal access to sexual and reproductive healthcare services and reducing maternal and under-five mortality rates. Bangladesh has achieved historic progress in expanding access to contraception use over the last 50 years with around 54% of the total eligible population now using modern contraception compared to 7% in 1974 [15]. This historic progress is largely due to the government efforts that began in 1972 by initiating an independent family planning division under the Ministry of Health and Family Welfare that has invested in and delivered major policies and programs in the subsequent years [23]. These include a declaration of rapid population growth as the number 1 problem in Bangladesh and its focus on the national population policy outline (1975-1980), formation of upazila (second administrative unit of Bangladesh)  family planning committee (to monitoring family planning activity) (1980-1985), formation of satellite clinics (to ensure family planning availability in remote and rural area) and initiation of unit-wise FWA registers (to keep . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint family planning and other demographic record) (1985-1990) and consideration of family planning programs in subsequent health, nutrition and population sector program (1998-present) [2]. However, Bangladesh’s family planning agenda remains incomplete with very slow progress since the early 2000’s - a time when non-use of modern contraception and unmet need for modern contraception were 50% and 27%, respectively. Research has shown that that this situation remains mostly unchanged [15]. The reasons for such stagnation are many, however, lack of sustainability of the policies and programs and reducing focus on family planning to prioritise other maternal and child health indicators as targeted in the Millennium Development Goals in 2000 have been suggested as core reasons [2, 15]. One of Bangladesh's most successful family planning initiatives involved FWAs visiting eligible women's homes every 14 days to provide contraception counselling and supplies, as well as the use of mass media to raise awareness about family planning [24]. In 2000, approximately 46% of eligible women received FWAs’ visits and 50% learned about contraception through mass media, with even higher contributions among rural and disadvantaged populations [24, 25]. However, despite its reported success, FWAs’ visits have declined over time, with only 18% of respondents reporting such visits in this study. This reduction in FWAs' visits is concerning given their past success in increasing contraception uptake and reducing unmet need for contraception, and several challenges at the respondent, provider, and policy levels may be contributing to this decline. Bangladesh has made significant progress in women's education and their participation in formal work, means less availability for at home services compared to the 2000s [26]. Consequently, the previous approach of providing family planning and contraception through FWAs visiting women's homes is no longer effective for many women. The main reason is that the visits are made during the day when women are at work, making it difficult for them to receive . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint information and methods for contraception, even if they have such intention [15, 27]. In addition, there is a shortage of FWAs, with approximately 40% of FWAs’ positions currently vacant [27]. The government has not been successful in addressing this issue due to structural challenges, including current vacancies of FWAs and educational requirements to undertake FWA roles. The minimum educational qualification required for FWAs recruitment is higher secondary level education, with no restriction on the maximum educational requirement. However, the flexible criteria for the maximum educational level required, combined with the current higher unemployment rate in Bangladesh among higher educated people, means that most FWAs have graduate or post-graduate degrees. They often continue to search for better jobs after working as an FWAs because the position is considered a low-status government job in Bangladesh, and the salary offer is much lower than other jobs with similar qualifications. Often, they leave the current position as FWAs once they find another better job, making it vacant. Furthermore, the current number of FWAs positions is inadequate, as the positions were created in the 1980s when the population of Bangladesh was around half of what it is now. This has led to reduced service quality, fewer visits, and a reduction in contraception use, resulting in an increase in unmet need for contraception. The challenges encountered by the FWAs are also responsible for the declining roles of community clinics and satellite clinics in the provision of contraception [28]. In this study, we found that community clinics and satellite clinics play an important role in increasing contraception uptake and reducing unmet needs. The underlying reason is that FWAs are mostly responsible for running satellite clinics, and their absence or lack of presence leads to the unavailability of satellite clinics. Even if the satellite clinics are available, a lack of FWAs may lead to inadequate focus on family planning and contraception because child vaccinations are given higher priority than family planning [25]. There are around 18,000 community clinics in Bangladesh run by the government's recruited MBBS doctors, but it also faces similar challenges like the satellite clinics. For instance, MBBS doctors do not feel comfortable staying in the village . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint due to the lack of accommodation, transportation, and other residential facilities. Consequently, community clinics mostly run without the presence of healthcare personnel, or limited healthcare personnel [28]. This irregularity of healthcare provision leads to other healthcare taking precedence (e.g., district level hospital). In addition, patients also do not feel comfortable accessing family planning services and contraception with others present, as it is a culturally sensitive issue. Since 2010, when family planning and contraception became available in upazila and district-level hospitals, healthcare personnel in community clinics may have thought that respondents should collect contraception from these sources, although, in reality, this is rare at the field level [2]. Along with these challenges, the current achievement of replacement-level fertility should motivate the government to reduce the focus on family planning and contraception at the field level and give more focus on population management [27]. However, whatever the challenges or intentions are, it is important to ensure that family planning and contraception are easily accessible at government-supported facilities to increase contraception use and reduce unmet needs, leading to the improvement of maternal and child health. The implications of our findings are that the government-supported family planning and contraception options, including FWAs visits, satellite clinics, and community clinics, continue to play a major role in increasing contraception uptake and reducing unmet need. Therefore, there needs to be substantial investment to strengthen these options and improve their ability to provide family planning and contraception services. It is also important to ensure an adequate number of FWAs at the field level by creating new FWAs’ posts and filling them, as well as providing them with better salaries and other opportunities for a better lifestyle. Separate family planning and contraception facilities should be established in all satellite clinics and community clinics, along with proper monitoring at all levels. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint The current study has several strengths and a few limitations. This is the first study of its kind that provides evidence of the importance of sustaining FWAs’ visits, satellite clinics, and community clinics at the community level at a time when the focus on all three levels is declining. The data for this study was also extracted from a nationally representative survey and analyzed through advanced statistical methods with a range of individual, household, and community-level factors. Therefore, the findings of this study are more precise and should be used for national-level policy and program-making. However, the data analyzed in this study was cross-sectional in nature, and the reported associations are therefore correlational only and not causal. The data was also collected retrospectively, and respondents were asked to answer on past events; therefore, there may be recall bias in the responses. However, to reduce this bias, respondents were asked several follow-up questions on their past events, and any biases are likely to be random. In addition, besides these three avenues of providing family planning and contraceptives by the government, there are also upazila (second administrative unit of Bangladesh) to tertiary level hospitals. Also, anyone can obtain contraception from local pharmacies and private facilities. However, these data are not available in the survey, so they were not considered in this study. Despite these limitations, the associations reported in this study provide an important direction about the massive role that these three-family planning and contraception providing options of the government at the field level play to ensure contraception use and reduce unmet need. Conclusion In this study, modern contraceptive use was found to be low with only half of women using these methods. In addition, a quarter of women were found to have an unmet need for contraception. FWAs’ visits to respondents' homes, as well as visits to community clinics and satellite clinics, were found to play an important role in increasing modern contraceptive uptake and reducing unmet need for modern contraception. These findings suggest the need for policies and . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint programs to strengthen these government-based family planning and contraception services at the field level to reduce the current high rates of contraception non-use and unmet need. A core part of these strategies will be to address the existing lack of FWAs at the field level by identifying ways to educate, recruit and retain these vital workers. Declaration of interests: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgement: The authors thank the MEASURE DHS for granting access to the 2011 and 2017/18 BDHS data. Funding: This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors. Authors’ contributions: MNK designed the study, performed the data analysis, and wrote the first draft of this manuscript. SJK, MAK, MMI and MLH critically reviewed and edited the previous versions of this manuscript. All authors approved this final version of the manuscript. Data availability: The datasets used and analysed in this study are available from the Measure DHS website: https://dhsprogram.com/data/available-datasets.cfm . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.06.23288229
doi: 
medRxiv preprint",1
"Bioenergetic failure caused by impaired utilisation of glucose and fatty acids contributes to organ dysfunction across multiple tissues in critical illness. Ketone bodies may form an alternative substrate source, but the feasibility and safety of inducing a ketogenic state in physiologically unstable patients is not known. Twenty-nine mechanically ventilated adults with multi-organ failure were randomised into a two- centre safety and feasibility trial of ketogenic versus standard enteral feeding. Ketogenic feeding was feasible, safe, well tolerated and resulted in ketosis. Patients receiving ketogenic feeding had fewer hypoglycaemic events (0% vs. 1.58%), required less exogenous insulin (0.0 IU (IQR 0-16) vs.78 IU (IQR 0-412) but had slightly more daily episodes of diarrhoea (53.5% vs. 42.9%) over the trial period.  Untargeted metabophenotyping revealed altered Cahill cycle flux and bioenergetic states, suggesting an advantageous metabolic profile. Ketogenic feeding is feasible and may be a novel intervention for addressing bioenergetic failure in critically ill patients. Clinical Trials.gov registration: NCT04101071; 19.09.2019. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 RUNNING HEAD: Ketogenic enteral feeding KEY WORDS: Enteral nutrition; nasogastric; critical care; ketogenic feeding; feasibility Take-home Message: Critical illness leads to altered metabolic states and bioenergetic failure caused by impaired utilisation of glucose, fatty acids and amino acids. This contributes to organ dysfunction across multiple tissues. Ketones may provide a safe and acceptable alternative metabolic fuel enabling energy production and maintaining tissue homeostasis. Tweet:  Ketogenic enteral feeding in early critical illness is feasible, safe and may decrease insulin requirements. ACKNOWLEDGEMENTS AM, DB, SE, KR, HM and ZP received a Research for Patient Benefit grant (PB-2006) from the National Institute for Health Research (NIHR). HM received funding from the NIHR’s Biomedical Research Centre (BRC) at University College London Hospitals, London, UK. The research was also supported by the NIHR BRC based at Guy's and St Thomas' NHS Foundation Trust and King's College London and by the NIHR BRC at Great Ormond Street Hospital. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, the Department of Health or the funders. Metabolomic data was supported by an educational grant from Nestle Health Sciences. Vitaflo International Ltd were involved in initial discussions about the study and provided the K.Quik® component for the ketogenic feed gratis. Neither Vitaflo International Ltd nor Nestle Health Sciences contributed to study design, study implementation, data analysis or interpretation. We would like to thank the patients (and their families) who took part, and the research nurses of both recruiting centres for their willingness to engage. Specifically: Maria Fernandez, Filipa Santos, Amaia Garcia, Fatima Seidu, Katie Sweet. We would also like to thank those who funded this study (below). FUNDING: NIHR Research for Patient Benefit (PB-PG-0317-20006: £249,560; plus £10,549 additional COVID-related funding). Education grant from Nestle Health Science (£25000). DECLARATIONS OF INTEREST: DEB has received speaker fees, conference attendance support or advisory board fees from Baxter, Cardinal Health and Avanos. ZP has received honoraria for consultancy from GlaxoSmithKline, Lyric Pharmaceuticals, Faraday Pharmaceuticals and Fresenius- Kabi and speaker fees from Orion and Nestle.  HM holds patents relating to intravenous hydration . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 3 and to regulation of metabolic efficiency using renin-angiotensin system antagonists. SE and SJH hold patents with Vitaflo International Ltd for compositions different from that used in this study, for managing drug resistant epilepsy and disorders associated with mitochondrial dysfunction, and also are in receipt of grant funding from Vitaflo International Ltd (not connected with this study). A patent has been submitted for the ketogenic feed regime used in this study (ZAP, AM, AL, DB). Vitaflo International Ltd were involved in initial discussions about the study and provided the K.Quik® component for the ketogenic feed gratis. Neither Vitaflo International Ltd nor Nestle Health Sciences contributed to study design, study implementation, data analysis or interpretation. Other authors have no conflicts of interest to declare. Author contributions: AM, DB,SE,KR,HM and ZP conceived and designed the  clinical trial. 
AM, DB,AL, KR, ZP, JP, RP, AP, TM, FS, FS1, KL carried out and delivered the clinical trial. 
AM, ZP, PA, DW, HC, IA, SE, TB, SH performed the analyses. 
All authors read and approved the manuscript. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 4 Introduction Critical illness is a state of ill health with vital organ dysfunction and a high risk of imminent death if care (pharmacological or mechanical) is not provided and has the potential for reversibility1. The physiological characteristics of critical illness have significant overlap across a wide range of presenting diseases, challenging commonly used disease-related taxonomies2. Multiple diverse stressors result in a unifying state of altered tissue metabolism and bioenergetics, compounding organ dysfunction and cell death in multiple tissues such as the brain, lung, kidney and skeletal muscle3-6. Specifically, substrate utilisation in the tri-carboxylic acid (TCA) cycle is impaired in critical illness, with tissue hypoxia and inflammation prevent glucose-derived pyruvate from being converted to acetyl-CoA, as a result of the Pasteur effect7 8. Amino acids may be recycled for pyruvate reconstitution in starvation, but such processes (e.g., the Cahill cycle) are affected by tissue hypoxia, inflammation, impaired Glucose Transporter Type 4 (GLUT-4) translocation, exogenous insulin therapy and other hallmarks of critical illness8-10. Finally, mitochondrial fatty acid oxidation is downregulated, and the resultant inability to use any of these three substrates efficiently leads to a bioenergetic crisis8 11 12. Under conditions of physiological stress, ketone bodies provide a source of substrate for ATP generation. In high intensity exercise, ketogenic diets provide ketone bodies for substrates, improving ATP production decreasing muscle protein breakdown and improving physical performance.13 During periods of starvation, brain metabolism relies on ketone bodies instead of fat or glucose14, and ketone bodies may provide up to 50% of total body basal energy, enabling the high-energy requirement of the human brain to be met whilst sparing muscle. 15 16 Ketogenic diets reverse the metabolic defects of non-alcoholic fatty liver disease.17 Patients with diabetes use ketone bodies for cardiac ATP synthesis.18  Ketone bodies such as beta-hydroxybutyrate and acetoacetate are the result of hepatic metabolism of fatty acids. Ketolysis occurs in mitochondria of extra-hepatic tissues, resulting in the formation of acetyl-CoA. The rate-determining step is the reconstitution of acetoacetyl-CoA from acetoacetate by the enzyme succinyl CoA-oxoacid transferase, which is not regulated by hypoxia or inflammation, unlike pyruvate dehydrogenase kinase19. Ketone bodies may therefore offer an alternative substrate source for energy production in critically ill patients.In addition, ketones may have other beneficial impacts in critically ill patients: in those with Acute Respiratory Distress Syndrome (ARDS), beta-hydroxybutyrate metabolically reprogrammes T-cells to improve functionality.20 However, the feasibility and safety of achieving ketosis in unstable patients in multi-organ failure has yet to be proven. Ketoacidosis might occur if ketones were not metabolised, exacerbating pre- existing systemic and cellular acidosis that carries a mortality risk to critically ill patients. Ketogenic diets minimise exogenous glucose delivery, which might predispose patients to hypoglycaemia, . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 5 which is harmful to patients21. Lastly, a ketogenic high lipid feed might increase the risk of vomiting (and therefore pulmonary aspiration), diarrhoea and pancreatitis 22. One specific physiological consequence of critical illness that may be attenuated by a ketogenic diet is that of muscle wasting. Critically ill patients lose 2-3% of their muscle mass per day23.  This is associated with increases in length of stay and mortality, and associated physical functional disability may persist for up to 5 years24. Patients, carers and health services are burdened by this physical function disability, which is recognised as a public health issue25 26 27.  Muscle wasting and subsequent weakness in these patients has proven resistant to all forms of exercise rehabilitation and increased nutritional delivery of energy and protein28 29. This loss of muscle mass appears driven by a decrease in muscle protein synthesis and unchecked muscle protein breakdown 23. This in turn may be a consequence of bioenergetic failure and a lack of adenosine tri-phosphate production8. Muscle protein synthesis is a highly energy-dependent process and is likely to remain depressed until muscle bioenergetics normalise. We therefore performed a randomised trial to determine the feasibility and safety of delivering a ketogenic enteral feed in critically ill patients and collecting physical function specific outcomes. We additionally performed an exploratory analysis of plasma metabolomic profiling to ascertain the presence or absence of a signal for efficacy in altering tissue metabolism, which might warrant further research in a larger trial. Methods We performed a single-blinded randomised controlled feasibility trial in two UK intensive care units (ICUs), with an allocation ratio of 1:1. Participants Adult (≥18 years) ICU patients were screened for inclusion on weekdays, being eligible for enrolment up to 48 hours after ICU admission. Inclusion Criteria: Requiring enteral nutrition via nasogastric tube; expected to be intubated and ventilated ≥48 hours; multi-organ failure (Sequential Organ Failure Assessment (SOFA) score >2 in >2 domains)30; likely ICU stay >5 days and likely survival >10 days (assessed as previously by senior ICU clinicians23). Exclusion Criteria: Primary neuromyopathy or significant neurological impairment at the time of ICU admission that would preclude physical activity; unilateral/bilateral lower limb amputation; requirement for sole or supplementary parenteral nutrition; need for specialist nutritional intervention; known inborn error of metabolism; participation in another clinical trial. Patients at risk of refeeding syndrome (based on NICE guidelines31) were assessed on an individual basis. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 6 Prospective informed assent was by nominated personal consultee (in person or by telephone) or professional consultee. Retrospective participant consent was obtained on return of each participant’s mental capacity. Permission to use participants’ data if capacity did not return or if they did not survive, was included in the assent process. The study received ethics committee approval (National Research Ethics Service Committee Wales 5 – Bangor; REC reference 19/WA/0209; IRAS project ID 266031) and was publicly registered prior to the first patient being randomised (ClinicalTrials.gov, NCT04101071). We used the CONSORT (Consolidated Standards of Reporting Trials) statement when reporting this trial32. Feeding Regimens The ketogenic enteral feed was reconstituted for each patient in a clean kitchen area of the ICU by research nurses, with the proportions of individual nutritional components used devised by a dietitian using K.Quik® (Vitaflo International Ltd, Liverpool, UK), Renapro® (Stanningley Pharma, Nottingham, UK), Maxijul® (Nutricia, Liverpool, UK) and Fresubin® 5kcal (Fresenius Kabi, Dublin, Ireland, if additional fat was needed). Ketogenic and standard enteral feeding regimens were provided continuously as per the standard protocol for each trust. Patients were ineligible if they received ≥12 hours of standard feed prior to randomisation. A dietitian assessed individual patients’ nutritional needs within 72 hours of randomisation. The Modified Penn State equation or a weight-based equation (e.g., 25 kcal/kg) was used to estimate energy targets. Protein targets were individualised to each patient with a range of 0.83 -1.5g/kg/d being used according to specific clinical need. Patients were considered to have received adequate nutrition if they achieved >80% of their prescribed targets. Ketogenic enteral feeding continued for the duration of the 10-day trial period as tolerated, before reverting to standard enteral feed, as per the clinician responsible for the patient’s care. Patients in the control arm received the site- specific enteral feed as per Trust protocols with an agreed daily energy target to meet their nutritional needs. Multivitamins were administered daily in the ketogenic arm as micronutrients were otherwise not in the modular feed. Intravenous glucose was only to be administered for the emergency treatment of hypoglycaemia. Blood glucose levels in both feeding arms were managed according to local protocols. Endpoints The primary endpoints were patient recruitment and consent rates during screening (with retention rates);   the ease of  reconstitution and administration of the novel ketogenic enteral feed by ICU and research staff (determined via questionnaire; safety  (reports of adverse events [AEs] and serious adverse events [SAEs]); parameters of enteral feed absorption and blood chemistry (including glucose levels and achievement of ketosis) post-recruitment; and plasma concentrations . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 7 of beta hydroxybutyrate and acetoacetate, glucose, lactate, pyruvate and medium-chain fatty acids from blood samples at timepoints during the 10-day study period (See Table 1) Secondary endpoints included arterial blood gas parameters, ultrasound-determined rectus femoris cross-sectional area (as a marker of muscle loss); clinical, physiological and nutritional data; non-invasive metabolic data via indirect calorimetry; measures of physical function and quality of life; employment status and health care resource usage; and urinary concentrations of beta- hydroxybutyrate and total nitrogen, and plasma metabolomics at timepoints during the 10-day study period. Measures of adverse safety impacts included daily rates of vomiting (>10mls); number of episodes of diarrhoea (Bristol Stool Score ≥533 on 3 or more days) and daily rates of diarrhoea; daily rates of high gastric residual volume (GRV>300ml), or impaired glycaemic control. Normoglycaemia was defined as a blood glucose concentration of 4-10mmol/l, and thus concentrations of >10.1 or <3.9mmol/l as hyperglycaemia or hypoglycaemia respectively. Details of longer-term secondary endpoints collected are provided in the Supplementary Information. Blood and Urine analyses Plasma medium chain fatty acids, lactate, and beta-hydroxybutyrate, whole blood acetoacetate pyruvate, and urinary beta-hydroxybutyrate were analysed by gas chromatography/mass spectrometry34. Ultra High-Performance Liquid Chromatography Untargeted Metabolomics Metabolites were extracted from plasma using a dual phase Bligh-Dyer extraction and analysed using a combination of hydrophilic interaction liquid chromatography (HILIC) and Reverse Phase Ultra High-Performance Liquid Chromatography-mass spectrometry (UHPLC-MS) for polar and non- polar/lipid metabolites respectively35. Metabolite features were extracted from raw files using XCMS, then quality-control filtered and normalised according to standard procedures. Validated metabolite features were annotated to provide putative metabolite ID’s. Metabolites were putatively identified by metID which utilises m/z and MS2 spectra matching from public metabolomics databases36. Mapping of metabolites to relevant pathways was performed by over representation analysis (ORA) using MetaboAnalyst37.  Pathway Impact scores represent an objective estimate of the importance of a given pathway relative to the global metabolic network38. A cut off value of 0.1 was used, in keeping with previous work across multiple comparisons to filter less important pathways38 39. Full details of sampling and analytical methods are provided in the Supplementary Information. Sample Size . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 8 Sample sizes of 12 per arm have been recommended where previous data on which to base a power calculation are lacking40.  We aimed to recruit at least 37 patients to allow for a possible high drop- out rate from early death and early recovery, and for protocol violations (common in many critical care trials), and to thus leave12 patients per arm. Randomisation and Blinding Randomisation was stratified for recruitment site (1:1 basis) and occurred once assent was obtained. Treatment arm allocation used an independent remote electronic web-based random allocation service to generate an unpredictable allocation outcome, and to conceal that outcome from research staff until assignment occurred. Whilst patients were blinded (by virtue of illness), knowledge of the feeding intervention they were receiving would not influence outcome.  Where possible individuals collecting outcome assessments were blinded in this feasibility study, but not those preparing and administering feed due COVID-19-related staffing limitations. DB (who assessed all ultrasound scans) was masked to allocation until data analysis was complete (see Supplementary Information). Statistical Analyses Descriptive analysis was performed for the continuous outcomes using mean (95% confidence intervals) and Student’s T-test, or median (range) analysed using Mann Whitney U test. Chi-squared testing was used for proportional data. Recruitment rates are shown as a percentage with 95% confidence interval. All analyses were performed on an intention-to-treat basis using GraphPad (Prism) Two-tailed tests were used, and statistical significance was indicated by P≤0.05. For metabolomic analyses, data were centred and scaled to perform sparse partial least squares discriminant analysis (SPLS-DA) validated by k-fold cross validation to establish differences between ketogenic and control diet groups at baseline and on day 10. Owing to the high variability in this data set, orthogonal projection to latent structures (OPLS) was utilised to maximise variation41. Models with error rate greater than 20% were considered to be overfitted (i.e., the model described random error in the data rather than relationships between variables)42 43. In non-overfitted models, all variables with a Variable Importance Projection (VIP)  score greater than 1 were retained for further analysis44. The VIP score is a quantitative assessment of the discriminatory power of each individual feature45. For time course analysis, a linear mixed effect model was fitted to each data matrix using the limma package46. Diet and time were fixed effects. Participant ID was a random effect to account for subject specific variation. Contrast matrices were set up comparing metabolite abundance at baseline and days 3, 5 and 10 of the intervention. Empirical Bayes moderated t-tests were . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 9 performed to obtain p-values. False discovery rate (FDR) was accounted for using the Benjamini- Hochberg procedure. Metabolites were deemed significantly different between comparisons when FDR<0.05. Further details on study methodology are available in the Supplementary Information. Data Availability Statement The data that support the findings of this study are available for research purposes on reasonable request from the corresponding author [ZAP]. The data are not publicly available since this was not included in research participant consent. Results Safety, Feasibility and Tolerability Participants were recruited between 26th September 2019 and 22nd April 2021 (including two COVID-19-pandemic related pauses) from two United Kingdom ICUs.  The CONSORT flow chart is available in the Supplementary Information (Figure S1). A total of 293 patients were screened, with 29 patients randomised after meeting inclusion criteria (see Supplementary Information Table S3) and a refusal rate for assent of 4.1% (12 patients). The rate of recruitment was 2.2 participants/month for the 13 months enrolment period (Figure S2).  Participant retention rate was 82.8% (24 patients). Reasons for withdrawal are shown in Supplementary Information Table S4. Participant demographics are shown in Table 2: mean (95%CI) APACHE II score was higher in the control arm than the ketogenic feeding arm (21.6 (18.4-24.8) vs. 16.4 (13.5-19.3); p=0.025), admission SOFA scores were similar (9.9 (95%CI8.4-11.4) vs. 10.1 (8.7-11.6); p=0.621).  At the end of intervention period, the number of days of vasopressor support and total daily propofol dose were also higher in the control arm. Process and Feasibility of Nutritional Delivery All patients randomised to the intervention received ketogenic enteral feeding.  Feedback was obtained from 23 staff (4 research nurses, 16 ICU nurses, 1 pharmacist, 1 dietitian, 1 ICU consultant). The trial process was considered acceptable and feasible (See Supplementary Information Figure S3), although the preparation of the modular feed was considered laborious. A mean score of 8/10 (with 10 scored as the most positive response) was obtained for the question ‘How keen would you be to work on another similar study?’ Serious Adverse Events . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 10 Four serious adverse events (SAEs) were reported, and all were deemed to be unrelated to the intervention.  Details of these are available in the Supplementary Information (Table S5). No episodes of pulmonary aspiration were reported. Adverse Events Similar proportions of gastrointestinal events were reported between arms, with the exception of diarrhoea. The proportion of patients with diarrhoea was greater in the ketogenic enteral feeding arm (intervention vs. control 76.9% vs. 52.3%) but the difference in proportion of daily episodes less marked (53.5% vs. 42.9%).  One patient in the intervention arm was transferred to total parenteral nutrition on Day 6 as a result of concerns regarding enteral feeding intolerance (Supplementary Information Table S6). Mean base excess and bicarbonate level were similar between arms, remaining within the normal ranges (+2 to -2) and (22mmol/L – 29mmol/L) respectively. One patient from each arm developed Acute Kidney Injury (Supplementary Information Table S7). Development and Establishment of Ketosis Ketosis was achieved within 48 hours and sustained for the 10-day intervention period, (Figure 1AB and Supplementary Information Figure S4).  Medium chain fatty acid (octanoic acid and decanoic acid) concentrations from the ketogenic feed were higher in the intervention arm. As expected, no differences were seen between arms in dodecanoic acid concentrations, which were not part of either feed (Supplementary Information Figure S5). As a result, the ratio of octanoic acid to dodecanoic acid  (C8+C10:C12) was higher in the intervention arm over time (Supplementary Information Figure S5D). Glucose Control Two hypoglycaemic events were reported in the control arm and none in the intervention arm (1.58% vs 0% respectively). Hyperglycaemia occurred in fewer patients in the ketogenic enteral feeding arm (intervention vs. control 26.85% vs. 57.48%). In keeping with this, the coefficient of variation of daily glucose was lower in the intervention arm (9.4% vs. 14.8%, Figure 2A) as was median (IQR) cumulative insulin use (0.0 IU (IQR 0-16) vs.78 IU (IQR 0-412). Nutritional Adequacy and Substrate Utilisation In per-protocol analysis, participants receiving control enteral feeding met 90.4% and 79.3% of energy and protein targets respectively; patients receiving ketogenic enteral feeding received 83.3% and 84.4% of energy and protein targets respectively (Figure 2B).  This was similar in the Intention-To-Treat group, although a lower proportion of control arm participants met their protein target compared with the ketogenic enteral feeding arm (71.4% vs 88.1%). Indirect calorimetry was . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 11 performed on a subset of patients. RQ was 0.83 in the control patients (n=6) and 0.78 in the intervention arm (n=8) (Supplementary Information Figure S6). Plasma pyruvate concentrations were similar between arms (Figure 2C). Plasma lactate concentrations were lower in the ketogenic enteral feeding arm at baseline and remained lower throughout the study period Figure 2D). Collection of 24-hour urine samples to obtain total nitrogen values was not feasible in the context of heightened infection control during the pandemic. Data Collection Completeness The completion rate of data collection (for blood gases, biochemistry, haematology, bedside physiology, nutritional data and propofol usage) from medical records into the electronic database was 98.7% for those participants still in the study. Rectus Femoris Ultrasound Twenty-seven (93.1%) patients had 73 ultrasound scans performed over the period of the study. Scans were not performed in 2 (6.9%) of patients due to transfer to palliative care before the Day 1 scan was performed. However, scan quality did not reach acceptability in scans on 61 days (in 25 patients) when examined independently (DB and ZP). Reasons for this included the turnover and disruption of shift patterns of research nurses during the pandemic, and difficulties of working in protective equipment (PPE) leading to issues with consistent training and quality control. Physical Functional Outcomes The Chelsea Critical Care Physical Assessment Tool (CPAx) was completed in 26/29 (90%) patients at ICU discharge and 17/29 (59%) at hospital discharge.  Collection of data for other physical function milestones (e.g. sit-to-stand 20.8%, bed-to-chair 29.2%, Short Physical Performance Battery 9.5%, 2 and 6 minute-walk tests <5%) was severely impeded by COVID-19-related limitations in access to physiotherapists for these assessments due to re-deployment. Longer-term Outcomes Quality of life assessed by EQ-5D-5L was measured at 3-, 6- and 12-months post-ICU discharge in 21 (72.4%), 21 (72.4%) and 19 (65.5%) of study participants respectively (including data from those who had died where available). Three non-responders needed a translator that was not available during the pandemic, two were in long-term care, and one had moved abroad. Primary healthcare usage data were available in 20/29 (69%) of patients. Questions of employment were abandoned due to the complexity of employment status during the COVID-19 pandemic, and the potential to cause patient distress.  Final discharge location data were completed for 15/29 patients (51%). Metabolomic Analysis . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 12 Plasma metabolomic analyses identified a total of 185 metabolite features. Exploratory Visualisations Sparse Partial Least Squares Discriminant analysis (SPLS-DA) demonstrated no difference in metabolite abundance between arms on Day 1 (Supplementary Information Figure S7). By the end of the intervention period, between-arm differences were seen in 31 non-polar negative and 67 non-polar positive metabolites with Variable Importance in Projection (VIP) scores of >1.  Similarly, 45 polar negative and 65 polar positive metabolites had a VIP score >1 (Figure 3ABCD). Each arm additionally demonstrated changes (VIP scores>1) in metabolite abundance over time. In the control arm, 37 non-polar negative, 39 non-polar positive, 41 polar negative and 18 polar positive metabolite abundances were differentially altered over time (Figure S8). In the ketogenic enteral feeding arm, 23 non-polar negative, 22 non-polar positive, 59 polar negative and 20 polar positive metabolite abundances were differentially altered over time (Figure S9). Pathway Analysis Metaboanalyst pathway analysis demonstrated differential metabolite abundance in ketogenic enteral feeding arm vs. controls in beta-alanine metabolism (Impact 0.5), glycerophospholipid metabolism (Impact 0.2) and pentose and glucoronate interconversions (Impact 0.14). Over time, changes in metabolite abundance in the ketogenic enteral feeding arm were seen in pantothenate and CoA biosynthesis (Impact 0.1) and alpha-linoleic acid metabolism (Impact 0.33). This was different to that seen in the control arm of caffeine metabolism (Impact 0.69), terpenoid backbone synthesis (Impact 0.11) and pentose and glucoronate interconversions (Impact 0.14). These data, and non-impactful pathways are summarised in Table 3. Specific metabolite alterations Specific metabolites driving the differential pathway abundance data were then examined. The ketogenic enteral feeding arm demonstrated increases in beta-alanine (17.2AU (17.0-17.4) vs 16.3AU (16.0-16.6); p=0.008) and ureidopropionic acid (16.1AU (15.7-16.4) vs 15.2AU (14.8-15.6); p=0.008) abundance and decreases in lithocholate 3-0-glucuronide  (14.9 AU(14.8-15.0) vs 15.3AU (15.2-15.4); p=0.0004) relative to the control feed. Differences in glycerophospholipid metabolism was driven by changes in phosphocholine residues (Figure 4ABCDEFGH). Over time, administration of ketogenic enteral feed was associated with a difference in ureidopropionic acid abundance (16.1AU (15.7-16.4) at day 10 vs. 15.0AU (14.7-15.2) at day 1; p=0.0002).In the  control feed arm , differences over time in paraxanthine (15.0AU (14.3-15.6) at day 10 vs 15.9AU (15.3-16.5) at day 1; p=0.04), palmitoyl glucoronide  (16.3AU (16.0-16.6) at day . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 13 10 vs 15.7AU( 15.5-15.9) at day 1; p=0.003) and mevalonic acid (14.5AU (13.7-15.2) at day 10 vs 15.9AU (15.0-16.7) at day 1; p=0.02) were noted. Discussion We have demonstrated that inducing sustained ketosis using a ketogenic enteral feeding regimen is safe, well tolerated, and feasible in critically ill patients with multi-organ failure. Although some secondary endpoints could not be collected due to COVID-19 restrictions, recruitment and retention rates were high. Variability in glycaemic control improved, and differences between arms in terms of hypogylycaemia, insulin dosing and glucose variability all point towards a favourable metabolic profile and ketone bodies being used as a substrate preferentially over glucose. In keeping with tissue ketone body metabolism, ketosis was not associated with development of arterial blood acidaemia. Exploratory untargeted metabolomic analyses showed clear separation of arms at the end of the intervention. Metabolite abundance data suggest that a favourable metabolic profile developed in response to the intervention, a hypothesis that requires prospective testing in a larger trial. Prior to this trial, concerns had been raised regarding safety and feasibility of inducing ketosis in physiologically unstable patients. Ketosis is traditionally associated with pathological states in clinical medicine47. However, staff and patient education and engagement led to excellent rates of recruitment and retention of patients. Staff questionnaires suggest not only a high level of enthusiasm for the study, but also that this was, in the main, scalable, if a pre-made enteral feed could be sourced, of which several are available commercially. Support for the study may be in part due to the safety profile observed, with little differences seen in adverse events or tolerability, except for incidence of diarrhoea. We had not pre-specified the definition of diarrhoea, and several such definitions exist. Diarrhoea is common in critically ill patients22, and it is not clear if the high medium chain triglyceride load led to an increased incidence although diarrhoea is a well-known consequence of enteral feeding when there is a high proportion of medium chain triglycerides48. Future trials should pre-specify such definitions for monitoring purposes. Octanoic acid and decanoic Acid were delivered as part of the intervention, and the increased presence of these in the circulation supports data from recent stable isotope studies that gastrointestinal absorption in not a limiting factor49. Moderate levels of both medium chain fatty acids and ketone bodies in the participants receiving ketogenic enteral feeding suggest that the pathway of medium chain triglyceride lipolysis, absorption, hepatic ketogenesis and extra-hepatic ketone body utilisation is functionally intact and operative in these patients, supporting the hypothesis that provision of a ketogenic enteral feed to critically ill patients provides an alternative substrate.  Dodecanoic acid is not present in either the ketogenic or standard enteral feeds and was no different between arms, acting as a form of internal control. Despite the altered composition of . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 14 the novel enteral feed formula, nutritional adequacy was achieved equally across arms. In an exploratory analysis the respiratory quotient (RQ) was lower in the ketogenic enteral feeding arm, adding further data suggestive of ketone metabolism occurring for energy generation50. While the significance of the equivalence in pyruvate concentrations is unclear, the lower lactate levels seen in the intervention arm are also suggestive of substrate switching. The collection of outcome data was significantly impacted by the COVID-19 pandemic. Bedside collection of 24-hour urinary data (and hence nitrogen levels) and reliable muscle ultrasound measurements were not possible due to a combination of infection control requirements, the use of PPE, and staff shortages due to redeployment. Routinely collected physical outcome data were limited to the minimum possible as a result. Staff acceptability of the study protocol indicated some level of difficulty with collecting the quality-of-life data both retrospectively (pre-ICU admission) and following ICU discharge. This is highly likely to be related to difficulties in communicating with family members due to COVID-19-related suspension of hospital visiting. Of note, measurement of muscle mass was neither an essential nor recommended outcome measure in the Core Outcome Set for metabolic and nutritional interventions in critically ill patients and would therefore be unlikely to form part of our subsequent efficacy trial51. Exploratory metabophenotyping demonstrated good metabolic separation between arms following the 10-day intervention. Given the multiple tissue metabolites contributing to plasma metabolite profiles, this separation lends weight to the hypothesis that ketone bodies are being used for substrate metabolism in diverse physiological pathways.  Pathways unrelated to metabolism of nutritional lipids were differentially regulated implying tissue metabolism was additionally altered. While suggestive of an advantageous metabolic profile, these data should be viewed as somewhat preliminary, given the small patient numbers. Alterations in Cahill cycle flux result in a differential metabolite abundance in the alanine pathway, suggesting a decrease in muscle protein breakdown for alanine production10. Pentose and glucoronate interconversions suggest a differential regulation of the bioenergetic state52, but no conclusions can be drawn regarding improvement of bioenergetic failure from this small sample. Strengths and Limitations This study has considerable strengths. First, the very comprehensive prospective data collection gives a high level of confidence in the safety data for future trials, and suggests that such trials are unlikely to require such extensive data collection (thus reducing the data collection burden). Second, the extensive biochemical and metabolomic analysis gives insight into metabolic processes brought into play by the delivery of ketogenic enteral feeding.  Limitations include the need to focus on safety and feasibility which determined the sample size.  Imbalances in the study cohort between arms are likely driven by this (e.g., APACHE score inequality versus SOFA equality). Regardless, all patients recruited were in multi-organ failure, and at risk of both altered substrate . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 3, 2023. 
; 
https://doi.org/10.1101/2023.03.30.23287849
doi: 
medRxiv preprint 15 utilisation, muscle wasting, and subsequent physical functional impairment.  All signals reported relating to efficacy should be viewed as hypothesis generating only as this trial was not powered to detect this. A further limitation is the missing data for some of the physical outcomes – an indication of the strain put on healthcare systems by the COVID-19 pandemic.  From a feasibility perspective, future trials would require dedicated time and funding for this, as these data expose the fragility of using routinely collected physical outcome data. Conclusions Ketogenic feeding in critically ill patients with multi-organ failure is safe and feasible. Patients who received ketogenic enteral feeding developed a different metabolic profile from controls. The efficacy of this altered metabolic profile in improving patient outcomes requires testing in prospective trials.",1
"Background. Creatine is an organic compound that facilitates the recycling of
energy-providing adenosine triphosphate (ATP) in muscle and brain tissue. It is a safe,
well-studied supplement for strength training. Previous studies have shown that
supplementation increases brain creatine levels, which might increase cognitive
performance. The results of studies that have tested cognitive performance differ greatly,
possibly due to different populations, supplementation regimens and cognitive tasks. This is
the largest study on the effect of creatine supplementation on cognitive performance to date.
As part of our study, we replicated Rae et al. (2003). Methods. Our trial was cross-over, double-blind, placebo-controlled, and randomised, with
daily supplementation of 5g for six weeks each. Like Rae et al. (2003), we tested participants
on Raven’s Advanced Progressive Matrices (RAPM) and on the Backward Digit Span (BDS).
In addition, we included eight exploratory cognitive tests. About half of our 123 participants
were vegetarians and half were omnivores. Results. There was no indication that vegetarians benefited more from creatine than
omnivores, so we merged the two groups. Participants’ scores after creatine and after
placebo differed to an extent that was not statistically significant (BDS: p = 0.064, η2
P = 0.029; RAPM: p = 0.327, η2
P = 0.008). Compared to the null hypothesis of no effect, Bayes factors indicate weak evidence in favour of a small beneficial creatine effect and strong
evidence against a large creatine effect. There was no indication that creatine improved the
performance of our exploratory cognitive tasks. Side effects were reported significantly more
often for creatine than for placebo supplementation (p = 0.002, RR = 4.25). Conclusions. Our results do not support large effects of creatine on the selected measures
of cognition. However, our study, in combination with the literature, implies that creatine
might have a small beneficial effect. Larger studies are needed to confirm or rule out this
effect. Given the safety and broad availability of creatine, this is well worth investigating; a
small effect could have large benefits when scaled over time and over many people. Key words. Creatine, cognition, intelligence, cognitive performance, Raven’s Advanced
Progressive Matrices, Backward Digit Span, working memory, deductive reasoning,
randomised controlled trial, RCT . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Introduction Given the important role cognition plays in daily life, substances that enhance cognition
safely and cheaply are highly desirable. Creatine is safe, well-tolerated, and cheap (Kreider
et al., 2017). Strength athletes have benefited from creatine supplementation for over 30
years (Branch, 2003; Butts et al., 2018). Slight weight gain due to water retention is the only
consistently reported side effect (Bender et al., 2008; de Souza e Silva et al., 2019; Kreider
et al., 2017; Kutz & Gunter, 2003). While the safety and athletic benefits of creatine are well established, its potential cognitive
benefits are still unclear. A systematic review tentatively suggests that creatine
supplementation may improve “short-term memory”/working memory and
“intelligence/reasoning” in healthy individuals (Avgerinos et al., 2018). The few studies that
have tested this have had heterogeneous results, but they have also used very different
populations (such as vegetarians, omnivores, varying age groups), supplementation doses
and durations and cognitive tasks (including different kinds of memory, reaction time,
reasoning, inhibitory control, attention and task switching). The study with the largest effect,
Rae et al. (2003), tested the effect of creatine supplementation in 45 young vegetarian adults
on working memory and abstract reasoning using the Backwards Digit Span (BDS) and
Raven’s Advanced Progressive Matrices (RAPM), respectively. Their study was
placebo-controlled, randomised and double-blind. Rae et al. (2003) found creatine
supplementation had a large and highly significant positive effect on both tasks. We deemed
this study particularly worth replicating. Why might supplementing creatine benefit cognition? Muscle and brain cells use creatine to
access more energy when demand is high. They store creatine as phosphocreatine, which
acts to regenerate the energy-providing adenosine triphosphate (ATP) (Lowe et al., 2013;
Persky & Brazeau, 2001). The energy demand of neurons can increase rapidly; maintaining
ATP concentration despite increased demand may explain the potential effect of creatine
intake on cognition (Ainsley Dean et al., 2017). The crucial role of creatine in brain
metabolism is supported by evidence from Cerebral Creatine Deficiency Syndromes.
Conditions causing brain creatine deficiency result in profound intellectual disability which
can be reversed by creatine supplementation (Clark & Cecil, 2015). Creatine can be produced by the body and is present in common foods, so why would we
expect supplementation to make a difference? Dietary creatine is primarily contained in
meat, fish, and a small amount in some dairy products (Balestrino & Adriano, 2019; Brosnan
& Brosnan, 2016). However, typical supplementation doses of creatine (5g per day) are
equivalent to more than 1kg of meat consumption per day (Brosnan & Brosnan, 2016), which
is substantially higher than the combined dietary intake and synthesis in most people
(Brosnan & Brosnan, 2016). Creatine intake increases the level of creatine in the blood serum (Harris et al., 1992)
(Schedel et al., 1999). Crucially, (Dechent et al., 1999) found brain creatine increased by
8.7% following a 20g/day 4-week supplementation regime; two further studies have
confirmed varying supplementation regimes can increase brain creatine (Lyoo et al., 2003;
Turner, Russell, et al., 2015). . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint It is unclear if creatine supplementation has similar effects on omnivores and vegetarians.
Rae et al. (2003) only included vegetarians. Another study comparing memory improvement
under creatine supplementation in omnivores and vegetarians found that creatine
supplementation improved memory only for vegetarians but not omnivores (Benton &
Donohoe, 2011). Vegetarians have been found to have lower serum and muscle creatine
concentration, but comparable total brain creatine to omnivores (Burke et al., 2003; Solis et
al., 2014, 2017). In this study, we included both omnivores and vegetarians to allow
comparison. We hypothesised that creatine supplementation would improve working
memory and reasoning ability in vegetarians. We also hypothesised that the improvement
would be greater in vegetarians than in omnivores. To test these hypotheses, we approximately replicated the study design and treatment (5g
per day of creatine for six weeks) used by Rae et al. (2003). We included the same primary
outcome measures, the Backwards Digit Span and 10-minute standardised subtests of
Raven's Advanced Progressive Matrices. In addition, to investigate a broader range of
cognitive functions, we included exploratory tests on attention, verbal fluency, task switching,
and memory. Methods Trial design We conducted a randomised, placebo-controlled, double-blind, cross-over study. The
primary endpoints are the scores in the cognitive tasks after 6 weeks of each
supplementation. Six weeks have been found to be a sufficient wash-out period (private
correspondence, (Turner, Byblow, et al., 2015)) and it is the duration used by Rae et al.
(2003). Unlike Rae et al. (2003), we did not have an extra washout period nor second
baseline testing after the first supplementation. The trial evaluated cognitive performance
after creatine compared to placebo. The trial design and participant flow are summarised in
Figure 1. The trial was prospectively registered (drks.de identifier: DRKS00017250,
https://osf.io/xpwkc/) and ethical approval was obtained from the ethics committee of the
University of Bonn (060/19). We follow the CONSORT guidelines. Participants Participants were 18 years or older (see appendix for full list of inclusion criteria). Half of
them reported to be on a vegetarian diet and half of them on an omnivore diet. Cognitive
assessments of participants took place in the clinic laboratory. Due to the contact restrictions
due to the COVID-19 pandemic, after 04/2020 participants were tested online via video call
instead. A screening questionnaire assessed if the eligibility criteria were met. Participants
who met these criteria went through the baseline assessment and were given their first
supplement to take home (or for participants tested online, received the two supplements via
the mail). . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Interventions and similarity of treatment groups Participants took the supplements daily for six weeks, including the day of the testing. The
creatine supplement consisted of creatine monohydrate powder “CreaPure PG” produced by
the company Alzchem (Trostberg, Germany). The placebo supplement consisted of
maltodextrin powder “Maltodextrin 6” produced by the company Nutricia (Frankfurt am Main,
Germany. The cans looked exactly the same except for clear markings of which one was the first and
which one the second supplement. The two powders looked exactly the same and were
flavourless. The solubility was somewhat different: While the placebo powder was
completely soluble in water and did not settle, the creatine powder slowly settled. We were
initially not aware of this difference in solubility. After we noticed it (after the first 40
participants), we asked participants to stir the powder into yoghurt or food with a similar
consistency, as we had found no perceptible difference then. To check to what extent
blinding was achieved, directly after the last testing participants were asked to guess what
their first supplement had been. Outcomes We had two primary outcomes: ●
A standardised 10-minute subtest of Raven Advanced Progressive Matrices (RAPM)
(Rae et al., 2003) ●
The Wechsler auditory Backward Digit Span (BDS) (Wechsler 1955) RAPM is a test of abstract reasoning. Each item in the test consists of a 3x3 matrix with
pictures of geometric forms. One of the pictures is missing and the task consists of choosing
the right picture to fill this gap out of eight alternatives. The full RAPM consists of 80 items
and has a time limit of 40 minutes. We used the same standardised 10-minute subtests of
the RAPM as Rae et al. (2003), consisting of 20 items each. The subtests are constructed to
have equal levels of difficulty based on the published normative performance data and Rae
et al. (2003) additionally verified this in an independent sample (N = 20). The RAPM score
consists of the sum of correct responses. The Backward Digit Span is a test of working memory. The tester reads increasingly longer
series of digits to the participant whose task it is to remember and repeat them in reverse
order. The task starts with two digits. Each length has two series of digits. The test ends after
wrong answers to two series of the same length. The BDS score consists of the sum of
correct responses. We had eight further exploratory outcomes: ●
The D2 Test of Attention (Brickenkamp, 2002), a test of sustained attention ●
The Trail-Making-Test A (TMT-A), a test of visual attention (Reitan, 1958) ●
The Trail-Making-Test B (TMT-B), a test of task switching (Reitan, 1958) ●
The Block-Tapping-Test (BTT), a test of visuospatial working memory (Schellig, 1997) ●
The Auditory Verbal Learning Test (AVLT, in German: VLMT), a word-learning test
including immediate recall, delayed recall and recognition (Lux et al., 2001) . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint ●
The Brief-Visuospatial-Memory Test – Revised (BVMT-R), a test of visuospatial
memory (Benedict et al., 1996) ●
The Stroop test (in German: Farb-Wort-Interferenz Test, FWI-T), a test of inhibitiory
control (Bäumler & Stroop, 1985) ●
Regensburger Wortflüssigkeitstest (RWT), a test of verbal fluency (Aschenbrenner et
al., 2000) Participants reported side effects experienced during the supplementation period in a free
text form on the day of testing. How side effects would be grouped for the report was
determined after evaluating all entries. At baseline testing, participants performed a test of
crystallised intelligence called “Mehrfach-Wahl-Wortschatztest (MWT-B)” (Lehrl, 2005). In
this test, participants had to identify real German words among made-up words. Sample size The sample size of 123 was powered (with power = 0.8, alpha = 0.05, calculated with
GPower) to detect effects of Cohen’s d = 0.45. The sample size (preregistered as 120) was
chosen based on a conservative estimate (see appendix) of the effect size in Rae et al.
(2003) (d = 1 for both RAPM and BDS) with a substantial buffer to account for smaller
effects. Block-tapping was originally performed with physical blocks and later on the website
Psytoolkit (Stoet, 2010, 2017) as part of remote testing during the COVID-19 pandemic.
Because the remote version was not immediately available, the participant number is lower
for this task. Randomisation and blinding The order of the two supplements was randomised with Excel by the pharmacy of the
university hospital Heidelberg. They labelled each of the cans of supplements with the
participant code and “A” or “B”, corresponding to the first and second supplement. The staff
members who tested participants also provided the participants with the supplement cans.
Allocation concealment was performed using sequentially numbered, opaque sealed
envelopes (SNOSE). Participants and all staff who interacted with them were kept blinded to
the allocation (also see intervention section). Statistical methods For each cognitive test, we conducted a mixed ANOVA with test score after supplementation
as the dependent variable, supplement (creatine vs placebo) as the within-subjects factor
and supplement order (creatine-first vs placebo-first) as the between-subjects factor. We did
not remove outliers in our main analysis, but conducted robustness checks which included
trimming and winsorising. We applied the Greenhouse-Geisser correction to all our analyses
but the correction did not change any value. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Confirmatory analyses As preregistered, our two confirmatory cognitive tasks are the Backward Digit Span and
Raven’s Advanced Progressive Matrices. All other cognitive tasks are analysed in an
exploratory fashion. There is one deviation from our preregistered analyses. We had preregistered t-tests, but
this was a mistake in the preregistration. The t-test is not appropriate here because
imbalances in the supplement order group sizes would bias the results. Instead we
conducted mixed ANOVAs with supplement (creatine vs placebo) as the within-subjects
variable, supplement order (creatine-first vs placebo-first) and diet (vegetarian vs omnivore)
as the between-subjects variables and test score after supplementation as the dependent
variable. Robustness checks We checked the robustness of our normality-assuming ANOVAs by performing: an ANOVA
on 20%-trimmed data, an ANOVA on 5%- and on 20%-winsorised data, and a robust
ANOVA which uses trimming and bootstrapping (performed with the sppb functions in the
WRS2 R package). The latter ANOVA provides the most robust estimate of these methods
(Field, 2013; Wilcox, 2011). Bayes factors For the calculation of the Bayes factors, we used the estimated marginal means (EMMs) of
the creatine and placebo score. The EMMs are the means weighed for the order groups
(creatine-first and placebo-first), so that imbalances in the sizes of the order groups do not
affect the means. So, we only had two groups for the Bayes factor calculation (creatine and
placebo), simplifying the analysis. The mean difference and standard error of the mean
difference were used to describe the data. Using the Bayesplay package (Colling, 2021), we
calculated the Bayes factors in several different ways. Approach 1 used point models for the
null hypothesis and the alternative hypotheses. Approach 2 compared a point null model
against half normal distributions centred on zero and with the standard deviation set to half
the maximum expected effect size. For the reasons behind this see the appendix. Exploratory analyses In addition to the confirmatory analyses of BDS and RAPM, we analysed the other cognitive
tasks in the same way in an exploratory fashion. We also looked in an exploratory fashion at the first supplementation and the second
supplementation separately and at participants with a low and high baseline performance
separately (see appendix). . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Results Participant flow Figure 1. Participant flow through the study. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Recruitment Participants were recruited through flyers and social media between 05/2019 and 05/2022
and tested between 05/2019 and 08/2022. Baseline data We analysed all available participant data apart from one task in the case of two participants
(see appendix). Participants were included irrespective of their adherence. For participant
characteristics, see Table 1. Total
Creatine-first
Placebo-first Age in years (M, SD)
30.6 (10.1)
31.5 (10.4)
29.8 (9.7) Sex (% female)
57%
54%
60% Weight in kg (M, SD)
70.3 (13.7)
71.8 (15.5)
68.8 (11.4) MWT-B (M, SD)
26.31 (4.35)
26.32 (3.99)
26.31 (4.72) Table 1. Participant characteristics. Data is given as mean (standard deviation) or as
percentage. The MWT-B (Mehrfach-Wahl-Wortschatztest) is a test of crystallised intelligence
(Lehrl, 2005). Blinding, adherence and side effects The last 73 participants were asked to guess the order of their supplements. Forty-three
(59%) guessed correctly and 30 (41%) guessed incorrectly. A binomial test reveals that the
probability of 43 or more correct guesses out of 73 by pure chance is p = 0.080. However,
most participants who guessed correctly reported being very unsure about their guess. We
recorded the reasons for the guesses of the last of the 33 participants. Of those participants
who had a reason for their guess, solubility was the most common, followed by negative side
effects and positive side effects. All three reasons seemed to improve guess accuracy (see
appendix). A z-score test for two population proportions revealed that the proportion of participants
reporting any negative side effect was significantly higher for the creatine than the placebo
condition, p = 0.002, RR = 4.25 (Table 2). In addition, although we did not assess this
systematically, some participants reported positive side effects such as improvements in
strength (several participants) and mood (one participant). No patients discontinued the
study due to an adverse event. Adherence (self-reported) was high (Table 2). All but one participant took the supplements in
the order assigned to them. This participant was analysed with their actual, not their
assigned, supplement order. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Creatine
Placebo Days supplemented per week (M, SD)
6.89 (0.26)
6.87 (0.26) Any side effects
Of these: Digestion problems
Weight gain
Other -
Tiredness -
Thirst -
Weight loss -
Nightmares -
Cramps -
Thoughts racing -
Problems concentrating -
nervousness 17% 6%
3% 1x
1x
1x
1x
1x
1x
1x
1x 4% 2%
0% 1x
1x
0x
0x
0x
0x
0x
0x Table 2. Adherence and negative side effects. Interaction with diet There was no significant interaction between diet and supplement nor between diet,
supplement and supplement order for neither BDS (p = 0.808 and p = 0.559) nor RAPM (p =
0.392 and p = 0.606), nor was the interaction in the predicted direction (we had hypothesised
that vegetarian participants would benefit more from creatine than omnivore participants).
This was also true when using the robust ANOVA based on bootstrapping. Bayes factors
favoured the null hypothesis. To be precise: They indicated strong support in favour of the
null hypothesis over the effect size in Benton and Donohoe (2011) (d = 0.36) and weak to
strong support in favour of the null hypothesis over smaller effect sizes (see appendix).
There was no indication for an effect of diet in the exploratory cognitive tasks either. For
more details on the analysis of diet, see the appendix. Confirmatory analysis There was a significant interaction between supplement and supplement order for both BDS
and RAPM. This seems to reflect a learning effect (see appendix). The effect of most
interest, the main effect of the supplement, was in the expected direction but not significant.
However, it bordered on significance for BDS (p = 0.067, η2
P = 0.028). This means that 2.8% of the variance in BDS scores that was not already explained by other variables was
explained by the supplement. For RAPM, it was 0.9%. The supplement effect was virtually
the same whether diet was included as a variable or not (Table 3). Thus we simplified
additional analyses (estimated marginal means, Bayes factors and robustness checks) by
dropping diet as a variable for these analyses. In terms of raw scores, the effect size for BDS was 0.41 additional correct items, i.e. a 0.2
digits longer digit span, because there were always two digit spans of the same length. For
RAPM, the effect was 0.23 more matrices solved (Figure 2). Cohen’s d based on the . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint estimated marginal means of the creatine and placebo scores was 0.09 for RAPM and 0.17
for BDS. If these were IQ tests, this would mean 1.4 and 2.6 IQ points. Task
N Supplement effect
3-way ANOVA, inkl. diet Supplement effect
2-way ANOVA Crea.
Score Pl.
Score Crea.-Pl. Scores
M(SE) [95% CI] total crea
first pl
first
F (df)
p
η2
P
F (df)
p
η2
P BDS
121
61
60 3.41
(1, 117) 0.067
0.028
3.49
(1, 119) 0.064 0.029 8.85
(0.28) 8.44
(0.25) 0.41(0.22)
[-0.24; 0.844] RAPM 118
60
58 1.02
(1, 114) 0.315
0.009
0.97
(1, 116) 0.327 0.008 12.39
(0.28) 12.16
(0.28) 0.23(0.23)
[-0.24; 0.70] Table 3. Mixed 3-way ANOVA with supplement (creatine vs placebo) as the within-subjects
variable, supplement order (creatine-first vs placebo-first) and diet (vegetarian vs omnivore)
as the between-subjects variable and test score after supplementation as the dependent
variable. Mixed 2-way ANOVA without diet. The test score is given as estimated marginal
mean (standard error). P-values are two-tailed. The two cognitive tasks are the Backward
Digit Span and Raven’s Advanced Progressive Matrices. Figure 2. a) Estimated marginal means for the Backward Digit Span (BDS) score. b)
Estimated marginal means for Raven’s Advanced Progressive Matrices (RAPM) score. Error
bars represent standard errors. Bayes factors To facilitate the interpretation of the results of the confirmatory analysis, we provide Bayes
factors. A Bayes factor (BF10) indicates how likely a null hypothesis is compared to an
alternative hypothesis given the data. A BF10 between ⅓and 3 indicates low sensitivity of the . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint data (i.e. not enough data to be certain), with weak evidence in favour of the null hypothesis
if BF10 is below 1 and weak evidence in favour of the alternative hypothesis if it is above 1. A
BF10 above 3 (below ⅓) is considered moderate and above 10 (below 1/10) strong evidence
(van Doorn et al., 2021). We compare several alternative hypotheses postulating small beneficial effects of creatine to
the null hypothesis. For RAPM, the data was very insensitive, very weakly favouring the
alternative hypotheses. For BDS, the data was more sensitive, providing weak to moderate
support in favour of the alternative hypotheses. Two different approaches to calculating
these Bayes factors were used (see statistical analysis) and the results were similar (Table
4). There was strong evidence in favour of the null hypothesis compared to the alternative
hypothesis postulating the effect size found by Rae et al. (2003). The data was insensitive
(BDS) or weakly favoured the null hypothesis (RAPM) when compared to the half normal
model based on Rae et al. (2003). The half normal model based on Rae et al. (2003) does
not assume their effect size is the true effect size in the population. Instead, the model
assumes their effect size is a moderate overestimation of the true effect size. The model
uses their effect size as a reference point to assign probabilities to effect sizes. It assigns
most of the probability weight to effect sizes that are smaller than this effect size, and some
probability to effect sizes up to twice that effect size. This is a common alternative model
when replicating studies. However, we did not use it as our only model, because we were
also interested in assessing the likelihood of smaller effect sizes and of the possibility that
the effect size in Rae et al. (2003) was the true population effect size. The results were similar whether using normal or cauchy distributions. For more details on
this and the aforementioned calculations see the appendix. In summary, this study provides weak evidence for a small cognitive benefit of creatine and
strong evidence against the effect size by Rae et al. (2003) being representative. Approach 1: point models
Approach 2: half normal Small effects
Rae-sized
Small effects
Max. = 2xRae-size Task
0.1
0.2
0.4
2.5
max. 0.4
max. 1
max. 5 BDS
2.1
3.6
5.7
< 2e-7
2.9
3.3
1.0 RAPM
1.4
1.6
1.3
< 2e-7
1.4
1
0.3 Table 4. Bayes factors (BF10) comparing a range of alternative hypotheses to the null
hypothesis. The effect size is given as the raw score difference. Approach 1 compared a
point null model to point alternative models with a range of small effect sizes (0.1-0.4, i.e. d =
0.04-0.17) as well as an equivalent of Rae et al.’s effect size (2.5, i.e. d = 1, see calculation
in appendix). Approach 2 compared a point null model against half normal distributions
centred on zero and with the SD set to half the maximum expected effect size. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Robustness checks We checked the robustness of our confirmatory analysis (the normal ANOVA) by performing:
an ANOVA on 20%-trimmed data, an ANOVA on 5%- and on 20%-winsorised data, and an
ANOVA which uses bootstrapping and 20% trimming. For RAPM, all of these methods gave overall similar results to that of the normal ANOVA
(Table 5). Task Better score Max. skew p (Supplement) normal
20% trim 5% winsorisation 20% winsorisation bootstrap and 20% trim RAPM
creatine
-059
0.327
0.412
0.361
0.672
0.354 BDS
creatine
1.14
0.064
0.17
0.009
0.05
0.37 Table 5. Creatine effect p-values (two-tailed) for different ANOVAs. The given trim and
winsorisation percentages are applied to each side. Better score based on estimated
marginal means. “Max. skew” gives the highest skewness statistic in any combination of
conditions (supplement and supplement order). For BDS, whose skewness statistic was slightly further from 0 than that of RAPM, these
methods gave results that differ from each other and from the normal ANOVA to a relevant
extent (Table 5). Most notably, the p-value for the supplement effect was 0.009 for the
5%-winsorisation and 0.370 for the bootstrap ANOVA. This seems to suggest that in the
normal ANOVA, the most extreme values made the effect of creatine appear smaller by
inflating the variance, while relying on possibly unjustified assumptions of normality made
the effect of creatine appear larger. Thus, the result for RAPM was robust and for BDS much less so. Exploratory cognitive tasks There was no indication that creatine improved the performance of our exploratory cognitive
tasks. The distribution of p-values was what one would expect if there was no effect. For the
exploratory cognitive tasks, Table 6 only includes the p-values of the supplement effect. For
the full results, including the interaction effect (reflecting a learning effect) and the order of
supplement effect, see the appendix. Task
N Better score p (Supplement) normal 20% trim 5% winsorisation 20% winsorisation bootstrap and 20% trim Blocktapping forward
71
creatine
0.779
0.564
0.865
0.678
0.826 Blocktapping backward
70
placebo
0.83
0.87
0.482
0.482
0.59 BVMT-R
119
creatine
0.543
0.809
0.746
0.112
0.67 . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint D2 test
104
placebo
0.394
0.382
0.446
0.291
0.62 Forward digit span
117
placebo
0.714
0.795
0.838
0.721
0.52 Stroop - colors
118
placebo
0.813
0.184
0.432
0.054
0.568 Stroop - colorletters
119
placebo
0.626
0.877
0.861
0.547
0.856 TMT A
123
placebo
0.129
0.04
0.068
0.021
0.224 TMT B
122
creatine
0.855
0.622
0.745
0.567
0.56 VLMT immediate recall
119
creatine
0.87
0.744
0.996
0.883
0.996 VLMT recall after interference
119
creatine
0.694
0.622
0.854
0.323
0.806 VLMT delayed recall
118
placebo
0.339
0.346
0.462
0.133
0.54 VLMT recognition
117
creatine
0.722
0.327
0.398
0.635
0.348 Word fluency
122
creatine
0.227
0.631
0.272
0.119
0.692 Table 6. Creatine effect p-values (two-tailed) for different ANOVAs. The given trim and
winsorisation percentages are applied to each side. Higher score based on estimated
marginal means. Discussion This is the largest study on the cognitive effects of creatine to date. As part of our study, we
aimed to replicate Rae et al. (2003), who found a large positive effect of creatine on the
abstract reasoning task Raven’s Advanced Progressive Matrices (RAPM) and on the
working memory task Backward Digit Span in healthy young adult vegetarians. In our study, half of the participants were vegetarians and half of them were omnivores. We
found no indication that our vegetarian participants benefited more from creatine than our
omnivore participants. This is in line with Solis et al (2014, 2017) who did not find a
difference in brain creatine content between omnivores and vegetarians. Our Bayesian
analysis of their data provides moderate support for the lack of a difference (see appendix).
In contrast, Benton and Donohoe (2011) found that creatine supplementation benefited
memory in vegetarians more than in omnivores, with no difference in baseline performance.
However, given the high number of tests in that study, the chance of a false positive was
high, so we regard their finding as only an exploratory hint. The conflicting findings might be
due to possible differences in the amount of dietary creatine (not measured in this study nor
in Benton and Donohoe (2011)). The preregistered frequentist analysis of RAPM and BDS found no significant effect at p <
.05 (two-tailed), although the effect bordered significance for BDS. The Bayes factors in this
study provide weak evidence for a small cognitive benefit of creatine and strong evidence
against the large effect size found by Rae et al. (2003). A larger sample size would be
necessary to provide stronger evidence on the question of a small benefit. In order for the
sample size to not have to be exceedingly large, we recommend being extremely careful in
reducing noise and choosing participants who are likely to benefit the most. In addition,
analogous to the compounding effect of creatine over time for strength training, it might be . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint possible to see a larger effect of creatine on cognition over time by training the tasks while
on the supplement. In their review, Avgerinos et al. (2018) reported that a creatine effect is more likely for
“intelligence/reasoning” and “short term memory”/working memory than for other cognitive
domains. In line with this, we found a weak indication for a creatine effect for the two
confirmatory tasks reflecting these two domains but not for other domains. However, two of
our exploratory tasks, the forward digit span and the immediate recall part of the VLMT also
tested short-term memory and there was no indication for an effect for these tasks. Another
review, (Dolan et al., 2018) report that a creatine effect is more likely for more cognitively
demanding tasks. In line with this, we found some indication for a creatine effect for the
backward digit span (BDS) but not for the less demanding forwards digit span. The VLMT
may also be less cognitively demanding than the BDS, but this comparison is less obvious to
make. There are a number of limitations to this study. Despite the large sample size compared to
other studies, a larger sample size would be needed to be powered for effects that are
smaller but still relevant. Some of the data (2%) could not be analysed because it was not
labelled with the participant and timepoint. The COVID-19 pandemic started in the middle of
the study, which likely added noise to the data, and meant that we had to switch from
in-person cognitive testing to testing via video call. Adherence was self-reported and not
checked with blood samples. Another limitation is that the proportion of participants who
correctly guessed their supplement order (59%) bordered on significance (p = 0.08).
However, most participants who guessed correctly reported being very unsure about their
guess. The largest contributing factor to correct guesses was likely the difference in the
solubility between the powders, followed by negative and positive side effects. We attempted
to counteract differences in solubility by recommending participants to stir the supplements
into yoghurt. For future studies we recommend cellulose as the placebo and a mixture of
cellulose and creatine as the treatment, as these two look extremely similar when dissolved
in water. The alternative solution with capsules would require participants to consume many
capsules per day. This would likely reduce adherence and massively increase costs.
Unfortunately, it is difficult to achieve perfect blinding when side effects occur with higher
frequency in the creatine condition. The side effects of creatine are well-known and not
dangerous (Bender et al., 2008; de Souza e Silva et al., 2019; Kreider et al., 2017; Kutz &
Gunter, 2003). Conclusion Supplementing creatine is safe, easy and very cheap. The real effect of creatine on cognition
is likely smaller than that reported in Rae et al. (2003). However, even small improvements
in cognition may be relevant, especially if accumulated over many people and over time. The
results of this study do not allow any strong conclusions, but it would be worthwhile to test
for a small effect of creatine in strategically designed, larger studies. . 
CC-BY 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288194
doi: 
medRxiv preprint Acknowledgements We thank the doctors of the University Clinic Bonn who collected blood samples. We thank
Dr. Lincoln Colling, Dr. Christian Stark, Jan Speller, Maximilian Meier and David Reinstein for
their feedback on statistical questions. We thank Thomas Szpejewski and Tom Lieberum for
their help with verifying data quality. We thank all data entry helpers. Funding Funding was provided by the non-profit organization Effective Ventures Foundation, 2443
Fillmore St., #380-16662, San Francisco, CA 94115. The trial funders had no role in the
design of the study, the collection, analysis or interpretation of data, the writing of the report,
or the decision to submit the article for publication. Data The appendix, data, code and output of this study are openly available at the Open Science
Framework, https://osf.io/xpwkc/.",1
"Background: Different SARS-CoV-2 variants can differentially affect the prevalence of Post Covid-19 
Condition (PCC). This prospective study assesses prevalence and severity of symptoms three months 
after an Omicron infection, compared to Delta, test-negative and population controls. This study also 
assesses symptomology after reinfection and breakthrough infections . Methods: After a positive SARS-CoV-2 test, cases were classified as Omicron or Delta based on ≥ 85% 
surveillance prevalence. Population controls were representatively invited and symptomatic test-
negative controls enrolled after a negative SARS-CoV-2 test. Three months after enrolment, 
participants indicated point prevalence for 41 symptoms and severity of four symptoms. Permutation 
tests identified significantly elevated symptoms in cases compared to controls. PCC prevalence was 
estimated as the difference in prevalence of at least one elevated symptom in cases compared to 
population controls. Findings: At three months follow-up, five symptoms and severe dyspnea were significantly elevated in 
Omicron cases (n = 4138) compared to test-negative (n= 1672) and population controls (n= 2762). 
PCC prevalence was 10·4% for Omicron cases and 17·7% for Delta cases (n = 6855). Prevalence of 
severe fatigue and dyspnea were higher in reinfected compared to primary infected Omicron cases, 
while severity of symptoms did not significantly differ between Omicron cases with a booster or 
primary vaccination course. Interpretation: Three months after Omicron, prevalence of PCC is 41% lower than after Delta. 
Reinfection seems associated with more prevalent severe long-term symptoms compared to a first 
infection. A booster prior to infection does not seem to improve the outcome of long-term symptoms. Funding: The study is executed by the National Institute for Public Health and the Environment by 
order of the Ministry of Health, Welfare and Sport. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 INTRODUCTION Worldwide an estimated 750 million SARS-CoV-2 infections have occurred up to March 2023, and numerous publications report that in a subgroup of COVID-19 infected individuals symptoms persist for months (1-3). This condition is commonly referred to as Post Covid-19 Condition (PCC), which  can burden health care systems and have a significant impact on individuals (3). A case definition of PCC by the WHO stipulated a general difficulty functioning in everyday life (4). However, prevalence and severity of symptoms associated with PCC may vary with different variants of SARS-CoV-2 (5). Compared to B.1.617.2 (Delta), B.1.1.529 (Omicron) has already been characterized by higher transmissibility, lower pathogenicity and a shorter duration of the acute phase (6). Moreover, acute symptoms differ with less involvement of the lower respiratory tract (6). Although it can be hypothesized that milder infections lower the risk of PCC, information on the prevalence and severity of symptoms associated with PCC after Omicron infections is still limited. Additionally, Omicron is better at immune escape than Delta which also raises the question to what extent vaccination protects against PCC-related symptoms after Omicron breakthrough infections and to what extent a previous infection may protect against PCC-related symptoms after an Omicron reinfection (7). For pre-Omicron variants of concern, the prevalence and severity of these sequela are already documented for several countries including the Netherlands (2, 8-10). In the Netherlands, over 8·5 million SARS-CoV-2 infections were reported up to March 2023 since the onset of the pandemic, of those nearly 4·2 million infections occurred from December 2021 on during the Omicron variant of concern emergence (11). This study aimed to assess PCC symptom prevalence and severity after infections with the Omicron variant compared to a Delta infection, test-negative controls and population controls. Moreover we assessed the possible protective effect of the booster vaccination against developing PCC-related symptoms after Omicron breakthrough infections. Finally we investigated the prevalence of PCC- . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 3 related symptoms after a first infection with Omicron compared to those with a reinfection with Omicron after a preceding COVID-19 infection. METHODS Design, participants and inclusion Data were collected in the context of the national Dutch prospective LongCOVID-study. Study design details are described in the previously published study protocol (12). This paper reports a follow-up study from our previous findings on long term prevalence and severity of symptoms 3 months after Alpha and Delta SARS-CoV-2 infection (8). In brief, here we report on Omicron cases aged 18 or older three months after testing positive for SARS-CoV-2 and were enrolled between January 3rd and May 31st 2022 (Figure 1). The start of the Omicron dominant period is defined by ≥ 85% prevalence of Omicron in the Dutch pathogen surveillance (13). Likewise we included Delta cases that enrolled during ≥ 85% prevalence pathogen prevalence between July 5th 2021 and December 19th 2021. Cases between December 19th 2021 and January 3rd were excluded as neither Delta nor Omicron was dominant during this period in the Netherlands. Cases were recruited, as defined in more detail in the study protocol, within seven days following a positive polymerase chain reaction or self- or professionally administered rapid lateral flow SARS-CoV-2 test. Test-negative controls who reported respiratory symptoms as reason to test for SARS-CoV-2 and population controls without previous suspected or confirmed COVID-19 were included to control for background prevalence of long term symptoms. Population controls from the Netherlands were randomly invited by direct mailing. Both control groups were included if they enrolled between July 5th 2021 and May 31st 2022. Participants could voluntarily self-register on the study’s website (longcovid.rivm.nl) and received questionnaires at baseline (T0) and after three months follow-up (T3). Outcomes and covariates . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 4 The primary outcome was the prevalence of PCC, which we defined – slightly adapted from our previous study – as the difference in prevalence of at least one significantly elevated symptom in Omicron cases compared to the prevalence in the population control group after three months (8). Likewise we assessed the prevalence of PCC in Delta cases. As secondary outcomes we assessed the prevalence of symptoms with a clinically relevant severity, using validated questionnaires with population norm scores. These included severe fatigue measured with the subscale fatigue of the Checklist Individual Strength (CIS, cut-off score ≥ 35) (14, 15); severe self-reported cognitive problems on the Cognitive Failure Questionnaire (CFQ, cut-off score ≥ 44) (16, 17); severe pain on the bodily subscale of the RAND SF-36 Health Status Inventory (SF-36, cut-off score ≤55) (18); and severe dyspnea on the modified Medical Research Council (mMRC, cut-off score ≥1 ) scale (19).  We compared the prevalence of severe fatigue, severe cognitive problems, severe pain and severe dyspnea for Omicron cases, Delta cases, test-negative controls and population controls. Furthermore we analysed the prevalence of severe symptoms in Omicron cases between a group that received a booster vaccination and  a group that only completed a primary course. A completed primary course was defined as having received two doses of the COVID-19 vaccine at least 14 days prior to a positive SARS-CoV-2 test, or one dose of JCOVDEN at least 28 days prior, or reporting a preceding suspected or confirmed COVID-19 infection and subsequently receiving one dose at least 14 days prior to a baseline positive SARS-CoV-2 test. A booster was defined as having received an additional dose after a complete primary course (for details see Supplementary methods).   Finally, we analysed the prevalence of severe symptoms in Omicron cases between a first infection group without a previous SARS-CoV-2 infection and a reinfection group with a previous suspected or confirmed infection. In the reinfection group, the prior infection could be either suspected or test-confirmed, since in the early phase of the pandemic testing was not sufficiently available yet. Reinfected cases were excluded if at T0 they self-reported long-term symptoms which they attributed to their prior infection. Information on demographics, vaccination status, general health status, use of health care and medication, and comorbidities (adapted from the TiC-P) (20)  were collected at baseline. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 5 Statistical analyses Statistical procedures were based on a predefined, published study protocol (12). Briefly, the primary analysis was a complete case analysis with only participants completing both T0 and T3. Four additional sensitivity analyses were used to substitute for missing data on symptoms at T3: multiple imputation, carry forward, best case and worst case scenario (See Supplementary Methods). Prevalence of 41 symptoms and prevalence of severe fatigue, severe cognitive impairment, severe pain and severe dyspnea were compared between Omicron cases, Delta cases and both control groups by permutation tests which stratified for predefined confounders age, sex, level of education and number of comorbidities. Significantly elevated symptoms in Omicron cases compared to controls were defined by a two-sided 5% significance level with p-values adjusted for multiple testing by the Benjamini-Hochberg procedure (21). Prevalence of at least one significantly elevated symptom at T3 was then assessed for Omicron and Delta cases and both control groups. Prevalence of PCC was then estimated by the difference in prevalence of at least one significantly elevated symptom in the cases compared to the population controls. Likewise, comparisons were made for Omicron cases with a booster and with only a completed primary vaccination course and for Omicron cases with a first infection and a reinfection. Lastly, to evaluate the sensitivity of the PCC definition we compared the severity scores between cases and population controls who did not have one of the significantly elevated symptoms at T3. Analyses were performed with R version 4·1·0 (packages listed in Supplementary methods). Ethics approval In February 2021 the Utrecht Medical Ethics Committee (METC) declared that the Medical Research Involving Human Subjects Act (WMO) does not apply to this study as it is survey based (protocol number 21-124/C). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 6 Figure 1: Timeline of, the current analysis period, the Dutch COVID-19 vaccination program and 
inclusion and classification of participants in the current study. RESULTS Baseline characteristics for cases and both control groups that completed both T0 and T3 questionnaires (complete case) are shown in Table 1. In total 4138 Omicron cases, 6855 Delta cases, 1672 test-negative controls and 2726 population controls are included. Differences between controls and cases in vaccination status are mostly due to differing inclusion times. Additionally, population control generally have less comorbidities and a lower education compared to cases. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 7 Table 1. Demographics and acute illness at baseline Complete case 
Omicron cases 
Delta cases 
Test-negative 
controls Population 
controls N 
4138 
6855 
1672 
2726 Age, median [IQR] 
55·8 [43·8 ; 65·9] 
52·1 [40·0 ; 62·8] 
57·3 [44·0 ; 66·0] 
53·2 [41·9 ; 60·7] Sex, % (n) Female 
62·0 (2567) 
62·9 (4315) 
64·4 (1076) 
68·6 (1871) Male 
37·7 (1561) 
36·8 (2526) 
35·1 (587) 
31·2 (850) Other 
0·1 (4) 
0·1 (9) 
0·4 (6) 
0·0 (1) Pregnancy, % (n) 
2·2 (19) 
2·4 (45) 
2·1 (7) 
4·1 (27) BMI, mean (SD) 
25·91 (4·68) 
25·76 (4·60) 
26·00 (4·86) 
25·84 (4·65) Smoking, % (n) Current smoker 
3·9 (160) 
4·4 (301) 
7·4 (123) 
5·7 (156) Former smoker 
27·0 (1117) 
25·6 (1754) 
30·1 (503) 
21·2 (577) Never smoker 
67·0 (2771) 
67·3 (4614) 
59·7 (999) 
71·0 (1935) Level of education, % (n) Low 
3·2 (132) 
3·4 (235) 
2·2 (37) 
5·2 (142) Medium 
32·0 (1324) 
35·0 (2402) 
26·6 (445) 
38·5 (1049) High 
64·8 (2682) 
61·5 (4218) 
71·2 (1190) 
56·3 (1535) History with COVID-19, % (n) 
10·9 (453) 
9·0 (617) 
0·0 (0) 
0·0 (0) Nr of comorbidities, % (n) 0 
44·2 (1827) 
47·1 (3231) 
40·0 (668) 
54·0 (1471) 1-2 
42·7 (1766) 
42·3 (2902) 
42·9 (718) 
36·5 (996) >2 
13·2 (545) 
10·5 (722) 
17·1 (286) 
9·5 (259) Respiratory disease, % (n) 
17·4 (720) 
16·6 (1137) 
21·5 (359) 
11·3 (309) Hypertension, % (n) 
14·1 (584) 
12·0 (826) 
14·8 (248) 
10·8 (294) Diabetes, % (n) 
3·7 (155) 
2·9 (202) 
3·8 (63) 
3·4 (93) Cardiovasculair disease, % (n) 
2·5 (102) 
2·0 (135) 
2·8 (47) 
1·4 (38) Use of healthcare, % (n) 
6·2 (257) 
10·6 (727) 
12·0 (200) 
5·5 (151) Medication use, % (n) 
74·5 (3083) 
77·2 (5292) 
68·7 (1148) 
22·2 (605) Admitted to hospital, % (n) 
0·2 (7) 
0·1 (5) 
0·1 (1) 
0·5 (6) Vaccination status at T0, % (n) Boostered  
76·1 (2970) 
0·3 (18) 
11·0 (160) 
1·9 (46) Complete primary course 
21·8 (853) 
93·4 (5274) 
83·1 (1206) 
94·9 (2346) Partially vaccinated  
0·5 (21) 
2·1 (118) 
3·3 (48) 
1·5 (37) Unvaccinated  
1·5 (60) 
4·2 (238) 
2·5 (37) 
1·7 (43) Number of symptoms at T0, 
median [IQR] 8 [5; 12] 
9 [6; 13] 
5 [3;8] 
0 [0; 2] Variant dominant at T0, % (n) Delta 
0·0 (0) 
100 (6855) 
83·0 (1250) 
94·6 (2551) Delta-Omicron 
0·0 (0) 
0·0 (0) 
3·8 (57) 
3·2 (87) Omicron 
100 (4138) 
0·0 (0) 
13·2 (199) 
2·2 (58) . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 8 Figure 2 shows that fatigue (24·3%; p.BH = 0·0077), dyspnea (11·1%; p.BH = 0·025) difficulties with a busy environment (8·4%; p.BH = 0·0077),  problems with memory (5·9%; p.BH = 0·00092) and brainfog (2·9%; p.BH = 0·014) were significantly elevated in Omicron cases compared to both control groups (shown p.BH-values here are compared to test-negative controls, all p.BH < 0·0001 compared to population controls) after three months follow-up in the complete case scenario. Yet, the prevalence of all five symptoms was significantly lower in Omicron cases compared to Delta cases. Prevalence of all 41 symptoms per group for T0 and T3 are available in Supplementary Table S1. Figure 2: Standardised prevalence (95% confidence intervals) of the 5 symptoms at T3 that were 
significantly elevated (p.BH<0,05) between Omicron cases and both control groups and their prevalence 
in Delta cases using complete case analysis without substituting for missing values at T3. Symptoms are 
ranked by prevalence in Omicron cases. *BH.adjusted p-value <0.05; **BH.adjusted p-value <0.01  ; 
***BH.adjusted p-value <0.001 compared to Omicron cases. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 9 Of the significantly elevated symptoms, fatigue and dyspnea were generally reported in both the acute phase and at T3 while difficulty with a busy environment, problems with memory and brainfog were generally reported more at T3 only (Figure S2). Severe dyspnea (mMRC score≥1) had a significantly higher prevalence in Omicron cases (8·3% prevalence) compared to test-negative and population control groups (6·3% and 4·2% respectively, with p.BH 0·02 and < 0·001); Figure 3).  Severe fatigue (CIS score ≥ 35) and severe cognitive problems (CFQ score ≥ 44)  were significantly more prevalent in Omicron cases (11·8%) compared to population controls (6,6%, p.BH < 0·001) but not compared to test-negative controls (10·6%, p.BH = 0·14). Finally, severe fatigue (25·6% vs 22·1%), severe cognitive problems (14·3% vs 11·8%) and severe dyspnea (12·2% vs 8·3%) were significantly more prevalent in Delta cases compared to Omicron cases (all p.BH <0·0001). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 10 Figure 3: Standardised prevalence (95% confidence intervals) of severity score cut-off values in cases 
and both control groups using complete case analysis without substituting for missing values at T3. 
Severe fatigue: Checklist Individual Strength (CIS), subscale fatigue ≥35, severe cognitive problems: 
Cognitive Failure Questionnaire (CFQ) ≥44, severe dyspnoea: modified Medical Research Council 
dyspnoea scale mMRC ≥1, severe pain: SF-36 subscale bodily pain ≤55. *BH.adjusted p-value <0.05; 
**BH.adjusted p-value <0.01  ; ***BH.adjusted p-value <0.001 compared to Omicron cases. In the complete case scenario, prevalence of PCC, i.e. the difference in prevalence of at least one of the five significantly elevated symptoms in cases compared to the population controls, was estimated at 10·4% in Omicron cases, compared to 17·7% in Delta cases. The overall prevalence of the five significantly elevated symptoms was found to be significantly lower for Omicron cases (30·0%) compared to Delta cases (37·2%) but higher compared to test-negative controls (26·2%) and population controls (19·6%) at T3 (Figure 4). Differences between Omicron cases and Delta cases and *** *** *** * *** *** *** . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 11 control groups were also noticeable in the multiple imputation, carry forward and worst case scenario when substituting for missing values (Supplementary Figure S1). In the carry forward scenario prevalence of PCC was higher largely due to differences in cases and population controls at T0, where cases and test-negative controls were in the acute phase of an infection. In the best case scenario differences between Omicron cases and controls were no longer significant, likely due to the substitution of missing values at T3 with complete recovery, but differences between Omicron and Delta were still present. Figure 4: Standardized prevalence (95% confidence intervals) of at least one of the five significantly 
elevated symptoms at T3 in Delta and Omicron cases compared to test-negative and population 
controls. Of the Omicron cases, in the complete case analysis 10·9% (453 out of 4138) of the participants reported a suspected or confirmed prior SARS-CoV-2 infection before enrolment. Of the reinfected cases, 438 had one infection prior to their inclusion Omicron infection and 15 had two or more. Baseline 30.0% 37.2% 26.2% 19.6% . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 12 demographics are available in Supplementary Table S3. Severe fatigue (28·0% ; p.BH = 0·030) and severe dyspnea (12·1%; p.BH = 0·020) were significantly more prevalent in Omicron cases with a reinfection compared to Omicron cases with a first SARS-CoV-2 infection (23·0% and 8·1% respectively). Additionally, of the five significantly elevated symptoms, here too fatigue (p.BH = 0·014) and dyspnea (p.BH = 0·045) were reported more frequently in the reinfection group (Supplementary Figure S3). Prevalence of at least one significantly elevated symptom was 37·8% for reinfected cases and 30·3% for cases with a first infection. Of the Omicron cases, 2970 (76·1%) participants received a booster vaccination while 853 (21·8%) participants only completed the primary course prior to infection in the complete case analysis. Baseline demographics are available in Supplementary Table S2. Severe fatigue and severe cognitive problems were less prevalent in cases with a booster (21·0% and 10·9% respectively) compared to the primary course cases (23·1% and 13·2%), though neither significantly (Supplementary Figure S3). Prevalence of at least one of the five significantly elevated symptoms was similar for boostered cases and cases with a completed primary course (27·4% and 28·8% respectively).  None of the five PCC- related symptoms were significantly different in prevalence between the two groups (Supplementary Figure S3). There were too few partially vaccinated cases (n = 21; 0·5%) and unvaccinated cases (n = 60; 1·5%) to analyse. Cases that did not fulfil our case definition had significantly worse scores for CIS- fatigue, CFQ and SF36-pain than population controls not fulfilling the case definition, but the absolute differences were less than 1 point on all three scales (Supplementary Table S4). . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 13 DISCUSSION In this prospective cohort study we found that three months after an Omicron infection, symptoms that were significantly elevated compared to both control groups were fatigue, dyspnea, difficulties with a busy environment, problems with memory and brainfog. Prevalence of PCC – i.e. the difference in prevalence of at least one of these symptoms in cases compared to the population controls – was 41% lower for Omicron cases compared to Delta cases: prevalence of at least one symptom possibly associated with PCC (30·0%) was lower compared to a Delta infection (37·2%) yet still exceeding the background prevalence in population controls (19·6%). Severity of symptoms was also lower after Omicron than after Delta for fatigue, cognitive impairment and dyspnea. Still these symptoms were reported significantly more often as severe in Omicron cases compared to population controls, whereas only severe dyspnea was increased compared to test- negative controls. Previous research in the same study on three month follow-up of prevalence and severity of symptoms following an Alpha or Delta infection indicated a total of 13 significantly elevated symptoms (8). In the Omicron analysis only 5 symptoms exceeded background prevalence in both control groups. Most notably, the COVID-19 characteristic symptoms of loss of smell and loss of taste are 5·5 and 4·2 times higher in prevalence at T3 for Delta cases than Omicron cases (Supplementary Table 1). A possible explanation for these findings could be that Omicron generally has less involvement of the lower respiratory tract and a milder acute phase than Delta (6). These findings are in line with a Danish and UK study that have shown lower odds for PCC with Omicron compared to Delta (22, 23).  Studies have shown that PCC is associated with the severity of the acute phase of COVID-19 (24, 25).  In the current study we only find that the median number of symptoms at T0 is slightly higher for Delta cases compared Omicron cases, but considerably more compared to both control groups (Table 1). However, prevalence of severe fatigue and severe bodily pain was significantly higher in the acute phase for Delta cases than Omicron cases. Interestingly, a Norwegian study did not find differences in long-term symptoms after Omicron and Delta infections when the variants co-circulated (26). This discrepancy with our and other studies may have to do with different . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 14 levels of immunity in the population when comparing Omicron and Delta infections in different calendar study periods or differences in subvariants analysed. Cases with a reinfection seemed to have a higher prevalence for PCC with a 1·2 times higher prevalence of PCC-related symptoms compared to a first SARS-CoV-2 infection with the Omicron variant. In fact, the prevalence of at least one of the significantly elevated symptoms at T3 amongst reinfected cases was very similar to the prevalence in the Delta case group (37·8% and 37·2% respectively). Estimates suggest over half a billion people globally have been infected SARS-CoV-2, which infers an increasing likelihood of reinfection occurring with (sub)variants more adept at immune escape. Research has shown that Omicron is indeed associated with a marked ability to evade immunity from prior infection (27). Natural immunity, and also hybrid immunity with vaccination, against subsequent infection has additionally been shown to wane over time (28). However, it cannot be excluded that reinfected cases had impaired health possibly putting them at higher risk both for reinfection and PCC. This would imply that the probability of developing symptomatic COVID-19 due to different health status rather than the reinfection itself would somehow be associated with a higher risk of developing PCC.  This would be in line with another study that has shown that hybrid immunity from prior infection and vaccination did not abrogate risk of long-term symptoms (29). Additional research on PCC and reinfection is needed to better understand the dynamics that govern these findings. Waning immunity may also infer a diminishing effect of vaccination on protection against PCC after an Omicron infection. Omicron cases with a primary course received their last vaccination a median 142 days prior to infection while boostered cases had a median time difference of 60 days. Still the booster vaccinations compared to a primary vaccination course seemed at most modestly protective for PCC: fatigue and cognitive problems were less frequently severe in cases with a booster, but these differences were not statistically significant and the study may lack the power to detect prevalence differences smaller than 5% (12) . Likewise, prevalence of at least one possible PCC-related symptom . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 15 was similar between both groups. Studies have shown a partial protection of the primary course compared to unvaccinated cases for pre-Omicron variants (8, 30, 31) and for Omicron (32). Research on the effect of the booster is limited but one study shows a lower association with PCC for three- dose vaccinated Omicron cases compared to two-dosed (23). Generally, there is some evidence to suggests that a booster vaccination provides an albeit temporary protection against infection with Omicron (33). Indirect effects by preventing infection and transmission compounded with a modest direct effect may still yield a more than modest reduction in PCC incidence. Currently, bivalent COVID- 19 vaccines are available which contain, in addition to the wild-type SARS-CoV-2 mRNA, Omicron BA·1 or Omicron BA·4-5 subvariants mRNA. Possibly, these bivalent vaccines may infer increased protection against PCC. Still, our findings suggest that monovalent booster vaccine induced immunity has either waned or offers limited direct protection against long-term symptoms following an Omicron breakthrough infection. Strengths and limitations A strength of this prospective cohort study is the inclusion of large numbers of Delta and Omicron cases as well as two control groups to be able to estimate the prevalence of PCC corrected for the background prevalence of symptoms in the population and symptoms likely due to other respiratory infections. Reporting of reinfections and changing vaccination status during follow up made it possible to investigate their association with the prevalence of PCC after SARS-CoV-2 infection. Moreover, recruiting through test sites rather than through hospitals likely resulted in a cohort that is representative of the long-term effects of COVID-19 in the general population. This study also has some limitations. Firstly, data was collected exclusively through online questionnaires without clinical evaluation of symptoms. Nevertheless our use of validated questionnaires with population norm scores and the aforementioned control groups made it possible to assess to what extent prevalence of reported symptoms exceeded the background prevalence. Secondly, the follow-up of the T3 survey had a response rate of 70%. It is possible that the missing 30% dropped-out due to lack of symptoms or, oppositely, becoming severely ill. Therefore we . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 16 substituted for missing values by multiple alternative imputation scenarios, which showed robustness of our finding that Omicron was less severe than Delta. Thirdly, recruitment of controls during the Omicron period was low. Background prevalence was therefore largely established on controls recruited during the Delta period which may have a different background prevalence than the Omicron period due to different COVID-19 restrictions and seasonal effects. However, most controls included during the Delta period were enrolled closely to the start of the Omicron period with only a 40 day median difference. Supplementary Table 1 additionally shows that between T0 and T3 background prevalence of symptoms in population controls fluctuates at most with 3,1 percentage point, indicating little fluctuation over time. Moreover, severe fatigue from our control groups (20·0% for test-negative and 14·0% for the population control) was similar to a large Dutch population cohort (18%, n=78363)(34). As Delta circulated during summer and autumn in the Netherlands and Omicron has, at the moment of analysis, circulated during winter and spring, correction for seasonal effects between Delta and Omicron cases was not possible. This means that some symptoms that significantly exceeded the background prevalence for Delta cases but not for Omicron cases might be explained by other factors than the variant. Variant attribution was performed on 85% Dutch pathogen surveillance proportion, which may lead to marginal misclassification for the weeks of transition between Alpha and Delta and between Delta and Omicron. As well, subvariants  BA·1 and BA·2 of Omicron have circulated in the Netherlands during the study period (13). Due to the relatively long co-circulation of subvariants, and the absence of SARS-CoV-2 PCR test sequence data, subvariant specific analysis are not possible. Finally, our case definition of PCC may not have been sensitive enough to capture all post-covid symptoms, since cases that did not fulfil our case definition had significantly worse scores for CIS-fatigue, CFQ and SF36-pain than population controls not fulfilling the case definition. However, the absolute differences in mean scores were at most minimal, see Supplementary Table S4), which suggests that our case definition still captured the vast majority of PCC. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 17 Implications These findings suggest that the prevalence of PCC is two fifths lower following an Omicron infection compared to Delta. Reinfection appears to be associated with more prevalent long-term symptoms compared to a first infection. A preceding booster vaccination does not seem to improve the outcome regarding PCC in Omicron cases. DECLARATIONS List of abbreviations: - 
CIS: Checklist Individual Strength - 
CFQ: Cognitive Failure Questionnaire - 
mMRC: Modified Medical Research Council dyspnea scale - 
PCC: Post-Covid-19 Condition - 
PCR: polymerase chain reaction - 
SARS-CoV-2: Severe acute respiratory syndrome coronavirus 2 - 
SF-36: SF-36 item Health Survey - 
TiC-P: Treatment Inventory of Costs in Patients with psychiatric disorders Competing interests We declare no competing interests. Author’s contributions TM, AH, CW and EF conceptualised the study. EM, CW, HK designed the study protocol and statistical analysis. SB, TM and JS analysed the data. SB, TM, EM, AT, AH, EF and CW contributed to the data interpretation. SDB and TM coordinated the data collection and SDB drafted the manuscript. All authors reviewed and edited revisions of the manuscript, had full access to all the data in the study, and had final responsibility for the decision to submit for publication . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 18 Acknowledgements We thank Caroline van den Ende for structural updating of literature on PCC. Role of the funding source The study is executed by the National Institute for Public Health by order of the Ministry of Health. The study is not the result of a competitive grant. The Dutch Ministry of Health, Welfare and Sport does not have a role in the design of this study, its execution, analyses and interpretation of results. Data sharing Supporting clinical documents including the study protocol and statistical analysis plan will be available immediately following publication of this Article for at least 1 year. Researchers who provide a methodologically sound proposal will within the applicable privacy legislation be allowed to access to the de-identified individual participant data that underlie the results reported in this article. Proposals should be sent to the corresponding author. These proposals will be reviewed and approved by the investigators on the basis of scientific merit. To gain access, data requestors will need to sign a data access agreement. . 
CC-BY-NC 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288157
doi: 
medRxiv preprint 19",1
"Introduction: Despite representing only 3% of the US population, immunocompromised (IC) individuals account for nearly half of the COVID-19 breakthrough hospitalizations. IC individuals generate a lower immune response following vaccination in general, and the US CDC recommended a third dose of either mRNA-1273 or BNT162b2 COVID-19 vaccines as part of their primary series. Influenza vaccine trials have shown that increasing dosage could improve effectiveness in IC populations. The objective of this systematic literature review and pairwise meta-analysis was to evaluate the clinical effectiveness of mRNA-1273 (50 or 100 mcg/dose) versus BNT162b2 (30 mcg/dose) in IC populations using the GRADE framework. Methods: The systematic literature search was conducted in the World Health Organization COVID-19 Research Database. Studies were included in the pairwise meta-analysis if they All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. 2 reported comparisons of mRNA-1273 and BNT162b2 in IC individuals ≥18 years of age; outcomes of interest were SARS-CoV-2 infection, hospitalization due to COVID-19, and mortality due to COVID-19. Risk ratios (RR) were pooled across studies using random-effects meta-analysis models. Outcomes were also analyzed in subgroups of patients with cancer, autoimmune disease, and solid organ transplant. Risk of bias was assessed for randomized and observational studies using the Risk of Bias 2 tool and the Newcastle-Ottawa Scale, respectively. Evidence was evaluated using the GRADE framework. Results: Overall, 22 studies were included in the pairwise meta-analysis. Compared with BNT162b2, mRNA-1273 was associated with significantly reduced risk of SARS-CoV-2 infection (RR 0.87, 95% CI 0.79–0.96; P=0.0054; I2=61.9%), COVID-19–associated hospitalization (RR 0.83, 95% CI 0.76–0.90; P<0.0001; I2=0%), and COVID-19–associated mortality (RR 0.62, 95% CI 0.43–0.89; P=0.011; I2=0%) in IC populations. Results were consistent across subgroups. Because of sample size limitations, relative effectiveness of COVID-19 mRNA vaccines in IC populations cannot be studied in randomized trials and evidence certainty among comparisons was type 3 (low) and 4 (very low), reflecting potential biases in observational studies. Conclusion: This GRADE meta-analysis based on a large number of consistent observational studies showed that the mRNA-1273 COVID-19 vaccine is associated with improved clinical effectiveness in IC populations compared with BNT162b2. Keywords: Severe acute respiratory syndrome coronavirus 2, SARS-CoV-2, COVID-19, mRNA vaccine, mRNA-1273, BNT162b2, immunocompromised, effectiveness All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 3 Introduction The global coronavirus disease 2019 (COVID-19) pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has resulted in 103 million reported infections and 1.1 million deaths to date in the United States (US) (1). In response to the pandemic, mRNA-1273 (Spikevax®, Moderna, Inc., Cambridge, USA) (2) and BNT162b2 (Comirnaty®, Pfizer/BioNTech, New York, USA/Mainz, Germany) (3), each employing novel mRNA technology, were developed and approved for the prevention of COVID-19 (4). Global phase 2/3 studies demonstrated that both mRNA vaccines given in a 2-dose series were highly efficacious at reducing symptomatic infections and hospitalizations in the immunocompetent population (5; 6). Although immunocompromised (IC) individuals comprise only approximately 3% of the total US population (7), they account for nearly half of the breakthrough COVID-19 hospitalizations (8). While there is a range of severity across conditions at the population level, adults considered immunodeficient had 2.68-fold greater adjusted odds of being hospitalized with COVID-19 compared with immunocompetent individuals due both to the underlying IC condition and therapies used for treatment (9; 10). In 1 study, use of immunosuppression in patients with autoimmune disease resulted in 1.35-fold (95% confidence interval [CI] 1.29–1.40) greater odds of developing life-threatening COVID-19 (11). Despite being at increased risk of COVID-19–related morbidity and mortality (10; 12-14), IC individuals and patients receiving immunosuppressive medications were excluded from participating in pivotal trials of mRNA-1273 and BNT162b2 (5; 6). Real-world COVID-19 data indicate that vaccine immune responses are generally impaired in IC populations (9; 15-17) and that vaccine effectiveness, as estimated as the odds of obtaining a positive SARS-CoV-2 test result using multivariate logistic regression models, is lower in IC versus immunocompetent individuals (18). In addition to severe COVID-19, IC populations are at higher risk of prolonged SARS-CoV-2 infection (19-26) and viral evolution (19-22; 24; 27; 28) due to poor humoral All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 4 responses. These risks are exacerbated by even lower antibody responses to SARS-CoV-2 variants (29-35). IC individuals may also contribute disproportionately to SARS-CoV-2 transmission to household contacts (36). High vaccine effectiveness is therefore critically important for this population and the US Centers for Disease Control and Prevention (CDC) recommended a third dose of either mRNA-1273 or BNT162b2 COVID-19 vaccines as part of their primary series. Influenza vaccine trials demonstrated that high dose vaccines led to improved immune responses in IC individuals compared with standard dose vaccines and suggested that a high dose vaccine offers greater effectiveness for IC populations (37-42). Although both mRNA-1273 and BNT162b2 employ the mRNA mode of action, the composition of each vaccine is different. For instance, the mRNA dosage and type of lipid nanoparticles used in the delivery system differs between vaccines. The mRNA-1273 primary series contains 100 mcg of mRNA and 50 mcg for the booster (2; 43), whereas BNT162b2 contains 30 mcg of mRNA for each primary and booster dose (3; 44). Observational studies have consistently shown differences between the two mRNA COVID-19 vaccines both in terms of immune response (15) and clinical effectiveness (45-47) in IC populations. As SARS-CoV-2 transitions from a pandemic to an endemic state, countries are transferring vaccination programs from central government purchasing to their respective national healthcare systems, which is triggering in-depth health technology assessments to recommend the best use of available vaccines in specific populations. Several national immunization technical advisory groups (NITAGs), including the Advisory Committee on Immunization Practices (ACIP) in the US, use the GRADE (Grading of Recommendations, Assessment, Development and Evaluations) framework for identifying questions relevant to healthcare, selecting outcomes of interest and assessing their importance, evaluating the available evidence, and synthesizing evidence to develop recommendations consistent with considerations of values and preferences of patients and the society in which they live (48; 49). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 5 This present analysis follows the GRADE framework to address the following healthcare question: Is the mRNA-1273 COVID-19 vaccine (50 or 100 mcg/dose) more clinically effective in IC populations compared with the BNT162b2 COVID-19 vaccine (30 mcg/dose)? Accordingly, we performed a systematic literature review and pairwise meta-analysis to compare COVID-19 vaccine effectiveness outcomes among IC individuals given either mRNA-1273 or BNT162b2. Methods Search strategy and study selection We performed a systematic literature review in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 framework (50). The main search was conducted in the World Health Organization COVID-19 Research Database on April 14, 2022 and updated on December 19, 2022. Databases searched were MEDLINE, International Clinical Trials Registry Platform, EMBASE, EuropePMC, medRxiv, Web of Science, ProQuest Central, Academic Search Complete, Scopus, and COVIDWHO. The search strategy is presented in Table S1. Clinical trials, observational studies, or any real-world evidence published as manuscripts, letters, commentaries, abstracts, or posters were included if they reported efficacy or clinical effectiveness outcomes in IC individuals ≥18 years of age vaccinated with mRNA- 1273 or BNT162b2 within the same study. IC individuals were defined as people with immunocompromising conditions falling into the clinically extremely vulnerable (CEV) groups 1 or 2, which include solid organ transplant, solid and hematological cancers, hemodialysis, poorly controlled human immunodeficiency virus (HIV) infection, and autoimmune conditions requiring immunosuppressive therapy (51). Outcomes of interest were vaccine efficacy or effectiveness against SARS-CoV-2 infection, COVID-19–associated hospitalization, and COVID-19– associated death. Recently published systematic literature reviews on the same topic were cross-checked to ensure relevant articles were included. Studies reporting outcomes in All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 6 pregnant women, current or former smokers, or physically inactive people; with heterologous vaccination schedule (i.e., mix of mRNA-1273 and BNT162b2); with only safety data; or study protocols or economic models were excluded (Table S2). Two independent reviewers selected studies following a two-level approach; a third reviewer arbitrated conflicts. Titles and abstracts were screened against inclusion criteria in level 1, followed by full-text appraisal of articles not excluded at level 1 against selection criteria in level 2. Data extraction and quality assessment Publication details, study and participant characteristics, vaccine type and vaccination status, at-risk condition, and clinical outcomes were extracted. Risk of bias was assessed in accordance with Cochrane review guidelines (52) using the ROB 2 tool (53) for randomized studies and the Newcastle-Ottawa Scale (54) for observational studies. Evidence was evaluated based on GRADE criteria (48; 49). Statistical analysis Random-effects meta-analysis models were used to pool risk ratios (RR) and calculate absolute effects as risk difference (RD) per 100,000 individuals across studies. Inverse variance weights were calculated for individual studies with the DerSimonian-Laird method (55). Chi- square testing to evaluate heterogeneity across studies was performed (56). The I2 statistic was estimated (0–100%, 0% meaning no evidence of heterogeneity). Subgroup analysis was performed for patients with cancer, autoimmune disease, and solid organ transplant. Results Overview of included studies Of 5,745 unique items retrieved, we identified 35 studies reporting COVID-19 clinical efficacy or effectiveness outcomes in IC individuals ≥18 years of age who received mRNA-1273 or BNT162b2 in the same study (Figure 1). Thirteen articles were excluded because the All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 7 population did not meet the inclusion criteria (i.e., participants had immunocompromising conditions not included in CEV groups 1 or 2), 1-dose vaccine regimen data were reported, or the outcome of interest data were not reported in sufficient detail for analysis. Characteristics of all nonrandomized (n=21) and randomized (n=1) studies included in the pairwise meta-analysis are shown in Table 1. Overall, 190,643 and 187,813 patients received mRNA-1273 and BNT162b2, respectively. Studies included mostly US populations (n=15) (18; 45; 47; 57-68), with the remaining trials reporting data on patients from Spain (n=3) (32; 46; 69), Italy (n=1) (70), Singapore (n=1) (71), Switzerland (n=1) (72), and multiple countries (n=1) (73). Specific at- risk and IC conditions included solid organ transplant (n=6) (45; 46; 57; 67; 68; 72), cancer (n=5) (18; 45; 62; 65; 70), hemodialysis (n=3) (32; 59; 66), rheumatologic disease (n=3) (18; 47; 73), multiple sclerosis or other neurological autoimmune disease (n=2) (61; 71), inflammatory bowel disease (n=1) (63), primary immunodeficiency with functional B cell defects (n=1) (64), hematological disorders (n=1) (69), and HIV infection (n=1) (72). Two studies did not specify the IC condition (58; 60). Individuals received ≥2 doses of mRNA-1273 or BNT162b2. Data on 2- dose regimens were considered if reported (n=15) (18; 45; 46; 57; 59; 61; 63-69; 72; 73), otherwise data from 3- or 4-dose regimens (n=7) (32; 47; 58; 60; 62; 70; 71) were used. Of studies reporting data from 2-dose regimens, outcomes were assessed ≥14 days after the second dose (n=13), ≥7 days after the second dose (n=1) (63), and other timepoints (n=1) (66). Timing of outcome assessment relative to the second dose was not specified in 3 studies. Variants of concern were delta (n=6) (18; 45; 46; 65; 67; 69), delta and omicron (n=2) (62; 71), delta and beta (n=1) (59), pre-omicron variants (n=1) (47), and omicron only (n=1) (58). Eleven studies did not directly specify the variant assessed (32; 57; 60; 61; 63; 64; 66; 68; 70; 72; 73). Risk of bias assessment found no serious risk of bias for 17 studies (randomized, n=1; nonrandomized, n=16) and serious risk of bias in 5 nonrandomized studies primarily due to the lack of description of comparability between cohorts or adjustment for confounding factors (Table S3; Table S4). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 8 SARS-CoV-2 infection (randomized study) Only 1 and 2 laboratory-confirmed, symptomatic SARS-CoV-2 infections occurred in the BNT162b2 and mRNA-1273 arms, respectively, of a single randomized controlled trial (RCT; RR 2.05, 95% CI 0.19–22.42; RD 499, 95% CI −1,137 to 2,136) (72). The small number of events led to uncertainty around the estimates of effect and no association between mRNA vaccine type and risk of SARS-CoV-2 infection was found in this RCT. Evidence certainty was downgraded from type 1 (high) to type 3 (low) for imprecision and limited evidence (Table 2; Table S4). SARS-CoV-2 infection (nonrandomized studies) Of the 17 nonrandomized studies reporting SARS-CoV-2 infection, mRNA-1273 was associated with a statistically significant reduction in the risk of SARS-CoV-2 infection compared with BNT162b2 (RR 0.87, 95% CI 0.79–0.96; P=0.0054; Figure 2). The RD (95% CI) of mRNA- 1273 versus BNT162b2 was estimated to be 412 fewer SARS-CoV-2 infections (from 665 fewer to 160 fewer). Heterogeneity between studies may be considered substantial (I2=61.9%). The certainty of evidence was graded as type 4 (very low) for imprecision and indirectness due to varying outcome definitions (Table 2; Table S3). Analysis of 4 studies each reporting SARS-CoV-2 infection in patients with cancer (45; 65; 69; 70) found that mRNA-1273 was associated with significantly reduced risk of infection compared with BNT162b2 (RR 0.73, 95% CI 0.57–0.92, P=0.0088; RD −346, 95% CI −598 to −94, P=0.0071). Similar findings were observed in 4 studies assessing patients with autoimmune diseases (RR 0.67, 95% CI 0.52–0.88; P=0.0032; RD −455, 95% CI −1,209 to 298) (47; 61; 63; 71). No association between mRNA vaccine type was found for the 4 studies reporting SARS-CoV-2 infection in solid organ transplant recipients (RR 1.05, 95% CI 0.87– 1.26; RD −93, 95% CI −573 to 386) (45; 57; 67; 68). No evidence of heterogeneity was All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 9 observed between any of the studies (I2=0% for all subgroups). As in the overall analysis of SARS-CoV-2 infection, the certainty of evidence was graded as type 4 (very low; Table 3). Hospitalization due to COVID-19 mRNA-1273 was associated with a significantly lower risk of COVID-19–associated hospitalization versus BNT162b2 in the 9 studies included in the overall analysis (RR 0.83, 95% CI 0.76–0.90; P<0.0001; Figure 2). The RD (95% CI) of mRNA-1273 compared with BNT162b2 was estimated to be 60 fewer hospitalizations due to COVID-19 (from 140 fewer to 20 more). No evidence of heterogeneity was observed between studies (I2 = 0%). The certainty of evidence for this outcome was type 3 (low) due to inclusion of nonrandomized studies and imprecision (Table 2; Table S3). In 2 studies reporting hospitalization in patients with cancer (18; 45), mRNA-1273 was associated with a significantly reduced risk of hospitalization compared with BNT162b2 (RR 0.54, 95% CI 0.37–0.79; P=0.0013; RD −585, 95% CI −1,655 to 485). No association between mRNA vaccine type and COVID-19–associated hospitalization was found for the 3 studies each reporting hospitalization in the subgroups of patients with autoimmune diseases (RR 0.97, 95% CI 0.70–1.35; RD −103, 95% CI −1,661 to 1,456) (18; 71; 73) or solid organ transplant (RR 0.91, 95% CI 0.78–1.06; RD −147, 95% CI −816 to 522) (45; 46; 68). No evidence of heterogeneity was observed between any of the studies for the subgroup analysis. The certainty of evidence in all subgroups was graded as type 3 (low; Table 3). Death due to COVID-19 Of the 4 studies reporting COVID-19–associated mortality (46; 57; 66; 67), mRNA-1273 was associated with a significantly reduced risk of death compared with BNT162b2 (RR 0.62, 95% CI 0.43–0.89; P=0.011; Figure 2). mRNA-1273 was estimated to lead to 56 fewer deaths associated with COVID-19 (from 559 fewer to 446 more) compared with BNT162b2. No evidence of heterogeneity was observed between any of the studies (I2=0%). The certainty of All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 10 evidence was rated as type 3 (low) due to inclusion of nonrandomized studies (Table 2; Table S3). Grading was reduced for imprecision and increased due to the strong association in RR. COVID-19–associated death was assessed only in the subgroup of solid organ transplant recipients (46; 57; 67). In these 3 studies, mRNA-1273 was associated with a significantly reduced risk of death compared with BNT162b2 (RR 0.56, 95% CI 0.37–0.84; P=0.0049; RD −3,528, 95% CI −12,002 to 4,945). No evidence of heterogeneity was observed between any of the studies in this subgroup (I2=0%). The certainty of evidence was type 3 (low) due to inclusion of nonrandomized studies as well as imprecision and limited evidence (Table 3). Discussion In this systematic review and pairwise meta-analysis of 22 studies, we found that mRNA- 1273 was associated with a significantly lower risk of SARS-CoV-2 infection, hospitalization due to COVID-19, and COVID-19–associated mortality compared with BNT162b2 in adults with a broad spectrum of severe immunocompromising conditions. The certainty of this evidence was type 4 (very low) for the SARS-CoV-2 infection outcome and type 3 (low) for the COVID-19– associated hospitalization and death outcomes (Table 4). As all included studies were pairwise comparisons between mRNA-1273 and BNT162b2, the research question was not biased by differences in time period assessed, population, viral variants within each study. When outcomes were assessed in subgroups, mRNA-1273 was associated with significantly lower risk of SARS-CoV-2 infection and COVID-19–associated hospitalization versus BNT162b2 in patients with cancer. Compared with BNT162b2, mRNA-1273 was also associated with a significantly reduced risk of SARS-CoV-2 infection in patients with autoimmune diseases and COVID-19–associated death in solid organ transplant recipients. IC individuals have a high burden of COVID-19 due to characteristics of their underlying disease or immunosuppressive treatments that impact their ability to mount productive immune All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 11 responses as well as increased susceptibility to severe COVID-19 (30). Physicians may seek to optimize COVID-19 vaccine type, timing, and number of doses to improve outcomes in IC patients (32). RCTs are ranked highly in the hierarchy of evidence; however, studying comparative efficacy with adequate power would require enrolling a prohibitive number of IC patients. Therefore, the research question can only be assessed using large real-world databases where individual medical and pharmacy information is available. Limitations of this systematic literature review were that non-English studies were excluded, and publication bias was not assessed in the meta-analysis. Inherent to the GRADE framework, evidence certainty is initially set to either high if the included studies are randomized studies or low if they are observational studies. As all but 1 of the 22 studies included in the pairwise meta-analysis were nonrandomized, the maximum certainty of evidence achievable in this meta-analysis was low despite the high number of observational studies and consistency of results. The pairwise meta-analysis was also limited by inconsistent outcome definitions across studies as well as differences in covariates between studies. For example, the vaccination scheme (2 vs 3 doses; booster) differed between studies, with a mix of primary series (100 mcg vs 30 mcg) and booster (50 mcg vs 30 mcg) pairwise comparisons included in the meta- analysis. Variants of concern changed over time, with risks of hospitalization and death (74) and vaccine effectiveness differing by variant (75). Vaccine effectiveness of 2-dose regimens could only be shown for the delta variant, whereas the omicron variant required a 3-dose schedule. Other sources of bias inherent to observational studies, such as prescribing differences by risk of severe COVID-19 and ability of patients to choose the mRNA vaccine type, could not be accounted for in this meta-analysis. In addition to differences in mRNA dosage between mRNA- 1273 and BNT162b2, other differences such as the lipid nanoparticle delivery system and mRNA translation efficiency may also have impacted clinical effectiveness between vaccines. Our meta-analysis of observational studies showed that mRNA-1273 (50 or 100 mcg/dose) was associated with a significantly reduced risk of SARS-CoV-2 infection, COVID- All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 12 19-associated hospitalization, and death due to COVID-19 when compared with BNT162b2 (30 mcg/dose) in IC populations. Based on the findings, vaccinating IC individuals in the United States with mRNA-1273 instead of BNT162b2 would prevent an additional 60 and 56 hospitalizations and deaths per 100,000 individuals, respectively. Considering the limited availability of data from RCTs and to provide needed clinical decision-making guidance, our results showed that mRNA-1273 offers better clinical outcomes compared with BNT162b2 in vulnerable IC populations. Author Contributions XW and KH designed and performed the systematic literature review and meta-analysis, and critically evaluated the manuscript. AS, LEP, AR, and AK designed and performed the systematic literature review and critically evaluated the manuscript. PS and SV collected data and critically evaluated the manuscript. MTB-J and NVdV conceptualized the article and provided oversight and critical evaluation of the manuscript. All authors contributed to the article and approved the submitted version. Disclosures XW, KH, PS, AK, and SV are employees of ICON plc, a clinical research organization paid by Moderna, Inc., to conduct the study. AS is an independent epidemiology consultant/director of Data Health Ltd, which provides health data consultancy services, and was paid by Moderna, Inc., to conduct aspects of this study. LEP is an employee and owner of Data–Driven LLC and AR is a contractor of Data–Driven LLC, a research organization paid by Moderna, Inc. to conduct aspects of this study. MTB-J and NVdV are employees of Moderna, Inc. and hold stock/stock options in the company. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 13 Acknowledgments Writing assistance was provided by Erin McClure, PhD, an employee of ICON (Blue Bell, PA, USA) in accordance with Good Publication Practice (GPP3) guidelines, funded by Moderna, Inc., and under the direction of the authors. Funding This study was funded by Moderna, Inc. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288195
doi: 
medRxiv preprint 14",1
"Background Psychosis is one of the most disabling psychiatric disorders. Paediatric traumatic brain injury (pTBI) has been cited as a developmental risk factor for psychosis, however this association has never been assessed meta-analytically. Methods A systematic review and meta-analysis of the association between pTBI and subsequent psychotic disorders/symptoms was performed. The study was pre-registered (CRD42022360772) adopting a random-effects model to estimate meta-analytic odds ratio (OR) and 95% confidence interval (CI) using the Paule–Mandel estimator. Subgroup (study location, study design, psychotic disorder vs subthreshold symptoms, assessment type, and adult vs adolescent onset) and meta-regression (quality of evidence) analyses were also performed. The robustness of findings was assessed through sensitivity analyses. The meta- analysis is available online as a computational notebook with an open dataset. Results We identified 10 relevant studies and eight were included in the meta-analysis. Based on a pooled sample size of 479,686, the pooled OR for the association between pTBI and psychosis outcomes was 1.80 (95% CI [1.11, 2.95]). There were no subgroup effects and no outliers. Both psychotic disorder and subthreshold symptoms were associated with pTBI. The overall association remained robust after removal of low-quality studies, however the OR reduced to 1.43 (95% CI [1.04, 1.98]). A leave-one-out sensitivity analysis showed the association was robust to removal of all but one study which changed the estimate to marginally non-significant. Conclusions We report cautious meta-analytic evidence for a positive association between pTBI and future psychosis. New evidence will be key in determining long-term reliability of this finding. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 3 Introduction There is consistent evidence indicating that traumatic brain injury (TBI) is associated with an increased risk of adverse neuropsychiatric outcomes in adults, including depression, anxiety, posttraumatic stress symptoms, cognitive impairment, personality change, and neurodegenerative disorders (Carroll et al., 2014; Cnossen et al., 2017; Fleminger, 2008; Perry et al., 2016; Rogers & Read, 2007; Schwartz, Jodis, Breen, & Parker, 2019; van Reekum, Cohen, & Wong, 2000). One association that has proved more controversial, however, has been the link between TBI and psychosis. Although there are clearly cases of post-TBI psychosis (Fujii & Ahmed, 2002), the extent to which TBI is a reliable population risk factor for psychosis has been debated. In a narrative review of the evidence, David and Prince (2005) concluded that it was unlikely brain injury reliably causes psychosis given the published data available at the time. In a subsequent narrative review, Batty et al. (2013) estimated that psychosis following TBI appears to be three times more prevalent than psychotic disorders in the general population. Looking specifically at the association between TBI and schizophrenia in case-control studies, Molloy et al.’s (2011) meta-analysis reported a significant association and, through the inclusion of family studies, suggested this effect was larger in those with a genetic predisposition to psychosis. Notably, however, the studies considered in these reviews largely examined the impact of adult TBI on later psychosis. Although clearly important, studies that focus solely on adult TBI may miss longer-term associations between TBI that occurs before the age of 18 and an increased risk of psychotic disorders or symptoms later in life. The association between paediatric TBI (pTBI) and psychosis is plausible given what we know about risk factors for psychosis in childhood and adolescence. Key developmental models of psychosis, including the psychosis-proneness-persistence-impairment model (Linscott & van Os, 2013; Os, Linscott, Myin-Germeys, Delespaul, & Krabbendam, 2009) and the developmental risk . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 4 factor model (Howes & Murray, 2014; Murray, Bhavsar, Tripoli, & Howes, 2017), are based on evidence that adverse experiences that impair typical neurodevelopment can maintain normally transient sub-threshold symptoms of psychosis during adolescence and increase the risk of later transition to psychotic disorders (Rubio, Sanjuan, Florez-Salamanca, & Cuesta, 2012; Trotta, Murray, & Fisher, 2015). It has been suggested that paediatric TBI could be one such neurodevelopmental risk factor (AbdelMalik, Husted, Chow, & Bassett, 2003), but this has never been subjected to systematic review and meta-analysis. Although Molloy et al. (2011) included a subgroup analysis on paediatric TBI cases in their meta-analysis, only three studies were available at the time, indicating a clear need for a more systematic analysis of this issue as new studies have emerged. Consequently, we conducted a pre-registered systematic review and meta-analysis to determine the association between paediatric TBI and later psychotic disorders or symptoms of psychosis. To the best of our knowledge, this is the first meta-analysis to examine paediatric TBI as a potential risk factor for psychotic disorders or symptoms. Methods The present systematic review and meta-analysis was undertaken and reported in compliance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines (Page et al., 2021). Eligibility criteria Participants We included studies that recruited participants of any age or gender with a diagnosis of paediatric traumatic brain injury. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 5 Exposures Paediatric traumatic brain injury was defined as an onset of TBI before adulthood (i.e., < 18 years old). Paediatric TBI could be determined by the age of the study population (e.g. children or adolescents with TBIs) or the time of onset of TBI (e.g. adults with a history of paediatric TBI). We included participants with a diagnosis of paediatric TBI based on validated screening tools, structured clinical interviews, medical records reviews, or clinical diagnosis. TBIs with severity ranging from mild (including concussion) to severe were included. For exclusion, we did not select studies where the occurrence of paediatric TBI could not be determined, and when psychotic disorders or symptoms were not measured. In addition, we did not include studies when exposure to TBI could not be differentiated from other non-TBI conditions within a single group. Comparators Studies with and without comparison groups were included, with no exclusion criteria applied. Outcomes The main outcome of interest was presence of psychotic disorders or psychotic symptoms based on validated screening tools, psychometric measures, structured clinical interviews, medical records reviews, or clinical diagnosis. Psychotic disorders included schizophrenia and related disorders, whilst psychotic symptoms included psychosis-risk syndromes, psychotic symptoms and psychotic-like experiences. We only included studies where the onset of the psychotic disorder/symptoms occurred after the TBI. We excluded studies reporting only broad neuropsychiatric outcomes (such as behavioural difficulties) without any specific assessment of psychosis. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 6 Types of studies included We included all peer-reviewed primary studies published in English with no date restrictions. The following types of design were included: randomised or non-randomised controlled trials, retrospective or prospective cohort studies, and case-control studies (including nested case-control and family studies). We excluded meta-analyses, systematic reviews, literature reviews, case reports, case series, qualitative studies, opinion pieces, editorials, comments, newsletters, book chapters, and congress papers. Information sources and search strategy The databases of PsycINFO (Ovid) (from 1806 onwards) and MEDLINE (Ovid) (from 1946 onwards) were searched based on the strategy outlined in Table 1 (see Appendix S1 for full search strategy), with the search carried out independently by two reviewers (KCY, GR). Studies were screened according to the above criteria. Prior to the final analysis searches were re-run on 1st December 2022 to identify any further studies that could be included in the review. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 7 Table 1. Search strategy Main term 
Search term with operator (PsycINFO) 
Search term with operator 
(MEDLINE) Traumatic 
brain injury (brain injuries/ OR traumatic brain 
injury/ OR brain concussion/) OR (TBI 
OR traumatic brain injur* OR brain 
injur* OR head injur* OR cerebral 
trauma OR craniocerebral injur* OR 
concussion* OR skull fracture*).ab,id,ti (brain injuries/ OR brain injuries, 
traumatic/ OR brain concussion/) OR 
(TBI OR traumatic brain injur* OR 
brain injur* OR head injur* OR 
cerebral trauma OR craniocerebral 
injur* OR concussion* OR skull 
fracture*).ab,kw,ti Psychotic 
disorders & 
psychotic 
symptoms (psychosis/ OR schizophrenia/) OR 
(psychosis OR psychotic OR psychotic 
disorder* OR psychotic exp* OR 
psychotic?like exp* OR schizophreni* 
OR delusional disorder* OR delusion* 
OR hallucinat* OR psychiatric illness* 
OR psychiatric disorder*).ab,id,ti (psychotic disorders/ OR 
schizophrenia/) OR (psychosis OR 
psychotic OR psychotic disorder* OR 
psychotic exp* OR psychotic?like 
exp* OR schizophreni* OR delusional 
disorder* OR delusion* OR 
hallucinat* OR psychiatric illness* OR 
psychiatric disorder*).ab,kw,ti Child 
(childhood birth 12 yrs OR preschool 
age 2 5 yrs OR school age 6 12 yrs 
OR adolescence 13 17 yrs).ag OR 
(infan* OR baby* OR babies OR 
toddler* OR preschool* OR child* OR 
pediat* OR paediat* OR prepubescen* 
OR prepuberty* OR puberty OR 
pubescen* OR teen* OR young* OR 
youth* OR minors* OR underag* OR 
juvenile* OR preadolesc* OR 
adolesc*).ab,id,ti (infant/ OR child, preschool/ OR 
child/ OR adolescent/) OR (infan* OR 
baby* OR babies OR toddler* OR 
preschool* OR child* OR pediat* OR 
paediat* OR prepubescen* OR 
prepuberty* OR puberty OR 
pubescen* OR teen* OR young* OR 
youth* OR minors* OR underag* OR 
juvenile* OR preadolesc* OR 
adolesc*).ab,kw,ti Note. ab = abstract; ag = age group; id = key concepts; kw = keyword heading; ti = title Study selection process Following removal of duplicates, two reviewers (KCY, GR) independently screened the titles and abstracts of all the records retrieved. A third reviewer (VB) was consulted when a consensus could not be reached. Two reviewers (KCY, GR) independently screened the full-text reports based on the above eligibility criteria, and processes of discussion between the two reviewers and consultation with the third reviewer (VB), in the case of disagreement, were held. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 8 Data extraction process A data extraction excel sheet was developed by one of the reviewers (KCY). Two reviewers (KCY, GR) independently extracted study characteristics and outcomes from all the included studies, and data were compared. A third reviewer (VB) was consulted when a consensus could not be reached. Data items Outcomes The main outcome was presence of a psychotic disorder or psychotic symptoms including schizophrenia, psychosis, hallucination, paranoia, psychosis-risk syndromes, and psychotic-like experiences. Diagnoses of schizophrenia, psychosis, hallucination, and paranoia based on the International Statistical Classification of Diseases and Related Health Problems (ICD), the Diagnostic and Statistical Manual of Mental Disorders (DSM), or Feighner et al. (1972) criteria were used. We also extracted sub-threshold symptoms of psychosis including psychosis-risk syndromes (McGlashan, Walsh, & Woods, 2010) and psychotic-like experiences (PLEs) (Lee et al., 2016). For methods of outcome measurement, validated screening tools and psychometric measures (including Prodromal Psychosis Questionnaire – Brief Child Version [PQ-BC] by Karcher et al. (2018)), structured clinical interviews, medical records reviews, and clinical diagnosis were included. Regarding the onset of psychotic disorders or symptoms, anytime time point was eligible (i.e. childhood, adolescence, or adulthood) provided the onset was after paediatric TBI. Regarding the major outcome data, we primarily extracted the number of participants experiencing psychotic disorders or symptoms after paediatric TBI. When studies used several methods for reporting the relevant data, we followed a priori defined rules of decision to select corresponding data. (i) When both the raw number of participants experiencing . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 9 psychotic disorders or symptoms and the calculated statistics (e.g., incidence rate ratios [IRRs]; odds ratio [ORs]) were available, we extracted the raw number. (ii) When descriptive statistics of interval measures of psychotic disorders or symptoms and the calculated statistics (e.g., p values or effect sizes) were available, we extracted the descriptive statistics. (iii) When both non-imputed and imputed data were reported, we chose the imputed. (iv) Lastly, we extracted the set of raw number based on primary analysis of the original study. Where the required data had not been published (three studies: Lopez et al., 2022; Orlovska et al., 2014; Timonen et al., 2002), authors were contacted for the required information (e.g., asking for total number of participants in the exposure group of paediatric TBI. Two authors responded but only one (Lopez et al., 2022) could provide the required raw data. The remaining two studies were only included in narrative synthesis but not meta- analysis. Exposures We included all TBIs with severity ranging from mild (including concussion) to severe. For methods of measurement, validated screening tools (including the Ohio State University TBI Identification Method [OSU TBI-ID]; Corrigan & Bogner, 2007), structured clinical interviews, medical records reviews, and clinical diagnosis were included. Regarding the major exposure data, we primarily extracted the number of participants experiencing TBI. Study characteristics For the characteristics of included studies, apart from the above exposure and outcome data items, we also extracted the (i) year and location of the study, (ii) study design, and (iii) participant characteristics (in the exposure and control groups, if any). . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 10 Quality assessment Two reviewers (KCY & GR) independently assessed the quality of included studies using Kmet et al.'s (2004) quality assessment scale. This consisted of a 14-item checklist on a 3-point scale (0 = criteria not met; 1 = partially met; 2 = fully met) generating a summary score (total sum / total possible sum) ranging from 0 to 100, to categorise the low (≤ 54), moderate (55–74), and high (≥ 75) quality of evidence. The areas of assessment included evaluation of appropriateness of research objectives, study design, sampling methods, recruitment of participants, adoption of measures, sample size, statistical analyses, estimate of variance, control for confounders, results reported, and conclusion drawn. All disagreements were resolved by consensus. Synthesis methods We estimated the meta-analytic odds ratio (OR) with 95% confidence interval (CI) of psychotic disorders or symptoms associated with preceding paediatric TBI among the included studies using the R package ‘meta’ (Balduzzi, Rücker, & Schwarzer, 2019). We computed the I² statistic to measure heterogeneity among included studies, and the levels of low, moderate, and high heterogeneity were assigned to I² values of 25%, 50%, and 75% respectively (Higgins et al., 2003). We expected a moderate-to-high I² value due to methodological heterogeneity, and subsequently we opted to use a random-effects model to estimate pooled estimates using the Paule–Mandel estimator (Paule & Mandel, 1982) given evidence for its lower risk of bias compared to other methods (Langan, Higgins, & Simmonds, 2017). We used a funnel plot to test for evidence of publication bias and Egger’s test was planned to provide a statistical test of funnel plot asymmetry (Ioannidis & Trikalinos, 2007). Subgroup analyses based on (i) study location, (ii) study design (i.e., case-control study versus cohort study), (iii) type of outcome being measured (i.e. psychotic disorders versus . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 11 symptoms or sub-threshold symptoms of psychosis), (iv) type of outcome measurement (i.e. clinical diagnosis vs validated/structured method), and (v) time of onset of the outcomes (i.e. childhood/adolescence versus adulthood) were conducted. We followed the suggested guidelines reporting any detections of statistically significant subgroup differences (Richardson, Garner, & Donegan, 2019). Afterwards, we conducted Viechtbauer and Cheung's (2010)’s outlier and influential study diagnostics and a leave-one-out sensitivity analysis to assess the presence of any overly-influential studies in estimating the pooled effect. We also completed a meta-regression to estimate whether study quality was related to study outcome. If the meta-regression was statistically significant, a sensitivity analysis was performed to assess whether the pooled association remained robust after removing studies of low quality. All analyses were conducted with R (version 4.2.1; R Core Team, 2020) and were conducted on a Linux x86_64 platform. All R code and data for the analyses are available online in the following archive: https://github.com/vaughanbell/pTBI_psychosis_meta- analysis For any studies that did not yield meta-analysed results, we planned to conduct a narrative synthesis to assess how the additional studies might affect the interpretation of the overall findings, using ESRC guidelines (Popay et al., 2006). Results Study selection A total of 850 records resulted from searching the PsycINFO (n = 365) and MEDLINE (n = 485) databases. After removing duplicates by Ovid’s automatic de- duplication feature, 688 records remained. Seventy records were eligible for full-text . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 12 screening, of these 60 were excluded. A total of 10 studies were included in this review. See figure 1 for PRISMA 2020 flow diagram. Figure 1. PRISMA 2020 flow diagram for literature search Records identified from: 
PsycINFO database 
(n = 365) 
MEDLINE database 
(n = 485) 
Registers  
(n = 0) Records removed before screening: 
Duplicate records removed 
(n = 162) 
Records marked as ineligible by 
automation tools (n = 0) 
Records removed for other 
reasons (n = 0) Records screened 
(n = 688) Records excluded 
(n = 618) Reports sought for retrieval 
(n = 70) Reports not retrieved 
(n = 0) Reports assessed for eligibility 
(n = 70) 
Reports excluded (n = 60): 
26: neither psychotic disorders 
nor psychotic symptoms 
20: TBI not specified in childhood 
8: review papers 
2: case reports or case series 
1: TBI or neurological disorders 
1: early brain trauma not TBI 
1: same dataset as Orlovska et 
al. (2014)  
1: congress paper Studies included in review 
(n = 10) 
Reports of included studies 
(n = 10) Identification of studies via databases and registers Ide
nti
fic
ati
on Sc
re
en
in
g Inc
lu
de
d . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 13 Study characteristics Among the 10 included studies, five adopted case-control designs (AbdelMalik et al., 2003; Deighton et al., 2016; Harrison et al., 2006; Helgeland & Torgersen, 2005; Wilcox & Nasrallah, 1987) and five adopted cohort designs (Ledoux et al., 2022; Lopez et al., 2022; Massagli et al., 2004; Orlovska et al., 2014; Timonen et al., 2002). Four studies were carried out in the United States (Deighton et al., 2016; Lopez et al., 2022; Massagli et al., 2004; Wilcox & Nasrallah, 1987), whilst the remaining six studies were undertaken in other places including Canada (AbdelMalik et al., 2003; Ledoux et al., 2022), Denmark (Orlovska et al., 2014), Finland (Timonen et al., 2002), Norway (Helgeland & Torgersen, 2005), and Sweden (Harrison et al., 2006). Five studies measured schizophrenia as an outcome (AbdelMalik et al., 2003; Harrison et al., 2006; Helgeland & Torgersen, 2005; Timonen et al., 2002; Wilcox & Nasrallah, 1987), four studies measured psychosis (Harrison et al., 2006; Ledoux et al., 2022; Massagli et al., 2004; Orlovska et al., 2014), and two studies investigated sub-threshold symptoms of psychosis (Deighton et al., 2016; Lopez et al., 2022). For the method of outcome measurement, six studies adopted clinical diagnosis (Harrison et al., 2006; Ledoux et al., 2022; Massagli et al., 2004; Orlovska et al., 2014; Timonen et al., 2002; Wilcox & Nasrallah, 1987), whilst the remaining four studies adopted validated psychometric measures or structured clinical interviews (AbdelMalik et al., 2003; Deighton et al., 2016; Helgeland & Torgersen, 2005; Lopez et al., 2022). Finally, in terms of the window of interest regarding the onset of a psychotic disorder or psychotic symptom, six studies reported psychotic disorders or symptoms in adulthood (AbdelMalik et al., 2003; Deighton et al., 2016; Harrison et al., 2006; Orlovska et al., 2014; Timonen et al., 2002; Wilcox & Nasrallah, 1987), whilst the remaining four studies reported childhood and adolescence (Helgeland & Torgersen, 2005; Ledoux et al., 2022; Lopez et al., 2022; Massagli et al., 2004). Detailed characteristics of all the included primary studies are shown in supplementary Table S1. A summary of included . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 14 study characteristics are presented in Table 2. Details of the quality assessment ratings are reported in supplementary Table S2, with seven studies rated as demonstrating high quality of evidence, one moderate, and two low. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 15 Table 2. Comparison data for probability of psychotic disorders or symptoms following pTBI First author, year 
Exposure n/Na 
Control n/Nb 
Location 
Design 
Outcome 
Outcome measure 
Time of onset (outcome) AbdelMalik, 2003c 
16/28 
51/141 
Canada 
Case-control – family Schizophrenia 
SCID-I 
Adulthood Deighton, 2016d 
232/287 
515/738 
United States 
Case-control 
Sub-threshold symptoms of psychosis 
SIPS 
Adulthood Harrison, 2006e 
18/455 
730/15,253 
Sweden 
Nested case-control 
Schizophrenia 
Clinical diagnosis 
Adulthood Helgeland, 2005f 
4/26 
5/115 
Norway 
Case-control 
Schizophrenia 
SCID-I 
Childhood/adolescence Ledoux, 2022g 
1,058/152,321 
1,705/296,482 
Canada 
Retrospective cohort 
Psychosis 
Clinical diagnosis 
Childhood/adolescence Lopez, 2022h 
45/128 
3,279/11,419 
United States 
Prospective cohort 
Sub-threshold symptoms of psychosis 
PQ-BC 
Childhood/adolescence Massagli, 2004i 
7/489 
7/1,470 
United States 
Prospective cohort 
Psychosis 
Clinical diagnosis 
Childhood/adolescence Orlovska, 2014j 
802/NR 
9,805/NR 
Denmark 
Prospective cohort 
Psychosis 
Clinical diagnosis 
Adulthood Timonen, 2002 
NR/256 
NR/10,678 
Finland 
Prospective cohort 
Schizophrenia 
Clinical diagnosis 
Adulthood Wilcox, 1987k 
22/23 
178/311 
United States 
Case-control 
Schizophrenia 
Clinical diagnosis 
Adulthood Note. NR = not reported; PQ-BC = Prodromal Questionnaire –Brief Child Version; pTBI = paediatric traumatic brain injury; SCID-I = Structured Clinical Interview for DSM Axis I Disorders; SIPS = Structured Interview for Psychosis-risk Syndromes aExposure n/N = (number of participants in pTBI exposure group having psychotic disorders or symptoms)/(number of participants in pTBI exposure group) bControl n/N = (number of participants in non-pTBI control group having psychotic disorders or symptoms)/(number of participants in non-pTBI control group) cTBI in childhood (≤ 10 years old) was chosen over throughout adolescence (≤ 17 years old) due to primary analysis of the original study . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 16 dComparison between clinical high risk (CHR) of psychosis and healthy controls (HC) was chosen due to primary analysis of the original study eSchizophrenia was chosen over non-affective psychosis due to more precise measurement of psychotic disorder fBoth concussion and head traumas were chosen and aggregated gRaw data on number of participants in relation to psychosis reported in the supplemental materials were used hData provided by the original author i3-year follow-up was chosen due to primary analysis of the original study jHospital contacts for head injury from 0-15 years old were chosen kSurgical control was chosen due to primary analysis of the original study . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 17 Synthesis of results Overall pooled analysis Among the 10 included studies, raw data from two were either not published or not provided by the original authors after contacts and were therefore excluded from the meta- analysis. Based on eight studies, with a pooled sample size of 479,686 (153,757 in the pTBI group; 325,929 in the control group), there was an overall significant positive association between exposure to paediatric TBI and outcomes of psychotic disorders or symptoms (pooled odds ratio [OR] = 1.80, 95% CI [1.11, 2.95]) with moderate between-study heterogeneity (I2 = 69%, τ2 = 0.35, p < 0.01). Figure 2 shows the comparison data and forest plot of the corresponding analysis. Figure 2. Comparison data and forest plot of odds ratio meta-analysis for psychotic disorders or symptoms Subgroup analyses Subgroup analyses based on study location (p = 0.34), design (p = 0.30), psychotic disorder vs subthreshold symptoms of psychosis (p = 0.48), measurement type (p = 0.77), time of onset (that psychotic disorder/symptoms emerged) (p = 0.62) were all non-significant, suggesting that these variables did not modify the effect of paediatric TBI on the probability . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 18 of psychotic disorders/symptoms. Forest plots of all the above subgroup analyses are reported as Figures S1–S5 in the supplementary material. In the subgroup analysis comparing psychotic disorder with subthreshold symptoms of psychosis, both remained reliably associated with pTBI (psychotic disorder OR = 2.11, 95% CI [1.01, 4.37]; subthreshold symptoms of psychosis OR = 1.58, 95% CI [1.17, 2.13]) Robustness and sensitivity analyses For the assessment of publication bias, visual inspection of the funnel plot (Figure 3) appeared to exhibit asymmetry. Egger’s test was completed (p = 0.052) although was likely under-powered given 10 studies are considered the minimum for a reliable assessment of publication bias (Ioannidis & Trikalinos, 2007). Figure 3. Funnel plot of standard error by odds ratio in meta-analysis No studies were identified as outliers but one study (Wilcox & Nasrallah, 1987) was identified as excessively influential using Viechtbauer and Cheung's (2010) outlier and influential diagnostics. A leave-one-out sensitivity analysis showed that removing Wilcox & Nasrallah (1987) reduced the pooled estimate although the association between pTBI and . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 19 psychosis outcomes remained reliable (OR = 1.52 95% CI [1.08; 2.14]). The removal of one study altered the estimate to the non-significant range, namely Massagli et al. (2004) revised estimate OR = 1.74, 95% CI [0.9997, 3.04]). A meta-regression analysis indicated that the quality of evidence summary score predicted the association between pTBI and psychotic disorders/symptoms, albeit weakly (random-effects estimate = -0.037, 95% CI [-0.06, -0.01], p = 0.003). Consequently, we completed a sensitivity analysis removing studies with evidence rated as low quality and recalculating the pooled estimate. The revised pooled estimate (see Figure 4) remained significant with narrower confidence intervals suggesting a more accurate estimate (OR = 1.43, 95% CI [1.04, 1.98]) and slightly reduced heterogeneity (I2 = 64%, τ2 = 0.10, p = 0.02). Figure 4. Comparison data and forest plot of odds ratio meta-analysis for psychotic disorders or symptoms – sensitivity analysis by the removal of studies with low quality of evidence Narrative synthesis including additional studies Two cohort studies were not included in the meta-analysis due to insufficient data, namely Orlovska et al. (2014) and Timonen et al. (2002). Orlovska et al. reported that, compared with individuals without hospital contact for head injury, those exposed to head injury between ages 0 and 5 years had higher rates of schizophrenia spectrum disorders . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 20 (adjusted incident rate ratio [aIRR] = 1.35, 95% CI [1.18, 1.54]). In addition, differing effects of head injury at 6–10 years (aIRR = 1.33, 95% CI [1.16, 1.50]) and 11–15 years (aIRR = 1.86, 95% CI [1.66, 2.07]) were observed. In Timonen et al, although the original analysis focussed on the association between preceding paediatric TBI and the broad outcomes of mental disorders, Molloy et al. (2011) contacted the original authors and reported no association between paediatric TBI and the subsequent development of schizophrenia (odds ratio [OR] = 1.1, 95% CI [0.41, 2.96]) although the wide confidence intervals indicate that the estimate would carry less weight in estimating an overall effect. Discussion We conducted a systematic review and meta-analysis to estimate the association between paediatric TBI and psychosis, including both frank psychotic disorders and psychotic symptoms. Based on a pooled sample size of 479,686, it was found that pTBI was associated with an increased probability of psychotic disorders and symptoms, with moderate between-study heterogeneity. Regarding the robustness of findings, the estimated association passed robustness tests for study quality, outliers, and excessively influential studies, although the removal of one would have reduced confidence in a reliable association in a leave-one-out sensitivity analysis. This reflects the fact that the lower bound of the confidence interval for the pooled estimate was only marginally above one and therefore confidence in the reliability of this estimate must be taken cautiously. Two studies were identified in the systematic review which could not be included in the meta-analysis, and these studies reporting conflicting results. However, given the characteristics of these additional studies, we consider that including them would have moderately increased our confidence in an association between pTBI and psychosis. Consequently, we conclude that this analysis provides additional evidence for an association between pTBI and psychosis. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 21 However, concerns remain about the long-run reliability of this estimate remain and new studies will be crucial in deciding this issue. In relating the above findings to the field, our results raise the feasibility of potential causal associations between pTBI and psychosis. Developmental models of psychosis suggest that pTBI could be a plausible risk factor for psychosis, given that the full spectrum of pTBI (from mild pTBI to severe brain injury) has established effects on neurodevelopment (Emery et al., 2016; Goh et al., 2021), and events that have an adverse impact on neurodevelopment are known risk factors for psychosis (Howes & Murray, 2014; Murray et al., 2017). Our meta-analytic results seem to suggest the role of pTBI as a risk factor for psychosis. However, reverse causality or shared risk factor pathways are also possible. Brain injuries have been hypothesised to be more common in young people who have a higher risk for psychosis, as they may already show subtle premorbid difficulties such as motor coordination leading to a higher risk for accidental injuries (AbdelMalik et al., 2003; David & Prince, 2005). Furthermore, psychotic symptoms, including in people without frank psychosis, are associated with higher rates of early-life bullying (Catone et al., 2015; Valmaggia et al., 2015), suggesting a possible reverse or reciprocal association between psychotic spectrum phenomena and acquired brain injury through victimisation violence. A well-designed prospective cohort study would be needed to reliably identify relationships between pTBI, psychotic symptoms, and any potential confounders and/or mediators. Nevertheless, given the concordance between the findings reported here and developmental models of psychosis (Howes & Murray, 2014; Murray et al., 2017), this may suggest that history of pTBI may be a useful addition when taking a history of patients with psychosis and that public health that prevent pTBI may have longer-term benefits for lifetime mental health. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 22 A strength of this study is the use of systematic procedures to comprehensively search for eligible studies. We also included sensitivity analyses to ensure robustness of the findings. In addition, we pre-registered the review to reduce the risk of bias. Moreover, the meta- analysis is available online as a computational notebook with an open dataset, to enhance openness and reproducibility. However, we note several limitations of this study. The first is that the included studies are heterogeneous in terms of their measured outcomes design (case control vs cohort), outcome (symptoms vs disorder), outcome measure (clinical diagnosis vs validated measure) and life-stage of measured psychosis outcome (adulthood vs childhood/adolescence). Our subgroup analyses found no evidence for difference of association between subgroups. We note the potential for low statistical power to make identifying associations within subgroups difficult, given than subgroups typically included 3-4 studies. However, the heterogeneity of studies reflects the fact that many were not primarily designed to assess the association with pTBI and psychosis spectrum phenomena, and more focused and better design studies are clearly needed. Inspection of the funnel plot indicated a potential for publication bias which could have reduced the accuracy or the direction of the estimate. In addition, studies either did not report severity of brain injury, or did not distinguish between severity, meaning it was not possible to examine whether there was a ‘dose-response’ relationship between TBI and later psychosis, a potentially important consideration when examining evidence for causality. We also note that we solely included studies published in English, and listed in primarily English language databases, potentially missing some potentially useful evidence. Based on the above discussions, it is recommended that future research focus on specifically assessing the association between pTBI and psychosis spectrum phenomena. In addition, to rule out the possible reverse association, a well-designed prospective cohort study . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 23 would be needed to reliably identify relationships between pTBI, psychotic symptoms, and any potential confounders and/or mediators. Additionally, future studies specifying the type of TBI (e.g. due to accidents or other sources), and location and type of injury, would enhance our understanding of the presence/absence of an aetiological relationship between pTBI and psychosis. Lastly, future reviews should consider including non-English language databases. In conclusion, our systematic review and meta-analysis reports evidence for a positive association between paediatric TBI and subsequent outcomes of psychotic disorders or symptoms but with caveats regarding our confidence in the long-term reliability of this association as new evidence emerges. . 
CC-BY-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted July 18, 2023. 
; 
https://doi.org/10.1101/2023.02.17.23286118
doi: 
medRxiv preprint 24",1
"Background:  Substance use disorders (SUDs) represent a major public health risk. Yet, our 
understanding of the mechanisms that maintain these disorders remains incomplete. In a recent 
computational modeling study, we found initial evidence that SUDs are associated with slower 
learning rates from negative outcomes and less value-sensitive choice (low “action precision”), 
which could help explain continued substance use despite harmful consequences. Methods: Here we aimed to replicate and extend these results in a pre-registered study with a 
new sample of 168 individuals with SUDs and 99 healthy comparisons (HCs). We performed the 
same computational modeling and group comparisons as in our prior report (doi: 
10.1016/j.drugalcdep.2020.108208) to confirm previously observed effects. After completing all 
pre-registered replication analyses, we then combined the previous and current datasets (N = 
468) to assess whether differences were transdiagnostic or driven by specific disorders. Results: Replicating prior results, SUDs showed slower learning rates for negative outcomes in 
both Bayesian and frequentist analyses (𝜂2=.02). Previously observed differences in action 
precision were not confirmed. Logistic regressions including all computational parameters as 
predictors in the combined datasets could differentiate several specific disorders from HCs, but 
could not differentiate most disorders from each other. Conclusions: These results provide robust evidence that individuals with SUDs have more 
difficulty adjusting behavior in the face of negative outcomes than HCs. They also suggest this 
effect is common across several different SUDs. Future research should examine its neural basis 
and whether learning rates could represent a new treatment target or moderator of treatment 
outcome. Keywords: substance use disorder; learning rate; active inference; decision-making; 
computational psychiatry; explore-exploit dilemma; reinforcement learning; transdiagnostic; 
replication; prediction. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 3 1. INTRODUCTION Substance use disorders (SUDs) have devastating mental and physical health consequences (1-5), and lead to large societal and financial burden (6). They also show high co-
morbidity with depression and anxiety disorders, which can be both a cause and consequence of 
substance use (1-3). Yet, our understanding of the mechanisms that lead to substance use, and of 
those that moderate recovery and abstinence, remains poor. Given current relapse rates with 
existing treatments (7), a better understanding of these mechanisms may be crucial for 
identifying effective clinical targets; and they could also offer important insights regarding 
etiology and prevention. Computational modeling offers a promising approach for characterizing these mechanisms in SUDs (8, 9), as it can disentangle processes underlying perception, learning, and 
decision-making that may contribute to these disorders (9, 10). A growing body of work has now 
applied this approach – highlighting several computational processes that differ from healthy 
participants and that may relate to vulnerability, severity, and/or treatment outcomes (11, 12). 
Much of this work pertains to deficits in “model-based” (i.e., goal-directed, prospective) control, 
which are thought to amplify habitual and impulsive choice (e.g., for recent reviews, see (12, 
13)). Other studies have also highlighted deficits in computational mechanisms subserving 
perception (particularly with respect to internal bodily states; i.e., interoception (14, 15)) and 
reinforcement learning (e.g., see (16-18)). Thus, this approach has already been helpful in 
elucidating a number of mechanisms that may underlie SUDs. Building on this body of work, we recently applied a computational approach to examine the way individuals with SUDs balance information-seeking and reward-seeking (19). Relative 
to healthy comparisons, modeling revealed slower learning rates for negative outcomes across 
multiple SUDs, as well as faster learning rates for positive outcomes and less precise (i.e., less 
value-sensitive) action selection. This pattern of results was also moderately stable when 
assessed in the same participants one year later (20). These findings suggested that substance use 
might persist despite painful life consequences due to slower learning from these negative 
outcomes. Returns to drug use after improvements in treatment could also be caused by 
imprecise action selection mechanisms (i.e., switching away from actions with positive 
outcomes). In the current pre-registered study (https://osf.io/u3sbg), we assess the reliability of 
these findings in a new, independent sample with similar characteristics to those in the original 
study. The same task and computational analyses were applied, and we report which results do 
and do not replicate. We subsequently extend these results by combining the two datasets, 
providing power to assess whether effects are driven by specific SUDs or co-morbid affective 
disorders or whether they represent a transdiagnostic marker across SUDs. 
 
2. METHODS 
 
2.1 Participants Participants were sampled from the confirmatory dataset (N=550) of the larger Tulsa 1000 (T1000) study (21). The T1000 project recruited a longitudinal community sample with 
dimensional measures designed around the NIMH Research Domain Criteria framework. The . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 4 sample in total consists of 1050 individuals, ages 18-55, recruited through radio, electronic 
media, word-of-mouth, and treatment center referrals. Participants were screened through 
various dimensional psychopathology measures, including Patient Health Questionnaire (PHQ-9 
(22)) ≥ 10, Overall Anxiety Severity and Impairment Scale (OASIS (23)) >= 8, and/or Drug 
Abuse Screening Assessment (DAST-10 (24)) ≥ 3. Healthy comparisons (HCs) did not show 
elevated symptoms on any of the previous measures. Exclusion criteria for participants were: (i) 
positive tests for drug use, (ii) meeting criteria for psychotic, bipolar, or obsessive-compulsive 
disorders, (iii) a history of moderate-to-severe traumatic brain injury, neurological disorders, or 
severe or unstable medical conditions, (iv) active suicidal intent or plan, or (v) change in 
medication dose within 6 weeks. Full inclusion/exclusion criteria are reported in (21). The study 
was approved by the Western Institutional Review Board. All participants provided written 
informed consent prior to completion of the study protocol, in accordance with the Declaration 
of Helsinki, and were compensated for participation. ClinicalTrials.gov identifier: 
#NCT02450240.  
 
Participants were divided into two groups: participants with one or more SUDs (with or without 
co-morbid anxiety/depression; N = 168) and participants without any mental health diagnosis 
(HCs; N = 99). Diagnoses were determined according to DSM-IV or DSM-5 criteria using the 
Mini International Neuropsychiatric Inventory (MINI) (25) and confirmed through clinical 
conferences with a board-certified psychiatrist. Table 1 provides summary statistics for 
demographic and clinical measures, as well as two-sample t-tests assessing group differences 
(and corresponding Bayes factors [BFs], calculated using the BayesFactor package in R (26)). 
Table 2 provides a summary of specific diagnoses within the SUDs group. Table S1.1 in 
Supplementary Materials 1 compares the demographic characteristics and clinical measures in 
this sample to those in our previous study (19). The only difference observed was that SUDs in 
the current sample had lower scores than the previous sample on the Wide Range Achievement 
Test reading score (WRAT) (27), a measure of premorbid intelligence (t(253) = 2.27, p = 0.02). 
This measure was included because of expected differences in baseline intellectual functioning 
relative to HCs. 
 
 
 
Table 1: Demographic and Clinical Characteristics HCs 
SUDs 
Test 
p-value 
BF* N = 
99 
168 Age 
32.29 (11.08) 
33.75 (8.34) 
t(265) = -1.22 
d = 0.15 0.22 
0.28 Sex (Male) 
39 (39.4%) 
63 (37.5%) 
𝜒2(1) = 0.031 
v = 0.01 0.86 
0.30 DAST 
0.1 (0.33) 
7.9 (1.81) 
t(265) = -42.33 
d = 5.2 < 0.001 
> 1000 PHQ 
1.22 (1.94) 
7.89 (6.58) 
t(265) = -9.84 
d = 1.21 < 0.001 
> 1000 OASIS 
1.19 (1.91) 
6.36 (4.51) 
t(265) = -10.84 
d = 1.33 < 0.001 
> 1000 WRAT** 
62.95 (5.33) 
56.67 (6.74) 
t(205) = 7.21 
d = 1.01 < 0.001 
> 1000 . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 5 Psychiatric 
Medication 0 (0%) 
95 (56.5%) 
NA 
NA 
NA Legend: DAST = Drug Abuse Screening Test. PHQ = Patient Health Questionnaire. OASIS = Overall Anxiety 
Severity and Impairment Scale. WRAT = Wide Range Achievement Test reading score. Effect sizes: d = Cohen’s d; 
v = Cramer’s v. 
*For unfamiliar readers, Bayes factors (BFs) here indicate the ratio of the probability of the data under a model with 
vs. without a group difference (e.g., BF = .5 indicates that data are twice as likely without a group difference, while 
BF = 3 indicates that data are three times as likely under a model with a group difference). 
**As mentioned in the text, some participants were missing WRAT scores – consequently, the Ns for this row are 
87 HCs and 120 SUDs. 
 
 
Table 2: Lifetime DSM-IV/DSM-5 Psychiatric Diagnoses within SUDs Note: Two individuals who met the inclusion threshold (DAST > 3) in the exploratory sample had polysubstance 
dependence and MDD. These individuals are reflected in the total counts, but not in specific SUD cell counts. GAD 
= Generalized Anxiety Disorder; MDD = Major Depressive Disorder; PTSD = Post-Traumatic Stress Disorder; 
Halluc. = Hallucinogens; Stimulants = Amphetamines, Methamphetamines, and/or Cocaine. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 6 2.2 Three-Armed Bandit Task Participants completed a standard three-armed bandit task (28). For 20 blocks of 16 sequential choices, they had to learn from trial-and-error which of three options was most likely 
to deliver a win (described in detail in our previous study (19); also see (20)). To maximize 
reward in each block, participants needed to strike a balance between sampling untested options 
(exploration) and capitalizing on the options that appeared most rewarding (exploitation). Figure 
1 shows the task interface and provides additional details. Figure 1: Upper Left: Screenshot of the task interface. In each block (N = 20), there were 16 trials where 
participants could choose the left, middle, or right options (buttons 1, 2, or 3 at the bottom). Each choice 
could lead to either a win (green circle) or a loss (red circle), which would appear above the chosen 
option. Left: A graphical depiction of the task model. Shaded circles denote inferred variables on each 
trial. White circles denote fixed model parameters. Arrows indicate dependencies between variables (see 
Table S1.2 for a detailed explanation). Right: Model equations for learning and choice. Reward 
probabilities were encoded in a matrix 𝐀; i.e., 𝑝(𝑜𝑡 𝑟𝑒𝑤𝑎𝑟𝑑|𝑠𝑡). Approximate beliefs about these reward probabilities, 𝑞(𝐀), were updated within a matrix 𝐚; i.e., 𝑞(𝑜𝑡 𝑟𝑒𝑤𝑎𝑟𝑑|𝑠𝑡). This matrix included a Dirichlet distribution over reward probabilities, the concentration parameters (𝑎𝑖,𝑗) of which were updated after 
each observation according to the specified learning rule (shown in the top-left box, which includes the 
learning rate 𝜂). Policy values were based on expected free energy (𝐺𝜋), which favors policies that are 
expected to maximize both reward (encoded in 𝐶) and information gain (i.e., the magnitude of expected 
change in beliefs about reward probabilities [𝑞(𝐀)] after a new observation). Randomness in action 
selection given approximate posteriors over policies, 𝑞(𝜋), was controlled by an action precision 
parameter 𝛼 (note that 𝜎 indicates a softmax function). All free parameters in the model are shown in red.  
 
 
2.3 Computational Modeling . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 7 A detailed computational model description can be found in Table S1.2. In brief, learning and choice are implemented by a set of inference and decision rules drawn from the active 
inference framework, which is designed to capture information-seeking under uncertainty. A 
detailed description of the mathematical framework can be found in (29, 30). The task model 
includes a set of observations (e.g., wins/losses), the subjective value assigned to those 
observations, choice states (option 1, 2, and 3), the probabilities of each observation under each 
choice state (e.g., the reward probabilities), and policies (actions) controlling transitions between 
choice states on each trial. Free parameters within the model offer mechanistic explanations for individual differences in choice behavior. These parameters are summarized in Table 3 and shown within 
the learning and choice equations in Figure 1. 
 
Table 3: Free Model Parameters Free Parameter Name 
Variable Description Action Precision 
α 
Controls the degree of randomness in behavior – lower values 
indicate that a participant is less likely to exhibit consistent 
choices in the same decision context (i.e., under the same 
expected reward probabilities). This stochasticity is associated 
with a random exploration strategy (information gathering via 
random sampling), but it can also index overarching decision 
uncertainty or insensitivity to the relative value of each option. Reward Sensitivity 
𝑐𝑟 
Indicates the subjective value of a win. Lower values lead to the 
prioritization of information-seeking, promoting goal-based 
(directed) exploration. Learning Rate 
𝜂 
Reflects the magnitude with which beliefs about reward 
probabilities are updated after each observation. This can be 
split into separate learning rates for observed wins and losses, 
reflecting the fact that participants can be more sensitive to wins 
or losses as sources of information in updating beliefs. Information 
Insensitivity 𝑎0 
Encodes the base levels of confidence in beliefs about reward 
probabilities before making initial choices at the start of each 
block. Higher values lead to reduced directed exploration. They 
also effectively slow learning, as observed wins/losses have a 
reduced impact on the normalized belief distribution over 
reward probabilities. We used a standard variational Bayes algorithm (variational Laplace (31)) to find parameter estimates that maximize the probability of participant behavior under the model, while 
also minimizing overfitting with a complexity cost (32). We then performed Bayesian model 
comparison (33, 34) across a set of 10 nested model variants (shown in Table S1.3) to determine 
the best model. The estimates from this best model were then used in subsequent statistical 
analyses. Results of parameter recoverability and model identifiability analyses are reported in previous work (20). Parameters show good recoverability across the region of parameter space 
describing participant behavior, and the best-fit model is identifiable within model comparison. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 8 2.4 Statistical Analysis Procedure 
 
2.4.1 Primary Replication Analyses Statistical analyses replicate those in our previous study (19) and were pre-registered (https://osf.io/u3sbg). We first ran correlations to confirm how model parameter estimates were 
related to descriptive behavioral measures, including number of wins, mean reaction times (RTs) 
(after trimming using iterative Grubbs at a threshold of p < 0.01 (35)), and stay vs. shift behavior 
after wins vs. losses (i.e., whether participants shift to a new option or stay at the current option 
after a win or loss on the prior trial). Our primary analyses then used parametric empirical Bayes (PEB) (36) to test general linear models with each model parameter estimate as the outcome variable, and using age, sex, 
baseline intellectual functioning (WRAT scores), and diagnostic group (SUDs and HCs) as 
predictors. PEB analyses allow incorporation of both posterior parameter means and variances 
when assessing these group-level models, as opposed to using the means alone as point 
estimates. As in our prior study, secondary frequentist analyses (equivalent t-tests and multiple regressions) were also performed to assess group differences in the posterior parameter means. 
Supplementary model comparisons with these regressions, using Bayes factors (BFs) to compare 
the probability of the data under models with different possible combinations of predictors, were 
also performed (see note below Table 4). For comparison to model-based results, these same analyses were also performed on the descriptive behavioral measures. PEB analyses were performed in MATLAB 
(https://matlab.mathworks.com/). All other analyses were performed in R (2018; www.R-
project.org/). Our previous study employed propensity matching (on the basis of age, sex, and WRAT scores; using the optmatch package in R [https://cran.r-
project.org/web/packages/optmatch/index.html]), which we initially planned to do here. 
However, the characteristics of this sample, and particularly the minimal overlap in WRAT 
scores between groups, prohibited our matching algorithm from finding a suitable subsample. 
Consequently, we only report analyses on the full sample using linear models that included age, 
sex, and WRAT as covariates. However, due to issues that arose during data collection, 12 HCs 
and 48 SUDs did not have available WRAT scores. In order to capitalize on the full sample, 
while also assessing possible effects of baseline intellectual functioning, we therefore ran each 
planned analysis once in the full sample (without including WRAT scores) and once in the 
subsample with available WRAT scores (while including this measure as a covariate). Results of 
analyses in the full sample are reported in detail below. Results within the analyses repeated in 
the subsample with WRAT scores are provided in detail within Supplementary Materials 1 
and 2, but we also note important findings here. Given the heterogeneity within our SUDs sample, we also planned to assess potential differences in computational parameters between different SUDs – specifically, differences . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 9 between individuals with opioid use disorders and stimulant use disorders. While there was a 
sufficient number of individuals with stimulant use disorders without co-morbid opioid use 
disorders (N = 109), the low number of individuals with opioid use disorders that did not have 
co-morbid stimulant use disorders (N = 10) was too small to permit this analysis. However, after 
completing all pre-registered replication analyses, we combined the exploratory and 
confirmatory samples to examine disorder-specific effects (see section 2.4.3 and 3.5). 2.4.2 Secondary Analyses We ran correlations within SUDs to examine the relationship between model parameters and dimensional measures of symptom severity. Although not pre-registered, we also used t-tests 
to assess whether model parameters differed in medicated vs. unmedicated individuals in the 
SUDs group. In addition to the 168 individuals with SUDs described above, the T1000 dataset also included 32 participants with an SUD diagnosis that had current symptom levels below the 
DAST ≥ 3 cutoff. These participants were not included in our primary analyses; however, they 
were used in some supplementary analyses described below. For more details about these 
additional participants, and comparison to the main sample, see Table S1.4. 2.4.3 Disorder-Specific Analyses in Combined Samples After completing all pre-registered replication analyses, we combined the exploratory and confirmatory datasets (N = 468) to ask questions about narrower diagnostic groups, which the 
previous or current dataset alone provided limited power to answer. Using logistic regressions, 
we first asked whether model parameters could differentiate HCs from those who had each of the 
specific substance use or affective disorders listed in Table 2. These regression models included 
each of the five parameters as predictors of diagnostic status (i.e., coding HCs = 0 and those with 
the specific disorder in question = 1, removing all other participants). We also examined those 
with no co-morbid SUDs when available sample size allowed, but high levels of co-morbidity 
did not permit this in most cases (i.e., only in stimulant use disorders; N = 55). These were 
considered post-hoc analyses done primarily to provide additional insights regarding whether 
specific overlapping disorders might drive the confirmed difference between HCs and SUDs 
overall. As such, we did not correct p-values for number of regressions; but we note that a 
Bonferroni correction for 11 regressions would entail a threshold of p < .0045. In similar logistic 
regressions, we then asked whether model parameters could differentiate one disorder vs. others 
(i.e., removing HCs and coding those with vs. without the specific disorder in question equal to 1 
and 0, respectively). 2.4.4 Predictive Categorization Finally, to evaluate whether model parameters could predictively classify individuals into disorder groups, we trained logistic regression models (with the 5 parameters as joint predictors) 
on the exploratory sample and then assessed whether the trained models could accurately 
categorize individuals as either HCs or SUDs, both in general and for each specific SUD or . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 10 affective disorder separately (i.e., HCs = 0, Disorder = 1 in each case, removing those without 
that the disorder being predicted). These predictive categorization analyses were performed using 
the glm function in R 
(https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm). Due to the different 
sample sizes per group, training data were also balanced using frequency weights. To assess 
performance, we report receiver operating characteristic curves (ROCs), reflecting the true 
positive rate (sensitivity) vs. false positive rate (1 - specificity) for different categorization 
thresholds, and the associated area under the curve (AUC). AUCs indicate how often a random 
sample will be assigned to the correct group with higher probability.  
 
3. RESULTS 
 
3.1 Model Validation The winning model matched that found in our previous study (including action precision, reward sensitivity, separate learning rates for wins/losses, and insensitivity to information; Table 
S1.3), with a protected exceedance probability of 1.0. The average probability of participant 
actions under the model was .56 (SD = .10). The model assigned the highest probability to 
participant actions on 61% (SD = .10) of trials (note that chance = 33.33%). Direct comparison of parameter values between the previous and current sample is provided in Table S1.5, with detailed results of associated linear models in Table S2.1 and S2.2 
within Supplementary Materials 2. The parameter values in SUDs did not differ significantly 
between samples, nor were these values different between the two HC samples. Relationships between model parameters and descriptive measures also replicated prior results (see Figure 2). Notable examples included: 1) number of wins was positively associated 
with action precision and reward sensitivity; 2) number of lose/shift choices was positively 
correlated with learning rate for losses; 3) stay choices were positively correlated with reward 
sensitivity and insensitivity to information; and 4) RTs were longer in those with slower learning 
rates for losses and faster in those with greater learning rates for wins, reward sensitivity, and 
insensitivity to information. The relationships with RTs are noteworthy, as model parameters are 
not fit to this aspect of behavior (i.e., the model itself does not simulate RTs). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 11 Figure 2: Correlations (and associated Bayes factors) between model parameter values and descriptive 
behavioral measures. 
 
 
3.2 Group comparison Replicating our prior results, PEB models with age and sex as covariates found strong evidence for group differences in learning rate for losses (slower in SUDs; posterior probability 
[pp] = 0.93; see Figure 3). These models also revealed some evidence for differences in reward 
sensitivity (greater in HCs; pp = 0.82). Note, however, that this latter finding was not present in 
our prior study. Including WRAT reading scores (in the subsample for which they were 
available) did not change these results (see Figure S1.1). See the Additional Effects section 
within Supplementary Materials 1 for further effects observed in relation to age, sex, and 
WRAT scores. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 12 Figure 3: The top-left panel depicts results from Parametric Empirical Bayes (PEB) analyses showing the 
posterior means and credible intervals for effects of group (when accounting effects of age and sex). 
Positive values indicate greater values in HCs relative to SUDs. Learning rates are estimated/displayed in 
logit space, while all other parameters are in log space. Results replicate the group difference in learning 
rate for losses found in our prior study. All other panels compare the means and standard errors for each 
parameter in our prior study (exploratory sample; 54 HCs, 147 SUDs) to those in the current study 
(confirmatory sample; 99 HCs, 168 SUDs), when separating parameters by group. Also shown are Bayes 
factors (BFs) evaluating the evidence for differences between the two samples (BF < 1 indicates greater 
evidence for the absence of a difference). BFs indicated that the data were between 2.6 and 7.1 times 
more likely under a model with no difference between samples. For an identical plot restricting analyses 
to participants with available WRAT reading scores (and including these scores as predictors in the PEB 
analyses), see Supplementary Figure S1.1, which shows the same overall pattern of results (and in some 
cases shows stronger consistency between samples; most notably with action precision values). As further assessment of successful replication, we also compared parameter estimates from the exploratory sample in our prior study to those in the present study for each group. We 
further computed BFs from Bayesian t-tests for each parameter/group to assess evidence for 
differences between samples. As shown in Figure 3, these BFs ranged from .14 to .38, indicating 
that the data are between 2.6 and 7.1 times more likely under a model with no difference 
between samples. This provides strong evidence that the new sample successfully replicates the 
findings in our prior study. Analogous results for the subsample with available WRAT scores are 
shown in Figure S1.1. 3.3 Supporting frequentist analyses In complementary frequentist analyses, initial two-sample t-tests for each parameter confirmed the group difference in learning rates for losses (t(265) = 2.26, p = 0.02, d = 0.28), but 
did not support differences in any other parameter. Table 4 reports results of subsequent . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 13 regressions in the full sample analogous to the PEB models above, assessing whether age and sex 
could account for observed group differences. Equivalent Bayesian regressions were also used to 
identify the model (combination of predictors) for which the data provided the most evidence 
and how this compared to models either adding or removing potential group effects (see note 
below Table 4). Here we observed the same expected effects of group on learning rates for 
losses (lower values in SUDs).  
 
 
Table 4: Parameter values (mean and SD) and group comparison Parameter 
HCs 
SUDs Coefficients & t-values p-
value Partial 
𝜼𝟐 BF* N =  
99 
168 Action  
Precision 2.61 
(1.06) 2.38 
(1.48) b = -0.2; 
t(263) = -1.2 0.23 
0.01 
Best Model (M1): Age (BF = 0.47) 
M2: Age + Group (BF = 0.08) Reward 
Sensitivity 4.67 
(1.87) 4.43 
(1.68) b = -0.32; 
t(263) = -
1.47 0.14 
0.01 
Best Model (M1): Age (BF = 
562.51) 
M2: Age + Group (BF = 29.76) Learning 
Rate (Wins) 0.48 
(0.14) 0.5 
(0.15) b = 0.02; 
t(263) = 0.85 0.4 
0.00 
Best Model (M1): Age (BF = 10.03) 
M2: Age + Group (BF = 0.27) Learning 
Rate 
(Losses) 0.41 
(0.14) 0.37 
(0.16) b = -0.04; 
t(263) = -
2.09 0.04 
0.02 
Best Model (M1): Age + Group 
(BF = 13.68) 
M2: Age (BF = 13) Information 
Insensitivity 0.77 
(0.33) 0.83 
(0.29) b = 0.05; 
t(263) = 1.41 0.16 
0.01 
Best Model (M1): Age (BF = 3.23) 
M2: Age + Group (BF = 0.17) Note: Inferential statistics reported here are based on linear models including age and sex as covariates. For 
analogous results in the subsample with available WRAT scores (including WRAT as an additional covariate), see 
Supplementary Table S1.6. 
*These Bayes factors reflect comparisons of evidence for models that did or did not include each possible 
combination of predictors (i.e., main effects of age, sex, group, and/or WRAT) in analogous Bayesian regressions. If 
the winning model included a main effect of group, the reported Bayes factor is relative to an intercept-only model, 
alongside the analogous Bayes factor of a model (M2) without the main effect of group (i.e., indicating the relative 
contribution of group effects in the model). If the winning model did not include group, this winning model and its 
Bayes factor are reported (relative to an intercept-only model), along with the Bayes factor for a model (M2) that 
added an effect of group. Bolded BF values indicate that the winning model included group as a main effect. 
 
 
 
Results of identical analyses on descriptive behavioral measures are reported in Table 5. We found greater wins in HCs than SUDs (p = 0.047; see Table 7 and Figure 3), as found in our 
prior study. We also found fewer win/stay choices in SUDs, which was not observed previously. 
As in our prior study, we also repeated these analyses for the first and second halves of each task 
block separately (i.e., first 7 choices vs. subsequent choices) to assess periods where exploration 
vs. exploitation would be expected to dominate, respectively (see Tables S1.8). This showed that 
the SUDs group only made fewer win/stay choices than HCs in the second half of each game (p 
= .02; and greater win/shifts, p = .04), while HCs only showed greater wins in the first half of 
each game (p = .007). Additional relationships observed between age or sex and these measures 
are reported in Table S2.3 and S2.4 and summarized in the Additional Effects section of 
Supplementary Materials 1. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 14 In most cases, similar results were seen in the subsample with WRAT scores (see Tables S1.6-8). The major exception was that this subsample did find higher action precision in HCs 
than SUDs, similar to our initial study. Evidence for the difference in learning rates for losses 
was also weaker in this subsample, but it was retained within the winning model in model 
comparison (BF = 2.93). Table 5: Descriptive behavioral measures and group comparison Measure 
HCs 
SUDs 
Coefficients 
& t-values p-
value Partial 
𝜼𝟐 BF* N =  
99 
168 Wins 
183.25 
(13.19) 180.44 
(10.65) b = -2.96; 
t(263) = -2.00 0.047 0.01 
Best Model (M1): Group (BF = 
0.77) Mean RT 
0.56 
(0.2) 0.53 
(0.25) b = -0.03; 
t(263) = -1.13 0.26 
0.00 
Best Model (M1): Sex (BF = 0.39) 
M2: Sex + Group (BF = 0.02) Win/Stay 
140.65 
(35.12) 133.51 
(33.08) b = -8.99; 
t(263) = -2.21 0.03 
0.02 
Best Model (M1): Age + Group + 
Sex (BF > 1000) 
M2: Age + Sex (BF > 1000) 
M1/M2: BF = 1.31 Win/Shift 
30.97 
(30.88) 34.79 
(28.6) b = 5.57; 
t(263) = 1.58 0.11 
0.01 
Best Model (M1): Age + Sex (BF > 
1000) 
M2: Age + Sex + Group (BF > 
1000) 
M1/M2: BF = 2.28 Lose/Stay 
47.27 
(27.07) 50.48 
(29.67) b = 2.04; 
t(263) = 0.58 0.57 
0.00 
Best Model (M1): Age (BF = 
565.78) 
M2: Age + Group (BF = 14.67) Lose/Shift 81.11 (28.11) 81.21 
(31.75) b = 1.38; 
t(263) = 0.37 0.71 
0.00 
Best Model (M1): Age (BF = 
478.04) 
M2: Age + Group (BF = 13.73) Note: Inferential statistics reported here are based on linear models including age and sex as covariates. For 
analogous results in the subsample with available WRAT scores (including WRAT as an additional covariate), see 
Table S1.5. For results dividing choices into early and late choices per block, see Table S1.6. 
*See note below Table 6 for interpretation of these Bayes factor analyses. For results in which BFs for both M1 and 
M2 are >1000 relative to an intercept-only model, we also report the BF for M1 relative to M2 for comparative 
interpretability. 
 
3.4 Relationships with symptoms 
 
Relationships between model parameters and symptom measures in SUDs are reported in Figure 
S1.2. No significant relationships were observed, which failed to replicate the relationships 
observed in our previous study between PHQ/OASIS and information insensitivity, and between 
OASIS and reward sensitivity (BFs also provided moderate evidence against these relationships: 
0.18 – 0.53). When comparing the main sample of SUDs to the 32 diagnosed individuals with symptom severity below the cutoff of DAST ≥ 3, we found no differences in parameter values 
(see Table S1.4). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 15 Comparison of those that were vs. were not on medication in the SUDs group revealed no significant differences for any model parameter (ts between -1.32 and .86, ps between .188 and 
.838). 
 
3.5 Analysis of specific use disorders in combined samples 
 
Summary statistics for computational parameters in specific disorder groups within the combined 
exploratory and confirmatory samples (N = 468) are shown in Table S1.9. Full results of the 
logistic regression models differentiating whether participants had each specific disorder relative 
to HCs are shown in Table S1.10. Accounting for other model parameters, these analyses found 
that action precision separately differentiated diagnostic status relative to HCs for those with 
each specific substance use and affective disorder (Wald z between -4.63 and -2.03, ps between < 
.001 and .042), with the exception of hallucinogen use disorder and PTSD. This was also true for 
those with stimulant use disorder without co-morbidities (z = -3.05, p = .002). Learning rates for 
wins separately predicted diagnostic status relative to HCs for those with stimulant use disorder 
(only marginally in those without co-morbidities; p = .079), opioid use disorder, sedative use 
disorder, major depressive disorder, and social anxiety disorder (z between -3.2 and -1.96, ps 
between .001 and .049). Learning rates for losses further predicted diagnostic status relative to 
HCs for those with the majority of specific disorders (including stimulant use disorders without 
co-morbidities; z between -3.57 and -2.08, ps between < .001 and .038), with the exception of 
hallucinogen use disorder, generalized anxiety disorder, panic disorder, or PTSD. Full results of the logistic regression models predicting whether participants in the SUDs group had a specific disorder relative to other disorders are shown in Table S1.11. Accounting 
for other model parameters, these analyses found that greater action precision and faster learning 
rate for wins predicted diagnosis with stimulant use disorders vs. other disorders (z = 2.47 and 
2.23, p = .014 and .026, respectively; and a marginal effect of learning rates for losses: z = 1.79, 
p = .073). Diagnosis with social anxiety disorder was also predicted by slower learning rates for 
wins relative to other disorders (z = -1.97, p = .049; and a marginal effect of learning rates for 
losses: z = -1.71, p = .09). Otherwise, model parameters did not differentiate between specific 
SUDs. Aside from social anxiety and learning rate for wins, they also did not indicate differences 
in SUDs with vs. without affective disorders for any model parameter. The overall variance explained by the 5 parameters in significant logistic regression models ranged from 9% to 16% (based on pseudo-R2; for details, see Table S1.10). Figure 4 
shows parameter values for those with each specific disorder and indicates which could be 
differentiated with the logistic regressions. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 16 Figure 4. Comparison of HCs and subsets of individuals with specific SUDs/affective disorders within 
the combined exploratory and confirmatory samples (sample size per group is indicated within each bar; 
based on the groupings detailed in Table 2). Red stars indicate that, in logistic regressions, model 
parameters could predict whether individuals were HCs or had a specific disorder (i.e., removing 
individuals from analyses without the disorder in question; *p<.05, **p<.01, ***p<.001). Blue stars 
indicate that model parameters could further predict whether an individual had one disorder relative to 
other disorders (i.e., removing HCs from analyses). Statistical results are reported in Tables S1.10-S1.11. 
For a similar plot of general task performance (number of wins) by specific disorder group, see Figure 
S1.3. Legend: Stim. Only = stimulant use disorders without co-morbidities, Can. = cannabis use . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 17 disorders, Op. = opioid use disorders, Alc. = alcohol use disorders, Sed. = sedative use disorders, Hal. = 
hallucinogen use disorders, SAD = social anxiety disorder, PTSD = post-traumatic stress disorders. 
 
3.6 Predictive categorization Results of predictive categorization analyses (ROCs, AUCs, accuracy, and confusion matrices) for specific disorders are shown in Figure 5. When tested on the confirmatory dataset, 
logistic regression models trained on the exploratory dataset varied in predictive classification 
accuracy (relative to HCs) between .55 and .66, depending on the specific disorder in question. 
AUCs ranged from .6 to .7, indicating poor to acceptable discrimination. When categorizing HCs 
vs. all individuals with SUDs together, accuracy = .63 and AUC = .66. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 18 Figure 5. Results of logistic regression models trained on the exploratory sample and then tested on the 
confirmatory sample (classifying individuals as HCs vs. having a specific SUD or affective disorder). 
Receiver operating characteristic curves (ROCs) illustrate the true positive rate (sensitivity) vs. false 
positive rate (1 - specificity) for different categorization thresholds. Performance is quantified by 
associated area-under-the-curve (AUC) scores, reflecting how often a random sample will be assigned to . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 19 the correct group with higher probability. Acceptable AUCs vary by application, but the following 
heuristic cutoff values have been proposed: 0.5 = No discrimination, 0.5-0.7 = Poor discrimination, 0.7-
0.8 = Acceptable discrimination, 0.8-0.9= Excellent discrimination, and > 0.9 = Outstanding 
discrimination (37). Accuracy at a neutral threshold of 0.5 is also shown. Confusion matrices for each 
disorder (0 = HCs, 1 = Disorder) illustrate the numbers of accurate and inaccurate classifications for each 
group based on this criterion. Note that training data were balanced using frequency weights prior to 
training. Gen. Anxiety = Generalized Anxiety Disorder; Soc. Anxiety = Social Anxiety Disorder. 4. DISCUSSION 
 
In this study, we found confirmatory evidence that substance users show selectively slower 
learning rates than HCs for negative outcomes. In contrast, previously observed differences in 
action precision and learning rates for positive outcomes did not replicate for most analyses in 
this new sample (with the exception of action precision in supplementary frequentist analyses 
accounting for WRAT scores). Also notable was the reduced reward sensitivity (suggesting 
greater directed exploration) observed within SUDs, which was not observed in our prior study.  
 
Subsequent logistic regressions across participants in the exploratory and confirmatory samples 
showed that both action precision and learning rates for losses (and in some cases learning rates 
for wins) could jointly differentiate the presence of most specific substance use and affective 
disorders relative to HCs. This highlights the way a multi-dimensional computational phenotype 
can offer categorical information not available from summary statistics or a single parameter 
alone. In contrast, analogous logistic regressions found that most disorders could not be 
differentiated from one another, except for stimulant use disorders (which showed greater action 
precision and faster learning rates for wins relative to other disorders) and social anxiety disorder 
(slower learning rates for wins relative to other disorders). When considering co-morbid 
affective disorders, this general lack of differentiability further indicated, for example, that 
learning rates for losses and action precision did not differ in SUDs with vs. without MDD or 
anxiety disorders.  
 
When logistic regressions were instead trained on the exploratory sample and tested on their 
ability to correctly categorize HCs vs. specific disorders in the confirmatory sample, 
performance was above chance but ranged from poor to acceptable discrimination across specific 
disorders. Thus, while the group differences we observed may offer important mechanistic 
insights, their potential clinical utility for diagnostic purposes should not be overstated. 
 
Overall, these results provide strong evidence that the differences we observed in SUDs are 
transdiagnostic. They also provide added support for the hypothesis that individuals with SUDs 
adjust their behavior more slowly in the face of aversive outcomes, which could help explain 
continued drug use despite harmful effects. This is also supported by previous literature showing 
reduced sensitivity to negative outcomes (or to affective stimuli generally) in this population 
within other contexts (e.g., (18, 38-49)). Given this replication, there are now several future 
directions that should be considered. One important question, for example, is whether this 
difference is caused by substance use or whether it represents a pre-existing vulnerability factor. 
Answering this question will likely require longitudinal analyses of at-risk populations, as well . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 20 as assessing potential relationships with pre-existing biological and psychological risk-factors. 
Another important topic to address will be whether this difference is affected by treatment and 
covaries with symptom severity. Randomized controlled trials assessing the effects of distinct 
treatments on learning rates will be necessary to shed light on this question. Additionally, 
neuroimaging studies designed to identify the neural correlates of these learning rate differences 
could highlight potential biological targets. Finally, it will be important to identify whether 
learning rates moderate treatment outcomes. In relation to this last point, we recently reported a 
1-year follow-up study in our previous sample and unexpectedly found that slower learning rates 
for losses at baseline predicted greater symptom improvement in stimulant users at 1-year 
follow-up (20). Here we suggested that, despite potentially reducing learning from the harmful 
outcomes of drug use, this difference might also attenuate reactions to unpleasant aspects of the 
recovery process and facilitate adherence. However, as it was not an expected result, we aim to 
replicate this longitudinal effect in future work. If confirmed, it could indicate one concrete way 
in which measures of learning rate differences could be clinically informative.  
 
Some remaining limitations are important to consider. First, our sample of SUDs was a 
heterogenous community sample from a particular region of the United States, the great majority 
of which were stimulant users, which may have limited our ability to detect effects specific to 
some substances and not others (with the exception of a moderate number of stimulant users that 
did not have co-morbidities in the combined sample), or to generalize findings to substance users 
in other regional or cultural contexts. It was also a treatment-seeking, abstinent sample, and thus 
may not generalize to non-treatment-seeking individuals. Many substance users also had 
affective disorders. However, parameters did not correlate with depression/anxiety symptoms 
and, with the exception of social anxiety disorder (slower learning rate for wins), model 
parameters in the combined sample could not differentiate SUDs with vs. without affective 
disorders. Finally, the correct interpretation is less clear for results that were significant in the 
previous or current sample, but not both (e.g., with respect to reduced action precision and 
reward sensitivity in SUDs). Future studies will therefore need to examine whether inconsistent 
results between samples are better understood as false positives in one sample or false negatives 
in the other. In summary, this study lends added confidence to the generalizability of previous results suggesting slower learning from negative outcomes in SUDs and motivates a number of future 
directions to test the neurobiological basis of this difference and whether it might represent, for 
example, a pre-existing vulnerability factor, a predictor of treatment response, an objective 
severity marker, and/or a novel intervention target. These represent important next steps toward 
evaluating the clinical utility of this potentially important individual difference.  
 
Software Note: All model simulations, model comparison, and parametric empirical Bayes 
analyses were implemented using standard routines (spm_MDP_VB_X.m, spm_BMS.m, 
spm_dcm_peb.m, spm_dcm_peb_bmc.m) that are available as MATLAB code in the latest 
version of SPM academic software: http://www.fil.ion.ucl.ac.uk/spm/. For the specific code used 
to build the three-armed bandit task model and fit parameters to data, see: 
https://github.com/rssmith33/3-armed_bandit_task_model. 
 
Role of Funding Source . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2023.04.03.23288037
doi: 
medRxiv preprint MODELING LEARNING RATES IN SUBSTANCE USE 21 Nothing declared. Contributors Samuel Taylor performed analyses and wrote the initial draft of the manuscript. Claire A. 
Lavalley and Navid Hakimi performed analyses and edited the manuscript. Jennifer L. Stewart, 
Maria Ironside, Haixia Zheng, Evan White, Salvador Guinjoan, and Martin P. Paulus reviewed 
and edited the manuscript. Ryan Smith performed analyses, reviewed and edited the manuscript, 
and supervised the project. All authors have approved the final article. Conflict of Interest No conflict declared. Acknowledgment This study was supported by the Laureate Institute for Brain Research.",1
"Recent research suggests that brain-heart interactions are associated with perceptual and self- consciousness. In this line, the neural responses to visceral inputs have been hypothesized to play a leading role in shaping our subjective experience. This study aims to investigate whether the contextual processing of auditory irregularities modulates both direct neuronal responses to the auditory stimuli (ERPs) and the neural responses to heartbeats, as measured with heartbeat-evoked responses (HERs). HERs were computed in patients with disorders of consciousness, diagnosed with a minimally conscious state or unresponsive wakefulness syndrome. We tested whether HERs reflect conscious auditory perception, which can potentially provide additional information for the consciousness diagnosis. EEG recordings were taken during the local-global paradigm, which evaluates the capacity of a patient to detect the appearance of auditory irregularities at local (short-term) and global (long-term) levels. The results show that local and global effects produce distinct ERPs and HERs, which can help distinguish between the minimally conscious state and unresponsive wakefulness syndrome patients. Furthermore, we found that ERP and HER responses were not correlated suggesting that independent neuronal mechanisms are behind them. These findings suggest that HER modulations in response to auditory irregularities, especially local irregularities, may be used as a novel neural marker of consciousness and may aid in the bedside diagnosis of disorders of consciousness with a more cost-effective option than neuroimaging methods. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Introduction Theoretical developments on consciousness and experimental research have rooted the basis of consciousness in how the brain responds to visceral inputs (Azzalini et al., 2019; Candia-Rivera, 2022; Park and Tallon-Baudry, 2014). In post-comatose patients, the consciousness diagnosis is primarily based on behavioral signs of consciousness (Bayne et al., 2017), which aims at distinguishing between patients showing only reflex-like responses to the environment, diagnosed as Vegetative State or Unresponsive Wakefulness Syndrome (VS/UWS; (Laureys et al., 2010), and patients with fluctuating but reproducible signs of non- reflex behavior, diagnosed as Minimally Conscious State (MCS), (Giacino et al., 2002), but see also (Naccache, 2018). However, recent results demonstrate that behavioral assessment is not sufficient and neuroimaging techniques are used to detect covert states of consciousness (Kondziella et al., 2020). The classification of MCS and UWS patients using EEG and cardiac features while undergoing processing of auditory regularities has shown an advantage over EEG features alone (Raimondo et al., 2017), implying that brain-heart interactions may be involved in the conscious processing of auditory inputs. Recent evidence on automatic classifications of heartbeat-evoked responses (HERs) in resting-state showed that these markers may capture residual signs of consciousness (Candia-Rivera et al., 2021a) suggesting that HERs might convey state-of-consciousness relevant information about how the brain responds to bodily- related stimuli. Further evidence exists in healthy participants, in which the processing of auditory stimuli may cause cognitive modulations to the cardiac cycle (Banellis and Cruse, 2020; Pérez et al., 2021; Pfeiffer and Lucia, 2017), and HERs correlate with perceptual awareness (Al et al., 2020; Park et al., 2014). We hypothesized that HERs can be modulated by contextual processing of different levels of auditory regularities, as presented in the local-global paradigm (Bekinschtein et al., . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint 2009). In this study, we analyze HERs following the presentation of auditory irregularities, with special regard on distinguishing UWS (n=40) and MCS (n=46) patients. Note that the automated classification of this cohort was previously performed in another study (Raimondo et al., 2017). Therefore, our aim is to characterize the group-wise differences between UWS and MCS patients that may allow a multi-dimensional cognitive evaluation to infer the presence of consciousness (Sergent et al., 2017), but also complement the bedside diagnosis performed with neuroimaging methods that capture neural correlates of covert consciousness (Sanz et al., 2021). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Materials and Methods Patients This study includes 46 MCS and 40 UWS patients. Patients were admitted at the Department of Neurology, Pitié-Salpêtrière Hospital (Paris, France) for consciousness evaluation through Coma Recovery Scale-Revised (CRS-R) (Giacino et al., 2004). The study was approved by the ethics committee of CPP Île de France 1 (Paris, France). Informed consent was signed by the patients’ legal representatives for approval of participation in the study, as required by the declaration of Helsinki. Experimental paradigm Patients were recorded with high-density EEG (EGI 256 channels, 250 Hz sampling rate, referenced to the vertex) under the local-global paradigm that aims to evaluate the cognitive processing of local–short term–, and global–long term–auditory regularities (fig. 1A) (Bekinschtein et al., 2009). The paradigm consists of two embedded levels of auditory regularities in trials formed by five consecutive sounds. The 5th sound defines whether the trial is standard or deviant at two levels: local and global. The local level of regularities is defined within the trial. The global level of regularities is defined across trials (frequent trials ~80% define the regularity, and rare ones ~20% violate this regularity). In fig. 1A, in the XX blocks, the frequent stimulus corresponds to 5 equal sounds (local standard and global standard). In contrast, the infrequent stimulus corresponds to 4 equal sounds followed by a fifth different sound (local deviant and global deviant). In the XY blocks, the frequent stimulus corresponds to 4 equal sounds and a fifth different sound (local deviant and global standard). The infrequent stimulus corresponds to 5 equal sounds (local standard and global deviant). The patients included in this study performed at least 4 blocks (2 XX and 2 XY), in which one block has an approximate duration of 200 s. Each trial is formed by five . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint consecutive sounds lasting 50 milliseconds, with a 150-millisecond gap between the sounds’ onsets and an intertrial interval ranging from 1,350 to 1,650 milliseconds. Figure 1. Experimental description and EEG analysis. (A) Local-global paradigm. (B) Heartbeat-evoked response defined by the R-peaks that follow the 5th sound from all the trials, and the Auditory-related potential defined by the EEG activity locked to the stimuli. Data preprocessing MATLAB and Fieldtrip toolbox were used for data processing and analysis (Oostenveld et al., 2011). EEG data were offline filtered with a 1-25 Hz Butterworth band- pass order 4 filter, with a Hamming windowing at cutoff frequencies). The channels with large artifacts were rejected based on the area under the curve of their z-score. Channels exceeding > 3 standard deviations were discarded iteratively (11 ± 1 SEM channels rejected on average). Following the procedure described in (Raimondo et al., 2017), electrocardiograms (ECG) were recovered from the cardiac field artefact captured in EEG data using Independent Component Analysis (ICA) (default parameters from Fieldtrip). From this, ICA-corrected EEG data and an electrocardiogram derived from independent component analysis (ICA-ECG) is obtained. Note that the use of ICA-ECG instead of a standard ECG measured from the rib cage was successfully used in other two studies (Candia-Rivera et al., . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint 2021a; Raimondo et al., 2017). Furthermore, it was shown that the differences between the R- peak timings obtained from the ECG and ICA-ECG differ in a range of 0-4 ms (Candia- Rivera et al., 2021a). To identify further noisy channels, the mean weighted-by-distance correlation of all channels between their neighbors was computed (36 ± 2 SEM channels rejected on average). Neighborhood relationships considered all channels up to distances of 4 cm. Channels with a mean weighted-by-distance correlation lower than 80% were replaced by spline interpolation of neighbors. EEG dataset was re-referenced using a common average and a subset of 64 channels was selected for data analysis (Candia-Rivera et al., 2021b). Heartbeats were detected on the ICA-ECG using an automated process based on a sliding time window detecting local maxima (R-peaks). Both peak detection and resulting histogram of interbeat interval duration were visually inspected in each patient. Ectopic interbeat intervals were automatically identified for review by detecting peaks on the derivative of the interbeat intervals time series. Manual addition/removal of peaks was performed if needed (23 ± 3 SEM manual corrections to individual heartbeats on average). Heartbeat-evoked responses (HERs) (Park and Blanke, 2019; Schandry et al., 1986) were computed by averaging EEG epochs from the R-peaks that follow the 5th sound from all the trials, up to 500 ms (fig. 1B). Epochs with amplitude larger than 300 μV on any channel, or where the next or preceding heartbeat occurred at an interval shorter than 500 ms, were discarded. The epochs in which the stimuli were located at less than 20 ms from the closest R- peaks were discarded as well. We also controlled that the average latency between the 5th sound and the next heartbeat did not differ between MCS and UWS patients (Wilcoxon tests, local standard: p = 0.2303, Z = 1.1991; local deviants: p = 0.3387, Z = 0.9567; global standard: p = 0.2047, Z = 1.2684; global deviant: p = 0.4182, Z = 0.8095). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Auditory event-related potentials (ERPs) were computed for contrast by averaging EEG epochs from the 5th sound onset from all the trials, up to 1000 ms. Epochs with amplitude larger than 300 μV on any channel were discarded. Data analysis Two neural signatures were computed to compare MCS and UWS patients: ERPs, that relate to the average of EEG epochs locked to the auditory stimuli, and HERs that relate to the average of EEG epochs locked to the heartbeats that follow the auditory stimuli. The experimental conditions, in which ERPs and HERs were used to compare MCS and UWS patients, are: - 
Local effect: average of the EEG epoch associated to local deviants (local deviant/global standard epochs + local deviant/global deviant epochs), minus the average of EEG epochs associated to local standards (local standard/global standard epochs + local standard/global deviant epochs). - 
Global effect: average of the EEG epoch associated to global deviants (local standard/global deviant epochs + local deviant/global deviant epochs), minus the average of EEG epochs associated to global standards (local standard/global standard epochs + local deviant/global standard epochs). Additionally, HERs average and HERs variance were analyzed during the whole experimental protocol, i.e., the neural responses to heartbeats were analyzed with respect all heartbeats independently of stimuli. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Statistical analysis Statistical comparisons were based on Wilcoxon rank sum and Spearman correlation, as specified in the main text. P-values were corrected for multiple comparisons by applying the Bonferroni rule or by using cluster-permutation analyses. Clustered effects were revealed using a non-parametric version of cluster permutation analysis (Candia-Rivera and Valenza, 2022). In brief, the cluster-based permutation test included a preliminary mask definition, identification of candidate clusters and the computation of cluster statistics with Monte Carlo’s p-value correction. The preliminary mask was defined through unpaired Wilcoxon test, with alpha = 0.05. The identification of neighbor channels was based on the default Fieldtrip channels’ neighborhood definition for 64 channels. A minimum cluster size of 4 channels was imposed. Adjacent candidate clusters on time were wrapped if they had at least one channel in common. Cluster statistics were computed from 10,000 random partitions. The proportion of random partitions that resulted in a lower p-value than the observed one was considered as the Monte Carlo p-value, with significance at alpha = 0.05. The cluster statistic considered is the Wilcoxon’s absolute maximum Z-value obtained from all the samples of the mask. Additionally, to confirm the presence of true effects in HERs, we compared the combined clustered effects with surrogates. We reallocated each heartbeat timing using a uniformly distributed pseudorandom process, between the first and the last sample of each recording. We computed 100 surrogates and repeated the aforementioned statistical analysis. We computed p-values as the proportion of the combined clustered effects found in the surrogates with a higher effect and cluster size, with respect to the real heartbeat timings. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Data availability statement The data used in this study can be made available upon reasonable request. Because of the sensitive nature of the clinical information concerning the patients, the ethics protocol does not allow open data sharing. To access the raw data, the potential interested researcher would need to contact the corresponding authors of the study. Together they would need to ask for an authorization from the local ethics committee, CPP Île de France 1 (Paris, France). The 
codes 
and 
pre-processed 
data 
are 
publicly 
available 
at https://github.com/diegocandiar/brain_heart_doc . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Results We evaluated auditory ERPs and HERs in patients with disorders of consciousness undergoing the local-global paradigm that aims to evaluate the cognitive processing of local– short term–, and global–long term–auditory regularities. First, unpaired non-parametric cluster analysis was performed between MCS and UWS patients for ERPs, global and local effects. In Figure 2A are shown the clustered effects found with respect to the 5th sound, in the ERP global effect (main positive cluster: p = 0.0001, Z = 3.684, latency = 800-850 ms; main negative cluster: p = 0.0013, Z = -3.1905, latency = 280-336 ms) and ERP local effect (main positive cluster: p = 0.0011, Z = 3.4416, latency = 236-328 ms). The clustered effects were combined to obtain a single value for each patient, corresponding to ERP global and local effects. The distribution of the combined clustered effects is depicted in Figure 2B. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Figure 2. Auditory event-related potentials (ERPs) in the global and local effect. (A) Scalp 
topographies indicate the average group differences between MCS and UWS patients. Thick 
electrodes indicate a clustered effect (Monte Carlo p < 0.05). (B) Average of the clustered 
effects per patient, in the ERP global effect (main positive cluster: p = 0.0001, Z = 3.684, 
latency = 800-850 ms; main negative cluster: p = 0.0013, Z = -3.1905, latency = 280-336 
ms) and ERP local effect (main positive cluster: p = 0.0011, Z = 3.4416, latency = 236-328 
ms). ERPs: auditory event-related potentials, MCS: minimally conscious state, UWS: 
unresponsive wakefulness syndrome Consecutively, cluster permutation analysis was performed between MCS and UWS patients for HERs, global and local effects. In Figure 3A are shown the clustered effects found with respect to the R-peak following the 5th sound, in the HER global effect (main positive cluster: p = 0.0037, Z = 3.0173, latency = 112-130 ms; main negative cluster: p = 0.0058, Z = -3.0173, latency = 340-360 ms) and HER local effect (main positive cluster: p = 0.0029, Z = . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint 3.0606, latency = 400-412 ms; main negative cluster: p = 0.0014, Z = -3.3983, latency = 0-40 ms). The clustered effects were combined to obtain a single value for each patient, corresponding to HER global and local effects. The distribution of the combined clustered effects is depicted in Figure 3B. The combined clustered effects were compared to 100 randomly distributed heartbeats to compute the surrogate p-value. The HER local effect was larger than what would be expected by chance as estimated from surrogate heartbeats (HER local effect, Monte Carlo p = 0.03; HER global effect, Monte Carlo p = 0.54). Figure 3. Heartbeat-evoked responses (HERs) in the global and local effect. (A) Scalp 
topographies indicate the average group differences between MCS and UWS patients. Thick 
electrodes indicate a clustered effect (Monte Carlo p < 0.05). (B) Average of the clustered 
effects per patient, in the HER global effect (main positive cluster: p = 0.0037, Z = 3.0173, 
latency = 112-130 ms; main negative cluster: p = 0.0058, Z = -3.0173, latency = 340-360 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint ms) and HER local effect (main positive cluster: p = 0.0029, Z = 3.0606, latency = 400-412 
ms; main negative cluster: p = 0.0014, Z = -3.3983, latency = 0-40 ms). HERs: heartbeat-
evoked responses, MCS: minimally conscious state, UWS: unresponsive wakefulness 
syndrome. We then tested whether the clusters found using cluster permutations at global and local effects, as measured from HERs and ERPs, come from a distribution with a median different from zero, i.e., whether the deviants differ from the standard 5th sounds within patients' groups (Table 1). We found a significant ERP and HER local effect in both MCS and UWS patients. On the other hand, the global effect was significant only for MCS patients in both ERP and HER analysis. This result extends previous reports highlighting the predictive power for conscious state of the global effect (Perez et al., 2021). Table 1. Wilcoxon sign test performed separately for MCS and UWS patients, to test whether 
the global and local effects as measured from HERs and ERPs come from a distribution with 
median different to zero. Bold indicates significance reached at α = 0.05/8 = 0.0063, 
according to Bonferroni correction for multiple comparisons. Patients 
HERs 
ERPs Global effect 
Local effect 
Global effect 
Local effect MCS 
Z = 2.7805 p = 0.0054 Z = 3.2175 p = 0.0013 Z = 3.7529 p = 0.0002 Z = 5.0311 p < 0.0001 UWS 
Z = -1.9759 p = 0.0482 Z = -2.9840 p = 0.0028 Z = -1.9624 p = 0.0497 Z = 2.9033 p = 0.0037 HERs: heartbeat-evoked responses, ERPs: auditory event-related potentials, MCS: minimally 
conscious state, UWS: unresponsive wakefulness syndrome In Fig. 4A are presented all pair comparisons between ERPs and HERs. for local and global effects. The figure depicts that the measured effects do not show apparent correlations (details on Spearman correlation tests in Table 2). Fig. 4B shows that the four markers: ERP global, ERP local, HER global, and HER local present complementary information for the separation of the diagnostic groups. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Figure 4. Multi-dimensional analysis of the clustered effects found when comparing MCS and 
UWS patients. (A) Pairwise comparison between all possible combinations for ERPs and 
HERs, for local and global effects. Individual points corresponding to a single patient, and 
dotted line indicates the trend, separately per diagnosis. (B) Three-dimensional 
representation of the clustered effects: left panel for ERP global, ERP, local and HER global; 
and right panel for ERP global, ERP local and HER local.  Each ellipsoid was constructed 
per diagnostic group, centered in the group means with ratio defined by the standard 
deviations, for the respective dimensions. HERs: heartbeat-evoked responses, ERPs: auditory 
event-related potentials, MCS: minimally conscious state, UWS: unresponsive wakefulness 
syndrome Table 2. Group-wise Spearman correlation analysis performed separately for MCS and UWS 
patients, between the combined clustered effects found when comparing MCS vs UWS in the 
ERP global effect, ERP local effect, HER global effect, and HER local effect. Significance 
was set at α = 0.05/8 = 0.0063, according to Bonferroni correction for multiple comparisons. MCS 
UWS ERP global vs ERP local 
R = 0.1077 p = 0.4748 R = 0.3099 p = 0.0591 . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint HER global vs ERP global 
R = 0.0575 p = 0.7033 R = 0.1580 p = 0.3290 HER global vs HER local 
R = -0.1193 p = 0.4283 R = 0.1480 p = 0.3607 HER local vs ERP local 
R = -0.0436 p = 0.7730 R = -0.4114 p = 0.0088 HER: heartbeat-evoked response, ERP: auditory event-related potential, MCS: minimally 
conscious state, UWS: unresponsive wakefulness syndrome HER average during the whole protocol presents a small, clustered effect when comparing MCS and UWS patients (Fig. 5A, left). In Fig. 5A, right panel, is shown that a higher HER variance is observed in MCS compared to UWS during the whole protocol. A wide scalp coverage presents higher HER variance in MCS, as compared to UWS (cluster permutation test, p<0.0001, Z= 4.0772, latency= 20-5000 ms). The time courses of the clustered effects in HER average and variance are shown in Fig. 5B. Figure 5. (A) HERs correlation analysis of global and local effect. (B) ΔIBI correlation 
analysis of global and local effect. (C) Results on HER average and HER variance for the 
whole protocol. Thick electrodes show significant differences after cluster permutation. (D) 
HER variance in MCS and UWS patients in the significant cluster. HERs: heartbeat-evoked 
responses, MCS: minimally conscious state, UWS: unresponsive wakefulness syndrome . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Discussion Considering that brain-heart interactions have been demonstrated to be involved in consciousness and relevant for the clinical assessment of disorders of consciousness (Candia- Rivera et al., 2021a; Pérez et al., 2021; Raimondo et al., 2017; Riganello et al., 2019), we analyzed neural responses to heartbeats during the processing of auditory irregularities to characterize MCS and UWS patients. The processing of short- and long-term auditory irregularities, i.e., the local and global effects, shows distinctive responses between MCS and UWS patients in their HERs. Correlation analyses showed that locking EEG to heartbeats provides complementary information (HERs), with respect to the ERPs locked to the auditory irregularities. The local effect, as evaluated with HERs, showed a better separability between MCS and UWS patients, and a greater specificity with respect to surrogate heartbeat analysis. These results support earlier findings that suggest the presence of auditory-cardiac synchrony (Banellis and Cruse, 2020; Pérez et al., 2021; Pfeiffer and Lucia, 2017). Additionally, our results suggest that heartbeat dynamics are involved in the conscious processing of auditory information and primarily on the distinction of short-term changes. Our results go in the same direction as previous evidence, in which automatic classifications of these patients showed a higher accuracy when locking EEG to heartbeats, with respect to the classification of EEG segments unrelated to the cardiac cycle (Candia- Rivera et al., 2021a). Nevertheless, the measured responses in ERPs and HERs do not separate MCS and UWS patients’ groups completely, suggesting that some patients do not react or only react to some trials that were attenuated when averaging all trials in the time- locked analysis. . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Our results contribute to the extensive experimental evidence showing that brain-heart interactions, as measured with HERs, are related to perceptual awareness (Azzalini et al., 2019; Skora et al., 2022). For instance, neural responses to heartbeats correlate with perception in a visual detection task (Park et al., 2014). Further evidence exists on somatosensory perception, where a higher detection of somatosensory stimuli occurs when the cardiac cycle is in diastole and it is reflected in HERs (Al et al., 2020). Evidence on heart transplanted patients shows that the ability of heartbeats sensation is reduced after surgery and recovered after one year, with the evolution of the heartbeats sensation recovery reflected in the neural responses to heartbeats as well (Salamone et al., 2020). The responses to heartbeats also covary with self-perception: bodily-self-identification of the full body (Park et al., 2016), and face (Sel et al., 2017), and the self-relatedness of spontaneous thoughts (Babo-Rebelo et al., 2016) and imagination (Babo-Rebelo et al., 2019). Moreover, brain-heart interactions measured from heart rate variability correlate with conscious auditory perception as well (Banellis and Cruse, 2020; Pérez et al., 2021; Pfeiffer and Lucia, 2017). We showed that ERPs and HERs are repeatedly larger in MCS patients, as compared to UWS, in both local and global effects. Furthermore, the ERPs and HERs (both for the local and global effects) are uncorrelated in all possible comparisons (see Figure 4A), in addition to the results showing differentiation of clustering effects in HER and ERP (see Figure 4B). These results suggest that the neuronal mechanisms behind these ERPs and HERs responses are independent. In addition, we found that HER variance is higher in MCS patients than in UWS patients, as previously reported in resting state (Candia-Rivera et al., 2021a). Put together these results suggest that two different neuronal signatures differentiate MCS from UWS patients. A first process probed with HER variability differentiates, irrespective of the current stimulus type being processed. This first process originates from central and right temporal scalp areas and has been linked with social cognition but could also correspond to a . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint self-consciousness-state marker (Candia-Rivera et al., 2021a). Second, a modulation of HER in response to local and global auditory irregularities. These responses present several properties related to a neural signature of conscious access to local and global deviant stimuli. Such ERPs and HERs modulations by conscious access to a new stimulus attribute may well correspond to a self-consciousness updating process occurring ‘downstream’ to conscious access (Sergent and Naccache, 2012), and enabled for instance in a global neuronal workspace architecture (Dehaene and Naccache, 2001). Note that outliers are expected in disorders of consciousness and exact physiological characterization of the different levels of consciousness remains challenging. First, the standard assessment of consciousness based on behavioral measures has shown a high rate of misdiagnosis in MCS and UWS (Stender et al., 2014). The cause of the misdiagnosis of consciousness arises because consciousness does not necessarily translate into overt behavior (Hermann et al., 2021). Unresponsive and minimally conscious patients, namely non- behavioral MCS (Thibaut et al., 2021), represent the main diagnostic challenge in clinical practice. Second, some of these patients suffer from conditions that may translate to no response to stimuli, even in presence of consciousness. For instance, when they suffer from constant pain, fluctuations in arousal levels, or sensory impairments caused by brain damage (Chennu et al., 2013). Third, these patients were recorded in clinical setups, which may lead to a lower signal-to-noise ratio, and lead to biased measurements in evoked potentials (Clayson et al., 2013). A plethora of complementary neuroimaging techniques have been proposed to enhance the consciousness diagnosis, including anatomical and functional magnetic resonance imaging and positron emission tomography (Kondziella et al., 2020; Sanz et al., 2021). However, those methodologies may not be accessible in all clinical setups, because of costs or medical contraindications. The foregoing evidence of EEG-based techniques to diagnose . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint consciousness (Bai et al., 2021; Engemann et al., 2018) shows promising and low-cost opportunities to develop diagnostic methods that can capture residual consciousness. Our results contribute more evidence of the potential of EEG as a diagnostic tool, but also to the role of visceral signals in consciousness (Azzalini et al., 2019; Candia-Rivera, 2022; Sattin et al., 2020). This study gives evidence that HERs detect auditory conscious perception, in addition to the residual signs of consciousness in resting-state (Candia-Rivera et al., 2021a). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint Funding C.T.B. is supported by a senior fellowship from the Canadian Institute For Advance Research (CIFAR) program in Brain, Mind and Consciousness, as well as by ANR-17-EURE-0017. J.D.S. is supported by the Paris Brain Institute (France) and the program “Investissements d’avenir” ANR-10- IAIHU-06, by the CARNOT maturation grant, Title: Con&Heart to JDS, by a Sorbonne Universités EMERGENCE grant, title: Brain-body interactions, a new window for conscious evaluation in brain-injured patients, as well as a the PerBrain study funded by the European Union (ERA PerMed JTC2019 “PerBrain”). P.P. received support from a Sorbonne PhD grant (""recherches interdisciplinaires émergentes "", France). . 
CC-BY-NC-ND 4.0 International license
It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 4, 2023. 
; 
https://doi.org/10.1101/2021.10.27.21265539
doi: 
medRxiv preprint",1
"Parkinson’s disease (PD) is a progressive neurodegenerative movement disorder with a latent phase and no currently existing disease-modifying treatments. Reliable predictive biomarkers that could transform efforts to develop neuroprotective treatments remain to be identified. Using UK Biobank, we investigated the predictive value of accelerometry in identifying prodromal PD in the general population and compared this digital biomarker to models based on genetics, lifestyle, blood biochemistry, and prodromal symptoms data. Machine learning models trained using accelerometry data achieved better test performance in distinguishing both clinically diagnosed PD (N = 153) (area under precision recall curve (AUPRC): 0.14",1
BACKGROUND: White matter (WM) microstructural changes in the hippocampal cingulum bundle (CBH) in Alzheimer’s disease (AD) have been described in cohorts of largely European ancestry but are lacking in other populations. METHODS: We assessed the relationship between CBH WM integrity and cognition or amyloid burden in 505 Korean older adults aged ,1
"Cardiac auscultation is an accessible diagnostic screening tool that can help to identify
patients with heart murmurs for follow-up diagnostic screening and treatment for
abnormal cardiac function. However, experts are needed to interpret the heart sounds,
limiting the accessibility of auscultation for cardiac care in resource-constrained
environments. Therefore, the George B. Moody PhysioNet Challenge 2022 invited
teams to develop algorithmic approaches for detecting heart murmurs and abnormal
cardiac function from phonocardiogram (PCG) recordings of the heart sounds.
For the Challenge, we sourced 5272 PCG recordings from 1568 pediatric patients in
rural Brazil, and we invited teams to implement diagnostic screening algorithms for
detecting heart murmurs and abnormal cardiac function from the recordings. We
required the participants to submit the complete code for training and running their
algorithms, improving the transparency, reproducibility, and utility of their work. We
also devised an evaluation metric that considered the costs of screening, diagnosis,
treatment, and diagnostic errors, allowing us to investigate the benefits of algorithmic
diagnostic screening and facilitate the development of more clinically relevant
algorithms.
We received 779 algorithms from 87 teams during the course of the Challenge,
resulting in 53 working codebases for detecting heart murmurs and abnormal cardiac
function from PCGs. These algorithms represent a diversity of approaches from both
academia and industry. April 5, 2023
1/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. The use of heart sound recordings for identifying heart murmurs and abnormal
cardiac function allowed us to explore the potential of algorithmic approaches for
providing accessible pre-screening in resource-constrained environments. The submission
of working, open-source algorithms and the use of novel evaluation metrics supported
the reproducibility, generalizability, and clinical relevance of the research from the
Challenge. Author summary Cardiac auscultation is an accessible diagnostic screening tool for identifying heart
murmurs. However, experts are needed to interpret heart sounds, limiting the
accessibility of auscultation in cardiac care. The George B. Moody PhysioNet Challenge
2022 invited teams to develop algorithms for detecting heart murmurs and abnormal
cardiac function from phonocardiogram (PCG) recordings of heart sounds.
For the Challenge, we sourced 5272 PCG recordings from 1568 pediatric patients in
rural Brazil. We required the participants to submit the complete code for training and
running their algorithms, improving the transparency, reproducibility, and utility of
their work. We also devised an evaluation metric that considered the costs of screening,
diagnosis, treatment, and diagnostic errors, allowing us to investigate the benefits of
algorithmic diagnostic screening and facilitate the development of more clinically
relevant algorithms. We received 779 algorithms from 87 teams during the Challenge,
resulting in 53 working codebases and publications that represented a diversity of
approaches to detecting heart murmurs and identifying clinical outcomes from heart
sound recordings. Introduction Cardiac auscultation via stethoscopes remains the most common and the most
cost-effective tool for cardiac pre-screening. Despite its popularity, the technology has
limited diagnostic sensitivity and accuracy [1, 2], as its interpretation requires many
years of experience and skill, leading to significant disagreement between medical
personnel [3, 4]. Digital phonocardiography has emerged as a more objective alternative
to traditional cardiac auscultation, enabling the use of algorithmic methods for heart
sound analysis and diagnosis [5]. The phonocardiogram (PCG) can be acquired by a
combination of high-fidelity stethoscope front-ends and high-resolution digital sampling
circuitry, which enable the registration of heart sounds as a discrete-time signal.
As acoustic signals, heart sounds are mainly generated by the vibrations of cardiac
valves as they open and close during the cardiac cycle and by the turbulence of blood
flow within the valves. Turbulent blood flow may cause enough vibrations within the
cardiac valves to create audible heart sounds and abnormal waveforms in the PCG,
which are called murmurs. Different kinds of murmurs exist, and they are characterized
in various ways, including location, timing, duration, shape, intensity, and pitch. The
identification and analysis of murmurs provide valuable information about
cardiovascular pathologies.
However, while cardiac auscultation itself is relatively accessible, experts are needed
to interpret heart sounds and PCGs, limiting the accessibility of auscultation for cardiac
disease management, especially in resource-constrained environments. The ability to
correctly interpret PCGs for murmur detection and for identifying different pathologies
requires time and broad clinical experience. Therefore, the objective interpretation of
the PCG remains a difficult skill to acquire.
The 2022 George B. Moody PhysioNet Challenge (formerly the April 5, 2023
2/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint AV
PV TV MV Fig 1. Auscultation locations for the CirCor DigiScope dataset [6], which was used for
the Challenge: pulmonary valve (PV), aortic valve (AV), mitral valve (MV), and
tricuspid valve (TV). PhysioNet/Computing in Cardiology Challenge) provided an opportunity to address
these issues by inviting teams to develop fully automated approaches for detecting heart
murmurs and abnormal cardiac function from PCG recordings. We sourced and shared
PCG recordings for up to four auscultation locations from a largely pediatric population
in Brazil, and we asked teams to identify both heart murmurs and the clinical outcomes
of a full diagnostic screening from the recordings. The Challenge explored the
diagnostic potential of algorithmic approaches for interpreting PCG recordings. Methods Challenge Data The CirCor DigiScope dataset [6] was used for the 2022 George B. Moody PhysioNet
Challenge. This dataset consists of one or more PCG recordings from several
auscultation locations. The dataset was collected during two screening campaigns in
Para´
ıba, Brazil from July 2014 to August 2014 and from June 2015 to July 2015. The
study protocol was approved by the 5192-Complexo Hospitalar HUOC/PROCAPE
Institutional Review Board under the request of the Real Hospital Portuguˆ
es de
Beneficiˆ
encia em Pernambuco. A detailed description of the dataset can be found in [6].
During the data collection sessions, the participants answered a socio-demographic
questionnaire, followed by a clinical examination (anamnesis and physical examination),
a nursing assessment (physiological measurements), and cardiac investigations (cardiac
auscultation, chest radiography, electrocardiogram, and echocardiogram). The collected
data were then analyzed by an expert pediatric cardiologist. The expert could
re-auscultate the participant or request complementary tests. At the end of the session,
the pediatric cardiologist either directed the participant for a follow-up appointment,
referred the participant to cardiac catheterization or heart surgery, or discharged the
participant as appropriate.
The PCGs were recorded using an electronic auscultation device, the Littmann 3200
stethoscope, from up to four auscultation locations on the body; see Fig 1: • Aortic valve: second intercostal space, right sternal border; • Pulmonic valve: second intercostal space, left sternal border; April 5, 2023
3/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint • Tricuspid valve: left lower sternal border; and • Mitral valve: fifth intercostal space, midclavicular line (cardiac apex). The choice of locations, the number of recordings at each location, and the duration of
the PCG recordings varied between patients. The recordings were made by potentially
different operators, but each patient’s PCGs was recorded by a single operator in a
sequential manner. The PCG were also inspected for signal quality and
semi-automatically segmented using the three algorithms proposed in [7], [8], and [9]
and then corrected, as necessary, by a cardiac physiologist.
Each patient’s PCGs and clinical notes were also annotated for murmurs and
abnormal cardiac function. These annotations served as the labels for the Challenge.
The murmur annotations and characteristics (location, timing, shape, pitch, quality,
and grade) were manually identified by a single cardiac physiologist independently of the
available clinical notes and PCG segmentation. The cardiac physiologist annotated the
PCGs by listening to the audio recordings and by visually inspecting the corresponding
waveforms. The murmur annotations indicated whether the expert annotator could
detect the presence or absence of a murmur in a patient from the PCG recordings or
whether the annotator was unsure about the presence or absence of a murmur. The
murmur annotations did not indicate whether a murmur was pathological or innocent.
The clinical outcome annotations were determined by cardiac physiologists using all
available clinical notes, including the socio-demographic questionnaire, clinical
examination, nursing assessment, and cardiac investigations. In particular, the experts
used reports from a echocardiogram, which is a standard diagnostic tool for identifying
abnormal cardiac function. The clinical outcome annotations indicated whether the
expert annotator identified normal or abnormal cardiac function. The clinical outcome
annotations were performed by different experts, and these experts were different from
the expert who performed the murmur annotations.
In total, the Challenge dataset consisted of 5272 annotated PCG recordings from
1568 patients. We released 60% of the recordings as a public training set and retained
10% of the recordings as a private validation set and 30% of the recordings as a private
test set. The training, validation, and test sets were matched to approximately preserve
the univariate distributions of the variables. Data from patients who participated in
multiple screening campaigns belonged to only one of the training, validation, or test
sets to prevent data leakage. We shared the training set at the beginning of the
Challenge to allow the participants to develop their algorithms and sequestered the
validation and test sets during the Challenge to evaluate the algorithms.
Table 1 summarizes the variables provided in the training, validation, and test sets
of the Challenge data. Table 2 summarizes the distributions of the variables. Challenge Objective We designed the Challenge to explore the potential for algorithmic pre-screening of
heart murmurs and abnormal heart function, especially in resource-constrained
environments. We asked the Challenge participants to design working, open-source
algorithms for identifying heart murmurs and the clinical outcomes from PCG
recordings. For each patient encounter, each algorithm interpreted the PCG recordings
and/or demographic data. Challenge Timeline This year’s Challenge was the 23rd George B. Moody PhysioNet Challenge [11]. As with
previous years, this year’s Challenge had an unofficial phase and an official phase. April 5, 2023
4/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint The unofficial phase (February 1, 2022 to April 8, 2022) introduced the teams to the
Challenge. We publicly shared the Challenge objective, training data, example
classifiers, and evaluation metrics at the beginning of the unofficial phase. We invited
the teams to submit their code for evaluation, and we scored at most 5 entries from
each team on the hidden validation set during the unofficial phase.
Between the unofficial phase and official phase, we took a hiatus (April 9, 2022 to
April 30, 2022) to improve the Challenge in response to feedback from teams, the
broader community, and our collaborators. During the hiatus, we sourced the clinical
outcomes for the patient encounters to enrich the Challenge.
The official phase (May 1, 2022 to August 15, 2022) allowed the teams to refine their
approaches for the Challenge. We updated the Challenge objectives, data, example
classifiers, and evaluation metric at the beginning of the official phase. We again invited
the teams to submit their code for evaluation, and we scored at most 10 entries from
each team on the hidden validation set during the official phase.
After the end of the official phase, we asked each team to choose a single entry from
their team for evaluation on the test set. We allowed the teams to choose any successful
model from the official phase, but most teams chose their best-scoring entries. We only
evaluated one entry from each team for each task on the test set to prevent sequential
training on the test set. The winners of the Challenge were the teams with the best
scores on the test set. We announced the winners at the end of the Computing in
Cardiology (CinC) 2022 conference.
The teams presented and defended their work at CinC 2022, and they wrote
four-page conference proceeding papers describing their work, which we reviewed for
accuracy and coherence. We will publicly release the algorithms after the end of the
Challenge and the publication of these papers through the Challenge website. Challenge Rules and Expectations While we encouraged the teams to ask questions, pose concerns, and discuss the
Challenge in a public forum, we prohibited the teams from discussing or sharing their
work during the unofficial phase, hiatus, and official phase of the Challenge to preserve
the diversity and uniqueness of the teams’ approaches.
For both phases of the Challenge, we required teams to submit their code for
training and running their models, including any code for processing or relabeling the
data. We first ran each team’s training code on the public training set to create trained
models. We then ran the trained models on the hidden validation and test sets to label
the recordings; we ran the trained models on the recordings sequentially to better reflect
the sequential nature of the screening process. We then scored the outputs from the
models using the expert annotations on hidden validation and test sets.
We allowed the teams to submit either MATLAB or Python code; other
implementations were considered upon request, but there were no requests for other
programming languages. Participants containerized their code in Docker and submitted
it by sharing private GitHub or Gitlab repositories with their code. We downloaded
their code and ran it in containerized environments on Google Cloud. We described the
computational architecture of these environments entries more fully in [12].
Each entry had access to 8 virtual CPUs, 52 GB RAM, 50 GB local storage, and an
optional NVIDIA T4 Tensor Core GPU (driver version 470.82.01) with 16 GB VRAM.
We imposed a 72 hour time limit for training each model on the training set without a
GPU, a 48 hour time limit for training each model on the training set with a GPU, and
a 24 hour time limit for running each trained model on either the validation or test set
either with or without a GPU.
To aid the teams, we shared example MATLAB and Python entries. These examples
used random forest classifiers with the age group, sex, height, weight, pregnancy status April 5, 2023
5/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint of the patient as well as the presence, mean, variance, and skewness, i.e., the first four
order statistics, of the numerical values in each PCG recording as features. We did not
design these example entries to perform well. Instead, we used them to provide minimal
working examples of how to read the Challenge data and write the model outputs. Challenge Evaluation To capture the focus of this year’s Challenge on algorithmic pre-screening, we developed
novel scoring metrics for each of the two Challenge tasks: detecting heart murmurs and
identifying clinical outcomes for abnormal or normal heart function.
As described above, the murmurs were directly observable from the PCGs, but the
clinical outcomes reflected the results of a more comprehensive diagnostic screening,
including the interpretation of an echocardiogram. However, despite these differences,
we asked the teams to perform both tasks using only PCGs and routine demographic
data, which allowed us to explore the diagnostic potential of algorithmic approaches for
interpreting relatively easily accessible PCGs.
The algorithms for both of these tasks effectively pre-screened patients for expert
referral. If an algorithm inferred abnormal cardiac function, i.e., the murmur outputs
were murmur present, murmur unknown, or outcome abnormal, then the algorithm
would refer the patient to a human expert for a confirmatory diagnosis and potential
treatment. If the algorithm inferred normal cardiac function, i.e., if the model outputs
were murmur absent or outcome normal, then the algorithm would not refer the patient
to an expert, and the patient would not receive treatment, even if the patient actually
had abnormal cardiac function. Fig. 2 illustrates this algorithmic pre-screening process
as part of a larger diagnostic pipeline.
For the murmur detection task, we introduced a weighted accuracy metric that
assessed the ability of an algorithm to reproduce the results of a skilled human
annotator. For the clinical outcome identification task, we introduced a cost-based
scoring metric that reflected the cost of expert diagnostic screening as well as the costs
of timely, delayed, and missed treatments for abnormal cardiac function. The team with
the highest weighted accuracy metric won the murmur detection task, and the team
with the lowest cost-based scoring metric won the clinical outcome identification task.
We formulated versions of both of these metrics for both tasks to allow for more
direct comparisons; see the Appendix for the additional metrics. We also calculated
several traditional evaluation metrics to provide additional context to the performance
of the models
Cost-based scoring is controversial, in part, because healthcare costs are an
imperfect proxy for health needs [13, 14]; we reflect on this important issue in the
Section . However, screening costs necessarily limit the ability to perform screening,
especially in resource-constrained environments, so we considered costs as part of
improving access to cardiac screening. Weighted Accuracy Metric
We introduced a weighted accuracy metric to evaluate
the murmur detection algorithms. This metric assessed the ability of the algorithms to
reproduce the decisions of the expert annotator. This weighted accuracy metric is
similar to the traditional accuracy metric, but it assigned more weight to patients that
had or potentially had murmurs than to patients that did not have murmurs. These
weights reflect the rationale that, in general, a missed diagnosis is more harmful than a
false alarm. April 5, 2023
6/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint Fig 2. Screening and diagnosis pipeline for the Challenge. All patients would receive
algorithmic pre-screening, and patients with positive results from algorithmic
pre-screening would receive confirmatory expert screening and diagnosis. (i) Patients
with positive results from algorithmic pre-screening and expert annotators would receive
treatment; they are true positive cases. Patients with positive results from algorithmic
pre-screening and negative results from expert annotators would not receive treatment;
they are false positive cases or false alarms. Patients with negative results from
algorithmic pre-screening who would have received positive results from the expert
annotators would have missed or delayed treatment; they are false negative cases.
Patients with negative results from algorithmic pre-screening who would have also
received negative results from expert annotators also would not receive treatment; they
are true negative cases. April 5, 2023
7/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint We defined a weighted accuracy metric for the murmur detection task as amurmur =
5mPP + 3mUU + mAA 5(mPP + mUP + mAP) + 3(mPU + mUU + mAU) + (mPA + mUA + mAA), (1)
where Table 3 defines a three-by-three confusion matrix M = [mij] for the murmur
present, murmur unknown, and murmur absent classes.
The coefficients were chosen to reflect the trade-off between false positives and false
negatives, where clinicians may tolerate multiple false alarms to avoid a single missed
diagnosis. In (1), murmur present cases have five times the weight of murmur absent
cases (and the murmur unknown cases have three times the weight of murmur absent
cases) to reflect a tolerance of five false alarms for every one false positive.
Like the traditional accuracy metric, this metric only rewarded algorithms for
correctly classified recordings, but it provided the highest reward for correctly
classifying recordings with murmurs and the lowest reward for correctly classifying
recordings without murmurs, i.e., recordings that were labeled as having or not having
murmurs, respectively. It provided an intermediate reward for correctly classifying
recordings of unknown murmur status to reflect the difficulty and importance of
indicating when the quality of a recording is not adequate for diagnosis.
We used (1) to rank the Challenge algorithms for the murmur detection task. The
team with the highest value of (1) won this task. Cost-based evaluation metric
We introduced a cost-based evaluation metric to
evaluate the clinical outcome algorithms for abnormal or normal heart function. This
metric considered the ability of the algorithms to reduce the costs associated with
diagnosing and treating patients, primarily by screening fewer patients with normal
cardiac function. We again emphasize that healthcare costs are an imperfect surrogate
for health needs [13, 14]. However, costs are still a necessary consideration as part of
resource-constrained environments.
For each patient encounter, the algorithm interpreted the PCG recordings and
demographic data for the encounter. If an algorithm inferred abnormal cardiac function,
then it would refer the patient to a human expert for a confirmatory diagnosis. If the
expert confirmed the diagnosis, then the patient would receive treatment, and if the
expert did not confirm the diagnosis, then the patient would not receive treatment.
Alternatively, if the algorithm inferred normal cardiac function, then it would not refer
the patient to an expert, and the patient would not receive treatment, even if the patient
had abnormal cardiac function that would have been detected by a human expert.
We associated each of these steps with a cost: the cost of algorithmic pre-screening,
the cost of expert screening, the cost of timely treatment, and the cost of delayed or
missed treatment.
For simplicity, we assumed that algorithmic pre-screening had a relatively small cost
that depended linearly on the number of algorithmic pre-screenings. We also assumed
that both timely treatments and delayed or missed treatments had relatively large costs
that, on average, depended linearly on the number of individuals. Given our focus on
the ability of algorithmic pre-screening to reduce human screening of patients with
normal cardiac function, we assumed that expert screening had an intermediate cost
that depended non-linearly on the number of screenings as well on as the infrastructure
and capacity of the healthcare system. Of course, treatment costs are also non-linear in
the number of treated patients for similar reasons, but non-urgent treatments can
arguably utilize the capacity of the broader healthcare system. Screening far below the
capacity of the healthcare system was inefficient and incurred a low total cost but high
average cost. Screening above the capacity of the healthcare system was also inefficient
and incurred both a high average cost and a high total cost. April 5, 2023
8/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint Therefore, we introduced the following cost-based evaluation metric for identifying
clinical outcomes. We defined the total cost of diagnosis and treatment with algorithmic
pre-screening as ctotal
outcome = falgorithm(npatients)
+ fexpert(nTP + nFP, npatients)
+ ftreatment(nTP)
+ ferror(nFN), (2) where Table 4 defines a two-by-two confusion matrix N = [nij] for the clinical outcome
abnormal and normal classes, npatients = nTP + nFP + nFN + nTN is the total number of
patients, and falgorithm, fexpert, ftreatment, ferror are defined below.
Again, for simplicity, we assumed that the costs for algorithmic pre-screening, timely
treatments, and missed or late treatments were linear. We defined falgorithm(s) = 10s
(3) as the total cost of s pre-screenings by an algorithm, ftreatment(s) = 10000s
(4) as the total cost of s treatments, and ferror(s) = 50000s
(5) as the total cost of s missed or delayed treatments.
To better capture the potential benefits of algorithmic pre-screening, we assumed
that the cost for expert screening was non-linear. We defined fexpert(s, t) =

25 + 397s t −1718s2 t2 + 11296s4 t4 
t
(6) as the total cost of s screenings by a human expert out of a population of t patients so
that
gexpert(x) = 25 + 397x −1718x2 + 11296x4
(7) was the mean cost of screenings by a human expert when x = s/t of the patient cohort
receives expert screenings; this reparameterization of (6) allowed us to compare
algorithms on datasets with different numbers of patients. We designed (6) and (7) so
that the mean cost of an expert screening was lowest when only 25% of the patient
cohort received expert screenings but higher when screening below and above the
capacity of the system. Fig 3 and Fig 4 show these costs across different patient cohort
and screening sizes, and the Appendix provides a fuller derivation of (6) and (7).
To compare costs for databases with different numbers of patients, e.g., the training,
validation, and test databases, we defined the mean per-patient cost of diagnosis and
treatment with algorithmic pre-screening as coutcome = ctotal
outcome npatients
.
(8) We used (8) to rank the Challenge algorithms for the murmur detection task. The
team with the lowest value of (8) won this task. April 5, 2023
9/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint 0.00
0.25
0.50
0.75
1.00 Fraction x of patient cohort
receiving expert screenings 0 2000 4000 6000 8000 10000 Mean per-patient
expert screening cost
gexpert(x) Fig 3. The expert screening cost gexpert(x) defined for the Challenge: mean cost for
screening a fraction x of a patient cohort for cardiac abnormalities. Mean per-patient
expert screening cost gexpert(x), i.e., the total expert screening cost for a patient cohort
divided by the number of patients in the cohort. Challenge Results Challenge Entries We received 779 entries from 87 teams throughout the course of the 2022 PhysioNet
Challenge, resulting in 77 submitted CinC abstracts, 62 accepted CinC abstracts, 43
published CinC proceedings papers, and 53 final codebases from 53 different teams.
These entries represent a diversity of approaches to the Challenge. A total of 81
teams submitted 293 entries during the unofficial phase of the Challenge, and a total of
63 teams submitted 486 entries during the official phase of the Challenge. Of the 779
entries, we received 652 entries from 75 teams that were implemented in Python, and
127 entries from 17 teams that were implemented in MATLAB. We received 519 entries
from 60 teams that requested a graphics processing unit (GPU) for their entries, and
260 entries from 49 teams requested only a central processing unit (CPU), i.e., no GPU.
In total, we received 473 successful entries that we were able to train on the public
training set and evaluate on the hidden validation set and 306 entries that we were
unable to train on the training set and/or evaluate on the validation validation set due
to various errors in the submitted code.
A total of 58 teams had a successful entry during the official phase that we were able
to train on the training set and evaluate on the validation set. Each team with a
successful entry during the official phase selected a single entry for evaluation on the
test set. Most teams chose their best-scoring entry from the official phase, but some
teams selected their most recent entry or another entry because the entry with the best
score on the validation set may not be the entry with the best score on the test set.
In some cases, a team’s best-scoring entry for the murmur detection task was
different from the team’s best-scoring entry for the clinical outcome identification task
because of the differences between the tasks and the evaluation metrics for grading
them; in these cases, we chose a different entry for each task. However, since the teams
could implement different approaches for each task, and since they could have
submitted the different approaches as part of the same entry, we did not distinguish
between teams and entries that submitted their best scoring entries in the same code
submission or different code submissions. We will share the best scoring entries for both
whether they were part of the same or different code submissions. April 5, 2023
10/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint 0.00
0.25
0.50
0.75
1.00 Fraction x of patient cohort
receiving expert screenings 0 2000 4000 6000 8000 10000 Mean per-screening
expert screening cost
gexpert(x)/x Fig 4. The expert screening cost gexpert(x) defined for the Challenge: mean cost for
screening a fraction x of a patient cohort for cardiac abnormalities. Mean per-screening
expert screening cost gexpert(x)/x, i.e., the total expert screening cost for a patient
cohort divided by the number of patients in the cohort and the fraction of screenings in
the cohort. Of the 58 teams with a successful entry during the official phase, 53 teams had code
that we were able to score on the training, validation, and test sets for both the murmur
detection and outcome identification tasks, resulting in 53 working entries to the
Challenge.
Fig 5 compares the performance of the working Challenge entries on the murmur
detection and clinical outcome identification tasks. Fig 6 and Fig 7 compare the
Challenge weighted accuracy and cost metrics with traditional scoring metrics,
respectively, including area under the receiver operating characteristic curve (AUROC),
area under the precision recall curve (AUPRC), the macro-averaged F-measure, and the
traditional accuracy metric.
To be officially ranked, we also required the teams to have successful unofficial and
official phase entries, have an accepted CinC abstract, publicly share their CinC
proceedings paper preprint by the CinC preprint submission deadline, have an accepted
final CinC proceedings paper by the CinC final paper deadline, and license their code
under an open source or similar license. We also allowed teams without a successful
unofficial phase entry and/or accepted CinC abstract who had a successful, high-scoring
official phase entry to submit a CinC abstract after the original abstract submission
deadline as a “wild card” team. The abstract and proceedings paper was subject to the
same review as the other participants.
We imposed these requirements in addition to the requirement for working code to
support the dissemination of research into the research community. Of the 53 working
entries, a total of 40 teams were officially ranked. Tables 5 and 6 summarize the
traditional and Challenge evaluation metrics for the officially ranked entries for the
murmur detection and clinical outcome identification tasks, respectively, on the hidden
test set.
To assess the robustness of the algorithms, we also trained them on a subset of the
training set with permuted labels and ran the retrained models on the validation and
test sets with the original labels; we did not change any of the publicly available
training data or labels but simply the size of the training set and the relationship
between the data and the labels in the training set. Algorithms that truly learned from
the training data performed much worse when retrained on the modified training set, April 5, 2023
11/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint 0.2
0.4
0.6
0.8 Weighted Accuracy 15000 20000 25000 Cost ρ = −0.62 Oﬃcial entry
Unoﬃcial entry
Best ﬁt Fig 5. Weighted accuracy metric (1) for the murmur detection task (x-axis) and the
cost metric (8) scores for the clinical outcome identification task (y-axis) of the final
Challenge entries on the hidden test set. Each point shows an entry, and the shading of
each point shows whether the entry was an official entry (dark red points) that satisfied
all of the Challenge rules or an unofficial entry (light red points) that did not. The
Spearman correlation coefficient ρ between the scores is given in the plot, and a line of
best fit (gray dotted line) is given by a robust linear regression. while algorithms that did not learn from the training set performed the same or better.
Only 35 of the 53 working entries were robust enough for us to complete this process.
The remaining 18 working entries either crashed or achieving the same or a better score
when retrained on the modified training set. While we encouraged teams to submit
robust code, we did not inform teams a priori how we would test the robustness of their
algorithms to avoid “gaming” the robustness criteria. Therefore, since we did not
provide advance notice of the exact requirements for robustness, we did not disqualify
teams whose algorithms failed this test. Voting algorithms While we ranked the official entries for the Challenge, we also recognized that entries
with lower overall performance could achieve higher performance than the top-ranked
entries on certain patient subgroups. Therefore, an algorithm that can learn the
different strengths of different algorithms can improve on the overall performance of the
individual entries. We developed several voting algorithms to leverage the diversity of
Challenge entries.
For the voting algorithms, we considered the discrete classifier outputs (see Tables 3
and 4) from the 39 algorithms from teams that were officially ranked for both the
murmur detection and clinical outcome tasks (see Tables 5 and 6). We considered these
tasks separately, and we used the relevant Challenge scoring metric to evaluate the
voting algorithms for each task: the weighted accuracy metric (1) for the murmur
detection task and the cost metric (8) for the clinical outcome identification task. We
used the common gradient-boosting trees (GBT) and random forests (RF) models for
the voting algorithms [15, 16, 17].
We developed and evaluated the GBT and RF voting models using the following
procedure. First, we trained the GBT and RF voting models on the k = 1, 2, . . . , 39
best-performing teams on the training set. Next, we chose the value of k that achieved
the best performance on the validation set; this step identified k = 14 for the GBT April 5, 2023
12/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint 0
0.25 0.5 0.75
1 AUROC 0 0.25 0.5 0.75 1 Weighted Accuracy ρ = 0.74 0
0.25 0.5 0.75
1 AUPRC ρ = 0.72 0
0.25 0.5 0.75
1 F-measure ρ = 0.89 0
0.25 0.5 0.75
1 Accuracy ρ = 0.65 Oﬃcial entry
Unoﬃcial entry
Best ﬁt Fig 6. Traditional evaluation metric (x-axis) and weighted accuracy metric (y-axis)
scores for the final Challenge entries with the murmur detection task on the hidden test
set. AUROC is area under the receiving operating character curve, AUPRC is area
under the precision-recall curve, F-measure is the macro-averaged F-measure, and
accuracy is the traditional accuracy metric. Each point shows an entry, and the shading
of each point shows whether the entry was an official entry (dark red points) that
satisfied all of the Challenge rules or an unofficial entry (light red points) that did not.
The Spearman correlation coefficients ρ between the metrics are given in the individual
plots, and a line of best fit (gray dotted line) is given by a robust linear regression. models and k = 2 for the RF models on the murmur detection task and k = 14 for the
GBT models and k = 4 for the RF models on the clinical outcome identification task.
Finally, we evaluated the resulting models on the test set.
The GBT and RF voting models performed slightly better than the highest-ranked
entry for the murmur detection task (weighted accuracy metric of 0.790 and 0.789,
respectively vs. 0.780; see Table 5) and performed slightly worse than the highest-ranked
entries for the clinical outcome identification task (cost of 11357 and 11687, respectively
vs. 11144; see Table 6). In each case, the voting algorithms had comparable performance
to the highest ranked individual entry, but they did not significantly outperform them
in either task by any the traditional or novel scoring metrics that we considered. Discussion Cardiac auscultation is one of the most cost-effective tools for helping clinicians to
identify heart murmurs, and the CirCor Digiscope dataset enriches our understanding
cardiac auscultation within resource-constrained environment. However, despite the
novelty and value of this dataset, it, like every dataset, has limitations.
No ages were available for the pregnant individuals in the CirCor DigiScope dataset.
It was unclear to the teams if the pregnant individuals belonged to the pediatric age
group of the rest of the patients, or if they had a different set of exclusion criteria from
the other patients, potentially limiting the performance and appropriateness of models
that use this dataset on pregnant individuals.
There was a single human annotator for labeling the heart murmurs; we did not have
information about the number of human annotators for labeling the clinical outcomes of
abnormal or normal heart function. We expect disagreements between annotators, and
several annotators are often needed to produce a single, consistent annotation of a
health record. Even several annotators may produce disparate annotations [58]. It is
likely that different or more annotators would result in different annotations.
Some heart murmurs are pathological, indicating a physiological problem that April 5, 2023
13/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint 0
0.25 0.5 0.75
1 AUROC 10000 15000 20000 25000 Cost ρ = −0.41 0
0.25 0.5 0.75
1 AUPRC ρ = −0.34 0
0.25 0.5 0.75
1 F-measure ρ = −0.06 0
0.25 0.5 0.75
1 Accuracy ρ = −0.35 Oﬃcial entry
Unoﬃcial entry
Best ﬁt Fig 7. Traditional evaluation metric (x-axis) and cost metric (y-axis) scores for the
final Challenge entries with the clinical outcome identification task on the hidden test
set. AUROC is area under the receiving operating character curve, AUPRC is area
under the precision-recall curve, F-measure is the traditional F-measure, and accuracy
is the traditional accuracy metric. Each point shows an entry, and the shading of each
point shows whether the entry was an official entry (dark red points) that satisfied all of
the Challenge rules or an unofficial entry (light red points) that did not. The Spearman
correlation coefficients ρ between the metrics are given in the individual plots, and a
line of best fit (gray dotted line) is given by a robust linear regression. requires monitoring and/or intervention. Other heart murmurs are innocent. The
human annotator did not identify which heart murmurs were pathological or innocent;
the full examination is the only evidence in the dataset that helps to characterize the
severity of any of the heart murmurs. We only evaluated the ability of algorithms to
identify heart murmurs, and not their ability to identify pathological heart murmurs,
but such as task is still valuable for screening.
The definition of a task and the choice of evaluation metrics for quantifying an
algorithm’s performance for the task affects the actual and perceived clinical relevance
of the algorithms [14]. The definitions of the Challenge evaluation metrics (1) and (8)
are attempts to make the evaluation metrics, and therefore the algorithms developed for
these metrics, more clinically relevant. The correlation in performance between the
traditional and Challenge metrics demonstrate that methods that perform better by one
metric tended to perform better by another (at least for the murmur detection task),
but the best-scoring method by one metric was often different from the best
best-scoring method by another metric, motivating the careful consideration of metrics;
see Fig 6 and Fig 7. We also recognize that these metrics are imperfect descriptions of
clinical needs; in particular, healthcare cost can be a poor proxy for health needs [13].
The relatively poor performance of the methods on the clinical outcome
identification task by all metrics suggests the difficulty of performing this task using the
PCG recordings alone; we note that echocardiography is a standard diagnostic tool for
assessing cardiac function, which is a more expensive and less accessible modality than
phonocardiography.
The voting algorithms had only modest performance improvements, if any, over the
individual algorithms. Indeed, the voting algorithms did not use any data-derived
features, which could help to provide context by associating different algorithms with
different patient subgroups. The features for the voting algorithms included the top k
algorithms, even when including a lower-ranked entry with a worse overall score or
excluding a higher-ranked entry with a better overall score would improve the
performance of the voting algorithm. This experiment only considered GBT and RF
models, but other types of models could potentially achieve better performance. The April 5, 2023
14/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint training procedure for the GBT and RF models did not use hyperparameter
optimization (beyond the selection of the number k of individual algorithms), limiting
their performance as well. Also, unlike the Challenge teams, we had access to the test
set, but, like the Challenge teams, our formal training procedure did not use it. Despite
these limitations, the ability of the voting algorithms to slightly improve on the
performance on the top-ranked algorithms for the murmur detection task, but not the
clinical outcome identification task, also suggests the differences in difficulty between
the two related tasks considered by the Challenge. Conclusions This year’s Challenge explored the potential for algorithmic pre-screening for heart
murmurs and abnormal cardiac function in resource-constrained environments. We
invited the Challenge participants to design working, open-source algorithms for
identifying heart murmurs and clinical outcomes from PCG recordings, resulting in 53
working implementations of different algorithmic approaches for interpreting PCGs. A
voting approach to combining the diverse approaches resulted in a superior performance
over the individual algorithms for murmur detection but not for clinical outcome
identification.
By reducing human screening of patients with normal cardiac function, algorithms
can lower healthcare costs and increase the effective screening capacity for patients with
abnormal cardiac function. The cost function proposed in this Challenge could be the
basis for cost-effective screening that balances both the financial and health burden of
correctly or incorrectly classifying patients. However, it will be important to optimize
the proposed cost function for a given healthcare system or population, since disease
prevalence, financial resources, and healthcare resources can differ enormously in
different settings. Acknowledgements This research is supported by the National Institute of General Medical Sciences
(NIGMS) and the National Institute of Biomedical Imaging and Bioengineering (NIBIB)
under NIH grant numbers 2R01GM104987-09 and R01EB030362 respectively, the
National Center for Advancing Translational Sciences of the National Institutes of
Health under Award Number UL1TR002378, as well as the Gordon and Betty Moore
Foundation and MathWorks under unrestricted gifts. GC has financial interests in
AliveCor, LifeBell AI and Mindchild Medical. GC also holds a board position in
LifeBell AI and Mindchild Medical. AE receives financial support through grant
PID2021-122727OB-I00 funded by MCIN/AEI/10.13039/501100011033 and “ERDF A
way of making Europe” and by the Basque Government under Grant IT1717-22. FR and
MC receive financial support by National Funds through the Portuguese funding agency,
FCT - Funda¸
c˜
ao para a Ciˆ
encia e a Tecnologia, within project UIDB/50014/2020. None
of the aforementioned entities influenced the design of the Challenge or provided data
for the Challenge. The content of this manuscript is solely the responsibility of the
authors and does not necessarily represent the official views of the above entities. Additional scoring metrics We defined additional scoring metrics to allow us to make more direct comparisons
between methods and tasks. April 5, 2023
15/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint In particular, we defined a weighted accuracy metric for the clinical outcome
identification task as aoutcome =
5nTP + nTN 5(nTP + nFN) + (nFP + nTN),
(9) where Table 4 defines a two-by-two confusion matrix N = [nij] for the clinical outcome
abnormal and normal classes.
We defined the total cost of diagnosis and treatment with algorithmic pre-screening
of murmurs as ctotal
outcome = falgorithm(npatients)
+ fexpert(oPA + oPN + oUA + oUN, npatients)
+ ftreatment(oPA + oUA)
+ ferror(oAA), (10) where Table 7 defines a three-by-two confusion matrix O = [oij] for the clinical outcome
abnormal and normal classes, npatients is the total number of patients, and falgorithm,
fexpert, ftreatment, ferror are defined above. Mathematical derivation of the cost-based scoring
metric We defined the cost of expert screening to reflect the non-linear costs associated with a
limited screening capacity of healthcare systems. While providing fewer screenings
incurs a lower total screening cost, the screenings are typically more expensive on a
per-screening basis because of the wasted capacity of the system. Similarly, while more
screenings incur higher costs, the screenings are also typically more expensive on a
per-screening basis because of the inadequate capacity of the system.
Let s be the number of expert screenings in a patient cohort of t patients, and let
x = s/t be the fraction of the cohort receiving expert screenings. We defined
gexpert(x) = a + bx + cx2 + dx4 as the mean expert screening cost for screening a fraction
x of a cohort, and we in turn defined fexpert(s, t) = gexpert(s/t)t = at + bs + cs2 t + ds4 t3 as
the total cost for s expert screenings in a cohort of t patients. These quantity are
quartic functions with four unknowns, allowing us to satisfy four criteria: 1. We set gexpert(0) = 25 to define a cost for maintaining the ability to perform
expert screening incurs a cost, even when screening x = 0 of a cohort, i.e.,
screening none of the cohort. 2. We set
d dx gexpert(x)/x |x= 1 4 = 0 so that mean expert screening cost cost achieved
its minimum when screening x = 1 4 of a cohort, which was roughly half of the
prevalence rate of abnormal cases in the database, 3. We set gexpert( 1 2) = 1000 so that the mean expert screening cost was $1000 when
screening x = 1 2 of a cohort, which is roughly the prevalence rate of abnormal
cases in the database. 4. We set gexpert(1) = 10000 so that the mean expert screening cost was $10000
when screening x = 1, i.e., screening all of the cohort, which is ten times the cost
of screening half of the database. The unique coefficients that satisfy these conditions are a = 25, b = 397, c = −1718,
and d = 11296. April 5, 2023
16/24 . 
CC-BY-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.08.11.22278688
doi: 
medRxiv preprint",1
"Background: Risk of suicide is complex and often a result of multiple interacting factors. It is 
vital research identifies predictors of suicide to provide a strong evidence base for targeted 
interventions. Methods: Using linked Census and population level mortality data we estimated rates of 
suicide across different groups in England and Wales and examine which factors are 
independently associated with the risk of suicide. Findings:  The highest rates of suicide were amongst those who reported an impairment 
affecting their day-to-day activities, those who were long term unemployed or never had 
worked, or those who were single or separated. Rates of suicide were highest in the White 
and Mixed/multiple ethnic groups compared to other ethnicities, and in people who reported 
a religious affiliation compared with those who had no religion. Comparison of minimally 
adjusted models (predictor, sex and age) with fully-adjusted models (sex, age, ethnicity, 
region, partnership status, religious affiliation, day-to-day impairments, armed forces 
membership and socioeconomic status) identified key predictors which remain important risk 
factors after accounting for other characteristics; day-to-day impairments were still found to 
increase the incidence of suicide relative to those whose activities were not impaired after 
adjusting for employment status. Overall, rates of suicide were higher in men compared to 
females across all ages, with the highest rates in 40-to-50-year-olds. Interpretation: The findings of this work provide novel population level insights into the risk 
of suicide by sociodemographic characteristics. Understanding the interaction between key 
risk factors for suicide has important implications for national suicide prevention strategies. Funding:  This study received no specific funding. Research in context 
Evidence before this study: Previous studies have identified key risk factors for suicide; 
being male and being aged 40 to 50 years of age have the highest rates of suicide. Suicide 
is a major public health concern, with prevention strategies imperative to minimising events. Added value of this study: For the first time we make population level estimates of suicide 
rates in England and Wales using death registration data linked to 2011 Census. 
Furthermore, we calculate incidence rate ratios for fully adjusted models which provide novel 
insights into the interplay between different risk factors. For instance, we see that people 
who report having day-to-day impairments risk of suicide is 2- to 3-times higher for men and 
women respectively compared to people who do not report day-to-day impairments, after . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint NOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice. adjusting for other characteristics such as socioeconomic status which are likely associated 
with impairments. Implications of this study: Understanding the groups most at risk of suicide is imperative 
for national suicide prevention strategies. This work provides novel population level insights 
into the risk of suicide by sociodemographic characteristics. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint Introduction Suicide is a major public health concern and a leading cause of death globally, responsible 
for more than 700,000 deaths each year [1]. In England, there are over 5000 suicides 
annually [2] and in 2012 a National Suicide Prevention strategy was published for the first 
time, with key areas of action including reducing the risk of suicide among high-risk groups 
and supporting research, data collection and monitoring of suicide in England [3]. The 
strategy, updated in 2017, emphasised the importance that quality data and linkage of 
sources are utilised to provide the best estimates of at-risk groups who should be targeted 
for intervention. In 2021 the World Health Organization (WHO) called for improved 
monitoring of suicide to support development and implementation of effective suicide 
prevention strategies [4]. The development of suicide risk is complex and often includes a 
combination of biological, psychological, clinical, social, and environmental risk factors [5] . 
Previous research has highlighted that sociodemographic risk factors for suicide include 
being male, being middle- and old-age, and belonging to a low socioeconomic group [2], [6], 
[7]. However, few studies have investigated sociodemographic risk factors for suicide in 
England, and none with population level data which can be used to account for confounding 
differences between at risk groups. In the current work we use a novel linkage of 2011 
Census and population level mortality data and modelling to assess which risk factors are 
important predictors of suicide. Methods Study Data and Population 
We used Census 2011 for England and Wales and Mortality data linked by NHS number. To 
obtain NHS numbers, the 2011 census was linked to the 2011-13 NHS Patient Registers. A 
total of 50,189,388 individuals who were valid Census respondents and could be linked were 
included in our sample. Of these, 50,189,220 were either alive at the end of study (31st 
December 2021) or died between 28th March 2011 and end of study. Our final sample 
comprised of 35,136,916 people were between the ages 18 and 74 years on census day 
[Supplementary table 1]. We included adults up to age 74 years on 2011 Census day as 
National Statistics Socio-economic Classification (NS-SEC) was constructed to differentiate 
positions within labour markets in people aged 16 to 74. In England and Wales, when someone dies unexpectedly, a coroner investigates to 
establish the cause of death through an inquest, resulting in a registration delay. For deaths 
caused by suicide, this generally means that around half of the deaths registered each year 
will have occurred in the previous year or earlier. To reduce the potential bias by this 
registration delay we used deaths occurring up to 31st December 2021 and registered by 
31st December 2022. Exposures  
We investigated a range of potential sociodemographic factors likely to be associated with 
the risk of suicide, selected based on the existing literature [5], [8], and data availability. 
Exposure variables explored in this analysis were sex, age (as a natural spline with 
boundary knots at the 1st and 99th percentile and four interior knots), ethnicity, marital status, 
day-to-day impairments, religion, region, and NS-SEC [Table 1]. All exposures were self-
reported from 2011 Census and age was calculated on census day. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint Outcome 
Our primary outcome was death due to suicide at any time during the study follow-up period 
(28th March 2011 to 31 December 2021). Suicide was defined as deaths from intentional 
self-harm for persons aged 10 years and over, and deaths caused by injury or poisoning 
where the intent was undetermined for those aged 15 years and over [2]. International 
Classification of Diseases, Tenth Revision (ICD-10) codes corresponding to intentional self-
harm (X60-X84) and injury/poisoning of undetermined intent (Y10-Y34) were used. Statistical analysis 
Descriptive statistics of population characteristics were presented stratified by all outcomes 
(alive, death by suicide, death due to all other causes). To model the association between 
the risk of suicide and demographic and socioeconomic characteristics, we fitted generalised 
linear models with a Poisson link function, with death by suicide being the outcome of 
interest. The natural logarithm of exposure time was included in the models as an offset term 
to account for the different time-at-risk periods between individuals. First, for each exposure, 
to estimate the difference in the rate of suicide, we fitted models adjusted for age and sex, 
with sex being interacted with age and the exposure of interest. The number of interior knots 
on the age spline was determined using the Bayesian Information Criterion (BIC). To 
estimate rates of suicide per 100,000 people for each level of the exposure, by sex for the 
average age, we calculated marginal means using the model with the lowest BIC. Estimated 
rates of suicide are reported per 100,000 people over the 10-year study period.  Second, to 
understand the different risk across the life course, we tested for an interaction between the 
factor of interest and age. We used the BIC to assess model fit between the models. Finally, to assess how each factor is independently associated with the risk of suicide, we 
fitted fully adjusted models. For each exposure, we compared the incident rate ratios (IRRs) 
from the fully adjusted model to those from the age and sex adjusted model. For both 
minimally and fully adjusted models, sex was interacted with the exposure of interest, and an 
age-sex interaction was included. IRRs were calculated separately for each exposure for 
males and females. Follow-up time was calculated as the time from 27th March 2011 (Census day) to date of 
death or end of study (31st December 2021), whichever was earlier. All statistical analyses 
were performed using R version 3.5.1. Results Estimated rates of suicide per 100,000 people  
Our study population consisted of 35,136,917 people; there were 35,928 suicides in our 
study period, with 73.9% of these suicides occurring in men [Supplementary Table 2]. 
Estimated rates of suicide were higher in men (19.8 per 100,000 people, 95% confidence 
interval (CI):19.3-20.2) compared with women (6.5 per 100,000 people, 95%CI:6.2-6.7), with 
the highest in males aged 40-to-50-years [Figure 1]. In women, the rates of suicide were 
highest in those aged 45-to-50-years but remained lower than men across all age groups. For all exposures, except day-to-day impairments, the model without interaction between the 
exposure and age had a lower BIC than the model with the interaction. For day-to-day 
impairments, the fully interacted model (sex, age and day-to-day impairments) was the best 
fit. The estimated rates of suicide per 100,000 people for each exposure were estimated 
from the best model fit. Rates have been calculated for each level of the exposure, by sex 
for several ages (20, 30, 40, 50, 60 and 70 years of age) when the size of the denominator . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint population is greater than 30. For the armed forces variable rates were only estimated up to 
age 50 years due to the underlying population distribution. In addition, for each model the 
rate was estimated by sex for the average age in our study population (45-years-of-age). For NS-SEC, the highest rates of suicide were seen in people who had never worked and or 
were long-term unemployed (men: 37.1 per 100,000 people, 95%CI:35.1-39.3, women: 12.0 
per 100,000 people, 95% CI:11.0-13.1) [Table 2, Figure 2]. Those classified as having higher 
managerial, administrative, and professional occupations (Class 1.1) had the lowest rates 
(men: 12.6 per 100,000 people, 95% CI:11.6-13.7, women: 4.6 per 100,000 people, 
95%CI:4.0-5.2). For estimated rates of suicide by age for each characteristic see 
Supplementary Table 3. Estimated rates of suicide were highest in the White (men: 21.0 per 100,000 people, 
95%CI:20.6-21.5, women: 6.8 per 100,000 people, 95%CI:6.5-7.1) and Mixed/Multiple ethnic 
groups (men: 23.56 per 100,000 people, 95%CI:21.3-26.0, women: 9.6 per 100,000 people, 
95%CI:8.3-11.1). Estimated rates of suicide were lowest for the Arab group (men: 3.8 per 
100,000 people, 95%CI:2.3-6.0, women: 2.5 per 100,000 people, 95%CI:1.3-4.9) [Table 2]. For religion, the lowest rates of suicide were in the Muslim group (men: 5.1 per 100,000 
people, 95%CI:4.6-5.8, women: 2.2 per 100,000 people, 95%CI:1.8-2.6) (Figure 3). The 
rates of suicide were highest in the Buddhist group (men: 26.6 per 100,000 people, 
95%CI:22.8-31.1, women: 8.9 per 100,000 people, 95%CI:7.0-11.3) and religions classified 
as ""Other"" (men: 33.2 per 100,000 people, 95%CI:29.0-38.1, women: 13.7 per 100,000 
people, 95%CI:11.4-16.3) [Table 2]. For men and females, the rates of suicide were lower 
across the Muslim, Hindu, Jewish, Christian and Sikh groups compared to the group who do 
not have religious beliefs. For marital status, people who described themselves as in a partnership, which is either 
married or in a registered same-sex civil partnership, had the lowest rates of suicide (men: 
12.9 per 100,000 people, 95%CI:12.5-13.2, women 4.2 per 100,000 people, 95%CI:4.0-4.4) 
(Figure 4). This was when compared with people who described themselves as single, 
separated or partner deceased [Table 2]. Rates of suicide were lowest in London (men: 14.9 per 100,000 people, 95%CI:14.3-15.5, 
women: 5.2 per 100,000 people, 95%CI:4.8-5.5) and highest in the North East for men (24.0 
per 100,000 people, 95%CI:22.7-25.3) and in the South West for women (7.3 per 100,000 
people, 95%CI:6.8-7.8) [Table 2]. At the time of the 2011 Census, rates of suicide were 
lower among serving members of the armed forces, than non-members for both men and 
women [Table 2]. The rate was lower for women who were dependants of a serving member 
of the armed forces (3.0 per 100,000 people, 95%CI:1.8-5.0) compared with those who were 
not members or dependants of a member (6.5 per 100,000 people, 95%CI:6.2-6.7). 
Conversely, for men the opposite was found, with dependants of serving members of the 
armed forces having the highest rate compared with those who were current members or not 
members. People who had day-to-day impairments had much higher rates of suicide (men: 48.4 per 
100,000 people, 95%CI:48.4-50.4, women: 18.9 per 100,000 people, 95%CI:17.8-20.1) 
compared with non-disabled people (men: 15.9 per 100,00 people, 95%CI:15.5-16.3, 
women: 4.5 per 100,000 people, 95%CI:4.3-4.7) [Table 2, Figure 3]. We found the model 
which fully interacted age, sex and day-to-day impairments was the best fitting model; 
differences between those (both men and females) who reported day-to-day impairments . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint were greater in younger individuals, with the difference in rates between groups being less 
visible in older individuals [Figure 3]. The groups with the highest rates of suicide in men were those in middle age who had day-
to-day impairments, were unemployed, single, separated or part of a non-main religious 
group. Men aged 30-to-50-years with day-to-day impairments had the highest rate of suicide 
(Rates: 54.4 to 36.0 per 100,000 persons). The next most vulnerable group were men ages 
40-to-50-years who were long term unemployed or never worked (Rates: 32.8 to 35.8 per 
100,000 persons). For women, the rates of suicide were highest amongst disabled women 
aged 30 to 50 (Rates: 12.7 to 20.9 per 100,000 persons). The next highest risk group was 
females aged 40-to-50-years who identified with a non-main religious group (Rates: 12.6 to 
12.7 per 100,000 persons). Independent association of the different sociodemographic characteristics Comparison of IRRs between a fully adjusted model (age, sex, ethnicity, marital status, day-
to-day impairments, region, NS-SEC, religion, and member of the armed forces status) and 
individual minimally adjusted models (each exposure plus age and sex) allowed us to 
identify independent associations been risk factors and suicide. Comparison of IRRs found 
that the values were generally closer to 1 in the fully adjusted model. For instance, the rate 
of suicide in the Mixed/multiple ethnic females was 1.4 times higher (IRR=1.4, 
95%CI=1.1;1.6) compared to white females in the minimally adjusted model, however when 
accounting for other characteristics in the fully adjusted model the rate remained elevated, 
but the difference was closer to the reference group (IRR=1.3, 95%CI=1.1;1.5) [Table 3]. 
Interestingly, the IRRs for day-to-day activities being limited a lot compared to not limited 
(Men IRR=2.6, 95%CI=2.5;2.7, Women IRR=3.5, 95%CI=3.3-3.6), which is a marker of day-
to-day impairments, remained elevated in the fully adjusted model (Men IRR=2.1, 
95%CI=2.1;2.2, Women IRR=3.1, 95%CI=3.3;3.6 [Table 3]. The rate of suicide was highest for people in NS-SEC Class 8 who are long term 
unemployed (Men IRR=2.9, 95%CI=2.7;3.2, Women IRR=2.6, 95%CI=2.2;3.1) [Table 3]. We 
see a gradual increase in rates moving from analytical Class 1.1 to Class 8 highlighting that 
socio-economic status is an important indicator for magnitude of risk. When accounting for 
other differences the risk is significantly reduced, due to adjusting for other factors such as 
day-to-day impairments which are likely confounding (Men IRR=1.9, 95%CI=1.7;2.1, Women 
IRR=1.5, 95%CI=1.3;1.7) [Table 3]. After accounting for other factors, being Buddhist was 
associated with an elevated risk of suicide compared to having no religion (Men IRR=1.4, 
95%CI=1.2;1.6, Women IRR=1.3, 95%CI=1.0;1.7) [Table 3]. For regional differences, after 
accounting for other factors such as ethnicity and socioeconomic status which are likely key 
drivers of population variation, disparities between regions were reduced but still remained 
with all regions having higher rates than London [Table 3]. Discussion This study explored the sociodemographic factors associated with the risk of suicide among 
35 million adults between 2011 and 2021. To our knowledge, this is the first study to 
investigate sociodemographic factors associated with the risk of death by suicide in England 
and Wales using population level data. We found that several sociodemographic 
characteristics were strongly associated with the risk of suicide; being male, or being 40-to-
50-years-of-age were found to have the highest risk of suicide. Reporting day-to-day 
impairments and never having worked or being long term unemployed were also found to be . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint associated with an increased risk of suicide. Those belonging to the White or Mixed/Multiple 
ethnic groups had the highest rates compared to other ethnic categories. We find the rate of suicide are 2.5-3.5 times greater in men and women reporting day-to-day 
impairments than those who do not. The relative rates are higher in women reporting day-to-
day impairments versus those without compared to men, suggesting this group should be a 
target for intervention. Overall, we find that the relative risk of suicide remains significantly 
elevated in people who have day-to-day impairments even after adjusting for other factors, 
indicating that the risk is independently associated with impairments. Day-to-day 
impairments is likely capturing factors associated with multimorbidity, mental health and old 
age. The census question asked respondents if their day-to-day activities were limited 
because of a health problem or disability which has lasted or is expected to last more than 
12 months. Our findings show people whose day-to-day activities are limited either a lot or a 
little, relative to those who are not limited, had higher rates of suicide. Some research has 
found that severe health conditions are associated with a higher risk of suicide [10], [11]. We find never having worked or being long term unemployed was associated with the 
highest rates of suicide in both men and women. Socioeconomic position has been found to 
be an important risk factor for suicide[14]. In our minimally adjusted model, the IRRs were 
2.5-3 times higher for women and men who were long term unemployed or never worked 
compared to those who were in higher managerial, administrative, and professional 
occupations. When adjusting for other characteristics we see the relative risk is 1.5-1.8 times 
higher, suggesting that accounting for factors such as day-to-day impairments could be 
capturing some of this variability. It is an established finding that suicides are lower in serving members of the armed forces 
compared to members of the general population [15].It may be due to the “healthy worker 
effect”; people in employment generally, and especially in physically demanding jobs like the 
armed forces, are less likely than the general population to be ill or disabled. In the England, 
recent work has shown that veterans are at no greater risk of suicide than the general 
population [16]. Our sample will likely capture individuals who since the time of 2011 Census 
are now veterans, as well as those who remained serving members at the end of study. We find the rates are highest in men aged 40 to 50 years of age. Middle-aged and older men 
have been shown to have the highest rates of suicide [2], [7]. In June 2022 the UK 
Government urged men to talk about mental health and engage with support networks, with 
suicide still reported to be the biggest cause of death in men under the age of 50 [18]. 
Previous evidence has shown that men are less likely than women to seek help for mental 
health problems and to engage with mental health services [19]. Future work should 
investigate the relative risk for individuals whose gender identity is different from their sex 
assigned at birth as previous literature has shown the risk is increased in this group [20]. Research investigating ethnic differences in the risk of suicide has produced inconsistent 
results [21]. Few studies from the UK have examined differences in the risk of suicide by 
ethnicity, although recent research found that White and Mixed ethnic groups had a higher 
suicide risk than Asian and Black ethnic groups [22]. Our results support these findings with 
suicide rate being highest in the White group compared to other ethnicities. Interestingly, 
previous work has highlighted complex interactions with other risk factors, for example, 
suicide risk is increased when individuals from ethnic minority groups live in areas with a low 
proportion of people from the same ethnic group. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint Research has consistently found religion to be protective against suicide [23], [24], with 
social support shown to mediated the relationship between suicide ideation and attempts 
and religious affiliation. Research from Switzerland found suicide rates were highest among 
non-religious individuals [25]. In our study we find that the rates are lower among Muslim, 
Jewish, Sikh, Hindu and Christian groups compared to those who do not report having a 
religion. Conversely, for those who identify as Buddhist, or an alternative non-main religious 
group the rates are found to be higher than those who do not belong to a religious group. 
Several factors could be driving these differences in rates, such as differences in religious 
beliefs and behaviours between faiths, or unmeasurable differences between groups such 
as pre-existing predisposition to poor mental health. Reverse causality may also be at play, 
as it is possible that people in the UK are more likely to turn to Buddhism or other religions 
during times of distress. Religion is also a factor which is known to be changeable over time 
[27], with some groups being more likely to vary than others. Subsequent work should be 
conducted in the UK to investigate causal relationships between religious beliefs and suicide 
risk accounting for additional social and mental health factors. Overall people who are unmarried experience a higher risk for suicide than married 
individuals [28]. Given the importance of episodic factors in relationship status, our finding of 
suicide rates being lower in individuals who were widowed compared to those who were 
single could be explained by survival bias. Previous work has shown that bereavement is a 
known risk factor for suicide, however the time between bereavement and suicide is 
important, with risk of suicide highest in the first week after death of a spouse [29]. 
Subsequent work is required in England and Wales to assess bereavement as a time-
dependent covariate using population linked data. Strengths & limitations The major strengths of this study include the use of a large, nationally representative cohort 
of adults who lived in England and Wales in 2011. The size of the cohort enabled us to 
explore differences in the risk of suicide across a wide range of socio-demographic factors 
for suicide, whilst also adjusting for other factors. One of the main limitations of this work is we did not have any data on adverse life events, 
such as violence or abuse, bereavement, or job losses, which may be important factors 
affecting mental health and suicide risk [30]. In addition, we have no information on mental 
health conditions, which are likely to mediate the relationship between some 
sociodemographic characteristics and the risk of suicide. Previous research has shown that 
previous self-harm and suicide attempt are the most important predictors for subsequent 
suicide [31]. Future work should aim to link health data to administrative records to account 
for poor mental health prior to outcomes. In addition, our analysis identified 
sociodemographic risk factors at one point in time (2011 Census) and it would be beneficial 
to have longitudinal measurements of risk factors which are likely to change over time, such 
as day-to-day impairments, marital status, or socioeconomic position. Furthermore, it is important to highlight that our population sample does not capture certain 
potentially vulnerable groups of people, such as those who migrated or did not link to patient 
demographic service (PDS), as all individuals in our study were enumerated in Census 
2011. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 6, 2023. 
; 
https://doi.org/10.1101/2023.04.05.23288190
doi: 
medRxiv preprint Conclusion Our results show that suicide risk varies by sex and age, with males aged 40-to-50-years 
who are long term unemployed or have never worked, disabled or single having the highest 
rates. In women, we find a similar pattern, with the absolute rates being lower. Interestingly, 
the relative risk is greater for disabled people compared to non-disabled people for women, 
indicating that this group in particular should be targets of intervention. The current work 
provides novel population level insights into the groups with the highest rates and factors 
which are independently associated with suicide. Data sharing In accordance with NHS Digital’s Information Governance requirements, the study data 
cannot be shared. Code used in this study is available on GitHub.",1
"Introduction Oral language skills are associated with children’s later self-regulation and academic skills; in turn, self-regulation in early childhood predicts successful functioning later in life. The primary objective of this study is to evaluate the separate and combined effectiveness of an oral language intervention (ENRICH) and a self-regulation intervention (ENGAGE) with early childhood teachers and parents for children’s oral language, self-regulation, and academic functioning. Methods and Analysis The Best Start study (Kia Tīmata Pai in te reo Māori, the indigenous language of New Zealand) is a cluster randomized controlled trial with teachers and children in approximately 140 early childhood centres in New Zealand. Centres are randomly assigned to receive either oral language intervention only (ENRICH), self-regulation intervention only (ENGAGE), both interventions (ENRICH + ENGAGE), or an active control condition. Teachers’ and parents’ practices and children’s oral language and self-regulation development are assessed at baseline at age 1.5 years and every 9-months to age 5, and academic performance at age 6. Teacher-child interactions will also be videotaped each year in a subset of the centres. Children’s brain and behavior development and parent-child interactions will be assessed every 6 months to age 6 years in a sub-group of volunteers. Ethics and Dissemination The Best Start trial and the two sub-studies (Video Project; Brain and Behavior Development) have been approved by the University of Otago Health Ethics Committee (H20/116), and reviewed for cultural responsiveness by: the Ngāi Tahu Research Committee (University of Otago), the Māori Advisory Group (University of Auckland, Liggins Institute) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 3 and an internal cultural advisory group. Results will be disseminated in international and national peer-reviewed academic journals and communicated to local, national, and international organizations serving early childhood teachers, parents, and young children. Trial registration This trial is registered with the Australian New Zealand Clinical Trial Registry (ANZCTR) as ACTRN12621000845831. Strengths and Limitations of This Study • Strength: The size and longitudinal design of this cluster RCT across early childhood will enable the assessment of the singular and combined effects of teacher-led oral language and self-regulation interventions on the development of children’s oral language, self-regulation, and academic functioning. • Strength: The trial represents a partnership between a large provider of early childhood education (BestStart), an external implementation service (Methodist Mission Southern), and a consortium of academics (Emotion Regulation Aoetaroa/New Zealand) to provide a culturally responsive intervention with co- designed implementation with the early childhood education provider (BestStart); the use of a single national provider simplifies co-design and the implementation of the intervention. • Strength: The efficacy of the interventions is assessed at multiple levels: via teacher and parent reports, behavioral observations, and measures of brain development (EEG/ERP). • Limitation: Because of the large size of the main trial, those assessments are restricted to teacher- and parent-report instruments. • Limitation: The single-provider feature with BestStart restricts the ability to collect primary outcomes on children who leave this service provider during early childhood. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 4 Keywords: oral language; self-regulation; early childhood education; teacher-child 
interactions; parent-child interactions; brain development . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 5 Introduction Self-regulation comprises the ability to regulate one’s thoughts, feelings, and actions [1]. Self-regulation skills are developing rapidly in early childhood [2] and predict later academic functioning and life success [1, 3]. Clinically relevant difficulties in these skills predict negative outcomes across domains of functioning, including psychological and physical health across the lifespan [4]. Self-regulation difficulties are linked to later problems in academic, occupational, and socioemotional functioning [5-7]. Therefore, it is vital to design preventive measures that can be implemented during early childhood when these skills are rapidly developing. A successful play-based preventive intervention called ENGAGE (Enhancing Neurocognitive Growth with the Aid of Games and Exercise) [8, 9] aims to improve children’s cognitive, emotional, and behavioral self-regulation through children’s games (e.g., Simon Says) that are interpersonal in nature and teach a range of skills. ENGAGE leads to improvements in parent-rated behavior problems equivalent to a gold-standard parent-management programme (Triple P), with treatment gains maintained 12 months later [9]. In an intervention in 28 early childhood education centres (ECEs) in Auckland New Zealand, ENGAGE was demonstrated to lower levels of hyperactivity, aggression, and inattention across a cohort of 940 preschool children relative to a wait-list control period [10]. Self-regulation can be also be fostered by enhancing children’s oral language development [11]. The way adults talk with children during everyday activities (book- reading, mealtimes, play) advances children’s early language and cognitive development [12], which in turn enhances their self-regulation [13]. Book-sharing [14] and conversing about everyday experiences [15] are two evidence-based methods that improve children’s oral language skills. For instance, a combined reading-and-conversation program . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 6 developed in New Zealand with parents and teachers called Tender Shoots improves preschool children’s oral narrative, early literacy, and socioemotional skills up to one year after the intervention when compared to activity-based controls [16-19]. Critically, children’s academic skills at school entry predict their later academic and socioemotional functioning [20]. Oral language is developing rapidly in the toddler years. Therefore, oral language interventions at this age can produce stronger benefits than similar interventions at older ages [21], but such interventions are rare [13]. Previous oral language interventions have focused primarily on training either parents or teachers to engage in the interactions, but not both [22]. The few combined interventions that have been conducted indicate larger benefits for preschool children when both parents and teachers are using the techniques compared to home-only or school-only [23, 24]. We have developed a shared reading-and- conversation program for even younger children with both teachers and parents called ENRICH (Enhancing Rich Conversations; see Table 1 and Figure 1). Objective To determine if an intervention, over and above the usual early childhood curriculum, that targets development of oral language plus self-regulation (ENRICH plus ENGAGE) has greater benefits on the development of children’s oral language, self-regulation, and academic functioning than interventions that target either oral language (ENRICH), or self- regulation (ENGAGE). Methods and Analysis Study design and participants This study is an open-label, cluster designed four-armed randomized controlled trial (RCT) with children aged 13 to 30 months at outset, their parents, and their teachers at BestStart early childhood centres in New Zealand. ECEs are randomly assigned to the . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 7 ENRICH (Enhancing RICH Interactions) and/or ENGAGE (Enhancing Neurocognitive Growth with the Aid of Games and Exercise) interventions or an Active Control group; teacher and family participation in the study is voluntary at all stages. Interventions are implemented with a factorial design. Professional development with teachers and workshops with parents began when children were 13 to 30 months (baseline). Interventions will continue until children are 5 years of age, and children will be followed until they are 6 years of age (see Figures 1 and 2; supplemental file 1). The ENRICH intervention is book- and conversation-based to foster toddlers’ oral language and literacy skills, with ENRICH+ an advanced version for preschoolers (see Table 1 and supplemental file 1). The ENGAGE intervention is games- based to foster preschool children’s self-regulation skills (see Table 2 and supplemental file 1). Recall that oral language is a pathway to self-regulation [11]. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 8 Table 1. Books and Cards for ENRICH and ENRICH+ Interventions ENRICH (1.5 to 3 years) 
ENRICH+ (3 to 5 years) Phase 1 
Books ➢ The Noisy Book by Soledad Bravi ➢ Hoihoi Turituri transl. by Ruia Aperahama ➢ Pukeko Shoes by Janet Martin and Ivar Treskon ➢ The Little Red Hen by Paul Galdone 
➢ Cuddly Dudley by Jez Alborough 
➢ Five Little Monkeys Sitting in a Tree by Eileen Christelow Phase 1 
Cards ➢ Overview of ENRICH, Phase 1 
➢ Kai Time 
➢ Book Time 
➢ Nappy Time 
➢ Play Time 
➢ Group Time ➢ Overview of ENRICH+, Phase 1 
➢ Kai Time 
➢ Book Time 
➢ Greetings/Farewells 
➢ Play Time 
➢ Group Time Phase 2 
Books ➢ Mahi/Actions by Kitty Brown and Kirsten Parkinson ➢ Kare ā-roto/Feelings by Kitty Brown and Kirsten Parkinson ➢ Who’s Driving? by Leo Timmers ➢ Ma Wai e Hautu transl. by Karena Kelly ➢ The Gingerbread Man by Eric A. Kimmel ➢ Bunny Cakes by Rosemary Wells 
➢ Louie the Tui by Janet Martin 
➢ Elephant Island by Leo Timmers Phase 2 
Cards ➢ Overview of ENRICH, Phase 2 
➢ Kai Time 
➢ Book Time 
➢ Nappy/Toilet Time 
➢ Play Time 
➢ Group Time ➢ Overview of ENRICH+, Phase 2 
➢ Kai Time 
➢ Book Time 
➢ Greetings/Farewells 
➢ Play Time 
➢ Group Time Phase 3 
Books ➢ 3 Billy Goats Gruff by Paul Galdone 
➢ Slinky Malinki Early Bird by Lynley Dodd ➢ That’s Not a Hippopotamus! by Juliette MacIver and Sarah Davis ➢ Jumblebum by Chae Strathie and Ben Cort Phase 3 
Cards ➢ Overview of ENRICH+, Phase 3 
➢ Kai Time 
➢ Book Time 
➢ Greetings/Farewells 
➢ Play Time 
➢ Group Time . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 9 Table 2*. Examples of ENGAGE Games and Activities Targeted Domain 
ENGAGE Game or Activity (3 to 5 years) Emotional/Feelings 
➢ Relaxing Yoga 
➢ Deep Breathing 
➢ Drawing Cognitive/Thinking 
➢ Object Copy 
➢ Puzzles 
➢ Cups Memory 
➢ Beading 
➢ Snap 
➢ Card Memory Behavioral/Doing 
➢ Ball Games 
➢ Musical Statues 
➢ Animal Speeds 
➢ Skipping 
➢ Ball and Spoon Race 
➢ Simon Says 
➢ Hop Scotch *Adapted with permission from Healey and Healey (2019; Table 1) . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 10 ECEs, and participants, will be randomized into the following four arms: 1) Language only: ENRICH (1.5 to 3 years) and ENRICH+ (3 to 5 years) 2) Self-regulation only: ENGAGE (3 to 5 years) 3) Combined: ENRICH (1.5 to 3 years) and ENRICH+ (3 to 5 years) and ENGAGE (3 to 5 years) 4) Active Control: Curriculum as usual plus child development webinars The Best Start trial contains two nested sub-studies: 1) the Video Project: a video teacher- child interaction sub-study to measure implementation within 24 ECE centres randomly selected from participating centres in two large cities (Auckland: 12; Christchurch; 8), two small cities (Dunedin: 1; Invercargill: 2), and a rural region (1); and 2) the Brain and Behavior Development sub-study: a study on a subset of children (n= 237) who volunteered to participate in additional neurophysiological and behavioral testing at university laboratories in either Auckland or Christchurch, New Zealand. Teachers are being trained in the interventions using a “train the trainer” model. Firstly, teacher managers (called professional practice leaders) within the BestStart early childhood organisation are being trained online and in person in the implementation of the ENRICH and ENGAGE interventions by the research team. These professional practice leaders have at least 10 years of experience in early childhood education. The professional practice leaders subsequently conduct training workshops with the ECE teachers, again via online and in- personal delivery, who then implement the techniques with children in their centres. Professional development workshops for the toddler phase of ENRICH (from approximately 1.5 to 3 years of age) were delivered in 2021 and 2022. Professional development workshops for ENRICH+ and ENGAGE will take place in early 2023 when . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 11 study children are approximately 3 years old (i.e. at the start of the preschool phase, which extends from approximately 3 – 5 years of age). Professional practice leaders will receive booster training sessions approximately every 9 months throughout the course of the study; these leaders will then deliver booster training sessions to teachers. As part of the study, professional practice leaders are required to document the delivery of teacher workshops and follow-ups with centres to answer questions. Every time teachers in either ENRICH or ENGAGE conditions receive new training, parents of children in intervention conditions are invited to learn about the new techniques through online information evenings and recorded video links. Teachers in ENRICH centres were trained in 2021 and 2022 and provided with new resources for the toddler phase of the study (two sets of informational cards, 6 new books, and instructional videos; see Table 1). In the preschool phase of the study, teachers in both ENRICH+ and ENGAGE conditions will receive new training and resources (sets of informational cards and/or books and instructional videos) approximately every 9 months (see Table 1). Teachers in ENRICH centres are encouraged to use the intervention techniques as often as possible throughout the day during five routines: mealtimes, book times, diaper changes, play, and group time. Provided books are all commercially available, but contain conversation prompts on each page that have been specially designed and inserted for this study [18]. In ENRICH+, teachers will be encouraged to select two books per week to read three times each, plus to continue to have enriched conversations on a daily basis during greetings/farewells, mealtime, play time, and group time. Teachers in ENGAGE centres are encouraged to use the games for 30 minutes a day. In the toddler phase of the study (1.5 to 3 years), teachers in centers in the ENGAGE- only arm and the Active Control arm participated in two webinars on childhood nutrition, a . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 12 topic chosen in consultation with BestStart. In the preschool phase, teachers in the Active Control arm will also receive child development webinars every 9 months on children’s friendships, a topic chosen in consultation with BestStart. All webinars are recorded and delivered by Ph.D.-level child development experts for approximately 1 hour each including questions. Parents of children in the ENGAGE-only arm (toddler phase) and the active- control arm of the study are invited to these webinars or to watch recordings, at their leisure. Participants: Eligibility criteria, study setting, and consent process The Best Start | Kia Tīmata Pai trial is a collaboration with a large national early childhood education organisation in New Zealand called BestStart. In 2021 and 2022, managers from 140 ECE centres invited teachers and parents of all children in the target age range (originally from 17-24 months but changed to 13-30 months at enrolment) to consent to participate via an online link or paper forms at the centre (see supplemental file 3). There are no gender, ethnic, language, or socioeconomic restrictions to participation. All enrolled children (13 to 30 months at enrolment; M = 20.6 months; SD = 3.4) are being exposed to the interventions within their early childhood education centre. Data are being collected only on those children whose families granted consent. Implementation checks Intervention fidelity is being monitored: 1) via fortnightly teacher self-ratings of the frequency of their delivery of the techniques, and each study child’s engagement in the techniques; and 2) via videotaped teacher-child interactions once a year in the 24 centres in the Video Project (approximately 100 children and their teachers each year). These implementation checks will serve as an audit on trial conduct and are being overseen by Methodist Mission Southern, a co-investigator body that is independent of the sponsoring organization (ERANZ). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 13 Outcomes Primary. Our primary outcomes are children’s oral language, self-regulation, and early literacy (see Figure 1). In the toddler phase (1.5 years to 2.25 years), we are measuring increases in vocabulary size and syntax with the New Zealand Communicative Development Inventories (short forms) for parents and teachers [25] and increases in the effortful control subscale and decreases in the negative emotionality subscale of the Early Childhood Behavior Questionnaire [26]. In the preschool phase (3 to 5 years), we will measure increases in oral language (semantic, phonological, syntactic, and pragmatic) and literacy (letter recognition, print concepts, and writing) with the Teacher Rating of Oral Language and Literacy [27] and the adapted companion Parent Rating of Oral Language and Literacy [28]. We will measure increases in self-regulation outcomes from ages 3 to 5 with teacher and parent versions of the Child Behavior Rating Scale [29]. Finally, global teacher ratings will assess improvement of children’s oral language, literacy, self-regulation, and social skills from ages 3 to 6 [19]. Secondary. Children’s B4 school check at age 4.5 years will be accessed through the Ministry of Health to assess improvements in school readiness [30]. Medical practitioners rate children’s cognitive, physical, and socioemotional development. Children’s Integrated Data Infrastructure (IDI) will also be accessed at age 6 years and beyond to provide long- term data on improvements in education, health, and employment outcomes. The IDI is administrative data held by the New Zealand government on a range of outcomes. Parents and teachers are self-reporting on their early learning practices at each assessment wave. Teachers are also reporting on their proficiency in te reo Māori, an official language of New Zealand and a target focus of some of the ENRICH materials (see Table 1). Each year, approximately 100 children and their teachers in the Video Project classrooms are being videotaped for a total of 25 minutes (5 minutes per targeted routine). . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 14 The videotapes will be transcribed and then coded for the quantity and quality of speech by coders who are blind to centre condition. Two waves of video observation data were collected in 2021 and 2022, with two more waves planned in 2023 and 2024. Every 6 months, the children participating in the Brain and Behavior Development sub-study are being administered additional neurophysiological measures (EEG/ERP and eye-tracking), behavioral measures of executive functioning (working memory, inhibition, and categorization), oral language and literacy (vocabulary, story comprehension, phonological awareness, and letter recognition); and a parent-child interaction measure (see Figure 1 and supplemental file 2). The first wave of data is now complete, with the second and third waves ongoing, and a fourth wave planned to begin in 2023. Participant and public involvement A key feature of the intervention is to co-design implementation with BestStart professional practice leaders. Academics designed the two interventions, but the way the techniques are presented to teachers at each phase is being developed through a co-design process with BestStart leaders, facilitated by implementation leaders at Methodist Mission Southern. The objective is to build on teachers’ existing knowledge and practices to enhance uptake of ENRICH and ENGAGE techniques. BestStart leaders also inform the administration of the fortnightly implementation checks and assessments at each wave. Sample size calculation Based on previous oral language and self-regulation interventions with early childhood teachers [10, 13, 24, 31], we anticipate small to medium effect sizes. Power analyses of final- phase primary outcomes suggested a total sample size at baseline of approximately 1600 children (i.e., approximately 400 per group, including control group) for the current study to afford precision for estimates with alpha = 0.05 and power between .75 and .90. This sample size of 400 per group at baseline allows for attrition of children over the course of the study. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 15 This sample size also takes into account multiple predictor variables to allow for up to 8-way analyses within groups for some outcomes at time of final analysis (see supplemental file 1 for more detail). Recruitment Recruitment of children in the target age range began with a first cohort A from May to August 2021. Due to pandemic-related disruptions, recruitment continued with a second cohort B from March to June 2022 for a total of 1495 children and their parents and 1638 teachers enrolled in the study to date.  New teachers are being recruited, consented, and trained as they begin employment at BestStart (see supplemental files 1, 3). Allocation to conditions (randomization) The procedure for allocating centres to conditions was central randomization by computer using a 2 (ENRICH vs no ENRICH) x 2 (ENGAGE vs no ENGAGE) factorial design (see Figure 2). The method used to generate the sequence was permuted block randomization by early childhood centre. Data collection In the Best Start main trial, assessments are being administered approximately every 9 months between ages 1.5 and 5 years, and at age 6 years. Teachers and parents are asked to complete instruments to assess children’s oral language and literacy, self-regulation, and socioemotional skills (see Figure 1), which takes approximately 35 minutes each wave using REDCap (Research Electronic Data Capture) hosted at the University of Otago [32, 33]. Paper forms are also provided for participants who do not wish to complete the forms online. In the Video Project, naturally occurring classroom practices at 24 centres are videotaped by researchers every year to age 5 years, which takes approximately an hour in each centre at each assessment wave. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 16 In the Brain and Behavior Development sub-study, a subset of 237 children from the main trial are taking part in additional neurophysiological and behavioral assessments: EEG/ERP, eye-tracking, and behavioral measures of attention, emotion processing, categorization, cognitive flexibility, inhibitory control, and memory, as well as a parent-child interaction task (see Figure 1; supplemental file 2 for protocol). Data collection for the Best Start main trial, the Video Project sub-study, and the Brain and Behavior Development sub-study are targeted to end in March 2025. Incentives to complete assessments at each wave in the main trial are in the form of raffles at each wave for centres (of ten $250 gift cards) and parents (of ten $250 gift cards). Parents and children participating in the Brain and Behavior Development sub-study receive a $20 gift card and small gift at each wave. Data management and security Source and format of data: All raw data files including the reports of teachers and parents are saved as CSV files, and media files including the naturalistic observations from 24 BestStart centres are saved as MTS video files at the University of Otago. Data sharing protocol between centres and the university: Data from all sites are collected by the REDcap server or sent directly to the research manager, Dr. Tugce Bakir- Demir, at University of Otago. Hard copies of any paper forms are couriered to the University of Otago in a secure envelope via New Zealand Couriers. Data sharing agreements have been established between the University of Otago, University of Auckland, and Boston Children’s Hospital (BCH). Participants were informed and consented to the data sharing process (see supplemental file 3). Data storage, security, and access: Raw digital data are stored in REDCap, which is a secure, web-based software platform. Back-ups of the digital data are also on the University of Otago server. Any original non-digital data (including any paper forms and micro-SD . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 17 cards with video footage) are securely stored at the research laboratory within the university to which only a limited number of approved researchers in the trial have access. Raw electrophysiological (EEG/eye-tracking) data are stored encrypted on secure servers within the University of Auckland. Due to the nature of this electrophysiological data, it could not be wholly deidentified prior to processing as it contains audio and video recordings of participants, which are naturally identifying. Confidentiality All information that we collect is used only by the research team working on this study. Any raw data (including video data) and personal information will be retained in secure storage for at least 5 years after the end of the project, as required by the University of Otago's research policy, after which time it will be destroyed. The overall results of the project will be published and will be available in university libraries and public good databases, but each individual participant's information will remain anonymous and confidential. Statistical analyses For the primary outcomes of the main trial, we will analyze data only from participants who have remained in their original BestStart centre. All enrolled children will be included in analyses of the B4 School Check and IDI. We will use intent-to-treat analyses for the participants in the Brain and Behaviour Development sub-study, who will continue to be followed even if they leave their BestStart centre. Mixed modelling and moderator/mediator analyses will be conducted to assess children’s oral language and self-regulation outcomes as a function of intervention condition (nested within centres) and as a function of implementation fidelity, adjusting for confounders. Where appropriate, we will use multiple imputation to account for missing data . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 18 and run sensitivity analyses that compare the results for each cohort (A and B) with the results for the entire sample. Ethical Considerations The proposed study does not involve any medication or invasive procedures and as such risk to children and their families is minimal. The techniques we will teach are designed to be positive and beneficial for adults and children alike; however, conversations can sometimes become negative, and young children can vary in their engagement with new activities. We anticipate that any of these issues will be within the realm of what teachers normally deal with on an everyday basis, and in fact, the techniques are designed to help educators tackle discussions of everyday negative events in ways that will support children’s coping and emotion regulation, and in ways that will facilitate children’s participation in activities. Therefore, we have determined that there is no need for a formal data monitoring committee. The University of Otago Health Ethics Committee approved this study on November 23, 2020 (H20/116). The cultural responsiveness of project has also been approved twice by the Ngai Tāhu Research Advisory Committee, once in 2020 and again in 2022. The cultural appropriateness of assessments and intervention materials is continually monitored by our own Cultural Advisory Group who are drawn from academia, the community, and the BestStart provider (currently Amanda Clifford, Elizabeth Schaughency, Mele Taumoepeau, Karen Salmon, Pip Laufiso, Barbara Backshall, and Waveney Lord). Trial Registration Status The study was registered in the Australia/New Zealand Clinical Trial Registry (ACTRN12621000845831) on 22 April 2021, prior to the date of the first enrolment on 4 May 2021. Confirmation of trial registration was delayed (1 July 2021) due to a COVID- related backlog. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 19 Discussion The Best Start study was developed to promote oral language and self-regulation development in children from 1.5 years of age using interventions that include both teachers and parents. To the best of our knowledge, this study is the first RCT to test the singular and combined effects of oral language and self-regulation interventions on children from such a young age. Our findings will inform both theory and practice. With respect to theory, we will be able to test our hypothesis that oral language is a key driver of later self-regulation [11]. We will also test our hypothesis that combining oral language and self-regulation interventions will produce the greatest benefits for children’s self-regulation and later academic functioning. Finally, via the Brain and Behavior Development sub-study, we will test our hypothesis that the intervention benefits are mediated by neurophysiological changes in brain development. With respect to practice, our findings will inform the design of early childhood curricula in New Zealand and internationally. Critically, each intervention is designed so that it can be readily scaled up to reach larger numbers of early childhood teachers and children — within a short-time frame. Dissemination We will disseminate the findings widely via newsletters to early childhood centres and parents twice a year, in-person and online presentations at national and international conferences, publications in academic journals, blogs and podcasts for professional and trade organizations, oral and written communication to the New Zealand Ministry of Education, and a study website. . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 20 Supplemental Information • File 1: Original protocol for Main Trial from Ethics application submitted in October 2020 • File 2: Original protocol for Brain and Behavior Development sub-study submitted in February 2021 • File 3: Information sheets and consent forms for the Main Trial and Brain and Behavior Development sub-study Acknowledgments We acknowledge with gratitude all of the BestStart teachers, professional practice leaders, centre managers, and staff who are contributing to the study; and to Julia Errington-Smith, Grace Lam, and other support staff at Methodist Mission Southern. Most of all, we thank the parents and children for their participation. Author Contributions Conceptualization: Richie Poulton, Stuart McNaughton, Elaine Reese, Clair Edgeler, Jimmy McLauchlan, Elizabeth Schaughency, Mele Taumoepeau, Karen Salmon, Charles Nelson Data curation: Tugce Bakir-Demir, Hayley Guiney, Natasha Maruariki Formal analysis: Hayley Guiney, Jesse Kokaua, Tugce Bakir-Demir Funding acquisition: Richie Poulton Methodology: Richie Poulton, Elaine Reese, Jesse Kokaua, Charles Nelson, Elizabeth Schaughency, Mele Taumoepeau, Karen Salmon, Amanda Clifford, Ran Wei, Valentina Pergher, Sophia Amjad, Anita Trudgen Project administration: Jimmy McLauchlan, Natasha Maruariki, Richie Poulton, Elaine Reese, Tugce Bakir-Demir, Hayley Guiney, Justin O’Sullivan Resources: Elaine Reese, Elizabeth Schaughency, Mele Taumoepeau, Jimmy McLauchlan, Julia Errington Scott . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 21 Supervision of postgraduate students: Elaine Reese, Amanda Clifford, Elizabeth Schaughency, Mele Taumoepeau, Tugce Bakir-Demir, Hayley Guiney Writing – original draft: Elaine Reese, Tugce Bakir-Demir, Hayley Guiney Writing – review & editing: Elaine Reese, Richie Poulton, Jesse Kokaua, Hayley Guinea, Tugce Bakir-Demir, Jimmy McLauchlan, Justin O’Sullivan, Clair Edgeler, Elizabeth Schaughency, Mele Taumoepeau, Karen Salmon, Amanda Clifford, Natasha Maruariki, Stuart McNaughton, Peter Gluckman, Charles Nelson, Ran Wei, Valentina Pergher, Sophia Amjad, Anita Trudgen . 
CC-BY-NC-ND 4.0 International license
It is made available under a 
 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 
(which was not certified by peer review) The copyright holder for this preprint 
this version posted April 5, 2023. 
; 
https://doi.org/10.1101/2022.10.18.22281232
doi: 
medRxiv preprint 22",1
"This paper presents macroeconomic model that is based on parallels between macroeconomic multi-agent systems and multi-particle systems. We use risk ratings of economic agents as their coordinates on economic space. Aggregates of economic or financial variables like Investment, Assets, Demand, Credits and etc. of economic agents near point x define corresponding macroeconomic variables as functions of time t and coordinates x on economic space. Parallels between multi-agent and multi- particle systems on economic space allow describe transition from economic kinetic- like to economic hydrodynamic-like approximation and derive macroeconomic hydrodynamic-like equations on economic space. Economic or financial transactions between economic agents determine evolution of macroeconomic variables This paper describes local macroeconomic approximation that takes into account transactions between economic agents with coordinates near same point x on economic space only and describes interaction between macroeconomic variables by linear differential operators. For simple model of interaction between macroeconomic variables as Demand on Investment and Interest Rate we derive hydrodynamic-like equations in a closed form. For perturbations of these macroeconomic variables we derive macroeconomic wave equations. Macroeconomic waves on economic space can propagate with exponential growth of amplitude and cause irregular time fluctuations of macroeconomic variables or induce economic crises. Keywords: Econophysics, Macroeconomic Theory, Economic Space, Economic Hydrodynamic-like Equations, Economic Wave Equations PACS Code: 89.65.Gh 2 1.Introduction Econophysics during last decades made a lot for economic and financial modeling [1- 8]. Financial markets and price dynamics, market trends and crises forecasting, market strategies, risk and insurance assessment use methods and models of theoretical 
and 
statistical 
physics. 
Meanwhile 
deep 
distinctions 
and misunderstandings between languages of economics and finance on one hand and theoretical physics on other hand [9] prevent mutual beneficial development. In this paper we try to overcome at least part of these distinctions and develop macroeconomic theory based on notions of economics and finance with help of certain parallels to kinetics and hydrodynamics. We agree with statements [6] that direct applications of physical models and methods to economic and financial problems give no effect. Distinctions between economic and physical systems are so vital that physical methods should be completely reconstructed to be useful for economic modeling. We hope that variety of new problems that should be solved to establish economic theory in a rigorous form alike to current state of theoretical physics should be interesting to physicists. As well we do hope that our models may present new treatment of economic and financial problems and deliver benefits for econometrics, economic modeling and forecasting. Perfect treatments of existing problems in economic theories are presented in numerous works of leading economists as Morgenstern [10], Lucas [11], Lucas and Sargent [12], Sims [13], Blanchard [14], McCombie and Pike [15]. These papers should be ground for any econophysics study. We do hope that our model might respond some of them. We develop economic theory on base of two common economic notions: economic agents and risk ratings of economic agents. Agent-based models argue decision- making of economic agents, describe trading strategies, game theories, behavioral economics, equilibrium theory [16-21]. Economic agent is a general term that describes any participant of economic or financial relations like Companies and Firms, Banks and Exchanges, Privet Investors and Households and etc. It is assumed that economic agents take rational or non rational decisions, follow personal expectations, develop and maintain its market strategy and that should explain behavior of economic agents – Producers and Consumers, Investors and Borrowers and etc. 3 We avoid discussions about agent’s strategies and their decisions and propose regard economic agents as primary, simple units of macroeconomic alike to particles in kinetic models in physics. Let treat economic agents like simple economic particles that have many economic variables that describe Demand and Supply, Investment and Production Function of economic agents and so on. Such approach to agent-based models allows develop bridge between language of economic modeling and language of physics and develop parallels between description of economic multi-agent system and description of physical multi-particle system. All macroeconomic variables are composed by corresponding variables of economic agents. Macroeconomic Demand is determined by aggregation of Demand of separate economic agents. Value Added of economic agents [22] define GDP of entire economics. Macroeconomic Consumption, Investment, Credits and Profits – all macroeconomic variables are determined by corresponding variables of separate economic agents. That is alike to relations between physical properties of macro system and physical properties of particles that constitute this macro system. There is no need to repeat that properties of agent-based economic system and physical multi- particle system are completely different. We outline parallels between them only. To develop similarities between economic multi-agent system and multi-particle physical system one should define certain economic analog of space that can allow describe economic agents alike to multi-particle systems in physics. Moreover, such economic space should have origin and roots in economics and finance and adopt general economic relations and phenomena’s. Usage of space in economics usually refers to spatial economics [23, 24], but that approach is helpless. To develop parallels between multi-agent and multi-particle systems we introduce economic space notion [25, 26] that reflects internal economic properties and allows develop general frame to macroeconomic and macro finance modeling. Economic space approach gives new look on option pricing theory and we derived generalization of Black-Scholes-Merton equations on n-dimensional economic space [25, 26, 27]. In this paper we describe macroeconomic multi-agent systems on n-dimensional economic space. We develop parallels to kinetics and hydrodynamics nevertheless phenomena’s of economic agent systems have nothing common with nature of kinetics and hydrodynamics of multi-particle physical systems. We propose use risk ratings of economic agents as their coordinates on economic space. Risk ratings of economic agents are provided for decades by international 4 rating agencies as Fitch’s [28], S&P [29], Moody’s [30]. Risk agencies define risk ratings of huge corporations and banks. These ratings are used as measure of assets security, sustainability and helps take investment decisions, estimate market prices of assets and etc. Risk ratings take finite number of values or risk grades. Let propose that risk ratings can be measured for all economic agents. Let assume that risk grades can be discreet as it is now or can be continuous R. Let call discreet or continuous space of risk grades as economic space. Let propose that simultaneous assessment of n different risks determine coordinates of economic agents on n-dimensional economic space. These assumptions allow describe macroeconomics alike to description of multi-particle systems and derive hydrodynamic-like equations on economic space. Our macroeconomic model is pure theoretical as no econometric data required to verify predictions of our theory exist. Current risk ratings data are not sufficient develop macroeconomic models on economic space. We do hope that required enhancement of risk assessments, econometric observations and data performance can improve economic modeling, forecasting and management. The rest of the paper is organized as follows: in Section 2 we introduce economic space and discus related economic and physical problems. In Section 3 and 4 we present kinetic-like and hydrodynamic-like economic models. In Section 5 we study simple interactions between two macroeconomic variables that allow derive hydrodynamic-like equations in a closed form and derive wave equations on macroeconomic perturbations alike to acoustic equations in fluids. Simple example in Section 6 demonstrates how waves can induce time fluctuations of macroeconomic variables. Derivation of acoustic-like wave equations is not too interesting for theoretical physics papers but as we know it is the first evidence and description of wave processes in macroeconomic and financial models. In Section 7 we argue diversity of macroeconomic models and some open problems that can be interesting for physicists. Conclusions are in Section 8. 2. Definition of Economic space Description of macroeconomic multi-agent system alike to multi-particle system requires introduction of economic analogy of space that allows define coordinates of economic agents alike to coordinates of physical particles. As analogy coordinates we 5 suggest risk ratings of economic agents [25, 26]. Here we present brief reasons for economic space definition. International rating agencies [28-30] estimate risk ratings of economic agents as Banks and Corporations, Firms and Enterprises. Risk ratings take values of risk grades and noted as AAA, BB, C and so on. Let treat risk grades like AAA, BB, C as points x1, x2,.. xm of discreet space. Let propose, that risk assessments methodologies can estimate risk ratings for all agents of entire economics: for huge Banks and for small households. If so, risk ratings distribute all economic agents of the entire economics over points of finite discreet space determined by set of risk grades. There are a lot of different risks those impact economic processes. Let regard grades of single risk as points of one-dimensional space and simultaneous assessments of n different risks as coordinates of economic agent on n-dimensional space. Let propose, that risk assessments methodologies can be generalized in such a way that risk grades can fill continuous space R. Thus risk grades of n different risks establish Rn. Let define economic space as any mathematical space that map economic agents by their risk ratings as space coordinates. Number n of risks ratings measured simultaneously determines dimension n of economic space. Let put positive direction along each risk axis as risk growth direction. Let assume that all economic agents of entire economics are “independent” and sum of extensive (additive) economic variables of any subset of agents equals economic variable of the entire subset. For example, sum of Assets of any two economic agents equal their collective Assets. Let assume that econometric data contains data about risk ratings and all economic variables of each economic agent. These assumptions require significant development of current econometrics and statistics. Quality and granularity of current U.S. National Income and Product Accounts system [22] gives hope that all these problems can be solved. Definition of economic space as grades of all economic and financial risks arises additional tough problem. There are a plentiful number of different economic and financial risks but their influence on macroeconomic evolution is very different. There are many risks that induce small influence and their action on entire macroeconomics might be neglected. We assume that current macroeconomic dynamics should be determined by action of one-two-three major risks and assessment of their ratings should define economic space. Thus definition of economic space Rn requires selection of n risks with major impact on economic agents 6 and macroeconomic processes. These n risks define initial state of economic space Rn. Selection of most valuable risks requires procedures that allow measure and compare influence of different risks on entire economics and economic agents. Assessment and comparison of different risks and their influence on economic agents establish tough problems and such models should be developed. Risk assessments methodologies and procedures, comparison of risk influence on performance of economic agents and on macroeconomic dynamics can establish procedures alike to physical measurement theory and measurement procedures. It allows develop relations between economic theory and econometric statistics alike to interdependence between physical theory and experimental measurements. Solution of this hard problem requires close collaboration between physicists and economists. Economic and financial risks have random nature and can unexpectedly arise and then vanish. Thus some current major risks that define initial representation of economic space Rn can accidentally disappear and other major risks may come to play. Thus economic space representation can be changed randomly. Description of economic dynamics and forecasting for time term T requires prediction of m main risks that can play major role in a particular time term and can define economic space Rm. Such set of m risks determine target state of economic space Rm. Transition of economic modeling on initial economic space Rn to target economic space Rm requires description of decline of action of initial set of n risks on entire economics and description of growth of influence of new m risks. Such stochastic scenarios are completely different from physical models that study complex dynamics of random fields and particles determined on constant physical space. Current macroeconomics describes relations between macroeconomic variables as Demand and Supply, Production Function and Investment, Economic Growth and Consumption each treated as function of time. Introduction of economic space gives ground for definition of macroeconomic variables as functions of time and coordinates. This small step opens doors for wide application of mathematical physics methods and models that should be transformed to adopt economic and financial phenomena’s. Below we present economic model on economic space Rn in the assumption that economic agents are under action of constant set of n major risks. We describe macroeconomics alike to kinetics and hydrodynamics and derive wave-like equations for macroeconomic variables. Up now notions of waves in economics and finance are 7 used to describe Kondratieff waves [31], inflation waves, crisis waves, etc. All these issues don’t describe any waves but time oscillations of economic variables only. Description of waves requires space and introduction of economic space notion gives ground for development of economic wave theory. 3. Macroeconomic kinetics Let treat macroeconomics as set of economic agents on economic space. Economic agents can move on economic space alike to particles. For convenience let call economic agents as economic particles or e-particles and economic space as e-space. Let assume that each e-particle is described by l extensive (additive) economic variables (u1,…ul) as Supply and Demand, Production Function and Capital, Consumption and Value and etc. Let study macroeconomics on e-space Rn that reflects action of constant set of n risks. Risk ratings of e-particles play role of their coordinates on e-space Rn. Each e-particle on e-space Rn at moment t is described by coordinates x=(x1,…xn), velocity υ=(υ1,… υn), and extensive economic variables (u1,…ul). Extensive economic variables of economic agents are additive and admit averaging by probability distributions. Intensive economic variables, like Prices or Interest Rates, cannot be averaged directly. Enormous number of extensive variables like Value and Capital, Demand and Supply, Profits and Savings, Consumption and Investment etc., describe economic and financial performance of each e-particle and are origin of extreme complexity of economic systems. As usual, macroeconomic variables are defined as aggregates of corresponding values of all economic agents of entire macroeconomics. For example, Demand of entire macroeconomics equal aggregate Demand of all economic agents and GPD can be calculated as aggregate Value Added of all economic agents [22]. Let introduce macroeconomic variables as aggregates of corresponding values of economic agents with coordinates x on e-space. Let assume that there are N(x) e-particles at point x. Let state that velocities of e- particles at point x equal υ=(υ1,… υN(x)). Let describe economics that has l macroeconomic variables and hence each e-particle has l economic variables (u1,…ul). Let assume that values of economic variables equal u=(u1i,…uli), i=1,..N(x). Each extensive economic variable uj at point x defines macroeconomic variable Uj as sum of economic variables uji of N(x) e-particles at point x 𝑈
௝= ∑ݑ௝௜ ;    ݆= ͳ, . . ݈
௜
;    ݅= ͳ, … 𝑁ሺ𝒙ሻ 8 For each macroeconomic variable Uj let define analogy of impulses Pj as ࡼ௝= ∑ݑ௝௜𝝊࢏ ;    ݆= ͳ, . . ݈
௜
;    ݅= ͳ, … 𝑁ሺ𝒙ሻ Let follow Landau and Lifshitz [33] and introduce economic distribution function f=f(t,x;U1,..Ul, P1,..Pl) on n-dimensional e-space that determine probability to observe macroeconomic variables Uj and impulses Pj at point x at time t. Uj and Pj are determined by corresponding values of e-particles that have coordinates x at time t. They take random values at point x due to random behavior of e-particles on e-space. Averaging of Uj and Pj within distribution function f allows establish transition from approximation that takes into account economic variables of separate e-particles to hydrodynamic-like approximation of macroeconomics that neglect e-particles granularity. Let define macroeconomic density function Uj(t,x) 𝑈
௝ሺݐ, 𝒙ሻ= ∫𝑈
௝ 𝑓ሺݐ, 𝒙, 𝑈ଵ, … 𝑈௟, ࡼଵ, . . ࡼ௟ሻ݀𝑈ଵ. . ݀𝑈௟݀ࡼଵ. . ݀ࡼ௟  
(1) and impulse density Pj(t,x) as ࡼ௝ሺݐ, 𝒙ሻ= ∫ࡼ௝ 𝑓ሺݐ, 𝒙, 𝑈ଵ, … 𝑈௟, ܲ
ଵ, . . ܲ
௟ሻ݀𝑈ଵ. . ݀𝑈௟݀ࡼଵ. . ݀ࡼ௟   
(2) That allows define e-space velocity υj(t,x) of density Uj(t,x) as 𝑈
௝ሺݐ, 𝒙ሻ𝒗࢐ሺݐ, 𝒙ሻ= ࡼ௝ሺݐ, 𝒙ሻ   
 
 
(3) Densities Uj(t,x) and impulses Pj(t,x) are determined as mean values of aggregates of corresponding economic variables of separate e-particles with coordinates x. Functions Uj(t,x) can describe macroeconomic e-space density of Demand and Supply, Assets and Debts, Production Function and Value Added and so on. Usage of distribution function f=f(t,x;U1,..Ul, P1,..Pl) allows describe any statistical moments of macroeconomic variables like <Uj
m>, correlations between economic variables <UjUi> and so on. Operators <..> define averaging by distribution function f.  We use (1-3) as tool to establish description of densities U=(U1,…Ul), Uj=Uj(t,x) as functions on e-space Rn and develop macroeconomic model alike to hydrodynamics. 4. Macroeconomic hydrodynamics Each macroeconomic density U1(t,x),… Ul(t,x) like Assets and Investment, Demand and Supply play role similar to fluid density ρ(t,x) in physical hydrodynamics. Such analogy allows call Uj(t,x) as densities of economic fluids or e-fluids. Extensive economic variables Uj of e-particles define corresponding amount of e-fluids Uj(t,x). Such hydrodynamic-like approximation of macroeconomics describes interactions between 
e-fluids 
U1(t,x),…Ul(t,x) 
and 
outlines 
parallels 
to 
multi-fluids 9 hydrodynamics. Parallels between physical and macroeconomic densities permit obtain e-fluid equations similar to Continuity Equation and Equation of Motion for hydrodynamics [34]. Continuity Equation on macroeconomic density Ui(t,x), i=1,..l takes form 𝜕𝑈𝑖
𝜕𝑡+ ݀݅ݒሺ𝒗௜𝑈௜ሻ= ܳଵ    
 
 
 
(4) υi(t,x) - is velocity of e-fluid Ui on e-space. Left side describes the flux of density Ui(t,x) through the unit volume surface on e-space Rn and right hand side Q1 describes factors that change density Ui(t,x). Macroeconomic density Ui(t,x)  can change in time and during motion of the selected volume on e-space due to economic reasons. For example macroeconomic Demand in unit e-space volume can increase in time due to economic growth and fall down due to economic crisis. As well, Demand density can decrease if unit volume moves in the direction of risk growth. Integral of Demand density over e-space determines Demand of entire macroeconomics and it changes its value in time due to phases of economic cycles. Equation of Motion for macroeconomic density Ui(t,x) takes form 𝑈௜[
𝜕𝒗𝑖
𝜕𝑡+ ሺ𝒗௜∙∇ሻ𝒗௜] = ࡽଶ 
 
 
 
(5) Left side describes the flux of Pi(t,x) = Ui(t,x)υi(t,x) through the surface of unit volume on e-space, taking into account Continuity Equation. The right hand side Q2 describes factors that induce changes of macroeconomic density and velocity. Economic and financial transactions between e-particles define evolution of densities of macroeconomic variables. This paper presents local model of economic and financial transactions between e-particles on e-space that takes into account transactions between e-particles with nearly same coordinates only. Local model of economic and financial transactions between e-particles simplifies description of macroeconomics. That assumption allows describe dynamics of macroeconomic densities alike to modeling collisions between e-particles and describe factors Q1 and Q2 by linear differential operators on conjugate macroeconomic densities and their velocities. We define conjugate densities below and use this assumption in the next Section. To determine right hand factors Q1 and Q2 let outline that same economic variables of different e-particles do not interact with each other. For example, Supply of e-particle 1 does not depend on the Supply of e-particle 2, but depends on other economic variables like Demand, Investment and so on. As well Consumption of e-particle 1 10 does not depend on Consumption of other e-particle, but is determined by Income, Savings, Inflation and etc. Let state that economic variables of different e-particles do not interact and do not depend on same variables. Let neglect any interaction between same economic variables of different e-particles. That causes lack of any self- interaction of macroeconomic densities Ui(t,x) and we state no economic parallels to such physical factors as pressure or viscosity. Let state that right hand side factors Q1 and Q2 in Continuity Equation and Equation of Motion for particular macroeconomic density Ui(t,x) do not depend on any factors determined by same variables Ui(t,x) but depend on economic densities Uj(t,x), υj(t,x) different from Ui(t,x), υi(t,x). Let call variables Uj(t,x), υj(t,x) that determine Q1 and Q2 factors in right hand side of hydrodynamic-like equations on variables Ui(t,x) and υj(t,x) as variables conjugate to Ui(t,x) or conjugate e-fluids. For example Supply may has conjugate variables like Demand, Investment, Credits and their velocities. Demand may conjugate to Supply and vice versa. Let state, that conjugate variables or conjugate e-fluids define right hand side of Continuity Equation and Motion Equations (4,5). Factors Q1 and Q2 in equations (4,5) describe action of conjugate e-fluids. Two conjugate e-fluids model is a simplest case that allows derive equations (4,5) in a closed form. Let study this model and possible Q1 and Q2 factors to obtain equations on two conjugate e-fluids in a closed form. As we show below, equations on two conjugate e-fluids allow derive wave equations on e-fluid densities perturbations. Existence of wave propagation of macroeconomic variable perturbations on e-space gives new look on macroeconomic modeling and description of economic shocks and their consequences. 5. Two conjugate e-fluids model Hydrodynamic-like Eq. (4,5) describe dynamics of e-fluids on e-space for given factors Q1 and Q2. Let study example of two conjugate e-fluids model and show possible advantages of economic hydrodynamic-like approximation. Let study relations between Investment and Interest Rates. Above we call macroeconomic density U2(t,x) or e-fluid U2(t,x) as conjugate to e-fluid U1(t,x) if e-fluid U2(t,x) or it’s velocity υ2(t,x) determine factors Q1 and Q2 in the right hand side of Eq. (4,5) on e- fluid U1(t,x) and it’s velocity υ1(t,x). Factors Q1 and Q2 can be determined by one, two or many different e-fluids Uj(t,x) and that makes macroeconomic modeling on e- space a very complex problem even in local approximation. There are two ways to use equations (4.5): 11 1. Study evolution of selected macroeconomic density U(t,x) for factors Q1 and Q2 determined by given functions of conjugate e-fluids. That allows describe dynamics of macroeconomic e-fluid U(t,x) and it’s velocity υ(t,x) in the given macroeconomic environment. All conjugate e-fluids are exogenous variables for e-fluid U(t,x) and one solves equations (4,5) that describe behavior of endogenous variable U(t,x) and υ(t,x) for given right side factors Q1 and Q2. 2. Study equations (4,5) in the assumption that e-fluids are self-conjugate. For example, e-fluid U2(t,x) is conjugate to e-fluid U1(t,x) and vice versa. So, factors Q1 and Q2 for equations (4,5) on e-fluid U1(t,x) are determined by e-fluid U2(t,x) and factors Q1 and Q2 for equations on e-fluid U2(t,x) are determined by e-fluid U1(t,x). Such model allows obtain hydrodynamic-like equations on e-fluids U1(t,x) and U2(t,x) in a closed form. Both approaches to Eq.(4,5) allow study macroeconomic models on e-space. Let study second case and derive self-consistent equations for simplest model of two self- conjugate e-fluids interactions. Such assumption simplifies the problem and allows study mutual relations between macroeconomic variables U1(t,x) and U2(t,x). 5.1. Model: Demand on Investment - Interest Rate Let study simple model that describe well-known relations between Demand on Investment and Interest Rate. Rise in Demand on Investment lead to Interest Rate growth. Interest Rate growth induce decline of Demand on Investment. Let neglect all other factors that have influence on Investment Demand and Interest Rate and simplify relations between core macro financial variables to obtain equations (4,5) in a closed form. Demand on Investment UI(t,x) is extensive variable and Interest Rate ir(t,x) is intensive economic variable. As we mentioned above, one can apply averaging procedure (1-3) to extensive (additive) variables of e-particles only. Intensive macroeconomic variables are determined as proportionality factor for relations between two extensive macroeconomic variables. Thus macroeconomic Interest Rate ir(t,x) determine proportionality factor between Cost of Investment UC(t,x) and Funds UF(t,x) available for Investment. For fixed value of UF(t,x), Cost of Investment UC(t,x)  for fixed time term equals: UC(t,x) = ir(t,x) UF(t,x) Thus for constant Funds UF(t,x) available for Investment, Cost of Investment UC(t,x) is proportional to Interest Rate ir(t,x) only. Rise in Investment Demand UI(t,x) lead to 12 growth of Interest Rate ir(t,x) and that induce Cost of Investment UC(t,x) growth. Growth Cost of Investment UC(t,x) induced by rise of Interest Rate ir(t,x) imply decline of Demand for Investment UI(t,x). Let replace Interest Rate ir(t,x) as intensive variable by Cost of Investment UC(t,x) as extensive variable taking into account that UC(t,x) depends on Interest Rate ir(t,x) only with Funds UF(t,x) being constant. That establish a model with two interacting conjugate e-fluids Demand for Investment UI(t,x) - UC(t,x) Cost of Investment. Due to above assumption that economic and financial transactions are local, density UI(t,x) at point x is determined by conjugate variables UC(t,x) and can be described by differential operators. Let study simples case that describes interaction of conjugate variables by operators div and grad. Let assume that Q1 factor in the right hand side of Continuity Equation (4) on Demand for Investment density function UI describe local action on Cost of Investment and is proportional to divergence of velocity υC of Cost of Investment Q1 ~ αCυC. Positive divergence of Cost of Investment velocity υC>0 describes growth of supply flux with the same Cost of Investment density function UC and that increase Demand for Investment UI, thus αC>0. Let assume that Q1 factor for Continuity Equation (4) on Cost of Investment density function UC is proportional to divergence of velocity υI of Investment Demand: Q1 ~ αIυI. Positive divergence of Demand for Investment velocity υI>0 describe source of demand flux and that increase Cost of Investment and αI >0. Let state that Q2 factor for Equation of Motion (5) for Demand on Investment velocity υI is proportional to gradient of Cost of Investment density UC: Q2 ~ ȕC  UC Demand for Investment velocity υI can decrease in the direction of positive gradient of Cost of Investment density UC. Flux of expensive Cost of Investment proposals will decline velocity of Demand for Investment flow and ȕC<0. Let state that Q2 factor for Equation of Motion (5) for Cost of Investment velocity υC is proportional to gradient of Investment density  UI: Q2 ~ βI  UI Our assumptions means that Cost of Investment velocity υC increase in the direction with positive gradient of Investment density UI. Indeed, Cost of Investment flow is directed in the domain with higher Demand for Investment and ȕI>0. Thus our assumptions give simple models of mutual dependence between Demand for 13 Investment UI and Cost of Investment UC on e-space. We remind that in our model Cost of Investment depends on Interest Rate ir(t,x) only. Thus equations (4,5) for two self-conjugate e-fluids Demand for Investment UI(t,x) and Cost of Investment UC(t,x) that depends on Interest Rate ir(t,x) only take form: 𝜕𝑈𝐼
𝜕𝑡+ ߘ∙ሺ𝒗𝐼𝑈𝐼ሻ= ߙ𝐶∇∙𝝊𝑐   ;    
𝜕𝑈𝐶
𝜕𝑡+  ߘ∙ሺ𝒗𝐶𝑈𝐶ሻ= ߙ𝐼∇∙𝝊𝐼 
 
(6.1) 𝑈𝐼[
𝜕𝒗𝐼
𝜕𝑡+ ሺ𝒗𝐼∙∇ሻ𝒗𝐼] = ߚ𝐶∇𝑈𝐶  ;  𝑈𝐶[
𝜕𝒗𝐶
𝜕𝑡+ ሺ𝒗𝐶∙∇ሻ𝒗𝐶] = ߚ𝐼∇𝑈𝐼  
(6.2) ߙ𝐼> Ͳ ; ߙ𝐶> Ͳ  ;   ߚ𝐼> Ͳ ; ߚ𝐶< Ͳ ; 5.2. Wave equations on e-fluid densities perturbations To derive macroeconomic wave equations on base of (6.1-6.2) let take small perturbations qI of Demand on Investment UI and small perturbations qC of constant Cost of Investment densities UC and assume that velocities υI and υC are small. Let put: 𝑈𝐼= 𝑈𝐼଴+ ݍ𝐼  ; 𝑈𝐶= 𝑈𝐶଴+ ݍ𝐶  
 
 
(7.1) and assume that derivations of UI0 and UC0 by time and coordinates in Eq.(6.1-6.2) are small to compare with similar derivations of qI , qC, υI and υC so we can neglect derivations by UI0 and UC0. In hydrodynamics similar approximations are used to derive acoustic wave equations [34]. Continuity Eq.(6.1) on small perturbations qI,C in linear approximation: 𝜕𝑞𝐼
𝜕𝑡+ 𝑈𝐼଴∇∙𝒗𝐼= ߙ𝐶∇∙𝒗𝐶  ;     
𝜕𝑞𝐶
𝜕𝑡+ 𝑈𝐶଴∇∙𝒗𝐶= ߙ𝐼∇∙𝒗𝐼  
(7.2) Equations of Motion in linear approximation: 𝑈𝐼଴
𝜕𝒗𝐼
𝜕𝑡= ߚ𝐶ߘݍ𝐶   ;        𝑈𝐶଴
𝜕𝒗𝐶
𝜕𝑡= ߚ𝐼∇ݍ𝐼      
 
(7.3) Derivation of equations on qI ,qC from (7.1-7.3) is very simple and we omit it here: [
𝜕4 𝜕𝑡4 −ܽ∆
𝜕2 𝜕𝑡2 + ܾ∆ଶ ] ݍ𝐼,𝐶= Ͳ   
 
 
(8.1) ܽ= ሺߙ𝐼ߚ𝐶+ ߙ𝐶ߚ𝐼ሻ ; ܾ= ߚ𝐶ߚ𝐼ሺߙ𝐼ߙ𝐶−ͳሻ It is easy to show that for a2>4b there exist two positive c2
1,2>0 and Eq.(8.1) take form of bi-wave equations ሺ
𝜕2 𝜕𝑡2 −ܿଵ
ଶΔሻ ሺ
𝜕2 𝜕𝑡2 −ܿଶ
ଶΔሻݍ𝐼,𝐶= Ͳ 
 
 
(8.2) Bi-wave equations (8.2) describe propagation of waves q=q(x-ct) with speed c equals c1 or c2 as in the direction of risks growth as in the direction of small risks. If coefficients a2<4b then equation (8.1) admits wave solutions with amplitudes 14 amplification in time as exponent. So, small perturbations of Cost of Investment may induce waves that propagate on e-space with exponential growth of amplitudes: ݍ𝐶= cosሺ࢑∙𝒙−𝜔ݐሻexp 
ሺߛݐሻ For Ȗ>0 the solution will grow up and for Ȗ<0 will dissipate. 𝜔ଶ= ݇ଶ√Ͷܾ+ ͵ܽଶ+ ʹܽ
8
> Ͳ   ;  ߛଶ= ݇ଶ√Ͷܾ+ ͵ܽଶ−ʹܽ
8
> Ͳ These examples demonstrate possible exponential amplification or dissipation of wave amplitudes of small macroeconomic perturbations in model of two interacting conjugate e-fluids. Derivation of above results is simple and we omit it here. Nevertheless even for simple model equations on disturbances of economic densities take form of bi-wave Eq.(8.2) and Green function for such bi-wave equations equals convolution of Green functions of two wave equations. Thus even simplest response on δ-function shock in economics is more complex then in physics. Existence of wave processes on e-space allows describe macroeconomic wave generation, propagation and interaction as possible wave response on macroeconomic shocks. Amplitudes amplifications of macroeconomic perturbations waves may model macroeconomic and financial crises evolution. 6. Time fluctuations of macroeconomic variables All economic variables follow time fluctuations and often these fluctuations are called waves [31, 35, 36]. Meanwhile all these “waves” are only fluctuations of economic variables in time. Nature of waves requires space where these waves can propagate. Macroeconomic models on e-space uncover existence of wave equations for macroeconomic variables and start studies of economic wave generation, propagation and interaction on e-space. Above we derive simple Demand on Investment - Interest Rate interaction model that admit wave equations and present simple wave solutions. Let show that even simple waves can cause irregular time fluctuations macroeconomic variables. Due to definition of e-space in Section 2 coordinates of e-particles define their risk ratings. Let consider simplest e-space R. Let assume that risk ratings of e- particles are reduced by minimum Xmin and maximum Xmax risk grades. For simplicity let take Xmin=0 and Xmax= X and coordinates x of e-particles on e-space R follow Ͳ ≤𝑥≤𝑋 
 
 
 
 
 (9.1) 15 Relation (9.1) defines simplest macroeconomic domain on e-space. Due to (7.1) macroeconomic density UI is presented as 𝑈𝐼ሺݐ, 𝑥ሻ= 𝑈𝐼଴+ ݍ𝐼   
 
 
 
(9.2) UI0 is constant or its derivatives are small to compare with derivatives of perturbations qI. Let take simplest wave solution qI of equation (8.1; 8.2) on e-space R as ݍ𝐼ሺݐ, 𝑥ሻ= cosሺ݇∙𝑥−𝜔ݐሻ   ;  𝜔ଵ,ଶ
ଶ= ݇ଶܿଵ,ଶ
ଶ 
  
(9.3) As we mentioned above, integral of macroeconomic density over e-space gives corresponding macroeconomic variable of entire economics as function of time. So, integral of Demand on Investment density UI(t,x) over e-space gives macroeconomic Demand on Investment UI(t). For assumption (9.1) integral qI(t,x) of (9.2, 9.3) by coordinate x  on e-space R2 gives 𝑈𝐼ሺݐሻ= 𝑈଴+ ݍ𝐼ሺݐሻ  ;   𝑈଴~ 𝑈𝐼଴ 𝑋 
 
 
(9.4) ݍሺݐሻ=
ଶ ௞ݏ݅݊ቀ
௞ ଶ𝑋ቁcos 
ሺ
௞ ଶ𝑋−𝜔ଵ,ଶݐሻ  
 
(9.5) Due to (9.5) macroeconomic Investment UI(t) follows time oscillations with frequency ω. For fixed wave speed c2
1,2 linear equations (8.1, 8.2) may have wave solutions with random k and random frequency ω that satisfy (9.3). Hence random wave vectors k may induce random time oscillations of macroeconomic variables. Relations between variables like GDP, Investment, Supply and Demand etc., of entire economics treated as functions of time can be determined by complex interaction of conjugate macroeconomic variables. Macroeconomic density perturbations waves on e-space may be origin of time oscillations of macroeconomic variables, origin of business cycles etc. 7. Diversity of macroeconomic models and open problems Macroeconomics is a very complex system and enormous number of economic variables and properties describe it state and evolution. That causes diversity and complexity of mutual dependence between macroeconomic variables. This paper presents simple model relations between macroeconomic Demand for Investment and Interest Rate. It is obvious that different macroeconomic densities can depend on conjugate variables in a different from. For example, dependence of Demand for Investment on Interest Rate can be different from dependence of Production Function on Capital or dependence of Consumption on Savings and so on. Different pairs of self-conjugate macroeconomic densities can have different forms of Q1 and Q2 factors. Let assume that dynamics of selected macroeconomic variable is 16 determined by one conjugate variable only. Even for such simplification macroeconomic modeling remains extremely difficult. Origin of complexity concern diversity of possible forms of Q1 and Q2 factors in the right hand side of hydrodynamic-like equations (4,5). What does that mean for macroeconomic hydrodynamic-like models? Let propose that right hand side factors Q1 and Q2 take form of simple linear operators on conjugate variables. Continuity Equations (4) have linear scalar right hand side factors Q1 that depend on density U or velocity υ of conjugate variable as: ͳ.  ܳଵ~ 𝑈 ;   ʹ. ܳଵ~
𝜕 𝜕𝑡 𝑈  ;   ͵. ܳଵ~∇∙𝒗  ;   Ͷ.  ܳଵ~Δ 𝑈  
 
(10.1) Equations of Motion (5) have linear vector right hand side factors Q2 that depend on density U or velocity υ of conjugate variable as: ͳ.  ࡽଶ~𝒗 ;  ʹ. ࡽଶ~
𝜕 𝜕𝑡 𝒗 ;  ͵.  ࡽଶ ~ߘ 𝑈 ;  Ͷ. ࡽଶ~ ݎ݋ݐ 𝒗 ; ͷ.  ࡽଶ~ 𝛥𝒗   
 
(10.2) Relations (10.1) describe four possible scalar linear operators on conjugate density and velocity. Relations (10.2) present five vector linear operators on conjugate density and velocity. These linear operators describe local action of conjugate variables due to Eq.(4,5). For example Continuity Equation on macroeconomic density UM can have Q1 factor that is proportional to conjugate density U or proportional to time derivation of density U etc. Let assume that different macroeconomic densities can depend upon conjugate variables in a different manner and present simple possible operators. It is obvious, that any linear composition of these operators can be used as a model for mutual dependence between macroeconomic variables on e-space. Usage of each possible form of Q1 and Q2 factors requires economical consideration and validation. Moreover, two self- conjugate macroeconomic variables may depend upon each other in a different manner. For example, variables U2 may define Q1 factor for Continuity Equation on variable U1 as Q1 ~ ∂U2/∂t and variable U1 may define Q1 factor for Continuity Equation on variable U2 as Q1 ~ ΔU1. That increases diversity of different models of two conjugate macroeconomic variables interaction up to 200 versions. To describe real interaction between macroeconomic variables one should take into consideration action of two, tree or more conjugate variables. To develop model equations in a closed form one need to consider a system of tree, four or more hydrodynamic-like equations like Eq.(4,5). Different macroeconomic variables may have different forms 17 of Q1 and Q2 factors and different substitutions of (10.1) and (10.2) relations make diversity of macroeconomic models incredibly huge. Let outline some vital distinctions between economic and physical systems on one hand and let mention extremely interesting and tough problems to be solved by methods of statistical physics to establish economic theory in a rigorous way. 1. 
Economic space is defined by methods that adopt current economic phenomena’s. It is impossible to define risk assessment on “empty economic space” without economic agents. Thus economics, at least in our model, has no analogies like free space, point particle mechanics, “fundamental” equations, conservation laws, symmetries and etc. It seems that economic theory begins with description of random multi-agent system. That is completely different from physics foundations. It seems that lack of economic conservation laws leads to lack of economic equilibrium states. That arises a lot of problems: How to develop non-equilibrium stationary states model for system that consists of many interacting subsystems and each particular subsystem does not have it’s own equilibrium state? How to develop theory starting with kinetic- like description? How to develop kinetic-like and hydrodynamic-like theory without models and equations that describe dynamics of separate e-particles? 2. 
Economic agents or e-particles as we call them are completely different from physical particles. E-particles have size that can be determined by probability distribution that estimate risk ratings or coordinates of particular economic agent on e-space Rn. But e-particles do not collide with each other. Any number of different e- particles can exist in the neighborhood of point x on e-space. No collisions between e- particles mean lack of economic analogy o pressure and viscosity. As we proposed in Sec. 4 for local model economic variables of e-particles depend on conjugate economic variables of different e-particle. Demand of e-particle do not depend directly on Demand of other particles, but on any other conjugate variables as Income, Saving, Supply and etc. It seems reasonable that set of e-particles can establish some stationary non-equilibrium state determined by interaction of conjugate variables. That arises problems: How to describe models for thermodynamic-like stationary state of multi-agents system on e-space. Such stationary states should be different for different set of interacting conjugate economic variables. How these states can interact with each other? 3. 
Economic analogy of kinetic distribution function helps develop transition from economic kinetic-like description to economic hydrodynamic-like description 18 and that arises questions: How to derive economic kinetic-like equations on distribution functions without underlining equations of e-particles “mechanics”? How possible kinetic-like equations on distribution functions determine right-hand side factors of hydrodynamic-like equations? Do equations on distribution functions depend on conjugate variables? That is only negligible number of problems that should be solved to establish some rigorous scheme for economic theory on economic space. 8. Conclusions Introduction of economic space opens doors for wide usage of mathematical and statistical physics methods for economic modeling. Economic space employers risk ratings methods that were developed for decades. Risks should be treated as drivers of economic evolution and absence of any risks delete reasons for economic growth. Reasonable economic space should be determined by set of major risks that define current economic evolution. Economic space has different representations for different economic conditions under action of different major risks. It is important to develop procedures that can compare influence of different risks and can chose major risks that define economic space. That may allow compare predictions of macroeconomic models with observed macroeconometric data and may help establish econometric measurements alike to measurements in physics. We assume that it is impossible establish determined macroeconomic description. Random nature of risks growth and decline and random nature of economic space representations insert internal 
stochasticity 
into 
economic 
evolution 
and 
forecasts. 
Long-term macroeconomic forecasts require development of macroeconomic dynamics on current economic space Rn and assumptions on future m risks configuration that will define economic space Rm in projected time term. Modeling on economic space uncovers extreme complexity of macroeconomics even for simplest models in the assumptions of local interaction between economic agents on economic space. This assumption allows derive hydrodynamic-like equations on macroeconomic variables in a closed form. Incredible diversity of relations between macroeconomic variables on economic space allows develop models of mutual dependence that reflect specific economic nature of particular problem. Economic space gives ground for wide usage of mathematical and statistical physics methods and models. Differences between nature of economics and physics are so vital that 19 leave no chance for direct application of physical methods and models. As well physical schemes and concepts like kinetics and hydrodynamics might be useful for economic theory. Macroeconomic wave equations for simplest Investment-Interest Rate interaction model uncovers existence of wide range of wave processes in macroeconomics and might be useful for crises forecasting. Even simple models of macroeconomic waves allow describe irregular time fluctuations of variables of entire economics. We believe that theory of economic waves could be very important for economic modeling and forecasting. Economic theory on economic space requires appropriate econometric foundations. To develop reasonable model on economic space one should solve many methodological and econometric problems. Definition of economic space requires cooperative efforts of Central Banks, Rating Agencies, Economic and Finance Research Communities, Regulators, Statistical Bureaus, and Business etc. Many problems should be solved to establish appropriate econometric models on economic space. It is obvious that macroeconomic models can be developed on continuous spaces as Rn and on discreet lattice as well. Lattice macroeconomic models require less changes of risk rating methodologies and can be developed within current risk ratings definitions. Acknowledgements This research did not receive any specific grant from TVEL or funding agencies in the public, commercial, or not-for-profit sectors and was performed on my own account. 20",0
"Previously, in [GR19], we derived a rational approximation of the solution of the rough
Heston fractional ODE in the special case λ = 0, which corresponds to a pure power-law
kernel. In this paper we extend this solution to the general case of the Mittag-Leﬄer
kernel with λ ≥0. We provide numerical evidence of the convergence of the solution. 1
Introduction In the case λ ≥0, the rough Heston model of [JR16] may be written in forward variance form
(see [GKR19]) as dSt St
=
p Vt

ρ dWt +
p 1 −ρ2 dW ⊥
t dξt(u)
=
p Vt κ(u −t) dWt,
u ≥t
(1.1) where ξt(u) = Et [Vu] , u > t is the forward variance curve, 1 2 < α = H + 1 2 ≤1, and the kernel
κ is given by
κ(x) = ν xα−1 Eα,α(−λ xα), where Eα,α(·) denotes the generalized Mittag-Leﬄer function.
Let X = log S and Xt,T := XT −Xt. According to [GKR19], the forward variance model
(1.1) has a cumulant generating function (CGF) of the form ϕT
t (a) := log E

ei a Xt,T 
 Ft

=
Z T t
ξt(s) g(T −s; a) ds
(1.2) if and only if g(t; u) satisﬁes the convolution integral equation g = −1 2 a (a + i) + ρ a i (κ ⋆g) + 1 2 (κ ⋆g)2,
(1.3) where (κ ⋆g)(t; u) :=
R t
0 κ(t −s) g(s; u) ds.
Previously, in [GR19], we derived various rational approximations to the solution of (1.3)
in the special case λ = 0 where the kernel simpliﬁes to κ0(x) = ν xα−1 Γ(α).
(1.4) 1 As pointed out in [BBRR22] for example, such rational approximations are extremely fast to
compute relative to the alternatives, enabling eﬃcient calibration of the rough Heston model
in this special case.
In the present note, we extend these rational approximations to the case λ > 0. This
enables fast calibration of the rough Heston model in the general case, with the extra param-
eter λ providing additional ﬂexibility to ﬁt market implied volatility smiles. Moreover, when
H = 1 2, we retrieve the classical Heston model for which there is a well-known closed-form
solution.
To proceed, let Dα and I1−α represent respectively fractional diﬀerential and integral
operators (see Appendix A of [GR19] for a very brief introduction to fractional calculus).
The following result was originally proved in [EER19]. Lemma 1.1. Let κ(τ) = ν τ α−1 Eα,α(−λ τ α) and h(t; a) = 1 ν (κ ⋆g)(t; a). Then h satisﬁes
the fractional ODE Dαh(t; a) = −1 2 a (a + i) + (i ρ ν a −λ) h(t; a) + 1 2 ν2 h2(t; a);
I1−αh(t; a) = 0.
(1.5) Proof. For ease of notation, we drop the explicit dependence of h and g on t and a. Let κ0
be the power-law kernel given by (1.4). The Laplace transforms of κ0 and κ are given (for
suitable p) by ˆ
κ0(p) = ν pα ;
ˆ
κ(p) =
ν pα + λ. Let λ′ = λ/ν. Then ˆ
κ0 −ˆ
κ = λ′ ˆ
κ0 ˆ
κ, and κ0 −κ = λ′ κ0 ⋆κ. Also, by deﬁnition of the
fractional integral operator, Iαf = 1 ν Z t 0
κ0(t −s) f(s) ds. Using that (κ0 ⋆κ) ⋆g = κ0 ⋆(κ ⋆g), it follows that h = 1 ν (κ ⋆g) = 1 ν (κ0 ⋆g −λ κ0 ⋆κ ⋆g) = Iαg −λ Iαh. Operating with Dα gives Dαh(t; a) = g(t; a) −λ h(t; a).
Substituting into (1.3) gives the
result. Given an approximate solution to the Riccati Equation (1.5), an accurate approximation
to the CGF (1.2) may be easily computed. European option prices may then be obtained
using the Lewis formula ([Lew00, Gat06]): C(S, K, T) = S −
√ SK 1 π Z ∞ 0 du u2 + 1 4
Re
h
e−iukϕT
t (u −i/2)
i
(1.6) where S is the current stock price, K the strike price and T expiration. Finally, implied
volatilities may be computed by numerical inversion of the Black-Scholes formula.
A key observation is that for option pricing with equation (1.6), we need only ﬁnd a good
approximation to the solution h(a, x) of the rough Heston Riccati equation for a ∈A with A = {z ∈C : ℜ(z) ≥0, −1 ≤ℑ(z) ≤0}
(1.7) where ℜand ℑdenote real and imaginary parts respectively. 2 1.1
Main results and organization of the paper Our paper is organized as follows.
In Section 2, we derive a short-time expansion of the
solution h of the rough Heston Riccati equation (1.5).
Then in Section 3, we derive an
asymptotic solution to (1.5) in the long-time limit τ = T −t →∞.
In Section 4, we
explain how to construct global rational approximations to h and present numerical results.
In particular, we exhibit (near) exponential convergence of the rational approximations with
respect to their order. Finally, in Section 5, we summarize and conclude. Some technical
details are relegated to the appendix. 2
Solving the rough Heston Riccati equation for short times First, we derive a short-time expansion of the solution h(t; a) of the fractional Riccati equation
(1.5). Inspired by the λ = 0 case, consider the small t ansatz h(t; a) = ∞
X j=1
bj tj α.
(2.1) Then, Dαh = ∞
X j=1
bj
Γ(1 + j α) Γ(1 + (j −1) α) t(j−1)α = ∞
X j=0
bj+1
Γ(1 + (j + 1) α) Γ(1 + j α)
tj α. Substituting into (1.5) and matching coeﬃcients of t0 gives b1 = −
1 Γ(1 + α)
1
2 a(a + i). Doing the same with the coeﬃcient of tα gives b2 = Γ(1 + α) Γ(1 + 2α) (i ρ a −λ′) ν b1, where as before, λ′ = λ/ν. This generalizes to the recursion b1
=
−
1 Γ(1 + α)
1
2 a(a + i) bk
=
Γ(1 + (k −1) α) Γ(1 + k α) 
 −˜
λ ν bk−1 + 1 2 ν2
k−1
X i,j=1 1i+j=k−1 bi bj 
 , where ˜
λ = λ′ −i ρ a. 3
Solving the rough Heston Riccati equation for long times The fractional Riccati equation (1.5) can be re-expressed as Dαh(t; a) = 1 2 (ν h(t; a) −r−) (ν h(t; a) −r+) ,
(3.1) 3 with A =
p a (a + i) + (λ′ −i ρ a)2;
r± = {λ′ −i ρ a ± A}; λ′ = λ/ν.
Let ν h∞(t; a) =
r−[1 −Eα(−A ν tα)] where Eα is the Mittag-Leﬄer function. Then, for t ∈R≥0 and a ∈A
(given in (1.7)), as in Proposition 3.1 of [GR19], h∞(t; a) satisﬁes ν h∞(t; a) −r−= −r− Aν
t−α Γ(1 −α) + O
",0
"Testing large covariance matrices is of fundamental importance in statistical anal- ysis with high-dimensional data. In the past decade, three types of test statistics have been studied in the literature: quadratic form statistics, maximum form statistics, and their weighted combination. It is known that quadratic form statistics would suﬀer from low power against sparse alternatives and maximum form statistics would suﬀer from low power against dense alternatives. The weighted combination methods were in- troduced to enhance the power of quadratic form statistics or maximum form statistics when the weights are appropriately chosen. In this paper, we provide a new perspective to exploit the full potential of quadratic form statistics and maximum form statistics for testing high-dimensional covariance matrices. We propose a scale-invariant power enhancement test based on Fisher’s method to combine the p-values of quadratic form statistics and maximum form statistics. After carefully studying the asymptotic joint distribution of quadratic form statistics and maximum form statistics, we prove that the proposed combination method retains the correct asymptotic size and boosts the power against more general alternatives. Moreover, we demonstrate the ﬁnite-sample performance in simulation studies and a real application. Key Words: Fisher’s method; high-dimensional hypothesis testing; joint limiting law; large covariance structure; power enhancement. ∗The authors thank the seminar/conference participants at the George Washington University, University of Southern California, Rutgers University, Yale University, ENAR 2019 Spring Meeting, and 2019 Joint Statistical Meetings for their helpful comments and suggestions. The preliminary result of this paper was included in the National Science Foundation (NSF) grant proposal (DMS-1811552). Lingzhou Xue’s research is supported in part by the NSF grants DMS-1811552 and DMS-1953189. 1 arXiv:2006.00426v1  [math.ST]  31 May 2020 1 Introduction Hypothesis testing on large covariance matrices has received considerable attention in the past decade. The covariance matrices not only have the fundamental importance in multivariate statistics such as discriminant analysis, principal component analysis, and clus- tering (Anderson, 2003), but also play a vital role in various research topics in biological science, ﬁnance, operations research including portfolio allocation (Goldfarb and Iyengar, 2003), gene-set testing (Chen and Qin, 2010), and gene-set clustering (Chang et al., 2017). Let X and Y represent two independent p-dimensional random vectors with covariance matrices Σ1 and Σ2 respectively. We are interested in testing whether these two covariance matrices are equal, that is, H0 : Σ1 = Σ2. This test is well studied in the classical setting where the dimension is ﬁxed and the sample size diverges (Anderson, 2003). For instance, the likelihood ratio test was shown to enjoy the optimality under mild conditions (Sugiura and Nagao, 1968; Perlman, 1980). However, the likelihood function is not well-deﬁned due to the singular sample covariance matrix in the high-dimensional setting where the dimension is no longer ﬁxed but diverges at a possibly faster rate than the sample size. Over the past decade, statisticians have made a lot of eﬀorts to tackle the challenges in the high-dimensional setting and proposed three diﬀerent types of statistics for testing large covariance matrices. Firstly, quadratic form statistics were studied to test against the dense alternatives, which can be written in terms of the Frobenius norm of Σ1 −Σ2 with many small diﬀerences between two covariance matrices. When the dimension is on the same order of the sample size, Schott (2007) proposed a test statistic based on the sum of squared diﬀerences between two sample covariance matrices, and Srivastava and Yanagihara (2010) used a consistent estimator of tr(Σ2
1)/ [tr(Σ1)]2 −tr(Σ2
2)/ [tr(Σ2)]2 to construct a new test statistic. Li and Chen (2012) introduced an unbiased estimator of the Frobenius norm of Σ1 −Σ2 to allow for the ultra-high dimensionality that the dimension grows much faster than the sample size. Recently, He et al. (2020) proposed the adaptive testing to combine the ﬁnite-order U-statistics that includes the variants of quadratic form statistics. Secondly, maximum form statistics were explored to account for the sparse alternatives with only a few large diﬀerences between two covariance matrices, which can be written in terms of the entry-wise maximum norm of Σ1 −Σ2. Cai et al. (2013) studied the maximal standardized diﬀerences between two sample covariance matrices to test against the sparse alternative, and Chang et al. (2017) proposed a perturbed-based maximum test using a data-driven approach to determine the rejection region. Thirdly, Li and Xue (2015), Yang and Pan (2017) and Li et al. (2018) used a weighted combination of quadratic form statistics and maximum form 2 statistics to test against the dense or sparse alternatives, which shares the similar philosophy with the power enhancement method (Fan et al., 2015) for testing cross-sectional dependence. Similar to these weighted combination tests, we are motivated by combining the strengths of quadratic form statistics and maximum form statistics to boost the power against the dense or sparse alternatives. It is also of great importance to combine the power of these two diﬀerent statistics in real-world applications such as ﬁnancial studies and genetic association studies. For instance, the anomalies in ﬁnancial markets may come from the mispricing of a few assets or a systematic market mispricing (Fan et al., 2015), and the phenotype may be aﬀected by a few causal variants or a large number of mutants (Liu et al., 2019). It is worth pointing out that these weighted combination tests critically depend on the proper choice of weights to combine two diﬀerent types of test statistics. There may exist a non-negligible discrepancy on the diﬀerent magnitudes between quadratic form statistics and maximum form statistics in practice, which makes the choice of weights a very challenging task. As a promising alternative to Fan et al. (2015), Li and Xue (2015), Yang and Pan (2017) and Li et al. (2018), we provide a new perspective to exploit the full potential of quadratic form statistics and maximum form statistics for testing high-dimensional covariance matrices. We propose a scale-invariant power enhancement test based on Fisher’s method (Fisher, 1925) to combine the p-values of quadratic form statistics and maximum form statistics. To study the asymptotic property, we need to solve several non-trivial challenges in the theo- retical analysis and then derive the asymptotic joint distribution of quadratic form statistics and maximum form statistics under the null hypothesis. We prove that the asymptotic null distribution of the proposed combination test statistic does not depend on the unknown parameters. More speciﬁcally, the proposed statistic follows a chi-squared distribution with 4 degrees of freedom asymptotically under the null hypothesis. We also show the consistent asymptotic power against the union of dense alternatives and sparse alternatives, which is more general than the designated alternative in the weighted combination test. It is worth pointing out that Fisher’s method achieves the asymptotic optimality with respect to Ba- hadur relative eﬃciency. Moreover, we demonstrate the numerical properties in simulation studies and a real application to gene-set testing (Dudoit et al., 2008; Ritchie et al., 2015). In the real application, the proposed test can detect the important gene-sets more eﬀectively, and our ﬁndings are supported by biological evidences. In recent literature, Liu and Xie (2019) proposed the Cauchy combination of p-values for testing high-dimensional mean vectors, and He et al. (2020) proved the joint normal limiting distribution of ﬁnite-order U-statistics with an identity covariance matrix and used 3 the minimum combination of their p-values.
The methods and theories of Liu and Xie (2019) and He et al. (2020) do not apply to the more challenging setting for testing two- sample high-dimensional covariance matrices. Speciﬁcally, Li and Xue (2015) and He et al. (2020) considered the one-sample test for large covariance matrices that H0 : Σ = I under the restricted complete independence assumption among entries of X, and Li et al. (2018) studied the one-sample test that H0 : Σ is a banded matrix under the Gaussian assumption. Li and Xue (2015), Li et al. (2018), and He et al. (2020) studied the one-sample covariance test and did not prove the asymptotic independence result for testing two-sample covariance matrices. However, it is signiﬁcantly more challenging to deal with the complicated dependence in the two-sample tests for large covariance matrices.
To the best of our knowledge, our work presents the ﬁrst proof of the asymptotic independence result of quadratic form statistics and maximum form statistics for testing two-sample covariance matrices, which provides the essential theoretical guarantee for Fisher’s method to combine their p-values. In the theoretical analysis, we use a non-trivial decorrelation technique to address the complex nonlinear dependence in high dimensional covariances. Recently, Shi et al. (2019) used the decorrelation to study the linear hypothesis testing for high-dimensional generalized linear models. But the nonlinear dependence in the two-sample covariance testing is much more challenging than the linear hypothesis testing. Moreover, we develop a new concentra- tion inequality for two-sample degenerate U-statistics of high-dimensional data, which makes a separate contribution to the literature. This result is an extension of the concentration inequality for one-sample degenerate U-statistics (Arcones and Gine, 1993). The rest of this paper is organized as follows. After presenting the preliminaries in Sec- tion 2, we introduce the Fisher’s method for testing two-sample large covariance matrices in Section 3. Section 4 studies the asymptotic size and asymptotic power, and Section 5 demon- strates the numerical properties in simulation studies. Section 6 evaluates the proposed test in an empirical study on testing gene-sets. Section 7 includes the concluding remarks. The technical details are presented in the supplementary note. 2 Preliminaries Let X and Y be p-dimensional random vectors with covariance matrices Σ1 = (σij1)p×p
and Σ2 = (σij2)p×p respectively.
Without loss of generality, we assume both X and Y have zero means. Let {X1, · · · , Xn1} be independently and identically distributed (i.i.d.) random samples of X, and {Y1, · · · , Yn2} be i.i.d. samples of Y that are independent of {X1, · · · , Xn1}. The problem of interest is to test whether two covariance matrices are equal, 4 H0 : Σ1 = Σ2.
(2.1) We ﬁrst revisit the quadratic form statistic (Li and Chen, 2012) to test against the dense alternative and the maximum form statistic (Cai et al., 2013) to test against the sparse alternative. The dense alternative can be written in terms of the Frobenius norm of Σ1 −Σ2
and the sparse alternative can be written using the entry-wise maximum norm of Σ1 −Σ2. Li and Chen (2012) proposed a quadratic-form test after reformulating the null hypothesis (2.1) into its equivalent form based on the squared Frobenius norm of Σ1 −Σ2, that is, H0 : ∥Σ1 −Σ2∥2
F = 0. To construct the test statistic, given the simple fact that ∥Σ1 −Σ2∥2
F = tr{(Σ1 −Σ2)2} = tr(Σ2
1) + tr(Σ2
2) −2tr(Σ1Σ2), Li and Chen (2012) proposed a test statistic Tn1,n2 in the form of linear combination of unbiased estimators for each term, speciﬁcally, Tn1,n2 = An1 + Bn2 −2Cn1,n2,
(2.2) where An1, Bn2 and Cn1,n2 are the unbiased estimators under H0 for tr(Σ2
1), tr(Σ2
2) and tr(Σ1Σ2) respectively. Then, the expected value of Tn1,n2 is zero under the null hypothesis. For details about An1, Bn2 and Cn1,n2, please refer to Section 2 of Li and Chen (2012). Li and Chen (2012) proved that the asymptotic distribution of Tn1,n2 is a normal distribution. Let zα be the upper α quantile of the standard normal distribution, and b
σ0,n1,n2 is a consistent estimator of the leading term σ0,n1,n2 in the standard deviation of Tn1,n2 under H0. Hence, Li and Chen (2012) rejects the null hypothesis at the signiﬁcance level α if Tn1,n2 ≥b
σ0,n1,n2zα.
(2.3) As an alternative to the quadratic form statistic (Li and Chen, 2012), Cai et al. (2013) studied the null hypothesis (2.1) in terms of the maximal absolute diﬀerence of two covariance matrices, i.e., H0 :
max
1≤i≤j≤p |σij1 −σij2| = 0. Cai et al. (2013) proposed a maximum test statistic Mn1,n2 based on the maximum of stan- 5 dardized diﬀerences between b
σij1’s and b
σij2’s. The maximum form statistic is written as Mn1,n2 =
max
1≤i≤j≤p
(b
σij1 −b
σij2)2 b
θij1/n1 + b
θij2/n2,
(2.4) where the denominator b
θij1/n1 + b
θij1/n2 estimates the variance of b
σij1 −b
σij2 to account for the heteroscedasticity of b
σij1’s and b
σij2’s among diﬀerent entries. Cai et al. (2013) proved that the asymptotic null distribution of Mn1,n2 is a Type I extreme value distribution (also known as the Gumbel distribution). Thus, Cai et al. (2013) rejects the null hypothesis at a signiﬁcance level α if Mn1,n2 ≥qα + 4 log p −log log p,
(2.5) where qα is the upper α quantile of the Gumbel distribution. 3 Fisher’s Combined Probability Test Li and Chen (2012) and Cai et al. (2013) have their respective power for testing high- dimensional covariance matrices. The quadratic form statistic Tn1,n2 is powerful against the dense alternative, where the diﬀerence between Σ1 and Σ2 under the squared Frobenius norm is no smaller than the order of tr(Σ2
1)/n1 + tr(Σ2
2)/n2. The maximum form statistic Mn1,n2 is powerful against the sparse alternative, where at least one entry of Σ1 −Σ2 has the magnitude larger than the order of
p log p/n. However, Tn1,n2 performs poorly against the sparse alternative and Mn1,n2 performs poorly against the dense alternative. More details will be presented in Subsection 4.3 and Section 5. Fan et al. (2015), Li and Xue (2015), Yang and Pan (2017) and Li et al. (2018) studied the weighted combination J = J0 + J1 to achieve the power enhancement, where J0 is built on the extreme value form statistic and J1 is constructed from the asymptotically pivotal statistic. It is worth pointing out that, with the proper weighted combination, J enjoys the so-called power enhancement principles (Fan et al., 2015): (i) J is at least as powerful as J1, (ii) the size distortion due to the addition of J0 is asymptotically negligible, and (iii) power is improved under the designated alternatives. For testing large covariance matrices, Yang and Pan (2017) proposed J1 = (1 −(sp + ξ1)−1)Mn and J0 = n 1 sp+ξ1 + 1 ξ2 · max1≤i,j≤p(b
σij1 −b
σij2)2, where Mn is a macro-statistic which performs well against the dense alternative, and sp is the number of distinct entries in two covariance matrices. Note that the quantities ξ1 and ξ2 are carefully chosen such that J0 →0 under H0. As a promising alternative, we propose a scale-invariant combination procedure based on Fisher’s method (Fisher, 1925) to combine both strengths of Tn1,n2 and Mn1,n2. Let Φ(·) 6 be the cumulative distribution function of N(0, 1) and G(x) = exp

−
1
√ 8π exp
",0
"This article reviews the current state of teaching and learning mathematical modeling in the context
of sustainable development goals for education at the tertiary level. While ample research on mathe-
matical modeling education and published textbooks on the topic are available, there is a lack of focus
on mathematical modeling for sustainability. This review aims to address this gap by exploring the
powerful intersection of mathematical modeling and sustainability. Mathematical modeling for sustain-
ability connects two distinct realms: learning about the mathematics of sustainability and promoting
sustainable learning in mathematics education. The former involves teaching and learning sustainabil-
ity quantitatively, while the latter encompasses pedagogy that enables learners to apply quantitative
knowledge and skills to everyday life and continue learning and improving mathematically beyond for-
mal education. To demonstrate the practical application of mathematical modeling for sustainability, we
discuss a specific textbook suitable for a pilot liberal arts course. We illustrate how learners can grasp
mathematical concepts related to sustainability through simple yet mathematically diverse examples,
which can be further developed for teaching such a course. Indeed, by filling the gap in the literature
and providing practical resources, this review contributes to the advancement of mathematical model-
ing education in the context of sustainability. Keywords: sustainable learning; mathematics education; mathematical modeling; sustainability, sus-
tainable development goal 4. 1
Introduction Sustainable learning plays a crucial role in education as it encompasses a comprehensive approach that
integrates environmental, economic, and social factors. This interdisciplinary field combines the elements
of education, sociology, and environmental studies to provide learners with the tools they need to navigate a
rapidly changing world. By recognizing the interconnectedness of individuals, communities, and the planet,
sustainable learning strives to strike a balance that ensures long-term and continuous learning [1,2]. The
goal of sustainable learning is to equip learners with the knowledge, skills, and attitudes necessary to lead a
sustainable life. It goes beyond traditional education by emphasizing the importance of informed decision-
making, active engagement, and responsible citizenship. Through sustainable learning, individuals become
catalysts for change, contributing to the creation of a more sustainable society and better future [3]. By
integrating sustainability principles into educational practices, we can cultivate a generation of learners
who are not only academically proficient, but also equipped to address the complex challenges our world
faces. Through holistic education that embraces sustainable learning, individuals can be empowered to
make informed choices, foster environmental stewardship, promote social equity, and drive positive change.
Sustainable learning aligns seamlessly with Sustainable Development Goal 4 (SDG4) set by the United
Nations (UN). SDG4 aims to ensure inclusive and equitable quality education, fostering lifelong learning
opportunities for all [4]. Sustainable learning embodies this goal by advocating accessible education that
transcends socioeconomic barriers and geographic boundaries. It emphasizes the importance of equipping
learners with relevant knowledge and practical skills that can be applied in their personal and profes-
sional lives [5]. By embracing sustainable learning practices, we can contribute to the realization of SDG4
and its overarching vision of quality education for all. Sustainable learning recognizes that education is a 1 arXiv:2307.13663v1  [math.HO]  20 Jul 2023 powerful tool for social transformation, empowering individuals to actively participate in shaping a sus-
tainable future. It promotes an inclusive and equitable learning environment, ensuring that every learner
has equal opportunities to acquire knowledge and skills applicable in real-world contexts. Through sus-
tainable learning, we can bridge the gap between education and the pressing challenges of our time, such
as environmental degradation, social inequality, and economic instability. By integrating sustainable prin-
ciples and values into educational systems, we equip learners with tools to address these complex issues
and contribute to building a more sustainable and equitable world [6,7].
In line with the UN SDG4 of quality education, proficiency in mathematics holds a prominent place
alongside reading skills. Mathematics education plays a pivotal role in attaining this goal, as it equips indi-
viduals with critical thinking abilities, analytical skills, and problem-solving proficiency, which are essential
for success in various aspects of life. Mastering mathematics not only fosters personal growth but also con-
tributes to sustainable economic and social development, empowering individuals to make meaningful
contributions to their communities and society at large. The significance of mathematics extends beyond
its fundamental nature. It provides individuals with the quantitative knowledge and critical thinking skills
necessary for tackling complex issues crucial to building a sustainable future [8]. Mathematical proficiency
plays a vital role in addressing global warming and climate change in order to promote environmental sus-
tainability and economic development. By nurturing a deep understanding of mathematics, learners are
equipped to effectively analyze and confront these multifaceted challenges. To ensure the realization of
SDG4 in quality education, it is paramount to provide high-quality mathematics education that is accessi-
ble and inclusive to all learners. This entails breaking down barriers based on socioeconomic backgrounds,
gender, or ethnicity, and creating an environment that fosters mathematical excellence for all individuals.
By prioritizing equitable access to mathematics education, we can enhance learners to develop the neces-
sary skills and knowledge to actively contribute to society and advance sustainable development [9,10].
One of the SGD4 targets includes equal access to higher education. Although it does not explicitly men-
tion mathematics education at the tertiary level, many subjects at the college level often encompass quan-
titative and analytical skills, particularly for Science, Technology, Engineering, and Mathematics (STEM)
majors. These skills are essential for solid preparation in mathematics-related topics, which are integral to
various fields, even in several non-STEM subjects. Some social science majors, such as economics, finance,
accounting, psychology, and sociology, mathematics remains extensively utilized. These disciplines rely
on mathematical models and techniques to analyze data, make predictions, and develop theories. For in-
stance, economics employs mathematical models to understand economic behavior, predict market trends,
and formulate policies. In finance and accounting, mathematics is used for analyzing financial data and
making informed investment decisions. Similarly, mathematical models contribute to the understanding of
human behavior and social phenomena in fields such as psychology and sociology [11–15].
The journey towards majoring in STEM disciplines often entails a sequence of mathematics courses
that serve as foundational pillars. These courses range from PreCalculus to Differential Equations and
encompass subjects such as Calculus, Vector Calculus, Linear Algebra, Discrete Mathematics, Engineering
Mathematics, Probability and Statistics, and several other courses. However, these courses very often lean
heavily toward the theoretical part rather than their practical applications. This imbalance is particularly ev-
ident for science and mathematics majors, let alone discussing sustainability in depth. Application-oriented
courses are typically found within the engineering realm. As a result, many mathematics and physical sci-
ence students who desire to explore broader applications of mathematics beyond their respective fields are
unaware of the existence of such courses. By developing a liberal arts course centered on mathematical
modeling for sustainability, our aim is twofold: to bridge this gap by introducing students to the realm of
mathematical modeling and acquaint them with the mathematics underlying sustainability. Through this
endeavor, we aspire to cultivate sustainable learning in mathematics education and equip students with a
holistic understanding of the subject matter.
The primary objective of this article is to present a comprehensive review of research and pedagogical
approaches pertaining to mathematical modeling at the undergraduate level. In addition to this, we aim
to address the often-overlooked aspects of sustainability in various academic disciplines. By intertwining
mathematical modeling with sustainability, we endeavor to establish a connection that facilitates learners 2 in their exploration of sustainability and promotes the adoption of sustainable learning practices, particu-
larly within the realm of mathematics education. This integration can be likened to the connection between
the theoretical and applied aspects of mathematics through mathematical modeling. Although the existing
body of literature offers numerous review articles and research papers on mathematical modeling in edu-
cation, a dearth of studies focusing on the teaching and learning of mathematical modeling in the context
of sustainability is evident. Therefore, this article aims to bridge this gap and contribute to the existing
knowledge base by providing a much-needed review of this domain.
Figure 1 illustrates the theoretical framework of this review. This conceptual structure highlights the in-
terconnection between learning about sustainability and sustainable learning in (mathematics) education.
The left-hand side features a blue horizontal ellipse, symbolizing the realm of “learning about sustainabil-
ity.” Here, students gain knowledge about various aspects of sustainability, including the intersection of
mathematics and sustainability, as well as the relationships between the economy, culture, society, and
the environment. It is our goal that through this learning, students will embrace a sustainable lifestyle
throughout their lives by reusing necessary resources and minimizing wastes, actively caring for ecolog-
ical health, limiting the use of Earth’s natural resources, as well as ensuring quality and vitality of living
environments [17–20].
On the right-hand side, the green vertical ellipse represents the realm of sustainable learning in (math-
ematics) education. This signifies learning that extends beyond the confines of formal education, empha-
sizing continuous renewal and improvement. Our aspiration is that students who engage in sustainable
learning will consistently enhance their quantitative skills, going beyond the scope of required course-
work, particularly in the mathematics and quantitative domains. They will adopt strategies in coping with
challenging circumstances that require renewing and relearning, independent and collaborative learning,
active learning, and lifelong learning—that is, the transferability of learning from formal to informal set-
tings [21–24].
Connecting these two realms is the red rectangular bridge that symbolizes the course of mathemat-
ical modeling for sustainability. Enrolling in this course offers students two primary objectives: First, it
aims to provide a deeper understanding of sustainability, sustainable development, and the mathematics
underlying sustainability concepts. Second, it aims to enhance learners’ quantitative skills, enabling them
to surpass the requirements of regular coursework. By integrating these objectives, we aspire to foster
a holistic approach to sustainability and sustainable learning, empowering students to make meaningful
contributions to society and the environment. Learning about sustainability
Mathematical modeling for sustainability Sustainable learning in education Figure 1: Illustration of the conceptual framework for this review. The blue horizontal ellipse on the left-
hand side denotes the realm of “learning about sustainability.” This includes learning the mathematical
aspects of sustainability. The green vertical ellipse on the right-hand side denotes the realm of sustainable
learning in (mathematics) education, or “learning that lasts.” Any learner who adopts sustainable learning
will continue to renew and improve themselves even after they graduate and leave the domain of formal
education, particularly in mathematics and quantitative skills. The red rectangle in the middle connects
these two domains, symbolizing the course on mathematical modeling for sustainability. The remainder of this paper is organized as follows. Section 2 explores the world of mathematical mod-
eling. We conducted an extensive literature review of research in education for mathematical modeling. 3 Section 3 continues with a liberal arts course on mathematical modeling for sustainability. We review a par-
ticularly relevant textbook and offer some additional examples that might help readers better understand
the various concepts used in mathematical modeling related to sustainability. Finally, Section 4 concludes
the paper. 2
The world of mathematical modeling Mathematical modeling is the process of using mathematics to represent, analyze, and solve real-world
problems by creating a mathematical representation of a scenario to make predictions or gain insights [25].
The acquisition of principles and techniques in mathematical modeling is a powerful tool that can be utilized
in various fields. Thus, teaching mathematical modeling to pupils at every level of education is essential
for sustainable learning in education. As explained by Pollak (2011), the heart of mathematical modeling
is problem finding before problem solving [26]. Furthermore, Maaß et al. (2018) also argued that offering
applicable mathematics education in schools will not only convince and motivate students who wonder
why they should study mathematics but also equip students with transferable skills that will be useful for
them along the road, such as problem-solving, critical thinking, and analytical reasoning, making it suitable
for promoting sustainable learning in mathematics education [27].
In what follows, we provide a literature review of the teaching and learning of mathematical modeling.
We continue with some textbooks that can be adopted as useful resources for teaching a course on mathe-
matical modeling at the tertiary level, while attempting to identify topics that are significantly relevant to
sustainability. 2.1
Literature review Greefrath and Vorhölter (2016) provided an overview of the German discussion on modeling and appli-
cations in schools, considering the development from the beginning of the 20th century to the present,
and discussed the term “mathematical model” as well as different representations of the modeling pro-
cess as modeling cycles [28]. Greer (1993) discovered that 13- and 14-year-old students often provide
unrealistic answers to word problems because of their tendency to rely on stereotyped procedures and as-
sumptions of direct proportionality, highlighting the need for a shift in perspective towards viewing word
problems as modeling exercises, while considering the underlying assumptions and appropriateness of the
model used [29]. Gainsburg (2006) reported an ethnographic study of structural engineers and revealed
that modeling is central to their work. This presents challenges for K-12 education because, on the one
hand, reformers in mathematics education advocated for incorporating mathematical modeling activities
into K-12 curricula to reflect real-world problem solving, and on the other hand, a lack of observational
descriptions of adult modeling behavior makes it difficult to assess the authenticity of classroom modeling
tasks [30].
Similarly, Niss and Blum (2020), Leung et al. (2021), Saxena et al. (2016), Arseven (2015), Dunn
and Marshman (2020), and Schukajlow et al. (2018) provide insights and perspectives on teaching and
learning mathematical modeling in various educational contexts [31–36]. With an emphasis on the sec-
ondary level, Niss and Blum (2020) included resources for teachers to acquire the knowledge and compe-
tencies that will allow them to successfully include modeling in their teaching [31]. Leung et al. (2021)
explored the teaching and learning of mathematical modeling and its applications, showcasing research
and collaboration between colleagues from China and other parts of the world, offering new perspectives
and resources for mathematical modeling education [32]. Saxena et al. (2016) presented the teaching
and learning of mathematical modeling from an Indian perspective, discussing the benefits and challenges
while attempting to improve the traditional way of the process [33].
In addition to providing an overview of the theoretical basis, concepts, and inclusion of modeling ac-
tivities in Turkish primary, secondary, and high school mathematics schools, Arseven (2015) explored the
importance of mathematical modeling in science and mathematics education, specifically in the context of
international exams such as PISA, where the report can be useful as a pathway to improving sustainable 4 education for each participant country [34,37]. Dunn and Marshman (2020) highlighted the importance of
connecting mathematics with the real world through mathematical modeling using real data, emphasized
the need to understand the implications and differences between validating and estimating with data, and
recommended an approach of mathematizing the context to promote students’ perception of the real-world
relevance of mathematics [35]. Schukajlow et al. (2018) surveyed the current status of empirical studies
in the teaching and learning of mathematical modeling, analyzed the development of studies focusing on
cognitive aspects of the promotion of modeling, and discovered that case studies and cognitively oriented
approaches tend to be more dominant than quantitative research or affect-related studies [36].
Kaiser (2017) offered valuable information and perspectives for researchers and educators interested
in the field of mathematical modeling in mathematics education. The article covers a range of topics,
such as the definition and characteristics of mathematical modeling, the role of teachers and students in
the modeling process, challenges and strategies for teaching modeling, and the assessment of modeling
skills [38]. In their theoretical study, Dundar et al. (2012) argued that mathematical modeling is a founda-
tional aspect of mathematics education, involving the conversion of real-world problems into mathematical
forms, and applicable across various areas of mathematics and educational levels [39]. Recently, Cevikbas
et al. (2022) reviewed the current discussion on mathematical modeling competencies, urging the need
for further the theoretical work, while highlighting the richness of developed empirical approaches and
their implementation at various educational levels [40]. 2.2
Textbooks on mathematical modeling Many books and monographs are dedicated to mathematical modeling, ranging from an introductory level
to analysis and simulations. In what follows, the list is far from exhaustive, and although they are not
particularly concerned with sustainability and sustainable development, some topics and examples can
still relate to, be applied to, and be useful for understanding issues in sustainability.
For example, the second part of Haberman (1988) deals with mathematical ecology and population
dynamics, which sparks recent interest in sustainable populations due to low birth rates in many parts of
the world [41]. When discussing population ecology, Mooney and Swift (1999) introduced the harvesting
effect to the population model, raising some awareness of the sustainability of a particular population
due to hunting, fishing, culling, or lumbering [42]. In addition to discussing experimental modeling of
harvesting bluefish (Pomatomus saltatrix) and blue crabs (Callinectes sapidus) from Chesapeake Bay in
Maryland and Virgina, Giordano et al. (2014) also presented the management of renewable resources,
such as the fishing industry [43]. Heinz (2011) covered both the deterministic and stochastic aspects of
global warming modeling and carbon dioxide concentration [44].
Although it does not contain the phrase “mathematical modeling” in its title, the second edition of
Shiflet and Shiflet (2014) covers a wide range of mathematical modeling and its applications, where sev-
eral dedicated modules are strongly related to sustainability, such as disease modeling, the carbon cycle,
global warming, mercury pollution, flu pandemics, and ornamental gardens [45]. Similarly, Tung (2007)
dedicated one chapter each to snowball Earth and global warming, as well as to El Niño and the Southern
Oscillation (ENSO), the topics that relate to climate patterns and their consequences toward sustainabil-
ity [46]. Meerschaert (2013) presented examples related to population dynamics of two similar species that
are competing between each other, that is blue whale (Balaenoptera musculus) and fin whale (Balaenoptera
physalus) [47]. Primarily intended for students with a working knowledge of calculus but minimal train-
ing in computer programming in the first course of modeling, Humi (2017) assists readers in mastering
the processes used by scientists and engineers in modeling real-world problems, including the challenges
posed by space exploration, climate change, energy sustainability, chaotic dynamical systems, and random
processes [48].
Other textbooks on mathematical modeling do not necessarily contain examples or applications that are
directly related to sustainability. However, instructors who plan to utilize such materials can be creative and
innovative in connecting mathematical topics with sustainability applications. Very often, these textbooks
cover a wide range of applications, not only in physical science and engineering, but also in biological
systems and social sciences, issues that have some implications for sustainability. 5 For example, Meyer (1984) featured independent sections on mathematical modeling that illustrated
the most important principles and a variety of applications from physical structures, biological systems, so-
cial sciences, and operational research [49]. Similarly, employing a practical learning-by-doing approach,
Bender (2000) offered more than 100 reality-based examples from various fields, while fostering the de-
velopment of skills needed to set up and manipulate mathematical models [50]. After introducing a set of
foundational tools in mathematical modeling, Dym (2004) applied these tools to a broad variety of fields
that range from biology to economics, including traffic flow, mechanical systems, optimization problems,
and social decision-making [51]. Howison (2005) demonstrated that applied mathematics is much more
than a series of academic calculations through a dozen applications, from the modeling of hair to piano
tuning, egg incubation, and traffic flow [52]. Using only basic knowledge of calculus and linear alge-
bra, Velten (2009) also selected many examples from various fields, such as biology, ecology, economics,
medicine, agriculture, chemical, electrical, mechanical, and process engineering [53].
The volume from Lin and Segel (1988) can be considered a classic in the pedagogy of applied math-
ematics, where it addressed the construction, analysis, and interpretation of mathematical models that
shed light on significant problems in the physical sciences. This book is divided into three parts. The first
part provides an overview of the interaction between mathematics and natural sciences. The second part
illustrates some fundamental procedures in ordinary differential equations. Finally, the third part discusses
the theory of continuous fields [54]. Fowkes and Mahony (1994) covered a wide range of mathematics in
their textbook, and explained why some techniques work well in certain situations but not in other circum-
stances. Using the apprenticeship approach, they also suggested when to stop analyzing mathematically
and start the numerical work instead [55].
Gernshenfeld (1999) discussed common techniques that are useful for mathematical modeling, such
as analytical techniques, numerical methods, and observational data analysis [56]. Bungartz et al. (2014)
not only introduced mathematical modeling but also emphasized the importance of computer-oriented
modeling and simulation as a universal methodology. In addition to addressing various model classes and
their derivations, the authors also showcased the versatility of approaches that can be applied, including
discrete, continuous, deterministic, and stochastic methods [57]. Eck et al. (2017) featured the use of
mathematical structures as an ordering principle instead of the fields of application in their modeling book
although the authors also discussed several applications in the fields of population and chemical reaction
dynamics, among others [58]. Garfinkel et al. (2017) specifically presented the modeling approach with
examples and applications in life sciences. It equips readers with the quantitative skills needed in explor-
ing complex feedback relationships and counterintuitive responses commonly found in natural dynamical
systems [59]. By targeting his textbook at newcomers to mathematical modeling, Banerjee (2021) of-
fered step-by-step guidance on model formulation and covered a wide range of examples from different
fields, providing an interdisciplinary overview and highlighting common themes in equilibrium, stability,
bifurcations, and parameter estimation [60].
Many textbooks in mathematical modeling focus on continuous variables instead of discrete ones. In
terms of randomness and uncertainty, they mostly focus on deterministic rather than stochastic or proba-
bilistic aspects. Although several textbooks offer both analytical and numerical approaches, most of them
focus on the former. Table 1 summarizes these observations. 3
Mathematical modeling for sustainability To the best of our knowledge, there are currently two textbooks dealing with mathematical modeling for
sustainability: Roe et al. (2018) [61] and Hersh (2006) [62]. Other books have some potential to be used
for teaching mathematical modeling for sustainability, such as Mordeson and Matthew (2020) [63], De
Lara and Doyen (2008) [64], Gupta et al. (2016) [65], Parkhurst (2006) [66], Walter (2011) [67], Fusaro
and Kenschaft (2020) [68], and Takeuchi et al. (2007) [69], among others. In what follows, we will focus
on reviewing Roe et al. (2018) [61].
The textbook belongs to the Texts for Quantitative Critical Thinking (TQCT) series, a collection of un-
dergraduate textbooks that develop quantitative skills and critical thinking through the exploration of real- 6 Textbook
Type of variables
Randomness and uncertainty
Approach Author(s)
Year
Discrete
Continuous
Deterministic
Stochastic/
Analytical
Numerical
probabilistic Banerjee
2021
✓
✓
✓
✓
✓
Bender
2000
✓
✓
✓
✓
✓
Bungartz et al.
2014
✓
✓
✓
✓
✓
✓
Dym
2004
✓
✓
✓
Eck et al.
2010
✓
✓
✓
Fowkes & Mahony
1994
✓
✓
✓
✓
Garfinkel et al.
2017
✓
✓
✓
Gershenfeld
2003
✓
✓
✓
✓
Giordano et al.
2014
✓
✓
✓
✓
✓
Haberman
1988
✓
✓
✓
Heinz
2011
✓
✓
✓
✓
Howison
2010
✓
✓
✓
Humi
2017
✓
✓
✓
Lin & Segel
1988
✓
✓
✓
✓
Meerschaert
2013
✓
✓
✓
✓
✓
Meyer
2004
✓
✓
✓
✓
✓
Mooney & Swift
1999
✓
✓
✓
✓
✓
Shiflet & Shiflet
2014
✓
✓
✓
✓
Tung
2007
✓
✓
✓
Velten
2009
✓
✓
✓
✓ Table 1: Summary of textbook content based on three main characteristics. First, whether the type of
variables involved in the modeling is discrete or continuous. Second, whether the factors of randomness
and uncertainty were considered, that is, deterministic vs. stochastic or probabilistic. Third, whether the
approach to solving modeling problems is analytical or numerical/computational. 7 world questions using mathematics and statistics, providing students from various disciplines the oppor-
tunity to enhance their understanding, evaluation, and communication of quantitative information. In the
foreword of the book, the former president of the Mathematical Association of America and Benediktsson-
Karwa Professor of Mathematics at Harvey Mudd College, Francis Edward Su, wrote that the book presents
a unique perspective on mathematics, emphasizing active participation, wisdom, and considering human
factors. It explores how mathematics can contribute to addressing sustainability challenges that encompass
economic, moral, and scientific aspects. As the authors have dedicated themselves to this impactful book,
they hope that readers will approach it with enthusiasm to address real-world problems for the benefit of
all.
This book is divided into three parts. The first part is the core of the material, where fundamental
concepts are presented in the order of complexity. It comprises six chapters that cover the various topics
of measuring, flowing, connecting, changing, risking, and deciding. The second part contains a collection
of nine case studies that apply the mathematical tools from the previous part to address sustainability-
related questions and explore real-world examples. The third part contains reference materials, including
suggestions for further reading.
The authors have done an excellent job in developing the theme by building progressive themes, from
measuring to deciding. However, further investigation of the mathematical topics presented in each chapter
may reveal different structures. Not all users, instructors and students alike, will find the mathematical
content progressive. By selecting different mathematical topics, the core of the book looks like a salad-style
of subjects, a mix of everything. Instead of focusing on a particular topic, let say, differential equations,
and building up from there, it requires a multidisciplinary background approach. Any instructor who
plans to use it for a course should have a broad knowledge of various topics in mathematics, in addition to
being aware of issues in sustainability and the ethics surrounding its implementation. A good knowledge of
economics would also be advantageous, as the market paradigm, Pareto efficiency, market failure, expected
utility theory, and prospect theory, among others, occur sporadically among the texts.
To provide some ideas, the second chapter on flowing requires solid knowledge of system theory, in-
cluding stocks, flows, equilibrium, feedback loops, their relationships, and their dynamics. The theory also
finds applications in business, management, industrial processes, organizations, and engineering, among
others [70–74]. The third chapter on connecting requires graph theory [75–77]. Both Chapters 3 and 4
on connecting and changing discussed a commonly utilized model in population growth, that is, exponen-
tial and logistic, albeit in the absence of calculus and differential equations. The fifth chapter on risking
requires a profound understanding of data analysis, probability, and statistics [78–82]. Finally, the sixth
chapter on deciding requires a solid understanding of game theory [83–88].
It is commonly found in many textbooks on mathematical modeling that growth and decay modeling
often employs initial value problems of an ordinary differential equation, a subject that requires under-
standing and application of calculus. Amazingly, Roe et al. (2018) presented the material in the absence of
calculus. However, they allowed instructors to have any liberty in inserting calculus and differential equa-
tions when discussing this subject. Modeling population growth has direct implications for sustainability.
Similar to the two sides of coins, population dynamics are closely linked to sustainability. On the one hand,
an increasing population can impact on the availability of resources and the environment because there
would be a greater demand for food, water, energy, and other resources that can strain ecosystems and lead
to resource depletion [89]. On the other hand, the low birth rates that are currently occurring in many
parts of the world can pose challenges for sustainable development due to the declining workforce and ag-
ing population that impact social safety net, healthcare systems, labor shortages, decrease in productivity,
and hindering economic growth [90,91].
Regarding the modeling of the decay rate, Roe et al. (2018) employed a discrete version of Newton’s
Law of Cooling, although their presented example does not really concern with sustainability. We would
like propose the following example on this topic. A similar case in point or other real-life applications of the
law associated with sustainability will enhance students’ understanding of the mathematics of sustainable
development. 8 Farmers were faced with the task of preserving a large quantity of harvested vegetables to prevent
spoilage. The vegetables had an initial temperature of 25°C, and they needed to be stored in a cold storage
facility maintained at a constant temperature of 5°C. The cooling process followed Newton’s Law of Cooling,
with a cooling rate of 0.099°C per hour. Determine the time it would take for the vegetable temperature to
reach 10°C. Additionally, if the initial temperature of the vegetables was lowered to 20°C and the cooling
rate was increased to 0.183°C per hour, calculate the time difference in electricity savings compared with
the previous scenario.
To solve this problem, we acquired Newton’s Law of Cooling, which can be formulated as the following
differential equation:
dT dt = k (T −Ts), where T denotes the temperature of the object at time t, Ts denotes the temperature of the surroundings,
and k is the cooling rate constant. It solutions is given by T (t) = Ts + (T0 −Ts) e−kt, where T0 denotes the initial temperature of the object. We have Ts = 5, T0 = 25, k = 0.099, and T(t) = 10,
then 10 = 5 + (25 −5) e−0.099 t 5 = 20 e−0.099 t e−0.099 t = 5 20 = 1 4
0.099t = ln4 t =
ln4 0.099 ≈14. Thus, it would take more than 14 hours for the vegetables to reach 10°C. For the second scenario, we have
Ts = 5, T0 = 20, k = 0.183, and T(t) = 10, then 10 = 5 + (20 −5) e−0.183 t 5 = 15 e−0.183 t e−0.183 t = 5 15 = 1 3
0.183t = ln3 t =
ln3 0.183 ≈6. Therefore, by decreasing the initial temperature and increasing the cooling rate, it would take at least 6
hours for the vegetables to reach 10°C. Hence, the time difference in saving electricity compared with the
previous case would be 14 −6 = 8 hours.
This example illustrates a simple application of Newton’s Law of Cooling in sustainability, that is, the
food preservation and storage of perishable goods. By understanding the cooling rates and temperature
changes, optimal storage conditions can be determined to extend the shelf life and reduce food waste [92].
Other applications are certainly available, although they might involve and be in combination with other
physical principles, such as building energy efficiency and thermal comfort analysis. Understanding how
buildings lose heat in the environment is crucial for a sustainable design. Newton’s Law of Cooling can be
used to model and optimize heating and cooling systems, improve energy efficiency, and reduce greenhouse
gas emissions [93]. The law can also help assess and ameliorate the thermal comfort of indoor spaces.
By analyzing the cooling rates and temperatures in different areas, sustainable building designs can be
developed to provide comfortable environments while minimizing the energy consumption [94]. 9 In terms of probability, Roe et al. (2018) presented examples that are commonly found in any textbook
discussing the topic, such as tossing a coin, rolling dice, drawing cards from a deck, and other similar ex-
periments. Although one example of inferential statistical analysis is closely related to sustainability, that
is, groundwater contamination by methane either due to its natural occurrence or via hydraulic fractur-
ing [95,96], there were no particular examples on conditional probability in sustainability-related contexts.
Many readers would appreciate more examples of the application of conditional probability to various sce-
narios related to sustainability. For example, in urban planning and transportation, conditional probability
can be utilized to assess the likelihood of achieving sustainable transportation outcomes, such as reduced
congestion or increased use of public transit, based on factors including infrastructure development, pol-
icy interventions, and behavior change [97,98]. In the area of natural resource management, conditional
probability can be used in analyzing the likelihood of sustainable resource extraction, such as by calculating
the probability of a fish population recovering to a desired level given certain conservation measures and
fishing quotas [99–101].
We consider the following example has a direct application to sustainability and simultaneously check
student’s understanding of conditional probability. Figure 2: Improper placement of wind turbines can result in unintended consequences for bird and bat
fatalities, despite their significance as a valuable source of intermittent renewable energy. The construction of a wind power infrastructure has been identified as a cause of bird and bat mortality.
See Figure 2. This is because turbines are often situated in the migration paths of birds, posing a risk to
birds with their spinning blades. Additionally, the presence of wind turbines can attract bats owing to
increased insect density, as they perceive the sites as a potential food source. However, it is worth noting
that the pesticides used in agricultural areas also pose a significant threat to bird and bat populations.
In a specific region examined by environmental activists, the annual mortality rates of migrating birds
in the presence of wind turbines and pesticides were recorded as 27,000 and 41,000, respectively. The
activists discovered that the spinning blades of the wind farm caused 9,000 bat fatalities per year, whereas
agricultural pesticides accounted for 23,000 bat mortalities annually in the area. What is the probability
that pesticides cause animal mortality, given that the flying creatures are birds? Find the probability of the
flying creatures are bats, given that the cause of fatality is wind turbines.
To answer these questions, constructing a two-way frequency table would definitely be helpful, as
presented in Table 2. Otherwise, we simply use the formula for conditional probability. Given that the
flying creatures are birds, the probability that pesticides cause animal mortality is given as follows: P (pesticides | bird) =
41,000 41,000 + 27,000 = 41 68 = 60.29%. Therefore, the probability that agricultural pesticides cause animal mortality in the examined region, given
that the animals are birds, is approximately 60%. If the wind turbines cause animal mortality, the proba-
bility that flying creatures are bats is given as follows: P (bat | wind turbines) =
9,000 9,000 + 27,000 = 9 36 = 25%. 10 Hence, the probability of animal mortality is bats, assuming that they were killed by wind turbines in the
examined region was 25%. Mortality causes and figures
Total
(in thousands) Wind turbines
Pesticides Flying creatures
Birds
27
41
68
Bats
9
23
32 Total
36
64
100 Table 2: Two-way frequency table for bird and bat fatalities due to wind turbines and pesticides. These
figures are useful for solving problems related to the conditional probability. Although the geographical location in this example is unspecified and the figures are fictitious, this
problem can serve the purpose in raising an awareness of sustainability. On the one hand, the construction
of wind turbines contributes to an additional source of renewable energy, in this case, using the wind’s ki-
netic energy to generate electricity. On the other hand, constructing such wind turbines and locating them
in correct locations concern not only better energy efficiency but also environmental and social problems.
Locations with the best wind currents often times coincide with migratory paths of birds [102–104]. Addi-
tionally, research on ecological sustainability suggests that bat fatalities at wind turbines can be caused by
the proximate category (direct means of death, such as collision with towers and rotating blades) and ul-
timate category (collisions due to bats’ attraction to turbines). Possible explanations for the latter propose
that bats could be drawn to turbines because of curiosity, misinterpretation, or the potential for activities
such as feeding, roosting, flocking, and mating [105–107]. It turns out that bird and bat fatalities caused
by wind turbines are just on a smaller scale in comparison to other causes. Communication towers, auto-
mobiles, pesticides, buildings, and cats killed more birds than wind turbines in the US alone [108–114].
Hence, adding pesticides to the considered example provides a better perspective for other causes of avian
and bat casualties.
Regarding game theory and the tragedy of the commons, Roe et al. (2018) presented some examples
related to sustainability, that is, the pollution control of a system of lakes and rivers between two territories
and two farmers who share a common grassland and decide whether to add cows, respectively. The former
is a bit complicated because the two countries share three lakes. A simplified example would be helpful
for newcomers to game theory. The latter is an excellent one because it not only employs some concepts
in game theory but also allows for some variations of different scenarios. The following example provides
a simpler version of game theory on sustainability.
Consider two small towns, Arang (A) and Bimo (B), that share a common border. They are currently
evaluating two waste management options: one that focuses on minimizing an environmental impact (E)
and another that aims to maximize cost-effectiveness (C). The town councils of Arang and Bimo have
calculated the annual savings associated with each option, depending on the priorities that each town
chooses to adopt. On the one hand, if both towns prioritize minimizing the environmental impact, Arang
and Bimo would save approximately $10,000 and $8,000 annually, respectively. On the other hand, if
both towns prioritize maximizing cost-effectiveness, Arang and Bimo would save approximately $8,000
and $7,000 annually, respectively. In the scenario where one town dedicates itself exclusively to either
minimizing environmental impact or maximizing cost-effectiveness, each town would save approximately
$6,000 annually. However, due to the difference in size between the two towns, Arang would only save
$5,000 if Bimo town focuses on maximizing cost-effectiveness, while Arang prioritizes minimizing the
environmental impact. Conversely, Bimo would save an additional $4,000 compared with Arang if the
roles were reversed. By carefully analyzing these savings and considering their different priorities, Arang
and Bimo can make informed decisions on the most suitable waste management option that aligns with
their sustainability goals and economic considerations. Construct a payoff matrix to represent the decision-
making process for the “game” of selecting a waste management system between two “players” of towns 11 and determine the Nash equilibrium in this waste management system game by analyzing the optimal
strategies for each pair of players.
Consider the towns of Arang and Bimo as the two players in a game. Their objective was to select the
most suitable waste management system for their respective towns. The payoff matrix, representing the
benefits or saving costs associated with different choices, is presented in Table 3. The given figures are in
thousands of dollars. Bimo Environment
Cost Arang
Environment
(10,8)
(5,6)
Cost
( 6,9)
(8,7) Table 3: The payoff matrix for the game theory problem involving two towns, Arang and Bimo, that need
to find a balance between minimizing the environmental impact and maximizing cost-effectiveness. To determine a Nash equilibrium, we must find each player’s strictly best response to the other player’s
strategy. We observe the following situations: • If Arang plays the environment, then Bimo’s payoffs for the environmental impact and cost-effectiveness
would be $8,000 and $6,000, respectively. Thus, Bimo plays the environment is the strictly best response
to this strategy. • If Arang plays cost, then Bimo’s payoffs for the environment and cost are $9,000 and $7,000, respectively.
Thus, Bimo plays environment is also still the strictly best response to this strategy. It turns out that playing the environment is a strictly dominant strategy for Bimo. Conversely, we note the
following observations: • If Bimo plays the environment, then Arang’s payoffs for the environmental impact and cost-effectiveness
are $10,000 and $6,000, respectively. Thus, Arang plays environment is the strictly best response to this
strategy. • If Bimo plays the cost, then Arang’s payoffs for the environment and cost are $5,000 and $8,000, respec-
tively. Thus, Arang plays cost turns out the strictly best response to this strategy. We observe that Arang does not have any strictly dominant strategy in this game. However, because playing
the environment is the strictly best response from each town to the other town’s strategy, both Arang and
Bimo play the environmental impact is a Nash equilibrium.
There are two remarks that we need to state. First, the word “approximately,” which appears twice in the
problem, does not really mean anything, and it can be dropped as well if the readers wish. What we meant
with this word is, for example, that $9,990 is approximately $10,000, in the same manner as $10,050.
However, figures $4,000, $5,000, and $6,000 are on different orders, and they are not approximately the
same values. Second, the sentence “Conversely, Bimo would save an additional $4,000 compared to Arang
if the roles were reversed.” might be confusing to interpret by non-native English speakers. This state-
ment should be understood from the preceding sentences. The sentence “In the scenario where one town
dedicates itself exclusively to either minimizing environmental impact or maximizing cost-effectiveness,
each town would save approximately $6,000 annually.” means AC = 6 and BC = 6. The next sentence
“However, due to the difference in size between the two towns, Arang would only save $5,000 if Bimo
town focuses on maximizing cost-effectiveness while Arang prioritizes minimizing environmental impact.”
means (AE,BC) = (5,6). Hence, if the roles are reversed, it means (AC,BE) = (6,5 + 4) = (6,9) and not
(AC,BE) = (6,6+4) = (6,10) because the latter argument might appear from reasoning that reversing the
role of BC to BE would add an additional $4,000.
This example illustrates some basic concepts in game theory: constructing a payoff matrix of a game,
understanding each player’s strategy and payoff relative to the other player, and finding the best response 12 strategy and Nash equilibrium. Indeed, game theory has been applied to various aspects of sustainability,
including, but not limited to, stakeholders and company leaders [115], preserving energy resources [116],
resource depletion strategy [117], decision making in the chemical industry [118], and agricultural supply
chain [119].
The following and our final example illustrates the mathematical modeling of the tragedy of the com-
mons [120–122]. Although it lacks an application of game theory, it employs the power of an exponential
model. This problem was adapted from the TED-Ed video lesson by Nicholas Amendolare [123]. ·
⊂
⋊ ·
⊂
⋊ Figure 3: The illustration depicts a pond with the potential for exponential growth of the fish population.
However, irresponsible fishing practices can lead to the tragedy of the commons, resulting in the depletion
of fish stock. In Green Village, there is a pond that serves as a crucial source of livelihood for villagers, providing
them with fish. See Figure 3. The number of fish in the pond doubled every month, ensuring a potential
increase in catch. Alice and Bob, two residents of the village, have the responsibility of catching fish to
sustain their community. They understood the importance of preserving the pond’s resources and decided
not to catch all the fish at once, allowing for reproduction. In the first month, both Alice and Bob catch one
fish each, and plan to double their catch every subsequent month. However, despite their best intentions,
they lack expertise in mathematical modeling, particularly in the field of sustainability. As a consequence
of their miscalculations, the tragic phenomenon known as the “tragedy of the commons” became a concern
when the fish stock in the pond faced depletion. If initially there are 5 pairs of fish in the pond, how many
months will it take for all fish to be gone? To prevent the tragedy of the commons by the end of the year,
what should be the minimum total number of fish in the pond at the beginning of the year?
To answer the first question, let m0 = 2q = 20+1(q −0) be the total number of fish at the beginning of
the month, where q ∈N. Let pn = 2n be the total number of fish that were caught by Alice and Bob at the
end of the month n ∈N. The following recursive relation indicates the total number of fish at the end of
each month: m1 = 2(m0 −p1) = 2(2q −2) = 4(q −1) = 21+1 (q −1) m2 = 2(m1 −p2) = 2[4(q −1) −4] = 8(q −2) = 22+1 (q −2) m3 = 2(m2 −p3) = 2[8(q −2) −8] = 16(q −3) = 23+1 (q −3)
.
.
. mn = 2(mn−1 −pn) = 2[2n (q −n) −2n] = 2n+1 (q −n). Because q = 5 and we want to solve mn = 0, then q −n = 5 −n = 0, which gives n = 5. Hence, after
5 months, all fish in the pond will disappear. Table 4 shows the situation describing fish stock depletion,
where the number of fish caught follows monthly exponential growth. Observe that despite the potential
for exponential reproduction, the initial stock of the 5 pairs of fish would be depleted after 5 months. 13 To answer the second question, we use the last equation for n = 12. Because we want m12 > 0 or
q −12 > 0, so q > 12. Hence, considering q = 13, then the total number of fish at the beginning of the
year should be at least 26 to avoid the tragedy of the commons after one year. Month n
Fish caught pn
Fish in month n, mn 0
0
10
1
2
16
2
4
24
3
8
32
4
16
32
5
32
0 Table 4: Situation describing fish stock depletion. The total number of fish at the end of each month with
exponential growth in catch is listed in the third column. Despite the potential for exponential reproduction,
the initial stock of 10 fish would be exhausted after five months. While this simplified illustration portrays the tragedy of the commons, its occurrence is evident in the
fishing industry, where fisheries frequently experience over-exploitation. When a fishery is regarded as a
common resource, both locally and internationally, fishermen often prioritize maximizing their catch to
serve their immediate self-interest, often disregarding long-term implications. Consequently, their unsus-
tainable practices endanger the future sustainability of fisheries, posing a threat to the provision of a reliable
food source for present and future generations. In addition, these actions contribute to the degradation of
fish habitats and ecosystems [124–128]. 4
Conclusion We have conducted a comprehensive review of the current state of teaching and learning mathematical
modeling at the tertiary level, with a particular emphasis on sustainability and sustainable learning in
education. This resonates with UN Sustainable Development Goal (SDG) 4, which promotes continuous
and lifelong learning for all individuals. Our analysis revealed a significant gap in the body of published
literature regarding mathematical modeling for sustainability and sustainable learning in mathematics ed-
ucation.
Through our examination of various textbooks on mathematical modeling, we discovered that most
of them cover a wide range of topics but lack a specific focus on sustainability. Although some textbooks
include examples related to sustainability, they are often buried within the content and require additional
effort from instructors to extract and tailor them to sustainability issues. However, many textbooks exhibit
common characteristics, including considerations such as the type of variables (discrete or continuous), the
incorporation of randomness and uncertainty in modeling (deterministic or stochastic), and the approach
employed to solve problems (analytical or numerical).
To address this gap, we proposed a liberal arts course on mathematical modeling for sustainability that
is suitable for students from diverse academic backgrounds. This course not only introduces students to
sustainable development and the mathematics of sustainability but also enhances their quantitative think-
ing skills for sustainable learning in mathematics education. By bridging these two domains, the course
aimed to promote a quantitative understanding of sustainability and the application of mathematical skills
in various contexts. Our aspiration is for students who have completed this course to embrace sustain-
able learning in mathematics education, which entails a commitment to ongoing learning, relearning, and
continuous improvement in mathematical skills beyond the confines of formal education.
We also explored the potential benefits of adopting a specific textbook in the course on mathematical
modeling for sustainability. Although it may not cover traditional college mathematics topics such as the
use of calculus and differential equations, it offers quantitative content and encourages critical thinking 14 through real-world problem-solving. The book’s accessible language appeals to students outside STEM
fields, while those with a strong mathematical background can gain a broader perspective on the mathe-
matics of sustainability. Additionally, we provided classroom examples that demonstrate how mathematical
concepts, such as probability and game theory, find practical applications in sustainability.
In conclusion, our research highlights the need for a focused exploration of mathematical modeling
for sustainability in higher education. By integrating sustainability into mathematics education, we can
equip students with the skills and knowledge necessary for addressing complex real-world challenges in a
sustainable and quantitative manner. Acknowledgments The author acknowledges the anonymous referees for reviewing this manuscript. Funding statement This research receives no funding. Conflicts of interest The author declares that he has no conflicts of interest to disclose. Ethics statement Not applicable.",0
"This paper investigates the relationship between economic media sentiment and individuals’ expectations and perceptions about economic conditions. We test if economic media sentiment Granger-causes individuals’ expectations and opinions concerning economic conditions, controlling for macroeconomic variables. We develop a measure of economic media sentiment using a supervised machine learning method on a data set of Swedish economic media during the period 1993–2017. We classify the sentiment of 179,846 media items, stemming from 1,071 unique media outlets, and use the number of news items with positive and negative sentiment to construct a time series index of economic media sentiment. Our results show that this index Granger-causes individuals’ perception of macroeconomic conditions. This indicates that the way the economic media selects and frames macroeconomic news matters for individuals’ aggregate perception of macroeconomic reality. Key words: Economic Media, Economic Expectations, Supervised Machine Learning, Consumer Confidence Survey, Scandinavia. Jel codes: D84, D83, E32. Kristoffer Persson, Lund University School of Economics and Management, Box 7082, 220 07 LUND, Sweden Correspondence: kristoffer.persson@nek.lu.se Acknowledgements: Funding from the Jan Wallander and Tom Hedelius Foundation is gratefully acknowledged. 2 1. Introduction Individuals gather information on economic conditions from several different sources. Some of the main sources are various media outlets, such as newspapers, magazines and online publications (Fogarty 2005). The media often reports on current factual events and, to some extent, re-reports news that is published by experts in public reports and scientific papers (Nadeu et al. 1999). However, the media is not entirely neutral and can sometimes put a filter on reality when newspapers select and frame news stories to capture individuals’ attention (Gentzkow & Shapiro 2008; McCarthy & Dolfsma 2014). Individuals using the media to inform themselves about economic conditions may thus become biased in their opinions and expectations about the future, causing them to make wrong decisions (van Raaij 1989). Public opinion about economic conditions is paramount to economic outcomes. Individuals’ expectations about the state of the economy influence their decisions in terms of savings, consumption, investments and entrepreneurial endeavours (e.g. Keynes 1936; Lachmann 1943; Muth 1961, Arenius & Minniti 2005; Koellinger et al. 2007; Brown & Taylor 2006; Moretti 2011; Hirshleifer 2001; Baker & Wurgler 2007; Burnside et al. 2011). Because the media has an incentive to put a filter on economic conditions, they may have a de-stabilizing effect on the economy. For example, since the media tries to capture individuals’ attention by over-emphasizing the negative aspects of economic conditions, they may cause individuals to overestimate the risk of the economy entering into a recession. As a consequence of this individuals may decide to postpone consumption, investment and entrepreneurial endeavours more than what is motivated by actual economic conditions. This could potentially be the decisive factor that causes the economy to go into a recession (Pigou 1927; Jaimovich & Rebelo 2009). In this paper, we test if the way Swedish media report on economic conditions affects individuals’ opinions and expectations about them. We do this by testing if individuals’ opinions and expectations about economic conditions are Granger-caused by economic media sentiment when controlling for 3 macroeconomic variables. We develop a measure of media sentiment using a supervised machine learning method for text classification of media items’ sentiment based on their full textual content. We use 179,846 media items, stemming from 1,071 unique media outlets including the major nationwide newspapers, local newspapers and online news during the 1993–2017 period, to observe the development of sentiment in the Swedish economic media. We are thus including several booms and busts in the real economy and in the financial markets. The data that we use gives us unique possibilities compared to other studies that typically are limited to a few time periods or a few media outlets. Our results show that the economic media sentiment is slightly positive on average. Individuals’ opinion about current macroeconomic conditions is Granger-caused by economic media sentiment and affected by macroeconomic variables. However, economic media sentiment does not Granger-cause individuals’ expectations about future macroeconomic conditions, individuals’ expectations about their own personal economic situation or individuals’ evaluation of current developments concerning their own private economic situation. The rest of the paper is outlined as follows. Section 2 develops some hypotheses concerning the effects of competition among media companies on economic media sentiment and the effect of economic media sentiment on individuals’ opinions and expectations. Section 3 presents economic media data, section 4 presents macroeconomic- and expectations data. Section 5 presents results and section 6 concludes the paper. 4 2. The News Media and Competition: Three Hypotheses Information published by the media is an important component of an individuals’ information set. Individuals are constrained in their attention and thus often use the media to form an opinion about matters that require high cognitive effort, such as predicting economic outcomes. Information presented by the media is easier for individuals to access and process than, for example, economic and meteorological data published in tables or reports by national statistics agencies and central banks. Individuals who consume information published in the media thus only need to use relatively little cognitive effort to form an opinion about complex matters, which allows them to allocate their attention efficiently. The way the media reports economic conditions may thus affect individuals’ information set and thereby the decisions they make (e.g. Carroll 2003, Sims 2001). In most economies, different media outlets compete in a market for news (Winseck 2008). Competition for market shares between different media outlets should theoretically cause the media to report on events that are of great importance to most individuals (Mullainathan & Schleifer 2005). Media that are successful in doing so in a fast, accurate and neutral manner will also be successful in selling their product and in generating returns to their shareholders. If the media reports relevant information, this will enable individuals to be well informed about current and important matters, which will allow them to make adequate decisions (Gentzkow & Shapiro 2008). Competition for market shares between different media outlets may, however, cause the media to not report objectively on economic conditions. The reason for this is that when media companies compete for market shares they are de facto competing for consumers’ attention. This may cause the media to exaggerate the importance of some events and underemphasize the importance of others. The daily press, for example, may potentially maximize its revenue by selecting their main stories based on how well the essence of the story can be translated into an eye-catching headline. Such a headline has a greater potential of capturing individuals’ attention, causing them to buy single copies of the newspaper (Tannenbaum 1953). The media may thus not report on economic conditions to the extent 5 that is actually motivated by economic data, but instead report more on other topics which the media regard as better headline material (Andrew 2007). Another possible consequence of media competition is that the media may over-report and exaggerate the consequences of negative aspects of economic conditions. This implies that the media, albeit reporting on relevant and important economic events, tend to exaggerate the sensational and eye-catching aspects of economic events such as economic crisis and stock market crashes in order to attract individuals’ attention (Hester & Gibson 2003). Newspapers may also capture the attention of customers by tailoring their reporting to fit the political- ideological conviction of their target market segment. Newspapers thus over-emphasize some topics, such as unemployment, if the opposing political party is in power (Larcinese et al. 2011). There is some evidence in the literature showing that the media sentiment is not neutral. The media generally reports on current events and we thus expect economic media to mainly report on current business cycle conditions rather than long-term economic developments, such as economic growth. A neutral economic media should thus publish equally many media items with positive and negative sentiment, in the long run. Most previous studies, however, find that the media tends to publish more content with negative sentiment than positive sentiment, which may be a consequence of the competition between media outlets (Fogarty 2005; Arango-Kure et al. 2014; Boydstun et al. 2018). This literature, however, mainly uses US data and few studies have investigated whether media in other economies behave in a similar way. Media non-neutrality is likely to exist also in Sweden where the media, for example, reported heavily on the negative consequences of the global financial crisis of 2008, despite the fact that economic data showed that the Swedish economy was less affected by the crisis than many other economies (Claessens et al. 2010; Olsson et al. 2015). This brings us to our first hypothesis: H1: Economic media is not neutral and therefore publishes unequally many items with positive and negative sentiment. 6 Since individuals use the media to inform themselves about economic conditions, they may become biased in their opinions and expectations about economic conditions (van Raaij 1989). There is, however, no consensus in the literature concerning the effect of economic media on individuals’ evaluations and expectations about economic conditions. Some studies find little or no effect of the media on individuals’ opinions about current and future economic conditions (Fogarty 2005). Others find that the economic media affects individuals’ opinions about both the current state of the macro economy and their expectations about the future developments of macroeconomic conditions (Hester & Gibson 2003). There is also some evidence that media sentiment that cannot be explained by economic data affects individuals’ opinions and expectations about economic conditions. This suggests that media non-neutrality may influence individuals’ perceptions of economic reality (Boydstun et al. 2018). This brings us to our second hypothesis: H2: Individuals’ opinions and expectations about current and future economic conditions are Granger-caused by economic media content that is not explained by economic data. Individuals are sometimes better informed than the media about changes in economic conditions. The economic media typically reports on changes in macroeconomic conditions that affect most individuals in the economy. Individuals may, however, have access to better sources of information about their own personal economic situation that the media cannot access (Wåhlberg & Sjöberg 2000). This information is likely to be private and more directly linked to individuals’ own economic situation thus causing them to use it instead of economic media information. Opinions about their own personal economic situation for individuals that have good unemployment insurance are, for example, not likely to be affected if the media reports that a recession is coming, compared to individuals without unemployment insurance. Individuals may also disregard media information about the macro economy because they suffer from the illusion of control bias. They are thus overconfident about their own economic situation in relation to the macro economy and may therefore underestimate individual level economic risk, such as the 7 risk of becoming unemployed. Individuals may thus disregard media reporting about negative macroeconomic events when they assess their own personal economic situation (Bovi 2011). This brings us to our third and final hypothesis: H3: Economic media content does not Granger-cause individuals’ opinions and expectations about their private economic situation. 3. Economic Media Data In order to test our hypotheses, we need to quantify economic media content. Some studies do this by using economic data as proxies for economic media content (Alwathainani 2010; Nguyen & Claus 2013). This approach is problematic since it is not a direct measure of media content. It thus assumes both that individuals use the media to inform themselves about economic conditions and that the media reports neutrally on these conditions. These assumptions are highly restrictive as they do not allow individuals to be affected by both the media and by economic data, and because they do not allow for non-neutral media content. Other studies quantify media content by classifying media items into classes based on sentiment, i.e., whether the texts have a positive or negative tone. One approach is to classify the headlines and economic content on the front pages of influential newspapers with respect to their sentiment (Hester & Gibson 2003; Doms & Morin 2004; Birz & Lott 2011). This approach relies heavily on the assumption that media content can be reduced to headlines and front- page material without any substantial loss of sentiment information. Such an assumption is not likely to hold in general, because headlines and front pages are tools that media outlets use to capture individuals’ attention and thus differ from media items’ actual sentiment (Andrew 2007). We follow a strand in the literature that classifies media items’ actual textual content (Nadaeu et al. 1999; Fogarty 2005; Soroka 2006; Goidel et al. 2010; Lamla & Maag 2012; Lamla & Lein 2014; Dräger 2015; Baker et al. 2016; Armelius et al. 2017; Boydstun et al. 2018; Lamla et al. 2019). We classify economic media items’ full textual content into positive and negative sentiment classes, which 8 we then use to quantify media content by counting the number of media items in each month that belong to the positive and negative classes. We are thus able to create a time series index of economic media sentiment. The process by which we create this index is comprised of six individual steps as shown by Figure 1. In the first step, we download economic media items from an online media database. In step 2, we parse the downloaded information by separating media items’ content from each other and by separating media items’ header, body, publisher and date of publication into separate pieces of information; in step 3 we create test data by manually classifying a sample of news items with respect to sentiment. In step 4 we estimate a probability model using the test data we created in step 3. In step 5 we classify all of the media items that we have downloaded using the probability model from step 4 and in step 6 we calculate our economic media sentiment index. Figure 1. Workflow for Quantifying Media Sentiment 1. Download Economic 
Media Items - Boolean 
Search Query. 2. Data Parsing - Separating  
Media Items, Header, Body, 
Publisher etc. 3. Manual Classification of Subset of Media Items 
into Positive, Negative and Non-economic content 
classes. 4. Estimate a Probability Model - 
Naïve Bayes. 5. Automatic Classification of 
Full Set of Media Items. 6. Economic Media 
Sentiment Index Data Collection Supervised Machine 
Learning 9 3.1 Data Collection We use media data from the Retriever database, which contains most editorial media items published in print and online in Sweden since the 1980s. The database contains around one hundred million searchable media items published by a wide range of media outlets including all the major newspapers. Before the year 2000, the database consisted primarily of printed newspapers with nationwide coverage. From the year 2000 and onwards, the database also includes media items published online. This caused the number of media outlets in the database to increase and changed the composition of media sources to include more local daily newspapers, purely Internet based news sites, weekly news magazines, trade magazines, union magazines and public authorities. The data contains information on whether media items are published in print or online which allows us to split the data accordingly. We are thus able to control for the effects of the influx of online media into the database in our analysis. We are interested in downloading as many media items with economic content as possible. We thus use a search query1 in the Retriever search engine, which is based on an economic dictionary we have downloaded from www.ekonomifakta.se. This webpage presents information and data on matters of importance to the Swedish economy and is run by the Confederation of Swedish Enterprise (Svenskt Näringsliv) which is Sweden’s largest employer organization. The dictionary contains 678 words and concepts used in the fields of economy, taxes, the labour market, energy, environment, entrepreneurship, education and the public sector. To download as many media items with economic content as possible from the Retriever database, we created our search query by requiring that at least one of the words in the dictionary is present in the media items’ texts. However, this may cause us to download a substantial number of media items that do not have economic content due to the comprehensiveness of the Ekonomifakta dictionary. We thus remove those items that do not have economic content at a later stage when we classify our downloaded media items using supervised machine learning. 10 One potential problem that arises when using the Ekonomifakta dictionary is that some words do not have explicit economic meaning when taken out of context. For example, if an environmental word occurs in a text, without any other economic words being present, this would not be a good indication that the media item has any economic relevance. Another potential issue with our search query is that the economic terms in the dictionary may give hits for very short economic media items that merely report on one topic without any sentiment. Finally, the query may result in media items that report economic matters in a naïve way that lacks realism compared to what the economic data actually suggests. Short newspaper notes may, for example, report that inflation is increasing or falling without presenting any information on other economic variables such as unemployment or GDP. Such media items do not contain any sentiment because higher and lower inflation can be both negative and positive for the economy. Chronicles published in small online publications may, on the other hand, report on a plenitude of economic conditions. These media items may report on economic conditions in a naïve way compared to other media items that are published by the larger and better funded media outlets. The sentiments of these naïve media items may thus make our measure less precise since they are likely to deviate from what the economic data actually suggests. We mitigate these issues by augmenting our search query to require that the word “economy” (“ekonomi” in Swedish) is present in the texts together with either of the words “forecast” (“prognos” in Swedish) or “report” (“rapport” in Swedish). This increases the probability that the search query returns media items that report on several economic conditions in the same text, since they will be reporting on findings and statements stemming from reports and forecasts made by economic bodies such as commercial banks and the central bank (Riksbanken). Such reports and forecasts are typically based on the outcomes of econometric models using economic data, and are thus well anchored in economic reality. These documents report general economic conditions, albeit with a main focus on one economic variable such as inflation or the interest rate (Blinder et al. 2001).2 Another advantage of this approach is that we ensure that our media items are especially suited for testing Hypotheses 2 and 3. Here, we use consumer sentiment data on individuals’ retrospective evaluations of recent economic outcomes and their prospective expectations about future economic conditions. There is thus 11 symmetry between our media data and the consumer sentiment data in the sense that media items containing the word “forecast” are prospective as they primarily deal with future economic developments, and media items containing the word “report” are retrospective in the sense that they primarily deal with realized economic outcomes. Some economic events, such as economic crises, have greater impact on the economic media than other events. The Retriever search engine allows us to measure the impact of economic events on the economic media because many media items reporting a particular event are re-reported by several other media outlets. A typical example of this is news items reported by Tidningarnas Telegrambyrå (TT) that have a big impact on the news media as a whole in times of major economic events such as economic crisis. This is especially true in the initial stages of unravelling events, when many smaller daily newspapers struggle to keep up with the larger and better-funded newspapers. Our search query resulted in 179,846 media items published in print and online. The Retriever home page delivered the data in text files in batches of max 500 media items per file. We parsed the information in these text files into separate pieces of information so that we could separately handle the media items’ header, body of text, source, publication date/time and place of publishing (print/online). This allows us to split the data based on the parsed information and conduct our analysis on different subsets of the data. 3.2 Supervised Machine Learning Supervised machine learning (SML) is a set of techniques often used to classify large bodies of data. Typical applications of SML are email spam filters (emails being spam or non-spam), topic-specific search engines (document topics, e.g., economics, politics, sports, etc.) and detecting sentiment in consumer reviews (Manning et al. 2010). SML techniques are often deployed in situations where manual classification of data is not possible due to time and resource constraints. These techniques have received limited attention by researchers in the field of economic media sentiment detection, with some promising exceptions (e.g. Boydstun et al. 2018). 12 Most of the SML-techniques used for text classification work on training data, which consist of text items that are pre-classified by humans, to estimate a probability model. These SML-techniques use the estimated probability model for out-of-training-data classification, i.e., to assign a class to text items that were previously unseen by the probability model. One can evaluate the accuracy of the probability model by partitioning the training data into a training set, which is used to estimate the probability model, and a validation set, which is disjoint with the training data set and is used to evaluate the accuracy of the models’ predictions. This process is known as K-fold cross validation (Sebastiani 2002), and can be used once or several times. 3.2.1 Training Data We randomly select a set of news items from the economic media items we have downloaded, and manually classify them into three groups: (i) positive sentiment, (ii) negative sentiment or (iii) irrelevant.3 This process results in 31 % of the media items in the test data being classified as positive sentiment, 29 % classified as negative sentiment and 40 % classified as irrelevant. The relatively high number of irrelevant media items in the test data is most likely due to the fact that we use a comprehensive search query to retrieve our data from the Retriever database. This class consists of media items that do not have any economic content. A frequent kind of irrelevant media item in the test data are TV-guides that end up in the search results because they contain an occasional word from the search query. We classify documents as negative sentiment if, for example, they report on the consequences of economic crises, rising unemployment and/or a slowdown in GDP growth. A typical example of a media item which we classify as negative is a report of a forecast from a commercial bank stating that the economy is losing its momentum or that a trade organization for the construction industries has released a report stating that building investments are down. We classify documents as positive sentiment if, for example, they report falling unemployment, increased GDP growth or general improvements of business cycle conditions.4 A typical example of a media item which we classify as positive economic sentiment is a report from the Swedish National Institute for Economic Research 13 saying that investments are growing in the economy, or that a commercial bank issued a forecast which predicts a recovery in the economy. However, media items do not have to report directly about macroeconomic business cycle conditions to be classified as positive or negative sentiment. If, for example, a media item reports that a major company is laying off (hiring) workers due to a decrease (increase) in demand, then we classify this as negative (positive) economic sentiment. Such actions taken by individual firms contribute to the general business cycle conditions via an increase (decrease) in unemployment. We take into account the fact that Sweden is a small and export-dependent economy which is sensitive to changes in foreign demand for goods and services (Gylfason 1999). We do this by classifying economic media items that report on improved (worsened) economic conditions in foreign economies as positive (negative) sentiment, since increases (decreases) in demand in foreign economies are likely to cause an increase (decrease) in the export of goods and services from Sweden to foreign countries. This will increase (decrease) aggregate demand in the small export-dependent Swedish economy, and therefore affect the Swedish economy in a positive (negative) way. One potential issue with our manual sentiment classification is that it may deviate from the general public’s perception of economic media sentiment in a systematic way. This is not likely to cause any substantial problems in our analysis, however. Many experimental studies that investigate to what extent individuals agree on sentiment in texts find an inter-annotator agreement rate in the range of 80 % – 90 % (e.g. Wilson et al. 2005; Bermingham & Smeaton 2010; Jain & Nemade 2010). The inter- annotator agreement rate is even higher when annotators are required to report on texts’ sentiment at a low-granular level, i.e., when annotators are not given any option to express their opinion of the strength of a text’s sentiment. The highest agreement among annotators, i.e., when all annotators agree completely, is achieved when an experiment requires annotators to conduct binary classification, i.e., to classify texts as positive or negative sentiment without a neutral option (O’Hare et al. 2009). 14 3.2.2 Prediction Model We use our training data to build a prediction model using the multinomial Naïve Bayes classifier (NB). NB is a robust and parsimonious SML-technique that assigns a class to a given text document by finding the maximum a posteriori class under the assumption that each word occurs independently of all other words in a language. In practice, this means that NB assigns a document, which is not in the training data, to the class which has the largest sum of log prior probability and log conditional probabilities across words in the documents. NB calculates the prior probability for a document belonging to a particular class as the relative frequency of documents that belong to the class in the training data. The conditional probabilities for words in a class are calculated as the relative frequency of words in documents belonging to the class in the training data. The NB classifier computes the probability of a class c, given a document d, as proportional to the product of the prior probability of a document class P(c) and the probabilities of each word, w, of which there are n in a document, d, given c: P(c|d) ∝ P(c)  ∏
P(wk|c)
1≤k≤nd
 
(1) Here, P(wk|c) is a measure of how much evidence wk contributes to the class, c, being correct for the document, d. We are interested in finding the maximum a posteriori class, cmap, for a given document. This means that we can formulate the classification problem as: cmap= arg max c∈C P(c|d) = arg max c∈C P(d|c)P(c)/𝑃𝑃(𝑑𝑑) = arg max c∈C P(d|c)P(c) 
(2) Where the first equality is due to Bayes’ Rule, and where the second equality comes from the fact that P(d), the probability of a document, is the same across all documents. Here, P(d|c) is the probability of a document, conditional on a class, c. Estimating this probability is not feasible in practice because it requires that one observes each possible sequence of words for each class, which implies that one needs a near-infinite number of documents in the test data. We thus make the conditional 15 independence assumption, i.e. that sequences of words are independent across classes. Additionally, we make a positional independence assumption, i.e. that the probability of a word, w, conditional on a document, d, is independent of the words’ position, k, in the sequence of words which constitute a document. This assumption is clearly naïve because it assumes that words occur in random sequences in language. The conditional- and positional independence assumptions allow us to rewrite (2) as: cmap= arg max
c∈C
P(c) ෑP(wk|c) 1≤k≤nd
 
(3) Here, we take logs to avoid numerical underflow when we estimate the model in practice: cmap=arg maxc∈C log൫P(c)൯+ ෍log 
(P(wk|c)) 1≤k≤nd
 
(4) We can now obtain estimators for P(c) and P(wk|c) by using maximum likelihood which renders the following closed form solutions: P
෡(c)= 𝑁𝑁𝑐𝑐 N  
(5) P
෡(wk|c)=
Wcw ∑
Wcw'
w∈V (6) Where 𝑁𝑁 is the total number of documents in the training data, 𝑁𝑁𝑐𝑐 is the number of documents in the training data that belongs to class c, Wcw is the word count for word w, in all documents in the training data that belong to class, c. The sum, ∑
Wcw
t∈V
, adds the counts of all words in the vocabulary V for documents that belong to class c, in the training data. The vocabulary is the list of all unique words in all of the documents in the training data. In practice, equation 3 may not be defined because we may have instances of word counts which are zero, i.e. Wcw= 0. We solve this issue by updating equation 16 (5) with Laplace smoothing, also known as add-one smoothing, which is a standard approach in the literature: P
෡(wk|c)=
Wcw+1 ∑
(Wcw'+1)
t∈V (7) We estimate (5) and (7) in the training phase using our training data. These estimates are then used in equation (4) to classify documents which are not in the training data.5 NB performs particularly well in terms of prediction compared to other SML-techniques, such as logistic regression, when the training data is small in relation to the size of the feature space (Ng & Jordan 2002). This makes NB well suited in our case since our feature space consists of 33,660 unique words stemming from the texts we use as training data. We use 50-fold cross-validation and find that our NB-model classifies media items into the classes (i)–(iii) with an estimated overall accuracy of 70 %. This is a good prediction rate in relation to other studies that classify texts on sentiment, which in general report average prediction rates ranging from 52 % - 64 % (Serrano-Guerrero et al. 2015; Boydstun et al. 2018). Our NB-model is likely to classify a document containing words like “strong”, “good” and “growth” as positive sentiment rather than negative sentiment. Our NB-model is, however, likely to classify a document containing words like “weaker”, “reduced” and “recession” as negative sentiment rather than positive sentiment.6 3.2.3 Classified Data We use our NB-model to classify the full sample of media items which results in 31% economic media items classified as positive sentiment, 24 % classified as negative sentiment and 45 % classified as irrelevant. The ratio of positive media items in the full data set is the same as in the test data. The ratios of negative and irrelevant data are, however, different. The test data contained 5 percentage points fewer irrelevant media items and 5 percentage points more negative media items. This difference is most likely due to a random overrepresentation of the negative class in the test data 17 sample. Figure 2 shows the number of news items for the 20 most frequent media outlets in the relevant data. Dagens Industri (DI), which is the Swedish equivalent to the Financial Times, is the most frequent media outlet in the data with 10% of the news items. The second most common outlet is Dagens Nyheter (DN) with 6% of the items in the data, followed by Affärsvärlden (AFV) with 5% of the items in the data, Svenska Dagbladet (SD) with 5% and Webfinanser (WF) with 4% of the media items in the data respectively. Out of these five, the most frequent outlets DI, DN and SD are daily newspapers with nationwide coverage, AFV is a weekly newspaper with nationwide coverage and WF is an online news platform. It should be noted that both digital and printed media items are included in the counts presented in Figure 2. Figure 2. Top-20 Most Frequent Media Outlets The top five outlets in the data are followed by a set of newspapers and websites with nationwide coverage, all of them with more than 1,500 news items in the data. The news outlets with around 1,000 news items in the data are all local newspapers. The average number of words in a media item for the top 20 media outlets is between 284.5 and 775.9, covering the bulk of the empirical distribution as presented in Figure 3. Most media items are short. The average number of words in an item is 569. 0 1000
2000
3000
4000
5000
6000
7000
8000
9000 10000 Items 18 There is, however, a wide variation in the average length of the outlets’ items, as illustrated in Figure 3. Most of the outlets in the data publish text items that are shorter than 1,000 words on average. This indicates that most of the media outlets in the data are news agencies that frequently publish short stories about economic developments. The media outlets that on average publish the longest texts (>1,000 words) are different from newspapers as they publish in-depth analyses, forecasts and minutes from board meetings, etc. at a low frequency. Figure 3. Media Outlets’ Average Number of Words per News Item In our analysis, we split the media items into six different subgroups in order to investigate if they affect individuals’ opinions and expectations about economic conditions in different ways. The first split we make concerns publishing format, i.e., whether media items are published in print or online. Printed media will probably follow stricter editorial procedures than online media, which are published more frequently than printed media. Their sentiment may differ from online media, which are published continuously throughout the day, therefore not allowing as strict an editorial process as can be expected in printed media. The second distinction we make concerns geographical affiliation, i.e., between media items published by media outlets which have nationwide coverage and those with 0.00% 0.05% 0.10% 0.15% 0.20% 0.25% 0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000 Frequency of Outlets Average Number of Words Per Item 19 only local coverage. We define nationwide coverage as media outlets that do not have an explicit local affiliation in their name or in their description of themselves as stated on their homepage. This split allows us to identify differences in media sentiment that stem from the local media, which is more likely to be more concerned with local economic conditions, and nationwide economic media sentiment, which is more likely to report nationwide economic conditions. The local media may thus affect individuals’ opinions and expectations about their own personal economic situation, since local economic conditions are more likely to be closely linked to individuals’ personal economic conditions than nationwide economic conditions. The third distinction we make is based on two features of the media outlets that can be found in the data. The first feature is the media outlets’ publication frequency, i.e., how many media items they have in the data. The second feature is the media outlets’ average number of words per media item in the data7. This will allow us to investigate the differences between media items that publish in-depth analysis, i.e., that are published infrequently and are relatively long, and media items that are published frequently with a relatively shallow content. 3.3 The Economic Media Sentiment Index We calculate an Economic Media Sentiment Index (EMSI) by taking monthly averages of the ratio of the net of positive and negative media items to total number of economic media items for each month, according to the following formula. EMSIt=Dt
-1 ෍netd Dt d=1
 
(8) Where d is day in the month t with Dt number of days in the month, and where: netd= ቐ # positived - # negatived
# positived + # negatived , if # positived + # negatived > 0 0                       , if # positived + # negatived = 0 (9) This gives us an index which ranges from -1 when there is only negative news during the month, to +1, when there is only positive news. In our data, the index has a mean value of 0.06 with a standard 20 deviation of 0.18. Figure 4 shows the development of the economic media sentiment index for the sample period. The EMSI is relatively volatile with monthly variations of +/- 50% being common. The index also contains some longer cycles, which are illustrated in the figure by using a 12-month centred moving average. In general, the longer cycle follows the business cycle, with peaks around the dot- com boom in 2000, before the international financial crisis in 2008 and during the recovery from the financial crisis in 2010. We can observe the largest troughs in the index during the international financial crisis of 2008/09 and the Euro-zone debt crisis in 2012/13. Other troughs in the EMSI are small in comparison with the financial and Euro crises, for example the trough following the dot-com boom and the one following the Swedish financial crisis in the early 1990s. In fact, the amplitude of the cycles appears to have increased over time. This may be due to more volatile economic conditions and/or to changed media behaviour where media outlets report economic events with stronger sentiment. Such a change in media behaviour may be the result of including media items published online after the year 2000, which do not necessarily follow the same editorial procedures as traditional printed media. Figure 4. EMSI and 12-Month Centred Moving Average of EMSI 1993–2017 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 EMSI
12-Month Centred Moving Average of EMSI 21 Figure 5 shows the development of the 12-month centred moving averages of the printed media and the online media, together with the output gap as calculated by using Swedish quarterly real GDP from the St. Louis Federal Reserve Economic Data (FRED) database (we have interpolated to obtain monthly observations and used the HP-filter to estimate the output gap). Printed and online EMSI follow the development of the output gap relatively closely throughout the sample, with troughs and peaks occurring around the same time as for the output gap. The output gap shows clear signs of increased volatility from 2006 onwards. Most of this volatility is due to the financial crisis. If we disregard the 2006–2011 period, which covers and is adjacent to the financial crisis, we do not observe any signs of increased volatility in the output gap. The online EMSI is, however, more positive than the printed EMSI particularly during the 2000–2005 and 2011–2017 periods. The increased variance of total EMSI may thus come from both increased volatility in the economy during the 2006–2012- period and also from the influx of online media in the data. Another observation is that both online and printed EMSI appear to lead the development in the output gap, particularly for the 2006–2011 time periods. Both print and online EMSI reach a pre-financial-crisis peak prior to 2007, while the output gap peaks after 2007. The EMSI measures and the output gap hit their lowest level at the end of 2008, and the EMSI measures recover more quickly from the trough than the output gap. This pattern, with EMSI reaching peaks and troughs before the output gap, is present throughout the sample but is more pronounced after the year 2000, and is stronger for online EMSI than for print EMSI. This may be due to changed media behaviour, where the media reports more about forecasts than previously. 22 Figure 5. 12-month Centred Moving Averages of Printed and Online EMSI and the Output Gap The different subgroups of the EMSI develop similarly to total EMSI during the sample period. Figure 6 shows that the development of nationwide, local, printed, online, frequent and infrequent EMSI develop in cyclical movements that are similar to those for total EMSI. The volatility of the subgroups are different, however. Printed media appears to be more volatile than nationwide media. This may be because nationwide media are better funded than other media outlets that are published in print, and may thus present a more balanced economic sentiment in their media items. The subgroups of EMSI (local, online and infrequent), which enter into the data in the year 2000, are more volatile at the beginning of the 2000s than later in the sample period. This may be because the number of online media outlets in the Retriever database has gradually increased from the year 2000 and onwards as the importance of online media increased in the media landscape. This should have a stabilizing effect on our EMSI measure since the larger number of online media outlets makes the EMSI less sensitive to the sentiment of merely a few online media outlets. -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 12-month Centred Moving Average of Printed EMSI 12-month Centred Moving Average of Online EMSI Output Gap scaled by 10 23 Figure 6. Subgroups of EMSI 1993–2017 4. Data on Individuals’ Opinions and Expectations Having quantified the economic media data, we now turn to the measurement of individuals’ evaluations and expectations of economic conditions. Researchers often measure these by using survey data (e.g. Boydstun et al. 2018; Fogarty 2005; Hester & Gibson 2003). We use data on individuals’ opinions and expectations about economic conditions that are collected in the Swedish Household Purchasing Plan’s survey (Hushållens inköpsplaner, henceforth referred to as “the survey”), which is a monthly survey conducted by the National Institute of Economic Research (NIER).8 The survey is representative of the adult population and each wave includes 1,500 respondents.9.The survey contains more than 16 questions about the household’s opinions concerning their current and future consumption and their opinions about current and future economic conditions. Van Raaij and Gianotten (1990) have shown that questions in consumer sentiment surveys can be divided into two main groups: questions that represent the individual’s sentiment about their own economic conditions and questions that represent the individual’s sentiment about the nation’s overall (macro) economic conditions. In order to make our analysis as general as possible, we have thus -0.6 -0.4 -0.2 0 0.2 0.4 0.6 1993 1996 1999 2002 2005 2008 2011 2014 2017 1993 1996 1999 2002 2005 2008 2011 2014 2017 1993 1996 1999 2002 2005 2008 2011 2014 2017 Nation-Wide (A)
Print (B)
Frequent (C) Local (A)
Online (B)
Infrequent (C) (A) 
(B) 
(C) 24 chosen to use four questions in the survey that are likely to best represent these two groups. These questions are presented in Table 1. Table 1. Survey Questions and Answers Name 
Question Asked to Respondents in the Survey ownFuture “How do you think the fiscal position of your household will change over the next 12 months?” sweFuture 
“How do you think the general economic situation in this country will develop over the 
next 12 months?” ownNow 
“How is the fiscal position of your household today compared with 12 months ago?” sweNow 
“How is the general economic situation in this country compared to 12 months ago?” Alternative Answers offered to Respondents in the Survey A1 
“Got/Get a lot worse in relation to 12 months ago/today” A2 
“Got/Get a little worse in relation to 12 months ago/today” A3 
“The same in relation to 12 months ago/today” A4 
“Got/Get a little better in relation to 12 months ago/today” A5 
“Got/Get a lot better in relation to 12 months ago/today” A6 
“I don’t know” One set of survey questions asks individuals to state their expectations about the development of their own private economic situation and the macroeconomic situation during the upcoming 12 months (ownFuture and sweFuture). Another set of questions asks respondents how they evaluate the development of their own economic situation and the macroeconomic situation during the past 12 months (ownNow, sweNow). Using the answers to these questions allows us to evaluate if an individual’s expectations and evaluations of past economic developments are caused by the EMSI. Respondents can give one out of six predefined answers to the questions. These are presented in Table 1, and range from very negative (A1) to very positive (A5), including a neutral option (A3) and the option to report a lack of knowledge (A6). In our econometric models, we use the average of within month-waves of the net of positive and negative replies to these questions, which are calculated as the sum of the number of positive answers (A5, A4) minus the sum of the number of negative answers (A1, A2) within each month. 25 4.1 Macroeconomic Data We need to control for macroeconomic variables in our analysis in order to isolate the sentiment that the news media adds when reporting economic conditions. We therefore include a set of key macroeconomic variables in our regression models. These macroeconomic variables are selected in order to capture the most important variations in the business cycle, the financial markets and exogenous shocks that hit the economy at random. We use quarterly real GDP which we download from the St. Louis FRED database as a measure of the business cycle in terms of economic activity. We interpolate this variable to obtain monthly observations and estimate the output gap using the HP-filter, which is a very common approach in the literature (Ravn & Uhlig 2002). Interpolating the GDP data may introduce some measurement error in the data (Friedman 1962). However, this measurement error is not likely to be a major problem in our analysis since GDP is a variable that changes slowly with relatively little variation between quarters. The variation between months is thus also likely to be relatively small and thus the measurement error as well. Furthermore, the media items in our data report findings from reports and forecasts published by economic bodies. Some of these reports and forecasts use models that also include past values of interpolated GDP from quarterly to monthly frequency, together with other key economic indicators available at monthly frequency, to forecast current and future GDP (e.g. Mitchell et al. 2005; Altissimo et al. 2010). Using the monthly interpolated GDP in our analysis should therefore not deviate systematically from what is being done in these reports and forecasts. This is especially so since we furthermore use monthly data available from statistics Sweden (SCB) for the unemployment rate (to account for business cycle fluctuations), and for the Consumer Price Index (CPI) to calculate the yearly inflation rate. We include two measures of exogenous shocks in our model. Firstly, we use monthly oil prices from the World Bank, which we convert to year-on-year percentage changes in real values, and secondly the monthly SEK/EUR-exchange rate from the Swedish Riksbank. Both of these variables may be important for explaining individuals’ expectations and perceptions of economic circumstances as well 26 as being important for the supply side of the economy. Individuals tend to attach a symbolic value to the national exchange rate, which may affect their perception of the performance of the macro economy (Hobolt & Leblond 2009). The oil price directly affects individuals’ personal economic situation through the price of petrol used for private transport (Johnson & Lamdin 2012). We use the monthly stock prices in terms of the OMXS30 index from Nasdaq OMX Nordic, which are transformed to real values using the CPI and then converted to year-on-year percentage changes. Stock prices are likely to matter both for individuals’ perceptions of their own personal economic situation and the nationwide economic situation. Swedish households’ participation in the stock market is high, with 45% of households owning stocks either directly as a part of their savings portfolio or indirectly through shares in mutual funds (Almenberg & Dreber 2015). Stock prices may also matter for individuals’ perceptions and expectations concerning the future of the macro economy, since stock prices tend to lead the development of the real economy (Fama 1990; Schwert 1990). We henceforth prefix all macroeconomic variables that we calculate using year-on-year percentage change with Δ%. 4.2 Descriptive Statistics Table 2 shows descriptive statistics for the variables that we include in our econometric models. EMSI ranges from -0.51 to 0.47, which means that it is at most approximately half of the extreme values (-1 and 1) for this measure, i.e., when the news media reports only negative or positive news items during all the days of the month. We further note that ownFuture, sweFuture and ownNow have positive averages, which indicates that individuals are, on average, optimistic about the state of their own present and future economic situation and about the future of the nationwide economic situation. However, the average net of replies to sweNow is negative, which indicates that individuals are, on average, pessimistic when evaluating the state of the present nationwide economic situation. The survey measures of individuals’ own economic situation are more stable than the survey measures concerning individuals’ evaluations and expectations about the future of the nationwide economy. The 27 standard deviations of sweNow and sweFuture are more than twice as large as the standard deviations for ownNow and ownFuture. This indicates that individuals base their expectations and evaluations about the state of their own personal economic situation and the nationwide economic situation on information sets that are not identical. The information set used by individuals to form an opinion about their own personal economic situation is likely to be more precise and less volatile compared to the information used by individuals to form their opinions about the nationwide economy. It is thus likely that the media plays an important role for individuals’ expectations and opinions about the nationwide economy given the rather choppy development of EMSI presented in Figure 1. The p- values from the Augmented Dickey-Fuller test including a constant shows that we reject the null of a unit root at a 5%-level for all of our variables except the inflation rate. Although inflation may be nonstationary in theory, it is counterintuitive that the inflation rate is nonstationary as it would imply, for example, infinite hyperinflation. A plausible explanation for this result is that the Swedish central bank (Riksbanken) was given an independent mandate to conduct monetary policy with an inflation target of 2% in 1995. This is likely to have caused the inflation rate to move from a higher level of stationary inflation to a lower level after 1995. In our analysis, we thus use an inflation rate which is demeaned with the pre- and post-1995 means for the respective periods. This inflation rate has an ADF-test p-value of 0.045. 28 Table 2. Descriptive Statistics Mean 
Median Minimum Maximum 
Std. Dev. ADF- test P- value EMSI 
0.06 
0.07 
-0.51 
0.47 
0.18 
0.00 ownNow 
4.88 
8.5 
-34.9 
19.5 
11.68 
0.02 ownFuture 
16.29 
18.4 
-28.6 
32.2 
9.02 
0.00 sweNow 
-12.16 
-11.6 
-92.8 
42.3 
27.47 
0.00 sweFuture 
6.9 
5.2 
-43.2 
49.9 
19.98 
0.01 Output Gap 
0.00 
0.00 
-0.04 
0.03 
0.01 
0.00 Unemployment Rate 
7.60 
7.65 
4.94 
10.48 
1.25 
0.03 Inflation Rate 
1.31 
1.14 
-1.55 
5.09 
1.35 
0.12 SEK/EUR 
9.14 
9.14 
8.24 
11.17 
0.49 
0.04 Δ%Oil Price 
8.54 
2.05 
-56.46 
168.00 
36.68 
0.01 Δ%Stock Prices 
8.03 
13.10 
-77.38 
96.34 
31.66 
0.00 The p-values are from the Augmented Dickey-Fuller test including a constant. The ADF-test p-value for the inflation rate is 
0.045 when we level-adjust it with regards to a change in monetary policy regime in 1995. This is the version of the inflation 
rate that we use in our analysis. The correlations in Table 3 confirm the observed pattern in Figures 3 and 4, i.e., that EMSI is correlated with the business cycle. The correlation between EMSI and the output gap is 0.30 and the correlation between EMSI and unemployment is -0.23. The EMSI is more volatile compared to the macroeconomic variables, which may explain the relatively weak correlations. Correlations based on a 12-month centred moving average of EMSI are more highly correlated with the macroeconomic variables (0.43 with the output gap and -0.35 with unemployment). EMSI is positively correlated with all of the net of individuals’ replies to the survey questions. The highest correlation, between EMSI and individuals’ evaluation of the current developments of the Swedish economy (sweNow), is 0.44, and is 0.60 for the 12-month centred moving average of EMSI. The strength of the correlation between the one month lag of EMSI and the survey- and macro variables is similar to the strength of the contemporaneous correlations. This indicates that there is a potential between-month causal relationship from the one period lag of EMSI to the contemporaneous survey variables. 29 Table 3. Correlations Contemporaneous 
One Month Lag EMSI 12- Month Centred Moving Average EMSI 12-Month Centred Moving Average sweNow 
0.44 
0.60 
0.47 
0.62 sweFuture 
0.18 
0.17 
0.16 
0.13 ownNow 
0.31 
0.44 
0.29 
0.43 ownFuture 
0.30 
0.45 
0.28 
0.44 Output Gap 
0.30 
0.43 
0.36 
0.50 Unemployment Rate 
-0.23 
-0.35 
-0.21 
-0.38 Inflation Rate 
-0.02 
0.02 
0.07 
0.10 SEK/EUR 
-0.08 
-0.06 
-0.12 
-0.09 Δ%Oil Price 
0.25 
0.33 
0.28 
0.35 Δ%Stock Prices 
0.20 
0.24 
0.18 
0.21 5. Results 5.1 Test of Hypothesis 1 We can now test Hypothesis 1, that economic media is not neutral and therefore publishes unequally many items with positive and negative sentiment. As our data consists of monthly observations spanning 24 years, it includes several booms and busts in the real economy. We thus expect a neutral media to publish an equal number of media items with positive and negative tone and a non-neutral media to publish an unequal number of media items with positive and negative tone. We thus test Hypothesis 1 by statistically testing if the mean of EMSI is significantly different from zero. Table 4 shows the sample means and t-ratios calculated using OLS with the EMSI as dependent variable and a constant as independent variable, using robust standard errors (Newey & West 1987). Table 4 also includes tests for our six different subgroups of EMSI including sources with nationwide coverage, local coverage, online and printed modes of publication and frequent and infrequent publications. We can see that the total EMSI, together with five out of the six subgroups, are positive and significantly different from zero, causing us to accept Hypothesis 1. The interpretation of the estimated mean values is straightforward: a positive and significant mean value means that the number of positive media 30 items is on average greater than the number of negative ones. The estimated mean for total EMSI is 0.06, which indicated that in a month with 300 economic media items there will be 18 more items during the month with a positive tone than with a negative tone. The estimated mean values of EMSI and the subgroups are thus only moderately positive on average. Table 4. Significance Test of Means EMSI 
Nationwide 
Print 
Local 
Online 
Frequent 
Infrequent 0.06*** (3.62) 0.07*** (4.94) 0.01 (0.69) 0.04** (2.21) 0.09*** (4.72) 0.04*** (4.20) 0.01*** (5.10) *= p-value<0.1, **= p-value<0.05, ***= p-value<0.01, t-ratios presented in parentheses. The means are estimated using OLS 
with the EMSI as dependent variable and a constant as an independent variable. The regressions are conducted using robust 
standard errors (HAC). Next, we investigate the difference in means and variances between the subgroups presented in Table 5. Firstly, we can note that economic news items that are published online have a predominately more positive tone than those published in print. This difference could be due to the fact that traditional printed media employs a stricter editorial process and is published less frequently than online news media, causing the traditional media to present a more neutral version of economic conditions on average. We further note that the nationwide economic media is more positive than the local media. This could be due to the local media mainly re-reporting macroeconomic news from the nationwide media, to which it adds local economic news with a more negative tone. We find no significant difference between the variances of the different subgroups. This indicates that the sentiment of the subgroups reacts to changes in economic conditions in a similar way, albeit with different averages. 31 Table 5. Significance Tests for Differences in Means and Variances EMSI-measures 
Difference 
in mean Difference in 
variance Print – Online 
-0.08*** (−5.30) 0.01 (1.14) Nationwide – Local 
0.05*** (3.70) 0.00 (1.21) Shallow – Deep 
-0.00 (−0.33) 0.00 (1.23) *= p-value<0.1, **= p-value<0.05, ***= p-value<0.01. The differences in means are estimated OLS with the difference in 
EMSI as dependent variable and a constant as independent variable. The regressions are conducted using robust standard 
errors (HAC). t-ratios are presented in parentheses for test of differences in means, and F-statistics for tests of differences in 
variances. 5.2 Tests of Hypotheses 2 and 3 We now turn to tests of Hypothesis 2, that individuals’ opinions and expectations about current and future economic conditions are Granger-caused by economic media content that is not motivated by economic data, and Hypothesis 3, that economic media content does not Granger-cause individuals’ opinions and expectations about their private economic situation. As both of these hypotheses regard causation of economic media on individuals’ expectations and evaluations of economic circumstances, we evaluate them using a set of Granger-causality tests, see Granger (1969). 5.2.1 Granger-causality Tests We acknowledge the fact that Granger-causality is not a test of true causality because it can only establish a causal relationship between two variables, X and Y, in the sense that one variable tends to occur before the other one in time. Thus, the Granger-causality test cannot confirm that there is a true causal relationship between X and Y but only rule out that there is no true causal relationship between them. We furthermore acknowledge that the Granger-causality test (i) does not rule out the possibility that two variables X and Y are caused by a third potentially unobserved variable Z and (ii) it does not account for instantaneous causality between X and Y (Maziarz 2015). Both the EMSI and the general public are likely to respond to changes in economic data. Any one-way Granger-causality established between EMSI and individuals’ expectations and evaluations of economic conditions may thus be due 32 to macroeconomic data and therefore not a true causal relationship. This is not likely to cause any problems in our analysis because we include key macroeconomic variables covering the most important sectors of the macro economy as controls in our regressions. It is thus likely that any Granger-causality we can establish between our variables of interest does not come from an unobserved variable that we do not capture with the macroeconomic variables. We test if a causal relationship exists between our survey measures and EMSI using the following specification: surveyt= α + ෍β0,k × EMSIt-k k=K k=1 + ෍෍βi,k × macrovariablei,t-k k=K k=1 i=6 i=1 + ෍β7,k × surveyt-k k=K k=1 + εt 
(10) EMSIt= α + ෍β0,k × surveyt-k k=K k=1 + ෍෍βi,k × macrovariablei,t-k k=K k=1 i=6 i=1 + ෍β7,k × EMSIt-k k=K k=1 + εt 
(11) We determine the number of lags, K, in equations 10 and 11 individually by selecting the most parsimonious specification that does not exhibit autocorrelation in the residuals. If the coefficient for EMSI is significant on at least a 5%-level in equation 10, then, we can conclude that EMSI Granger causes individuals’ opinions and expectations about economic conditions, rather than this coming from our economic control variables. We can determine the direction of causation by investigating the significance of the coefficient estimates for our survey variables in equation 8. If all of them are insignificant, then we can conclude that there is a one-way Granger causal relationship going from EMSI to our survey variables, we denote this result as EMSI → survey. If, however, the coefficients in equation 8 are also significant, then we can conclude that there is a two-way Granger causal relationship between EMSI and our survey variables, we denote this result EMSI ↔ survey. If we observe that the coefficient for EMSI is insignificant in equation 10 and that the coefficient for our survey variables is significant in equation 8 then we can conclude that there is a one-way Granger causal relationship going from our survey variables to EMSI. This may be due to the media reporting on recent results from the survey as part of, for example, the NIER’s Economic 33 Tendency Survey (Konjunturbarometern). We must thus rule out the possibility that our survey measures cause the EMSI and that we thus have a feedback loop between the media and the general public. We denote this result as survey → EMSI. Table 6. Results from Granger-causality Tests sweNow sweFuture ownFuture ownNow EMSI 
→ 
← Online 
→ 
 
← Nationwide 
→ Print 
→ Frequent 
 
 
 
← Local Infrequent The symbols show Granger-causality between the row variable on the left side of the symbol and column variable on the 
right side of the symbol, e.g. the top left symbol represents EMSI → sweNow. The results from our Granger-causality tests presented in Table 6 show a clear pattern. Four out of seven tests result in one-way Granger-causality between EMSI and sweNow and three out of seven tests result in no Granger-causality between EMSI and sweNow. These results give support to Hypothesis 2 because the news media Granger-causes individuals’ opinions about the macro economy in terms of sweNow, i.e. that EMSI → sweNow, Online → sweNow, Nation-Wide → sweNow and Print → sweNow. The results also show that there is a one-way Granger-causal relationship going in the other direction from the survey variables to EMSI in terms of sweFuture → EMSI, ownFuture → Online and ownNow → Frequent. These results are, however not robust across different subgroups of EMSI. Hypothesis 3 is supported by the results presented in Table 6 since EMSI does not Granger- cause ownFuture and ownNow. This result indicates that individuals do not base their opinions and expectations about their own economic situation on information published by the economic media. Individuals’ opinions about the current state of economic conditions are Granger-caused by EMSI and Δ% Stock prices. The results for equation 10 with sweNow as dependent variable are presented in Table 7. These show that EMSI is significant for 4 out of 7 specifications with estimates in the range of 5.39–8.57. This implies that the long run effect of EMSI on sweNow is in the range of 59.89–85.7. 34 The long run effect of EMSI on sweNow in a month with a large share of positive news, such as the sample maximum of 0.47, is thus in the range of 28.15–40.28. This roughly corresponds to an interval ranging from a 1–1.5 standard deviations increase in the net number of positive replies in sweNow. The estimated coefficient for Δ%Stock Prices is positive and significant in all of our specifications of equation 10, indicating that individuals use the stock market as an indicator of current macroeconomic conditions. Lags of the other macroeconomic variables are insignificant in most of the specifications, with the exception of inflation which is significant at the 10% level in the model specifications with EMSI and Print as independent variables. Table 7. OLS Regression Results from Equation Specification 7 Dependent variable 
sweNow 
sweNow sweNow sweNow 
sweNow sweNow 
sweNow Economic Media 
Sentiment EMSI 
Online 
Nationw
ide Infrequent Print 
Frequent 
Local 𝛂𝛂 
-8.78 (12.26) -8.14 (13.4) -7.10 (12.37) -11.24 (13.87) -9.10 (12.56) -8.00 (14.16) -7.94 (14.51) 𝐄𝐄𝐄𝐄𝐄𝐄𝐄𝐄𝐭𝐭−𝟏𝟏 
5.39** (2.48) 7.53*** (2.47) 5.48** (2.48) 8.57** (4.22) 5.8* (2.95) 5.81 (4.43) 5.04 (3.22) 𝚫𝚫%𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒 𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐭𝐭−𝟏𝟏 
0.06*** 
(0.02) 0.14*** 
(0.02) 0.06*** 
(0.02) 0.14*** 
(0.03) 0.06*** 
(0.02) 0.13*** 
(0.03) 0.13*** 
(0.03) 𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐭𝐭−𝟏𝟏 
-0.96* (0.58) 0.16 (0.95) -0.95 (0.58) 0.09 (1.01) -1.04* (0.58) 0.14 (1.01) 0.17 (1) 𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐭𝐭−𝟏𝟏 
0.16 
(0.48) -0.02 
(0.69) 0.09 
(0.48) 0.14 
(0.71) 0.11 
(0.48) -0.19 
(0.76) -0.1 
(0.73) 𝐎𝐎𝐎𝐎𝐎𝐎𝐎𝐎𝐎𝐎𝐎𝐎 𝐆𝐆𝐆𝐆𝐆𝐆𝐭𝐭−𝟏𝟏 
23.69 (73.1) -5.08 (80.8) 19.95 (72.77) 6.64 (82.74) 25.09 (73.02) -11.53 (84.56) -17.61 (84.92) 𝐒𝐒𝐒𝐒𝐒𝐒/𝐄𝐄𝐄𝐄𝐄𝐄𝐭𝐭−𝟏𝟏 
0.70 (1.2) 0.64 (1.4) 0.57 (1.2) 0.87 (1.48) 0.81 (1.22) 0.82 (1.43) 0.75 (1.49) 𝚫𝚫%𝐎𝐎𝐎𝐎𝐎𝐎 𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐭𝐭−𝟏𝟏 
-0.01 (0.01) -0.01 (0.02) -0.01 (0.01) -0.01 (0.02) -0.01 (0.01) -0.02 (0.02) -0.02 (0.02) 𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐭𝐭−𝟏𝟏 
0.91*** 
(0.03) 0.90*** 
(0.03) 0.92*** 
(0.03) 0.90*** 
(0.03) 0.91*** 
(0.03) 0.91*** 
(0.03) 0.91*** 
(0.03) Adjusted R-sq. 
0.93 
0.95 
0.93 
0.95 
0.93 
0.95 
0.95 Obs. 
291 
208 
291 
208 
291 
208 
208 *= p-value<0.10, **= p-value<0.05, ***= p-value<0.01, Robust standard errors (HAC) presented in parentheses. 35 The estimated coefficients for the one period lagged values of sweNow range from 0.90–0.92. These estimates have 95 % confidence intervals that do not include unity, a result which is further reinforced by the fact that the Augmented Dickey Fuller test rejects the null hypothesis of a unit root for all of our survey measures.10 The residuals in the regressions presented in Table 7 are not autocorrelated and the results are robust to adding more lags of the dependent variable and the independent variables. The output gap Granger-causes EMSI, online EMSI and nationwide EMSI in a majority of the regression specifications of equation 8, i.e., with our 7 different EMSI measures as dependent variables and our 4 different survey variables as independent variables using the same macroeconomic control variables in each regression. Figure 7 presents the number of regressions for which lags of the macroeconomic control variables are significant. The lags of the output gap are significant in a majority of the regressions with EMSI, nationwide EMSI and local EMSI as dependent variable. EMSI is also Granger-caused by Δ% Stock prices in 3 out of 4 regressions, which shows that EMSI is Granger-caused both by economic data which changes frequently and infrequently. This indicates that the media are reporting both on economic events that change on a daily basis such as the stock market, as well as changes in other more long-term economic conditions that affect the real economy. The subgroups of EMSI are Granger-caused by different macroeconomic variables. Online EMSI is Granger-caused by the output gap, while printed EMSI is Granger-caused by measures of exogenous supply-side shocks in terms of the SEK/EUR-exchange rate and Δ% Oil price. The nationwide EMSI is Granger-caused by the output gap, while the local EMSI is Granger-caused by unemployment. This suggests that local media report on topics that are directly linked to individuals’ own financial situation to a greater extent than nationwide media, which instead focuses on economic conditions that affect the macroeconomic situation and thus have a weaker direct link to individuals’ own economic situation. 36 Figure 7. Significant Lags of Macroeconomic Variables in Equation Specification 2 across 4 Different Survey Variables as Independent Variables The general picture is that EMSI is Granger-caused by data on economic conditions and not by individuals’ expectations and opinions about economic conditions. The results thus indicate that EMSI is Granger-caused by economic conditions and that the EMSI affects individuals’ perception of macroeconomic conditions. These are, however, not Granger-caused by economic data apart from stock prices. This indicates that individuals’ perception of economic conditions primarily comes from the media. 0 2 4 6 8 10 12 14 EMSI
Frequent
Nation-Wide
Print
Local
Online
Infrequent 37 5.2.2 Contemporaneous Effects We would ideally like to conduct Granger-causality tests using data with a higher frequency than monthly. The macroeconomic data does not allow for this, however, and any attempt at identifying Granger-causality using, for example, daily data will thus suffer from the unobserved variable problem described in section 5.2. We can, however, get an indication of whether a causal relationship exists within a month, albeit without pinpointing the direction of the causality. We do this by running the following regression: surveyt = α + β0 × EMSIt + ∑
βi × macrovariablet
6
i=1
 + ∑
β7,k × surveyt-k
k=K
k=1
 + εt 
(12) Here, we include the same macro variables as in equations (10) and (11). Although the output gap and the unemployment and inflation rates are not available in real time, they may act as proxies for other, unobserved information about economic conditions that individuals have access to. Including the macro variables thus allows us to isolate the contemporaneous correlation between EMSI and our survey variables. We again determine the number of lags, K, individually by selecting the most parsimonious specification that does not exhibit autocorrelation in the residuals. If the coefficient for EMSI is significant in equation 12, then it is an indication that EMSI affects individuals’ opinions and expectations about economic conditions, other than that coming from our economic control variables. We cannot say anything about the direction of causality, i.e. whether EMSI causes our survey measures or if our survey measures cause EMSI or if they both cause each other. We are, however, able to rule out that EMSI and our survey measures are unrelated within months. Table 8 presents the regression results for model 3:11 38 Table 8. OLS-Regression Results for Equation 12 sweNow 
sweFuture 
ownNow 
ownFuture 𝛂𝛂 
3.67 (11.9) 12.86 (10.3) 4.18 (4.24) 12.89** (5.47) 𝐄𝐄𝐄𝐄𝐄𝐄𝐄𝐄𝐭𝐭 
9.11*** (2.47) 5.59** (2.32) 0.02 (0.97) 0.38 (1.28) 𝚫𝚫%𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒 𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐭𝐭 
0.05*** (0.02) 0.04** (0.02) 0.02*** (0.01) 0.02* (0.01) 𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐢𝐭𝐭 
-1.22** (0.53) 0.55 (0.42) -0.38* (0.21) -0.23 (0.28) 𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐮𝐭𝐭 
0.33 (0.47) 0.01 (0.42) -0.49** (0.21) -0.66** (0.27) 𝐨𝐨𝐨𝐨𝐨𝐨𝐨𝐨𝐨𝐨𝐨𝐨 𝐠𝐠𝐠𝐠𝐠𝐠𝐭𝐭 
9.24 
(7.37) -18.42*** 
(4.57) -0.38 
(2.44) -2.78 
(2.55) 𝐒𝐒𝐒𝐒𝐒𝐒/𝐄𝐄𝐄𝐄𝐄𝐄𝐭𝐭 
-0.85 (1.12) -1.39 (0.99) 0.01 (0.44) -0.59 (0.49) 𝚫𝚫%𝐎𝐎𝐎𝐎𝐎𝐎 𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐏𝐭𝐭 
0.00 (0.02) -0.03** (0.01) -0.01** (0.00) -0.01 (0.01) 𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐭𝐭−𝟏𝟏 
0.88*** (0.03) 0.92*** (0.02) 0.63*** (0.07) 0.63*** (0.06) 𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐭𝐭−𝟐𝟐 
- 
- 
0.22*** 
(0.07) -0.02 
(0.08) 𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐬𝐭𝐭−𝟑𝟑 
- 
- 
0.07 (0.05) 0.24*** (0.06) Adjusted R-sq. 
0.93 
0.88 
0.94 
0.86 Obs. 
291 
291 
291 
291 *= p-value<0.10, **= p-value<0.05, ***= p-value<0.01, Robust standard errors (HAC) presented in parentheses. The 
Breusch–Godfrey test does not reject the null of no autocorrelation in the residuals in all of the model specifications. The results in Table 8 give support to Hypothesis 3. The coefficient estimates for EMSI are not significant for ownNow and ownFuture, which means that EMSI is not contemporaneously correlated with ownNow and ownFuture. We can thus rule out that the public’s opinions and expectations about their own personal economic situation are affected by EMSI within the same month. The regression results give some indicative support to Hypothesis 2 since the coefficients of EMSI are positive and significant for sweNow and sweFuture. We are thus able to deduce that there is a Granger-causal relationship between EMSI and sweNow and sweFuture within the same month. The magnitude of the coefficient for EMSI is 63 % larger in the regression with sweNow as dependent 39 variable compared to the regression with sweFuture as dependent variable. This result agrees with the contemporaneous correlations presented in Table 2, which are stronger between EMSI and sweNow than between EMSI and sweFuture. This indicates that individuals may use the economic media to form an opinion about the current state of the Swedish economy to a greater extent than when they form their expectations about its future developments. The estimated coefficients for the year-on-year percentage change of the stock market index are significant and positive for all the survey variables. This is a reasonable result given that stock prices are a leading indicator for the macro economy and also important for individuals’ personal savings. The estimated coefficients for the unemployment rate are significant at a 5-% level for ownNow and ownFuture but not for sweNow and sweFuture. This implies that individuals’ perception of their own economic situation correlates with the unemployment rate. This may be due to the fact that individuals’ perception of their own economic situation is mostly determined by their employment status; if people expect to become unemployed, then they tend to perceive and expect the future of their personal economic situation to deteriorate. The estimated coefficients for the inflation rate are negative and significant in the regression with sweNow and ownNow as dependent variables at the 5 % and 10 % level respectively. This shows that individuals tend to become more pessimistic about the current state of economic conditions as inflation increases. The estimates for the year-on-year percentage change of the oil price is negative and significant at the 5 % level for sweFuture and ownNow. This indicates that individuals tend to become more negative if oil price increases, both about their own economic situation and about the nationwide economic situation. This is an expected result since changes in oil prices can be the result of an exogenous supply-side shock, but they also affect expenses for households via the price of petrol used for private transport. 40 6. Conclusion We develop a new measure on Swedish economic media sentiment using a supervised machine learning technique. Our measure is calculated based on the sentiment of the full textual content of 179,846 media items, stemming from 1,071 unique media outlets including the major nationwide newspapers, local newspapers and online news during the 1993–2017 period. The measure has more good characteristics compared to other studies that leave out important sentiment information or are limited to a few time periods and/or media outlets. We use our measure to test three hypotheses regarding how the media reports on the economy, and how this reporting affects individuals’ opinions and expectations. Our results support the second hypothesis; that individuals’ opinions and expectations about current and future economic conditions are Granger-caused by economic media content that is not motivated by economic data. There is also support for the third hypothesis; that economic media content does not Granger-cause individuals’ opinions and expectations about their private economic situation. We accept the first hypothesis; that the economic media is not neutral and therefore publishes unequally many items with positive and negative sentiment. We find that the economic media reports slightly more items with positive than negative sentiment which differs to that found in the previous literature, which has shown negative reporting to be more common than positive reporting (Fogarty 2005; Arango-Kure et al. 2014; Boydstun et al. 2018). These different results may be explained by the fact that Swedish media is different to media in other economies and that we have more data stemming from a larger set of media outlets compared to other studies. We provide evidence that individuals’ opinions and expectations about their own personal economic situation are not Granger-caused by the economic media sentiment index. This implies that individuals use other sources of information for forming opinions about their own economic situation. Individuals may do this because they have access to other sources of information that are more relevant for their own personal economic situation. Alternatively, individuals may be overconfident in their opinions and expectations concerning their own economic situation in relation to the nationwide situation, 41 which may cause individuals to ignore information about macroeconomic conditions that is published by the media. Our results are in line with some of the previous literature, such as Hester and Gibson (2003), Boydstun et al. (2018) and Lamla et al. (2019), which have found that the public’s perception of general economic conditions is affected by economic media, controlling for macroeconomic data. This indicates that the way the media frames news about economic conditions matters for individuals’ perceptions about economic conditions. The general picture is thus that individuals’ perception of macroeconomic conditions is a reflection of the information presented to them by the media. This is most likely due to individuals having few other information sources available to them that they can use to form an opinion about macroeconomic conditions. The economic media sentiment index that we have developed in this paper may thus be useful for forecasters who use data on individuals’ opinions about macroeconomic conditions in their models for forecasting monthly macroeconomic activity. 42",0
"Systems biology seeks to create math models of
biological systems to reduce inherent biological
complexity and provide predictions for applica-
tions such as therapeutic development. However,
it remains a challenge to determine which math
model is correct and how to arrive optimally at the
answer. We present an algorithm for automated
biological model selection using mathematical
models of systems biology and likelihood free in-
ference methods. Our algorithm shows improved
performance in arriving at correct models without
a priori information over conventional heuristics
used in experimental biology and random search.
This method shows promise to accelerate biologi-
cal basic science and drug discovery. 1. Introduction Biological cellular systems exhibit super exponential scaling
in the number of biological states achieved arising from
different combinations and sequences of cell regulators,
such as messenger proteins and transcription factors (Letsou
& Cai, 2016). This complexity impedes our understanding
of diseases and development of therapeutics. We focus here
on the combinatorial complexity of biology, deﬁned by the
vast number of models and their parameters that describe
biological systems. This combinatorial problem in biology is exempliﬁed by
promiscuous signaling, which is the phenomenon of mul-
tiple protein ligands in a pathway being able to bind to
multiple receptors in a competitive manner. The Bone Mor-
phogenetic Protein (BMP) pathway exempliﬁes this type of
signaling with multiple protein ligands, and type I and II
receptors present in the pathway, each combining with one
another at different rates to form a complex of ligand, type I,
and type II receptor to phosphorylate SMAD 1/5/8 to send a
downstream gene expression signal. The BMP pathway can 1Department of Biomedical Engineering, University of Califor-
nia Irvine, Irvine, CA, USA. Correspondence to: Vincent Zaballa
<vzaballa@uci.edu>, Elliot Hui <eehui@uci.edu>. The 2022 ICML Workshop on Computational Biology. Baltimore,
Maryland, USA, 2022. Copyright 2022 by the author(s). be mathematically modeled by mass action kinetics (Antebi
et al., 2017) and previous work demonstrated how to opti-
mally infer BMP models’ parameters using Likelihood Free
Inference (LFI), also known as Simulation Based Inference
(SBI), using the SBIDOEMAN algorithm (Zaballa & Hui,
2021). However, since multiple models have been proposed
for the BMP pathway (Antebi et al., 2017; Su et al., 2022),
there remains ambiguity in determining which model best
describes observed experimental data. We propose to use the previously developed SBIDOEMAN
algorithm and a novel method to approximate a model’s
marginal probability, p(M|xo, θ), within Bayesian Model
Averaging (BMA) to select a correct model from a set of
models proposed. This method, which we call SBIDOE-
MAN BMA, uses models’ prior distributions of parame-
ters, p(θ), to design optimal experiments using a mutual
information approximation I(θ, x; d) between model pa-
rameters and data, then determines the posterior distribu-
tion of parameters given observed data, p(θ|xo), by LFI,
and ﬁnally approximates a marginal likelihood of a biolog-
ical model given observed data points, p(M|xo, θ). This
marginal probability is used as a probability measure of a
given model, M, and can be used in BMA to determine
the next experiment to evaluate and a weighting of possible
models. Previous work for optimal experimental designs in biologi-
cal systems studied graphical models describing gene reg-
ulatory networks, modeled using Bayesian graphs, (Cho
et al., 2016) and M-estimators applied to Gaussian Markov
Random ﬁelds, (Zheng et al., 2018) both of which have
closed-form information measures. By contrast, we evaluate
methods exclusive to the LFI setting where likelihoods and
closed-form information measures are not tractable. Regard-
ing model selection, trained classiﬁers have been proposed
to classify whether data can ﬁt a proposed model or not
(Radev et al., 2021). While useful in model selection, this
method does not provide a posterior distribution of models’
parameters or design optimal experiments. Our method pro-
vides an alternative for scientists who have a model of their
system that they can simulate but not evaluate its likelihood
function, compare models, and design experiments towards
the most promising model. Additionally, our method has the
potential to be used with biological highthroughput screen-
ing (HTS) systems to increase the efﬁciency of such systems arXiv:2208.02344v1  [q-bio.QM]  3 Aug 2022 An Optimal Likelihood Free Method for Biological Model Selection for ICML-WCB 2022 to discover novel biology and therapeutics. In summary, the key contributions of this paper are: • A method to determine the marginal probability of a
model given observed data. • BMA applied to optimal experimental designs to de-
sign experiments for a given model. • An automated algorithm to design and evaluate exper-
iments in biological models that is compatible with
HTS of biological systems. 2. Background 2.1. Modeling the BMP pathway Two mass action kinetics models have been proposed for the
BMP pathway. The one-step model in Equation (1) models
type I (A) and type II (B) receptors and a ligand (L) forming
a trimer complex in a single step (Su et al., 2022) A + B + L
K
−
→T.
(1) The two-step model in Equations (2) and (3) adds a param-
eter to model a ligand ﬁrst binding with a type I receptor
before forming a trimeric complex with a type II receptor
(Antebi et al., 2017) as follows A + L
K1
−
−
→D
(2) B + D
K2
−
−
→T.
(3) Both models have a complex, T, that phosphyrolyates
SMAD to send a downstream gene expression signal, S,
with a certain efﬁciency, ϵ as ϵT = S.
(4) Steady-state signals can be simulated using convex opti-
mization (Su et al., 2022). 2.2. Normalizing Flows Given a dataset, one may ask what is the probability of a
certain data point in the dataset, px(x), of a variable x with
RD dimensions. However, this probability density is usually
intractable or unknown. Normalizing ﬂows provide a way
to answer this question by creating a transformation from a
known simple distribution, pu(u), such as a Gaussian distri-
bution, to the data distribution, px(x), by a series of nonlin-
ear and invertible composition of functions, f : RD →RD,
where f is composed of N functions, f = f N ◦· · · ◦f 1.
We can map from a base distribution to target distribution using the change-of-variables formula for random variables
as px(x) = pu(u)| det J(f)(u)|−1,
(5) where J(f)(u) is the Jacobian matrix of f evaluated at u.
See Murphy (2023) for details about normalizing ﬂows. 2.3. Likelihood Free Inference For models with an implicit or intractable likelihood func-
tion, p(x|θ), but whose response may be simulated, we can
use LFI methods to approximate the posterior q(θ|x) or
likelihood q(x|θ). This can be done by drawing N samples
from the prior p(θ) and generating a dataset {(θn, xn)}N
n=1
by sampling θn ∼p(θ). Each (θn, xn) is a joint sample
from p(θ, x) = p(θ)p(x|θ), and can be used to train a
normalizing ﬂow to approximate the posterior q(θ|x) con-
ditioned on an observed xo (Greenberg et al., 2019; Papa-
makarios & Murray, 2016) or approximate the likelihood
q(x|θ) conditioned on θ. See Papamakarios et al. (2019)
for details on applying normalizing ﬂows to LFI. While LFI provides a method to approximate a model’s
posterior or likelihood, practical considerations, such as
difﬁculty in rejection sampling in in sequential neural pos-
terior estimate (SNPE) (Greenberg et al., 2019) or pro-
hibitively slow MCMC sampling for sequential neural like-
lihood estimate (SNLE) (Papamakarios et al., 2018), make
LFI methods difﬁcult to implement. In response to this
difﬁculty, recent methods have developed variational meth-
ods to approximate the posterior or likelihood (Gl¨
ockler
et al., 2022; Wiqvist et al., 2021).
These methods, re-
ferred to here as sequential neural likelihood variational
inference (SNLVI), train another normalizing ﬂow, qφ(θ),
to minimize the divergence from an estimated likelihood,
φ∗= argminφ D(qφ(θ)||qψ(x|θ)). We use SNLVI meth-
ods to overcome prior practical difﬁculties in LFI methods. 2.4. Optimal Experimental Design for Implicit
Likelihood Model Selection Optimal experimental designs (OEDs) can be formulated as
an optimization (Boyd & Vandenberghe, 2006) or informa-
tion theoretic problem (MacKay, 1992). Assuming designs
are independent of model parameters, we formulate this
problem as maximizing the information gain (IG) (Foster
et al., 2019), or, the difference in entropy given a proposed
design, d, as IG(x, d) = H[p(θ)] −H[p(θ|x, d)].
(6) This objective function can be rewritten as a utility func-
tion, U(d), that maximizes the mutual information (MI),
I(v; y|d) between a variable of interest, v, and the observed An Optimal Likelihood Free Method for Biological Model Selection for ICML-WCB 2022 data, x, at particular design, d. The MI variable of interest,
v, can be adapted to the scientiﬁc question at hand (Ryan
et al., 2016). A gradient-based approach for OEDs was
recently proposed for likelihood free models that provides
a way to both select a model, M, by BMA and determine
its parameters, p(θ|M) with a minimum number of exper-
iments (Kleinegesse & Gutmann, 2021). Finding designs
that optimally discover a model and its parameters can be
formulated as the following utility function U(d) =
X M Z
p(x|θM, M, d)p(θM, M) log
p(θM, M|x, d) p(θM, M) 
dx. (7) We implement equation 7 by simply averaging each model’s
Mutual Information Neural Estimation (MINE) (Belghazi
et al., 2018) MI estimate. The estimated MI is then used
as the objective function in Bayesian Optimization using a
Gaussian Process (Kleinegesse & Gutmann, 2020). 2.5. Bayesian Model Averaging and the Bayes Factor The weighting of model probabilities is also known as
the Bayes Factor (BF), which we deﬁne here as BF =
p(M1)/p(M0), and can be used as a form of model se-
lection where BF > 10 is strong evidence for M1 and
BF < 1/10 is strong evidence for M0. We only use the
BF for model selection as it uses marginal probabilities that
prefer simpler models by the Bayesian Occam’s razor effect.
Although, this relies on an accurate estimate of the model’s
marginal probability. See Murphy (2022) for further discus-
sion on various model selection techniques. 2.6. Approximating Model Marginal Probability To perform model selection, we need an estimate of each
model’s marginal probability in order to calculate the BF. To
do this, we can use a normalizing ﬂow with a Gaussian base
distribution pu(u) that can provide a probability of a model
given the posterior parameter distribution and observed data,
p(M|xo, θ, d), which is the same as marginal likelihood,
p(xo|θ, M, d), when assuming uniform priors over models,
p(Mi) = 1/|M|. This ﬂow is trained by sampling data
from the simulator of M to produce x ∼px(x|xo, M, θ)
that can be used to train a reverse ﬂow function to a base
Gaussian distribution u = f −1(x). We propose the follow-
ing method to approximate the marginal likelihood.
Proposition 2.1. The marginal likelihood of a model, M,
given an observed data vector, xo, and the model’s pa-
rameters, θ, can be approximated as p(xo|M) ≈1 −
Φ(f −1(xo)), where f −1 is the pullback of a trained nor-
malizing ﬂow from the observed data distribution, px(xo),
to a Gaussian base distribution, pu(u), and Φ is cumulative
distribution function of a Gaussian distribution. We provide a proof of Proposition 2.1 in Appendix B. Al-
gorithm 1 in Appendix A brings these parts together in
SBIDOEMAN BMA to optimally determine models and
their parameters. POLICY
MEDIAN BF
25%
75% ONE-STEP RANDOM
0.05
0.02
0.17
ONE-STEP EQUI
0.55
0.09
3.72
ONE-STEP SDM BMA
0.03
0.01
0.05 TWO-STEP RANDOM
0.74
0.22
1.28
TWO-STEP EQUI
2.12
0.79
16.11
TWO-STEP SDM BMA
5.70
1.38
34.66 Table 1. Median and interquartile range (IQR) Bayes Factor (BF)
values after 5 rounds of experiments for both one-step and two-step
datasets compared to random and equidistant experimental design
policies. Lower BF is better for the series of one-step models while
higher BF is better for the two-step model. For both models, both
the median and IQR values are better than competing approaches. Figure 1. Final Bayes Factor (BF) after 5 design rounds and an
ensemble of models. Compared to controls for both models, SBI-
DOEMAN BMA performed an order of magnitude better on the
one-step model and performed more than two times better than
control policies of the two-step model. 3. Results We evaluated SBIDOEMAN BMA for model selection by
evaluating the BF over ﬁve rounds of experiments when
the one-step BMP pathway was true and when the two-step
BMP pathway was true by holding out a single set of param-
eters for each model, θ{1,2}
T
. When evaluating performance
across designs, we compared to random search, as shown in
Figure 2. We also compared ﬁnal BF to random and equidis- An Optimal Likelihood Free Method for Biological Model Selection for ICML-WCB 2022 Figure 2. Change in Bayes Factor (BF), p(twostep)/p(onestep),
over design round when the one-step (top) and two-step (bottom)
models are true. The strong evidence threshold for both models
is labeled in red. Top: When the one-step model is true, SBI-
DOEMAN BF model trends down, indicating the one-step model
is true and outperforms random search by the ﬁnal design. The
median BF value for the SBIDOEMAN model strongly suggests
the one-step model is true by the ﬁfth round. Bottom: When the
two-step model is true the median value of the SBIDOEMAN BF
trends upwards, indicating the two-step model is true, and has a
median trend that outperforms the competing random search by
the last three designs. The two-step model’s ﬁnal value indicates
only moderate evidence in favor of the true two-step model. tant ligand titrations which is a heuristic commonly used in
biology to evaluate the response of an assay. Equidistant
designs are logarithmically-equal spaced designs across a
domain of interest. Here, this would be ﬁve equally spaced
designs in concentrations from 10−3 to 103ng/ml. Results
of the ﬁnal design comparison are shown in Figure 1 and
Table 1. Examining the change in BF across designs Figure 2, we
see that across an ensemble of independent and identically
distributed (iid) SBIDOEMAN models that the median per-
formance outperforms random search for both the one-step
and two-step models. When looking at the ﬁnal BF after a
budget of 5 designs, as shown in Table 1 and Figure 2, we
see that the median performance of SBIDOEMAN BMA outperforms random and equidistant data, with SBIDOE-
MAN BMA interquartlie range (IQR) values performing
better, or almost better, than competing policy median val-
ues. While random search performed as well as SBIDOE-
MAN BMA in the one-step model, it performs worse in the
more complex two-step model, suggesting that principled
heuristics and optimal experimental design algorithms are
needed for more complex models of biology. 4. Discussion Robustness and Performance of SBIDOEMAN BMA
We demonstrated the ability of SBIDOEMAN BMA to se-
lect a true model of the BMP pathway over competing meth-
ods, including a standard heuristic in biological systems.
Across an ensemble of models, prediction of optimal de-
signs and subsequent posterior evaluation are more efﬁcient.
In the process of comparing SBIDOEMAN BMA, we have
shown how to estimate a model’s marginal probability using
normalizing ﬂows. We have also shown that averaging the
mutual information estimate between models still results in
designs that outperform competing methods in improving
the quality of experiments. Future Work We demonstrated application of SBIDOE-
MAN BMA to two simple models, the one-step and two-
step models, of the BMP pathway, each with two and three
parameters, respectively. The efﬁcacy of the SBIDOEMAN
algorithm to scale and infer larger implicit likelihoods, such
as a BMP model with up to 275 parameters for a BMP two-
step model with up to 10 unique ligands, 4 type I receptors,
and 3 type II receptors, remains to be seen. Recent innova-
tions in LFI methods, such as SNLVI (Gl¨
ockler et al., 2022),
show promise to ease such problems, demonstrating robust
inference of a neuroscience model with 31 parameters. We
also only consider the case without noise or batch effects
observed in the model, which true biological systems have.
Future work will examine how SBIDOEMAN BMA scales
to larger models and how robust it is to noise and batch
effects in biological systems. Computationally, we only used averaging of the mutual in-
formation among models to design optimal experiments.
Weighting each model’s mutual information by its respec-
tive marginal probability may lead to improved designs for
for the model with more evidence. We used a simple ensem-
ble method to evaluate the performance of iid models using
our algorithm, which allows us to measure uncertainty in
models’ predictions. Mixtures of Experts (MoEs) (Bengio
et al., 2013; Shazeer et al., 2017) have also shown promise
to improve training and can be combined with ensembling
methods to also perform uncertainty quantiﬁcation (Alling-
ham et al., 2021). These methods could both improve per-
formance and uncertainty quantiﬁcation in optimal designs
for biological models. An Optimal Likelihood Free Method for Biological Model Selection for ICML-WCB 2022 Software and Data We used the hydra conﬁguration manager to track hyperpa-
rameters and seeds of experiments (Yadan, 2019). To per-
form SBI, we used the SBI software library (Tejero-Cantero
et al., 2020). The model marginal probability calculation
was performed using JAX and Distrax libraries (Bradbury
et al., 2018; Babuschkin et al., 2020). Acknowledgements This research was funded by the National Institute of Gen-
eral Medical Sciences (NIGMS) of the National Institutes
of Health (NIH) under award numbers R01GM134418 and
1F31GM145188-01. We would like to thank Matthew Lan-
gley for helpful conversations about the BMP pathway,
Christina Su for making the simulator that made this work
possible, and other members of the Elowitz Lab for helpful
discussions. We thank Zelda Mariet for revising our proof.",0
"Some types of statistical convergence such as statistical order and de-
ferred statistical convergences have been studied and investigated in Riesz
spaces, recently. In this paper, we introduce the concept of deferred sta-
tistical convergence in Riesz spaces with order convergence. Moreover,
we give some relations between deferred statistical order convergence and
other kinds of statistical convergences. Keywords: deferred statistical convergence, order convergence, de-
ferred statistical order convergence, Riesz space 1
Introduction and Preliminaries Statistical convergence is a generalization of the ordinary convergence of a
real or complex sequence. It was introduced by Steinhaus in [18]. Maddox
discussed the statistical convergence in more general abstract spaces such
as locally convex spaces in [15]. K¨
u¸
c¨
ukaslan and Yılmazt¨
urk introduced
and investigated the deferred statistical convergence in [12]. It is enough
to mention the theory of statistical convergence (cf. [1, 10, 11, 13, 15]).
On the other hand, Riesz space (or, vector lattice) is another concept of
functional analysis that was introduced by Riesz [16]. Then, many authors
developed the subject. Riesz space is an ordered vector space that has
many applications in measure theory, Banach space, operator theory, and
applications in economics (cf. [2, 3, 5, 14, 20]). The present paper aims to
combine the concepts of deferred statistical convergence of real sequences
and order convergence in Riesz spaces. 1 A real-valued vector space E with an order relation is said to be ordered
vector space if, for each x, y ∈E with x ≤y, we have x + z ≤y + z and
αx ≤αy for all z ∈E and α ∈R+. An ordered vector space E is called
Riesz space or vector lattice if, for any two vectors x, y ∈E, the inﬁmum
and the supremum x ∧y = inf{x, y} and x ∨y = sup{x, y} exist in E, respectively. For an element x in a vector lattice E, the positive
part, the negative part, and module of x are respectively deﬁned as follows: x+ := x ∨0,
x−:= (−x) ∨0 and |x| := x ∨(−x). Thus, in the present paper, the vertical bar |·| of elements in vector lattices
will stand for the module of the given elements. A subset A of a vector
lattice E is called solid if, for each x ∈A and y ∈E, |y| ≤|x| implies
y ∈A. A solid vector subspace of a vector lattice is referred to as an ideal.
A vector lattice is called σ-order complete if every nonempty bounded
above countable subset has a supremum (or, equivalently, whenever every
nonempty bounded below countable subset has an inﬁmum) (cf. [2]).
A sequence (xn) in a Riesz space E is said to be increasing whenever
x1 ≤x2 ≤· · · and is decreasing if x1 ≥x2 ≥· · · holds. Then, we denote
them by xn ↑and xn ↓, respectively. Moreover, if xn ↑and sup xn = x,
then we write xn ↑x. Similarly, if xn ↓and inf xn = x, then we write
xn ↓x. Then, we call that (xn) is increasing or decreasing as monotonic.
On the other hand, order convergence is crucial for this paper, and so, we
continue with its deﬁnition. Deﬁnition 1.1. Let (xn) be a sequence in a vector lattice E. Then, it is
called order convergent to x ∈E if there exists another sequence yn ↓0
(i.e., inf yn = 0 and yn ↓) such that |xn −x| ≤yn holds for all n ∈N,
and abbreviated it as xn
o
−
→x. For the deﬁnition of statistical convergence, the important point is the
natural density of subsets of natural numbers. Recall that the density
of a subset K of N is the limit limn→∞1 n |{k ≤n : k ∈K}| whenever
this unique limit exists, and it is mostly abbreviated by δ(K), where
|{k ≤n : k ∈K}| is the cardinality of K, and it does not exceed n. A
sequence (xn) of real numbers is called statistical convergent to a real
number l if, for every ε > 0, we have lim
n→∞
1
n {k : n ≥k, |xn −l| > ε}

 = 0. Now, consider a sequence x := (xk) and take the sequences (pn) and
(qn) of non-negative integers such that pn < qn for each n and qn divergent
to inﬁnity. Then, deﬁne a new sequence (Dp,qx)n :=
1 qn −pn qn
X k=pn+1
xk, for each n ∈N. The sequence (Dp,qx)n is called the deferred Ces´
aro mean
as a generalization of Ces´
aro mean of real (or complex) valued sequence; 2 see [1]. On the other hand, x is said to be strong Dp,q-convergent to l if
the following limit exists lim
n→∞
1 qn −pn qn
X k=pn+1
|xk −l| = 0. Then, we abbreviate it as xk
D[p,q]
−
−
−
−
→l. In this article, unless otherwise,
when we mention p and q sequences, they always hold the above proper-
ties, and also, these properties are said to be the deferred property.
A sequence x := (xk) is called deferred statistical convergent to l ∈R
whenever, for all ε > 0, we have lim
n→∞
1 qn −pn {pn < k ≤qn : |xk −l| ≥ε}

 = 0 holds; see [12]. In this case, we write xk
DS[p,q]
−
−
−
−
−
→l.
A characterization of statistical convergence on vector lattices was
introduced by Ercan in [9], and also, some kinds of statistical convergence
in Riesz spaces were introduced and studied by Aydın in [4, 6, 7, 8]. Deﬁnition 1.2. Let (xn) be a sequence in a Riesz space E. Then, (xn)
is called - statistical order decreasing to 0 if there exists a set K = {k1 <
k2 < · · · } ⊂N with δ(K) = 1 such that (xkn) is decreasing and
inf
n∈K(xkn) = 0, i.e., (xkn)kn∈K ↓0, and it is abbreviated as xn ↓sto 0; - statistical order convergent to x ∈E if there exists a sequence
qn ↓sto 0 with an index set K = {k1 < k2 < · · · } ⊂N such that
δ(K) = 1 and
|xkn −x| ≤qkn for every kn ∈K, and so, we write xn
sto
−
−
→x. It is clear that every order convergent sequence is statistical order
convergent to the same point. 2
Deferred statistical decreasing Tripathy [19] introduced the statistical monotonicity for real sequences,
and also, statistically monotone sequences in Riesz spaces were investi-
gated. We extend it to deferred statistical decreasing in Riesz spaces. Deﬁnition 2.1. Let (pn) and (qn) be sequences of nonnegative integers
satisfying the deferred property. Then, a sequence (zn) in a Riesz space
E is called deferred statistical order decreasing to 0 if there exists a set
K ⊆N such that the deferred density of K δp,q(K) := lim
n→∞
1 qn −pn {pn < k ≤qn : k ∈K}

 = 1 and (zkn)kn∈K ↓0 holds on K. Then, we abbreviate it as zn ↓
Dsto
p,q
0. 3 Remark 2.2. (i) If q(n) = n and p(n) = 0, then Deﬁnition 2.1 coincides with the
deﬁnition of statistical order decreasing. (ii) If (zn) is monotone decreasing to zero, then it is deferred statistical
order decreasing to zero. But, the converse does not need to be true
in general. To see this, consider the Euclidean space R2 with the
coordinatewise ordering and the sequences q(n) = n and p(n) = 0
and (zn) denoted by zn := (
(0, n2)
if n = k3 (0,
1 n2 )
if n ̸= k3 , where k ∈N. Hence, we get zn ↓
Dsto
p,q
(0, 0). But, observe that the
whole sequence (zn) is not monotonic. (iii) A deferred statistical order decreasing to zero sequence may contain
a subsequence of decreasing or incomparable elements of E but the
index set of such a subsequence has deferred density zero. (iv) In Riesz spaces, it is well known that zn ↓0 implies zkn ↓0 for
every subsequence (zkn) of (zn). However, this may not hold in the
setting of deferred statistical monotone decreasing sequences.
For
example, take the sequences in the (ii) with the subsequence (zkn),
where kn = j3 for some j ∈N, does not have a supremum. In the general case, the example in Remark 2.2(iv) shows that a sub-
sequence of deferred statistical monotone decreasing sequence need not be
deferred statistical monotone decreasing. But, we give a positive result in
the following theorem. Theorem 2.3. Let (zn) be a sequence in a Riesz space E. If zn ↓
Dsto
p,q
0,
then any subsequence (zkn) of (zn) with index set δp,q(K) = 1 such that
(zkn) is decreasing on K is deferred statistical order decreasing to 0. Proof. Suppose that zn ↓
Dsto
p,q
0 holds in E.
Then, there exists a set
K ⊂N such that δp,q(K) = 1 and (zkn)kn∈K ↓0 on K. Let us consider
any arbitrary index set M ⊆N such that K ̸= M, δp,q(M) = 1 and (zn) is
decreasing on M. It can be observed that if there is not such a set M, then
the poof is complete. It follows from zkn ↓0 that 0 ≤zkn for all kn ∈K.
Also, we have δp,q(K ∩M) = 1. Thus, for some km ∈K and mn ∈M, we
have kn = mn. Hence, we have zm1 ≥zm2 ≥· · · ≥zmn = zkn ≥0. We
can ﬁnd inﬁnitely many of such pair of indices. By continuing this way,
we obtain zmn ≥0 for every mn ∈M, i.e., zero is a lower bound of (zmn).
Take another lower bound u of (zmn). Therefore, we have u ≤zmn for
every mn ∈M. Then, we can ﬁnd some znkt such that znkt = zmk ≥u
for some mk ∈M. By following this way, we can construct a subsequence
(znk1 , znk2 , · · · ) of (zkn) such that u is a lower bound of (znkt ) for t ∈N.
It follows from zkn ↓0 that the inﬁmum of every subsequence of (zkn)
is zero.
Hence, we get u = 0.
Therefore, we get the desired result,
zmn ↓
Dsto
p,q
0. 4 In the next results without proof, we give the linear property of de-
ferred statistical order decreasing sequences. Proposition 2.4. Let xn ↓
Dsto
p,q
0 and yn ↓
Dsto
p,q
0 be a sequence in a Riesz
space E and λ ∈R. Then, we have (i) (xn + yn) ↓
Dsto
p,q
0; (ii) λxn ↓
Dsto
p,q
0. 3
Deferred statistical order convergence Deﬁnition 3.1. Let p and q be sequences of positive integers satisfying
the deferred property. Then, a sequence (xn) in a Riesz space E is called
deferred statistical order convergent to x if there exists a sequence zn ↓Dsto
0 with an index set K ⊆N such that δp,q(K) = 1 and |xkn −x| ≤zkn holds for all kn ∈K. Then, we write xn
Dsto (p,q)
−
−
−
−
−
−
→x. Remark 3.2. It can be seen that, in the case of xn
Dsto (p,q)
−
−
−
−
−
−
→x, we have δp,q({n ∈N : |xn −x| ≰zn}) = 0. Remark 3.3. It can be observed that the deferred statistical order con-
vergence of the sequence (xn) in Deﬁnition 3.1 with sequence (zn) to x implies that xkn
Dsto (p,q)
−
−
−
−
−
−
→x with the same sequence (zn). The converse is also true, i.e., if there exists a subsequence (xkn)
Dsto (p,q)
−
−
−
−
−
−
→x of a se- quence (xn) with a sequence zn ↓Dsto 0, then xn
Dsto (p,q)
−
−
−
−
−
−
→x with the same
sequence (zn). It is clear that deferred statistical order decreasing sequence is deferred
statistical order convergent. But, the converse does not hold in general. Remark 3.4. Let q(n) = n and p(n) = 0. Then, we have the following
observations: (i) an order convergent sequence is deferred statistical order convergent
to its order limit; (ii) the statistical order convergence and deferred statistical order con-
vergence coincide; One can observe that a subsequence of a deferred statistical order
convergent sequence need not be deferred statistical order convergent. Proposition 3.5. Let (xn) be a sequence in a Riesz space E.
Then, xn
Dsto (p,q)
−
−
−
−
−
−
→x holds if and only if there exists another sequence (yn) in E such that δp,q({n ∈N : xn = yn}) = 1 and yn
Dsto (p,q)
−
−
−
−
−
−
→x. 5 Proof. Suppose that there exists a sequence (yn) in E such that δp,q({n ∈ N : xn = yn}) = 1 and yn
Dsto (p,q)
−
−
−
−
−
−
→x. Then, there is another sequence
zn ↓Dsto 0 in E with δp,q(K) = 1 such that |xkn −x| ≤zkn for each
kn ∈K. Thus, it follows from the including {pn + 1 ≤m ≤qn : |xm −x| ≰zm} ⊆{pn + 1 ≤m ≤qn : xm ̸= ym} ∪{pn + 1 ≤m ≤qn : |ym −x| ≰zm} that we have lim
n→∞
1 qn −pn |{pn + 1 ≤m ≤qn : |xm −x| ≰zm}| ≤lim
n→∞
1 qn −pn |{pn + 1 ≤m ≤qn : xm ̸= ym}| because of δp,q({pn + 1 ≤m ≤qn : |ym −x| ≰zm}) = 0. Thus, we obtain lim
n→∞
1 qn −pn |{pn + 1 ≤m ≤qn : |xm −x| ≰zm}| = 0. Therefore, we get the desired result, xn
Dsto (p,q)
−
−
−
−
−
−
→x. The other part of
proof is obvious, and so, we omit it. Proposition 3.6. The deferred statistical order limit is linear and uniquely
determined. Proof. Assume that xn
Dsto (p,q)
−
−
−
−
−
−
→x and xn
Dsto (p,q)
−
−
−
−
−
−
→y hold in a Riesz
space E. Then, there are sequences zn ↓Dsto 0 with δp,q(K) = 1 and
tn ↓Dsto 0 with δp,q(M) = 1 such that |xkn −x| ≤zkn and |xmn −y| ≤tmn
for all kn ∈K and mn ∈M. Thus, it follows that |x −y| ≤|x −xjn| + |xjn −y| ≤zjn + tjn for every jn ∈J := K ∩M. By using (zjn + tjn)jn∈J ↓0, we obtain that
|x −y| = 0. Thus, we get the equality of x and y.
Now, for the linearity of the deferred statistical order limit, take se- quences xn
Dsto (p,q)
−
−
−
−
−
−
→x and yn
Dsto (p,q)
−
−
−
−
−
−
→y in a Riesz space E. Then, there
are sequences zn ↓Dsto 0 and tn ↓Dsto 0 such that δp,q({n ∈N : |xn −x| ≰
zn}) = 0 and δp,q({n ∈N : |yn −y| ≰tn}) = 0. It follows from the
triangular inequality in Riesz spaces that {n ∈N : |(xn + yn) −(x + y)| ≰zn + tn} ⊆{n ∈N : |xn −x| ≰zn} ∪{n ∈N : |yn −y| ≰tn}. Therefore, we obtain δp,q({n ∈N : |(xn + yn) −(x + y)| ≰zn + tn}) = 0, i.e., we obtain xn + yn
Dsto (p,q)
−
−
−
−
−
−
→x + y. In the following result, we observe some relations between deferred
statistical order convergence and lattice properties. 6 Theorem 3.7. Let xn
Dsto (p,q)
−
−
−
−
−
−
→x and yn
Dsto (p,q)
−
−
−
−
−
−
→y in a Riesz space E.
Then, we have the following statement: (i) xn ∨yn
Dsto (p,q)
−
−
−
−
−
−
→x ∨y; (ii) xn ∧yn
Dsto (p,q)
−
−
−
−
−
−
→x ∧y; (iii) x+
n
Dsto (p,q)
−
−
−
−
−
−
→x+; (iv) x−
n
Dsto (p,q)
−
−
−
−
−
−
→x−; (v) |xn|
Dsto (p,q)
−
−
−
−
−
−
→|x|. Proof. It is enough to show the ﬁrst statement because the other case
can be obtained by applying [3, Thm.1.7] and Proposition 3.6.
Now, from xn
Dsto (p,q)
−
−
−
−
−
−
→x and yn
Dsto (p,q)
−
−
−
−
−
−
→y, we have sequences zn ↓Dsto 0 and
tn ↓Dsto 0 with indexes sets δp,q(K) = δp,q(M) = 1 such that |xkn −x| ≤
zkn and |ymn −y| ≤tmn hold for all kn ∈K and mn ∈M. On the other
hand, by applying [3, Thm.1.9] and by taking J := N ∩M, we get |xjn ∨yjn −x ∨y| ≤|xjn −x| + |yjn −y| ≤zjn + tjn for every jn ∈J. Hence, we obtain δp,q({n ∈N : |xjn ∨yjn −x ∨y| ≰zjn + tjn}) = 0. Therefore, we get the desired result, xn ∨yn
Dsto (p,q)
−
−
−
−
−
−
→x ∨y. Corollary 3.8. The positive cone E+ = {x ∈E : 0 ≤x} of a Riesz space
E is closed under the deferred statistical order convergence. Proposition 3.9. If xn
Dsto (p,q)
−
−
−
−
−
−
→x, yn
Dsto (p,q)
−
−
−
−
−
−
→y and xn ≥yn satisfy
for every n ∈N in a Riesz space, then x ≥y holds. Proof. Assume that yn ≤xn holds for each n ∈N. Then, we have 0 ≤
xn −yn ∈E+ for each n ∈N. It follows from Corollary 3.8 that xn − yn
Dsto (p,q)
−
−
−
−
−
−
→x−y ∈E+ because of (xn−yn) ∈E+. Thus, we get x−y ≥0,
i.e., x ≥y. Theorem 3.10. If (xn) is a monotone and deferred statistical order con-
vergent in a Riesz space, then it is order convergent. Proof. Suppose that (xn) ↓and xn
Dsto (p,q)
−
−
−
−
−
−
→x in a Riesz space E. Fix
any k ∈N. Then, we have xk −xn ≥0 for all n ≥k. It follows that xk −xn
Dsto (p,q)
−
−
−
−
−
−
→xk −x ≥0, i.e., xk ≥x. Thus, x is an lower bound
of (xn) because k is arbitrary. Choose another lower bound z of (xn). Hence, we have xn −z
Dsto (p,q)
−
−
−
−
−
−
→x −z ≥0, i.e., x ≥z. Therefore we get
the desired result, xn ↓x. 7 Remark 3.11. Let A be an ideal in a vector lattice E and (an) be a
sequence in A. One can observe that if an
o
−
→0 in A, then an
o
−
→0 in E.
Hence, it is clear that an ↓Dsto 0 in A implies an ↓Dsto 0 in E. For the
converse, if an
o
−
→0 in E and order bounded, then an
o
−
→0 in A, and so,
an ↓Dsto 0 in E implies an ↓Dsto 0 in A for order bounded sequences. Thanks to Remark 3.11, we give the following two results. Theorem 3.12. Let A be an ideal in an σ-order complete vector lattice and (xn) be a sequence in A. Then, xn
Dsto (p,q)
−
−
−
−
−
−
→0 in A if and only if xn
Dsto (p,q)
−
−
−
−
−
−
→0 in E. Proof. Assume that xn
Dsto (p,q)
−
−
−
−
−
−
→0 in A. Then, there exists a sequence
zn ↓Dsto 0 in A with index set δp,q(K) = 1 such that |xkn| ≤zkn for all
kn ∈K. Now, by using Remark 3.11, it follows from (zkn)kn∈K ↓0 in A
that (zkn)kn∈K ↓0 in E, i.e., we get zn ↓Dsto 0 in E Therefore, we have xn
Dsto (p,q)
−
−
−
−
−
−
→0 in E. Conversely, assume xn
Dsto (p,q)
−
−
−
−
−
−
→0 in E.
Then, there is a sequence
zn ↓Dsto 0 in E with index set δp,q(K) = 1 such that |xkn| ≤zkn for all
kn ∈K. Thus, Remark 3.11 implies that zn ↓Dsto 0 in A. Therefore, we get xn
Dsto (p,q)
−
−
−
−
−
−
→0 in A. The following result is similar to [13, Thm.3.1.]. Theorem 3.13. Let (xn) be a sequence in a Riesz space E and (xkn)kn∈K
be a subsequence of (xn). If the limit lim inf
n→∞
1 qn −pn {pn < kn ≤qn : kn ∈K}

 > 0 holds and xn
Dsto (p,q)
−
−
−
−
−
−
→x for some sequences p and q satisfying the deferred property, then xkn
Dsto (p,q)
−
−
−
−
−
−
→x. Proof. Assume that xn
Dsto (p,q)
−
−
−
−
−
−
→x satisﬁes in E.
Then, there is a se-
quence zn ↓Dsto 0 in E such that δp,q({n ∈N : |xn −x| ≰zn}) = 0. It
can be seen that {pn < kn ≤qn : kn ∈K, |xkn −x| ≰zn} ⊆{pn < n ≤qn : |xn −x| ≰zn}. Thus, by taking Hn := {pn < kn ≤qn : kn ∈K} for all n, we have 1 |Hn| {pn < kn ≤qn : kn ∈K, |xkn −x| ≰zn} ≤
1 |Hn|{pn < n ≤qn : |xn −x| ≰zn}. Therefore, it is enough to show lim sup
n→∞ 1 |Hn|

{pn < n ≤qn : |xn −x| ≰ zn}

 = 0 for proving the convergence xkn
Dsto (p,q)
−
−
−
−
−
−
→x. We observe the
following inequality lim inf
n→∞
|Hn| qn −pn lim sup
n→∞
|{pn < n ≤qn : |xn −x| ≰zn}| |Hn| 8 ≤lim sup
n→∞
|{pn < n ≤qn : |xn −x| ≰zn}| qn −pn
. Therefore, we have lim sup
n→∞
1 |Hn| {pn < n ≤qn : |xn −x| ≰zn}

 = 0 because of xn
Dsto (p,q)
−
−
−
−
−
−
→x. Thus, we obtain the desired result. In Remark 3.4, we give a relation between statistical order and deferred
statistical order convergences by taking q(n) = n and p(n) = 0. We give
another relation under a new condition in the next theorem. Theorem 3.14. If the sequence
",0
"Application of adeles in modern mathematical physics is brieﬂy
reviewed. In particular, some adelic products are presented. 1
Introduction p-Adic numbers are invented by K. Hansel in 1897. Ideles and adeles are
introduced by C. Chevalley and A. Weil, respectively, in the 1930s. p-Adic
numbers and adeles have many applications in mathematics, e.g. represen-
tation theory, algebraic geometry and modern number theory. Since 1987,
p-adic numbers and adeles have been used in construction of many mod-
els in modern mathematical physics and related topics. Here we consider
applications of adeles in mathematical physics. ∗E-mail:dragovich@phy.bg.ac.yu 1 2
Adeles On the ﬁeld Q of rational numbers any non-trivial norm is equivalent either
to the usual absolute value |·|∞or to a p-adic absolute value |·|p (Ostrowski
theorem). For a rational number x = pν a b, where integers a and b are not
divisible by prime number p, by deﬁnition p-adic absolute value is |x|p = p−ν and |0|p = 0. This p-adic norm is a non-Archimedean (ultrametric) one,
because |x+y|p ≤max{|x|p , |y|p}. As completion of Q gives the ﬁeld Q∞≡R
of real numbers with respect to the | · |∞, by the same procedure one get the
ﬁelds Qp of p-adic numbers (p = 2, 3 , 5 · · · ) using |·|p . Any number x ∈Qp
has a unique canonical representation x = pν(x)
+∞
X n=0
xn pn ,
ν(x) ∈Z ,
xn ∈{0, 1, · · · , p −1}.
(1) Real and p-adic numbers, as completions of rationals, unify by adeles. An
adele α is an inﬁnite sequence α = (α∞, α2, α3, · · · , αp , · · ·) ,
α∞∈R , αp ∈Qp ,
(2) where for all but a ﬁnite set P of primes p one has that αp ∈Zp = {x ∈Qp :
|x|p ≤1}. Zp is the ring of p-adic integers. The set AQ of all adeles can be
presented as AQ =
[ P
A(P) ,
A(P) = R ×
Y p∈P
Qp ×
Y p̸∈P
Zp .
(3) Endowed with componentwise addition and multiplication AQ is the adele
ring.
The multiplicative group of ideles A×
Q is a subset of AQ with elements
η = (η∞, η2 , η3 , · · · , ηp , · · ·) , where η∞∈R× = R \ {0} and ηp ∈Q×
p =
Qp \ {0} with the restriction that for all but a ﬁnite set P one has that
ηp ∈Up = {x ∈Qp : |x|p = 1} . Thus the whole set of ideles is A×
Q =
[ P
A×(P),
A×(P) = R× ×
Y p∈P
Q×
p ×
Y p̸∈P
Up .
(4) A principal adele (idele) is a sequence (x, x, · · · , x, · · ·) ∈AQ , where
x ∈Q
(x ∈Q×).
Q and Q× are naturally embedded in AQ and A×
Q ,
respectively. 2 Let P be set of all primes p. Denote by Pi , i ∈N, subsets of P. Let us
introduce an ordering by Pi ≺Pj if Pi ⊂Pj. It is evident that A(Pi) ⊂A(Pj)
when Pi ≺Pj.
Adelic topology in AQ is introduced by inductive limit:
AQ = lim indPA(P). A basis of adelic topology is a collection of open sets of
the form V (P) = V∞×Q p∈P Vp ×Q p̸∈P Zp , where V∞and Vp are open sets
in R and Qp , respectively. A sequence of adeles α(n) ∈AQ converges to an
adele α ∈AQ if (i) it converges to α componentwise and (ii) if there exist
a positive integer N and a set P such that α(n), α ∈A(P) when n ≥N.
In the analogous way, these assertions hold also for idelic spaces A×(P) and
A×
Q. AQ and A×
Q are locally compact topological spaces.
For various mathematical aspects of adeles one can see books [1, 2, 3]. 3
Adelic models Recall that results of measurements are rational numbers, and physical mod-
els have been treated using real and complex numbers. Since Q is dense not
only in R but also in Qp, it has been natural to expect some applications of
p-adic numbers in mathematical modeling of physical systems. First signiﬁ-
cant employment of p-adic numbers in physics started in 1987 by successful
construction of p-adic string amplitudes. From the very beginning there has
been an opinion that all prime numbers should be equally important and
that p-adic models should be somehow connected with standard ones (over
real or complex numbers). According to the Hasse local-global principle an
equation has a solution over Q if and only if it has solutions over R and all Qp.
These ideas naturally gave rise to an application of adeles and construction
of adelic physical models (for an early review, see [4, 5]).
Especially so-called adelic products have been attracted much attention.
They are of the form φ∞(x1 , · · · , xn ; a1 , · · · , am)
Y p∈P
φp(x1 , · · · , xn ; a1 , · · · , am) = C ,
(5) where φ∞and φp are real or complex valued functions, xi ∈Q ,
aj ∈
C , and C is a constant (often C = 1). It is obvious that expressions of
the form (5) connect real and p-adic characteristics of the same object at
the equal footing. Moreover, the real quantity φ∞(x1 , · · · , xn ; a1 , · · · , am)
can be expressed as product of all p-adic inverses. This can be of practical 3 importance when functions φp are simpler than φ∞, but may also lead to
more profound understanding of physical reality.
For the reason of better understanding, let us ﬁrst present two simple
examples: |x|∞×
Y p∈P
|x|p = 1 , if x ∈Q× ,
and
χ∞(x) ×
Y p∈P
χp(x) = 1 , if x ∈Q , (6)
where χ∞(x) = exp(−2πix) and χp(x) = exp 2πi{x}p are real and p-adic
additive characters, respectively, and {x}p denotes the fractional part of x.
It follows from (6) that d∞(x, y) = Q p∈P d−1
p (x, y), where d∞(x, y) = |x−y|∞
and dp(x, y) = |x−y|p, i.e. the usual distance between any two rational points
can be regarded through product of the inverse p-adic ones. One can also
write χ∞(ax + bt) = Q p∈P χp[−(ax + bt)] when a , b , x , t ∈Q, and consider
a real plane wave as composed of p-adic plane waves.
Let us also notice some adelic products related to number theory: λ∞(x)
Y p∈P
λp(x) = 1 ,
x, y ∞  Y p∈P x, y p 
= 1 ,
(7) where x is presented by (1) and λp(x) = 




 



 1,
ν(x) = 2k ,
p ̸= 2 ,
r
−1 p
 
x0 p

,
ν(x) = 2k + 1 ,
p ̸= 2 , exp [πi(x1 + 1/4)] ,
ν(x) = 2k ,
p = 2 ,
exp [πi(x2 + x1/2 + 1/4)] ,
ν(x) = 2k + 1 ,
p = 2 , (8) λ∞(x) = exp

−πi 4 sgn x

,
x, y ∞ 
=

−1,
x < 0, y < 0 ,
1,
otherwise ,
(9)

x
p

and

x,y p

are Legendre and Hilbert symbols [5], respectively.
Gauss integrals satisfy adelic product formula [6]
Z R
χ∞(a x2+b x) d∞x
Y p∈P Z Qp
χp(a x2+b x) dpx = 1 ,
a ∈Q× , b ∈Q , (10) what follows from
Z Qv
χv(a x2 + b x) dvx = λv(a) |2 a|
−1 2
v
χv

−b2 4a 
,
v = ∞, 2 , · · · , p · · · . (11) 4 These Gauss integrals apply in evaluation of the Feynman path integrals Kv(x′′, t′′; x′, t′) =
Z x′′,t′′ x′,t′
χv

−1 h Z t′′ t′
L( ˙
q, q, t) dt

Dvq ,
(12) for kernels Kv(x′′, t′′; x′, t′) of the evolution operator in adelic quantum me-
chanics [7] for quadratic Lagrangians. In the case of Lagrangian L( ˙
q, q) = 1
2

−˙
q2 4 −λ q + 1

for the de Sitter cosmological model (what is similar to a
particle with constant acceleration λ) one obtains [8, 9] K∞(x′′, T; x′, 0)
Y p∈P
Kp(x′′, T; x′, 0) = 1 ,
x′′, x′, λ ∈Q , T ∈Q× ,
(13) where Kv(x′′, T; x′, 0) = λv(−8T) |4T|
−1 2
v
χv

−λ2 T 3 24 +[λ (x′′+x′)−2]T 4 +(x′′ −x′)2 8T 
. (14)
The adelic wave function for the simplest ground state has the form ψA(x) = ψ∞(x)
Y p∈P
Ω(|x|p) =

ψ∞(x),
x ∈Z,
0,
x ∈Q \ Z ,
(15) where Ω(|x|p) = 1 if |x|p ≤1 and Ω(|x|p) = 0 if |x|p > 1. Since this wave
function is non-zero only in integer points it can be interpreted as discreteness
of the space due to p-adic eﬀects in adelic approach.
The Gel’fand-Graev-Tate gamma and beta functions [4, 5] are: Γ∞(a) =
Z R
|x|a−1
∞χ∞(x) d∞x = ζ(1 −a) ζ(a)
, Γp (a) =
Z Qp
|x|a−1
p
χp(x) dpx = 1 −pa−1 1 −p−a ,
(16) B∞(a, b) =
Z R
|x|a−1
∞|1 −x|b−1
∞d∞x = Γ∞(a) Γ∞(b) Γ∞(c) ,
(17) Bp(a, b) =
Z Qp
|x|a−1
p
|1 −x|b−1
p
dpx = Γp(a) Γp(b) Γp(c) ,
(18) 5 where a, b, c ∈C with condition a + b + c = 1 and ζ(a) is the Riemann zeta
function. With a regularization of the product of p-adic gamma functions
one has adelic products: Γ∞(u)
Y p∈P
Γp(u) = 1 ,
B∞(a, b)
Y p∈P
Bp(a, b) = 1 ,
u ̸= 0, 1 ,
u = a, b, c , (19)
where a+b+c = 1. It is worth noting now that B∞(a, b) and Bp(a, b) are the
crossing symmetric standard and p-adic Veneziano amplitudes for scattering
of two open tachyon strings. There are generalizations of the above product
formulas for integration on quadratic extensions of R and Qp, as well as on
algebraic number ﬁelds, and they include scattering of closed strings [4, 10].
Introducing real, p-adic and adelic zeta functions as ζ∞(a) =
Z R
exp (−π x2) |x|a−1
∞d∞x = π−a 2 Γ
a 2 
,
(20) ζp(a) =
1 1 −p−1 Z Qp
Ω(|x|p) |x|a−1
p
dpx =
1 1 −p−a ,
Re a > 1 ,
(21) ζA(a) = ζ∞(a)
Y p∈P
ζp(a) = ζ∞(a)ζ(a) ,
(22) one obtains
ζA(1 −a) = ζA(a) ,
(23) where ζA(a) can be called adelic zeta function. Let us note that exp (−π x2)
and Ω(|x|p) are analogous functions in real and p-adic cases. Adelic har-
monic oscillator [7] has connection with the Riemann zeta function. Namely,
the simplest vacuum state of the adelic harmonic oscillator is the following
Schwartz-Bruhat function: ψA(x) = 2
1
4 e−π x2
∞Y p∈P
Ω(|xp|p) ,
(24) whose the Fourier transform ψA(k) =
Z
χA(k x) ψA(x) = 2
1
4 e−π k2
∞Y p∈P
Ω(|kp|p)
(25) has the same form as ψA(x). The Mellin transform of ψA(x) is ΦA(a) =
Z
ψA(x) |x|a d×
Ax 6 =
Z R
ψ∞(x) |x|a−1d∞x
Y p∈P 1 1 −p−1 Z Qp
Ω(|x|p)|x|a−1 dpx =
√ 2 Γ
a 2 
π−a 2 ζ(a) (26)
and the same for ψA(k). Then according to the Tate formula one obtains (23).
It is remarkable that such simple physical system as harmonic oscillator is
related to so signiﬁcant mathematical object as the Riemann zeta function.
Recently [11] adelic properties of dynamical systems, which evolution is
governed by linear fractional transformations f(x) = ax + b cx + d ,
a, b, c, d, ∈Q ,
ad −bc = 1
(27) is investigated. It is shown that rational ﬁxed points are p-adic indiﬀerent
for all but a ﬁnite set P of primes, i.e. only for ﬁnite number of p-adic cases
a rational ﬁxed point may be attractive or repelling. 4
Concluding remarks We presented a brief review of some important applications of adeles in mod-
ern mathematical physics. We considered above simple cases of adeles AQ
consisting of completions of Q. There is also ring of adeles AK related to the
completions of any global ﬁeld K. There is a straightforward generalization
of AQ to the n-dimensional vector space An
Q = Qn
i=1 A(i)
Q (see, e.g. [1]). Adelic
algebraic group G(AK) is an adelization of a linear algebraic group G over
completion ﬁelds Kv of a global ﬁeld K [1, 2, 3].
For a more detail insight into this attractive and promising ﬁeld of in-
vestigations let us also mention a few additional topics.
Adelic quantum
cosmology (for a review, see [9]) is an application of adelic quantum me-
chanics [7] to explore very early evolution of the universe as a whole. Adelic
path integral [12] is a suitable extension of the standard Feynman path inte-
gral and serves to describe quantum evolution of adelic objects. Conjecture
on the adelic universe with real and p-adic worlds, as well as p-adic origin
of dark matter and dark energy are discussed in [9]. Adelic summability
[13] of perturbation series is an approach to summation of divergent series
in the real case when they are convergent in all p-adic cases. Use of eﬀec-
tive Lagrangians on real numbers for p-adic strings has been very eﬃcient in
their application to string theory and cosmology. Paper [14] is an attempt 7 towards eﬀective Lagrangian for adelic strings without tachyons. Further de-
velopment of adelic analysis and, in particular, adelic generalized functions
[6, 15, 16] is one of mathematical opportunities.
One can conclude that there has been a successful application of adeles
in modern mathematical physics and that one can expect a growing interest
in their further mathematical developments as well as in applications. Acknowledgements. The work on this article was partially supported by
the Ministry of Science and Environmental Protection, Serbia, under contract
No 144032D.",0
"In computational science and in computer science, research software is a central asset for research. Computational science is the application of computer science and software engi- neering principles to solving scientiﬁc problems, whereas computer science is the study of computer hardware and software design. The Open Science agenda holds that science advances faster when we can build on existing results. Therefore, research software has to be reusable for advancing science. Thus, we need proper research software engineering for obtaining reusable and sustainable research software. This way, software engineering methods may improve research in other disciplines. However, research in software engineering and computer science itself will also beneﬁt from reuse when research software is involved. For good scientiﬁc practice, the resulting research software should be open and adhere to the FAIR principles (ﬁndable, accessible, interoperable and repeatable) to allow repeatabil- ity, reproducibility, and reuse. Compared to research data, research software should be both archived for reproducibility and actively maintained for reusability. The FAIR data principles do not require openness, but research software should be open source software. Established open source software licenses provide sufﬁcient licensing options, such that it should be the rare exception to keep research software closed. We review and analyze the current state in this area in order to give recommendations for making computer science research software FAIR and open. We observe that research soft- ware publishing practices in computer science and in computational science show signiﬁcant differences. 1 arXiv:1908.05986v1  [cs.SE]  16 Aug 2019 Key Insights • For good scientiﬁc practice in computer science research, evaluating and publishing re- search software is gaining attention. • For reproducibility and for reusability of research software, speciﬁc solution approaches are required: archives such as Zenodo serve for reproducibility, while development plat- forms such as GitHub serves for use, reuse and active involvement. • Research software publishing practices in computer science and in computational science show signiﬁcant differences: computational science emphasizes reproducibility, while computer science emphasizes reuse. Research Software Research software is software that is employed in the scientiﬁc discovery process or a research object itself. Computational science (also scientiﬁc computing) involves the development of research software for model simulations and data analytics to understand natural systems an- swering questions that neither theory nor experiment alone are equipped to answer. Computa- tional science is a multidisciplinary ﬁeld lying at the intersection of mathematics and statistics, computer science, and core disciplines of science and research. Despite the increasing importance of research software to the scientiﬁc discovery process, well-established software engineering practices are rarely adopted in computational science [24], but the computational science community has started to appreciate that software engineering is central to any effort to increase its research software productivity [34]. Computer science, in particular software engineering, may help with reproducibility and reuse to advance computa- tional science. Publishing research software as open source is an established practice in science; a popular open source repository is GitHub [5]. Researchers also disseminate the code and data for their experiments as virtual machines on repositories such as DockerHub.a Open source software practices have enabled the more general open science agenda. Open Science principles af- fect the research life-cycle, in the way science is performed, its results – including software – published, assessed, discovered, and monitored, as will be discussed in the following section. ahttps://hub.docker.com/ 2 Open Science Replicability and reproducibility are the ultimate standards by which scientiﬁc claims are judged [33]. Reproducibility and reuse of research can be improved by increasing transparency of the re- search process and products via an open science culture [31]. Of the variations of open science [12], in this paper, we consider the pragmatic and in- frastructure views. The pragmatic view regards open science as a method to make research and knowledge dissemination more efﬁcient. It thereby considers science and research as a process that can be optimized by, for instance, modularizing the process of knowledge cre- ation, opening the scientiﬁc value chain, including external knowledge and allowing collabora- tion through online tools such as Github, etc. The infrastructure view is concerned with the technical infrastructure that enables emerging research practices on the Internet, for the most part software tools and applications, as well as (high-performance) computing systems. An example of such infrastructures is the envisioned European Open Science Cloud EOSC [1]. Research Software Engineering for Sustainable Research Software These above-mentioned views of open science require software engineering to enable sustain- able research software. We need proper research software engineering for obtaining sustain- able research software. Research Software Engineers (RSE) combine an intimate understand- ing of research with expertise in software engineering.b It is a relatively new role in academia, but a highly popular one that has shown signiﬁcant growth in countries around the world.c The RSE community has displayed a particular interest in adopting and promoting open science, and as such are perfectly placed to help researchers adopt FAIR and open software practices. Many researchers and research software engineers spend signiﬁcant time creating and con- tributing to software, a resource which is currently under-represented in the scholarly record. This creates some problems: 1. Trust in research relies on the peer review system but without ready access to the software used to perform a given experiment or analysis, it is difﬁcult for readers to check a paper’s validity. 2. Lack of access to the software underlying research makes it signiﬁcantly more difﬁcult to build new research results on top of existing ones: it is difﬁcult to ‘stand on the shoulders of giants’. bhttps://rse.ac.uk/who/
chttps://www.software.ac.uk/what-do-we-know-about-rses-results-our-international-surveys 3 3. Promotion and hiring in academic research are highly dependent on building a portfolio of well-cited papers, and researchers whose main work is software development often have fewer research papers published. One of the biggest obstacles to making research software sustainable is ensuring appropriate credit and recognition for researchers who develop and maintain such software. The goal is to address the challenges that researchers face regarding sustainable research software. It is also essential to sustain software by sustaining its communities (researchers, developers, maintainers, managers, active users). Approaches to Software Publishing The scientiﬁc paper is central to the research communication process. Guidelines for authors deﬁne what aspects of the research process should be made available to the community to evaluate, critique, reuse, and extend. Scientists recognize the value of transparency, openness, and reproducibility. However, it remains unclear, how this may be achieved with software. Various journals allow one to add supplementary material to an article, which may include software. Such additional material is usually just put into zip archives, whose content is neither reviewed nor further explained with metadata information or other documentation. Thus, this may fail to promote reuse. More publishing-oriented practices were also explored. For instance, Elsevier conducted the “Executable Paper Grand Challenge” to enhance how scientiﬁc information is used and commu- nicated, addressing both computational science and computer science [14]. Several projects presented their concepts of “Executable Papers” which were published in the corresponding conference proceedings. The example paper [29] from this competition uses literate program- ming to present a Curry program within the paper. Thus, it contains the complete concise source code of their software application, which is directly executable, together with sufﬁcient documentation to be understandable. Research software may also be published in software journals such as JOSSd, JORSe or Software Impactsf but this is rarely adopted in computer science. Some research communities are also building online platforms for sharing research software services. The SoBigData Lab,g for instance, provides a cloud service for data analytics, with dhttps://joss.theoj.org/
ehttps://openresearchsoftware.metajnl.com/ fhttps://www.journals.elsevier.com/software-impacts
ghttp://sobigdata.eu 4 a focus on social mining research. OceanTEA provides an online service for analyzing ocean observation data [22]. The integrated toolchain LabPal for running, processing, and including the results of computer experiments in scientiﬁc publications is presented in [16]. The tool Qresp for curating, discovering and exploring reproducible scientiﬁc papers is presented in [15]. Generic services such as BinderHubh support online execution of reproducible code. Artifact Evaluation as a Review Mechanism The code quality of research software often is a hindrance for reuse. For example, there are typically no tests, documentation is often lacking, and the code does not usually adhere to any coding standards. This is not necessarily caused by the scientist’s bad work, but rather it is the natural result of what scientists are judged on, namely the scientiﬁc quality of the papers they put out, as opposed to the quality of software that enables such papers. To address these issues, several ACM conferences initiated artifact evaluation processes, in which supplementary material is part of the review process [28]. The ACM distinguishes be- tween repeatability (same team, same experimental setup), replicability (different team, same experimental setup), reproducibility (different team, different experimental setup), or reusability (artifacts are carefully documented and well-structured to the extent that reuse and repurposing are facilitated) of research artifacts [4]. Artifacts can be software systems, scripts used to run experiments, input datasets, data col- lected in experiments, or scripts used to analyze results. Artifact evaluation processes help to check their quality via peer review. In some subdisciplines of Computer Science these artifact evaluation processes have been established: Databases (ACM SIGMOD), Software Engineer- ing (ACM SIGSOFT) and Programming Languages (ACM SIGPLAN), see [28]. SIGMOD calls the process reproducibility evaluation and also offers the ‘Most Reproducible Paper Award’. The Super Computing Conference Series introduced a reproducibility initiative with an artifact evaluation process in 2016.i However, some subdisciplines of Computer Science are still dis- cussing whether they should adopt the artifact evaluation process. Such a subdiscipline is Information Retrieval (ACM SIGIR), which started an initiative to implement the ACM artifact review and badging process [13]. Recently, the Empirical Software Engineering journal initiated an open science initiative in- cluding an artifact evaluation process [30]. Once a manuscript gets minor revision, the authors are encouraged to prepare a replication package. When the manuscript gets accepted, the hhttps://binderhub.readthedocs.io/ ihttps://sc18.supercomputing.org/submit/sc-reproducibility-initiative/ 5 authors are invited to submit the replication package for evaluation. Childers and Chrysanthis [7] examine how artifact evaluation has incentivized authors, and whether the process is having a measurable impact. They observe a statistical correlation between successfully evaluated artifacts and higher citation counts of the associated papers. This correlation does not imply a cause-and-effect conclusion, but the hypothesis is that authors who participate in artifact evaluations for whatever reason may have a tendency to be more active and visible in the community. The FAIR Principles for Research Software The FAIR principles are originally intended to make data ﬁndable, accessible, interoperable, and reusable [40]. However, for open science it is essential to publish research software in ad- dition to research data. Extended to research software, the FAIR principles can be summarized as follows: Findable: The ﬁrst step in (re)using data and software is to ﬁnd it. Accessible: Once the user ﬁnds the required data and software, she or he needs to know how to access it, possibly including authentication and authorization if data is involved. Interoperable: The data and software often need to be integrated with other data and software. Reusable: For reusability, metadata, data and software should be well-described such that they can be reused, combined and extended in different settings. Some communities also use source code itself as data. For example, the Mining Software Repositories community analyzes the rich data available in software repositories to uncover interesting information about software systems and projects [17]. Data from GitHub, Stack- overﬂow etc. is harvested into repositories such as GHTorrent to be employed in research [27]. Thus, these principles can also be applied to software, which can be stored and treated as data. However, at present research software is typically not published and archived using the same practices as FAIR data, with a common vocabulary to describe the artifacts with metadata and in a citable way with a persistent identiﬁer. GitHub is not a platform for archival publishing. Zen- odo supports archiving and publishing snapshots from GitHub with persistent DOIs,j however, jhttps://guides.github.com/activities/citable-code/ 6 it remains a great challenge to collect, preserve, and share all the software source code. Re- search software is the result of creative work that can continue to evolve over time. In general, software must be continuously maintained to function. Computer science and software engineering play an important role in the implementation of the FAIR principles, which usually have a focus on helping other disciplines to be FAIR. However, computer science research itself is often also based on software; thus, computer science research software should also consider the FAIR principles. To analyze the current state of research software publication, we conducted an initial study of research software publication and development behavior, as presented in the following section. Relating Research Software to Research Publications To study the relationship between research software and research publications, we conducted an analysis of research software dissemination practices. For our analysis, research software is identiﬁed either by • research publications that cite software repositories or • software repositories that cite research publications. Research software is analyzed in our initial study using a combination of research publication metadata and software repository metadata. Figure 1 illustrates our approach. Assumptions
We have to make some assumptions for analyzing the relationships between research software and research publications. First, we assume that a research publication refers to some GitHub repository for the related research software. Second, we assume that somewhere in a GitHub repository a publication identiﬁer (DOI) is available. We do not assume bi-directional links. We are well aware that these assumptions restrict the coverage of our analysis, but the analysis becomes tractable and repeatable with these assumptions. Analysis Data Set
Over 5,000 Github software repositories have been identiﬁed as research software according to the criteria explained previously: either a research publication referenced the software repository, or the software repository referenced a research publication. This data set is formed from three investigations: (i) 1,204 Github repositories that contain a DOI, (ii) 1,091 Github repositories that are mentioned in publications in the ACM digital library, (iii) 2,872 repositories that are mentioned by e-prints in the arXiv service. In the following section, these will be referred to as the GitHub, ACM and arXiv sets, respectively. 7 <<repository>>
Publication Fulltext <<persistent identifier>>
Software ID <<search result>>
Publication Search Result <<metadata>>
Publication Metadata <<repository>>
Software Code <<persistent identifier>>
Publication ID <<search result>>
Software Search Result <<metadata>>
Software Metadata 0..*
0..* relates to generate search generate search Figure 1: Conceptual model (as UML class diagram) for our approach to relating research soft- ware to research publications. Research publications are searched in their reposi- tories (such as digital libraries). Publication metadata is generated from the search results. The publications and the search results contain the persistent software iden- tiﬁers as reference to the software repository. The generated metadata contains the publication identiﬁer. A similar process is applied to software code (orange classes). Publication and software identiﬁers are related in various ways, as will be discussed below. Covered research areas
An ﬁrst interesting observation is that our three data sets cover quite different research areas: • The GitHub research software set is drawn mainly from the computational sciences, par- ticularly the life sciences (Figure 2a). This is determined by resolving the DOI to obtain publication metadata at datacite.org and classifying the publication venue (e.g. journal or conference) in which the paper appeared. The most popular venues were PLOS One, PLOS Computational Biology, Scientiﬁc Reports, PNAS, Nature, Nature Communications, Neuroimage, Molecular Biology and Evolution, and Science. • The ACM research software set is dominated by software engineering, information sys- tems, social and professional topics and human-centered computing (Figure 2b). This was determined by inspecting the top level of the ACM Computing Classiﬁcation Scheme (CCS) descriptors which were applied to the publications by the authors. • The arXiv research software set is dominated by computer science topics (Figure 2c),k which is mainly composed of AI topics (computer vision, machine learning, computational linguistics, Figure 2d). This was determined by inspecting the thematic “primary category” to which the e-print was submitted (e.g. arXiv categories rcs.AI or hep-th). Thus, these kRemind that we only collect arXiv publications that refer to GitHub repositories. 8 Life Science General Science Unpublished / Archives Earth Science Chemistry Physics Comp Sci Psychology
Engineering Maths Education Social Sci (a) Research areas of publications cited from Github repositories General & 
reference Mathematics 
of computing Information systems Security and privacy Networks Human-centered 
computing Social and 
professional topics Theory of 
computation Computing 
methodologies Applied computing Computer 
systems 
organization Hardware Software and its 
engineering (b) Research areas of ACM computer science publications citing GitHub repositories Computer Science Astrophysics Statistics Quantitative Biology Physics Mathematics QuantumPhysics High Energy Physics - Phenomenolo gy Condensed Matter Electrical Engineering and Systems Science General Relativity and QuantumCosmology High Energy Physics - Theory Others (c) Research areas of arXiv publications citing GitHub repositories Computer Vision and Pattern Recognition Machine Learning (cs) Computation and Language Robotics Information Retrieval Artificial Intelligence Sound Information Theory Social and Information Networks Neural and Evolutionary Computing Data Structures and Algorithms Distributed, Parallel, and Cluster Computing Computers and Society Others (d) Computer science publications in arXiv from Figure 2c reﬁned into sub-areas Figure 2: Research areas of publications with related GitHub repositories. 9 computer science sub-areas at arXiv seem to emphasize a “publish and share as early as possible” attitude, which is encouraged by the review-less publication repository arXiv. Sustainability of research software
For this study, we consider research software as sus- tainable, if it has a greater lifespan and is still live. We consider a repository as live if some activity occurred during the last year, otherwise it is considered dormant. The “lifespan” of a software repository is the length of time between its ﬁrst and last commit activity. To analyze the sustainability of research software, we divide the software repositories between “live” and “dormant” repositories. As presented above, publications cited from GitHub repositories mainly belong to computa- tional science, for which we observe an even split between live and dormant software repos- itories. Publications from the ACM digital library mainly belong to computer science, for their cited software repositories, we also observe an even split between live and dormant software repositories. However, the computer science software repositories lifespan is hugely higher than the computational science software repositories lifespan: • As Figure 3a shows, the computer science software repositories’ lifespan is distributed with a median of 5 years. Our hypothesis is that in computer science research, often commercial open-source soft- ware frameworks are employed. These software frameworks are maintained over long times by employees of the associated companies. • As Figure 3b shows, the computational science software repositories’ lifespan has a dis- tribution with a median lifespan of 15 days. A third of these repositories are live for less than 1 day. Our hypothesis is that in computational science research, often the research software is only published when the corresponding paper has been published. The software is then not further maintained at GitHub, but at some private place as before (if it is further maintained at all). • As Figure 3c shows, the arXiv repositories are somewhere in between with a median of 8 months lifespan. Furthermore, 75% of the arXiv repositories are live. Our hypothesis is that the attitude of publishing as early as possible in parts of the artiﬁcial intelligence community also motivates the researchers to develop their research software openly from the start of research projects. 10 0 20 40 60 80 100 120 140 160 180 200 1
2
3
4
5
6
7
8
9
10
11
12 Respoitories Year Dormant
Active (a) Lifespan of Github repositories cited in ACM computer science publications 0 100 200 300 400 500 600 1
2
3
4
5
6
7
8
9 Repositories Years Dormant
Active (b) Lifespan of Github repositories citing publications via DOIs 0 200 400 600 800 1000 1200 1400 1600 0
1
2
3
4
5
6
7
8
9 Repositories Year Dormant
Active (c) Lifespan of Github repositories cited in arXiv publications Figure 3: Lifespan of software repositories in years. 11 Relationships and categories of research software
In addition to the lifespan, it is interest- ing to take a closer look at the activity of the repositories; such as the number of commits per time unit. Due to limited space in the present paper, we leave a detailed analysis and discussion of the repositories’ activities to future work. However, by manually inspecting the most active of the ACM repositories, we were able to identify particular kinds of relationship between the research publications and the software repositories, and different kinds of research software. We observe different categories and relationships between research publications and research software: • Software as an output of research, collaboratively constructed and maintained through an active open source community. For instance, Caffe is a deep learning framework that has been developed as research software [21]. It has meanwhile been maintained at GitHub for ﬁve years with a large user community and even commercial forks. • Software as an output of research, privately developed but published openly and aban- doned after publication. For instance, the software for the genetics study by Hough et al. [20] has been published at GitHubl in 2014 in parallel with the paper. This repository is now ﬁve years old with a lifespan of one day (all commits on September 5th, 2014). • Software itself as an object of study or analysis. For instance, Costa et al. [9] studied the performance of the Google Guava core libraries for Java. They did not develop or adapt this software. • Software that then leads to a fork (in GitHub) that is independently developed as a re- search output and published openly (if successful, it may be fed back into the original project via GitHub pull requests). For instance, Bosagh Zadeh et al. [6] extended the Apache Spark analytics engine as a GitHub fork in their research and managed to merge their software extensions back into the master software repository. • Software used as a tool or framework to do the research. For instance, O’Donovan et al. [32] used the three.js Javascript 3D library to study 3D modeling approaches. lhttps://github.com/arvidagren/Cytonuclear 12 Besides these relationships, software is cited as related work, background, or example. GitHub repositories are also used to publish data and reference lists to collections of software. Related Work
Collberg & Proebsting [8] studied the extent to which computer systems re- searchers share their code and data. Their focus is on re-building the research software for repeatability and on sharing contracts, not FAIR and open publishing. Russell et al. [35] conducted a large-scale analysis of bioinformatics software on GitHub looking at relationships between code properties, development activity, and their mentioning in bioinformatics articles. Similar to our observations, they observed that certain scientiﬁc topics are associated with more active code development and higher community interest in the repos- itory. Russell et al. [35] focus on bioinformatics research software, while we focus on computer science research software, and the differences in computational science. The Research Software Directory is a content management system for research software that aims to improve the ﬁndability, citability, and reproducibility of the software packages advertised in it, while enabling a qualitative assessment of their impact [37]. This related initiative collects research software, but does not analyze the relationship to research publications. Threats to Validity
As mentioned above, we had to make some assumptions for this initial study. To make our analysis tractable and repeatable, we assume that a research publication refers to some GitHub repository for the related research software or that somewhere in a GitHub repository a publication identiﬁer is available. We are well aware that these assumptions restrict the coverage of our analysis, but even with this limited coverage, we already observed interesting differences in software publication behaviors in different research domains. In our future work, we intend to extend and reﬁne this analysis, for instance to perform a deeper analysis of the repository activities. Research software is not always cited with a link to the GitHub repository. It could also be published, for instance, in Bitbucketm or GitLabn repositories. Alternative citations may refer to papers, manuals or books that introduce the software. Our initial analysis does not cover such additional citation links. To allow for a more comprehensive study of the relationships between research software and research publications, so-called Research Software Observa- tories could provide appropriate citation links and citation graphs, as will be discussed in the following section. mhttps://bitbucket.org
nhttps://gitlab.com 13 Observatories for FAIR and Open Research Software Based on our experience with analyzing the relationships of research software and research publications, we propose the deployment of Research Software Observatories to better sup- port research software retrieval and analysis. Discovery and analysis of data resources have been considered in the conceptualization of web observatories [39] and later in data observa- tories [38]. A data observatory is a catalogue of data resources and of observations (analytic applications) on those resources. Data observatories envisage decentralized, interoperable catalogues of data resources, hosted by different organizations.o Thus, data observatories are distributed, federated collections of datasets and tools for analyzing data, each with their own user community. Decentralization in this sense can provide for agile architectures in ways that centralized, one-size-ﬁts-all solutions cannot. In the context of open science and research software, research software observatories can be considered in three different ways. First, in terms of describing a research software obser- vatory for FAIR and open research software, that will allow scientists to share software and observations on the status of this software, such as those described in this publication and illustrated in Figures 2 and 3. A research software observatory could support open science research and encourage best practice among research communities. Second, one could con- sider the research software used for processing scientiﬁc data and producing observations (analytics) in ways that respect the FAIR and open principles. Third, the opportunities and challenges of cataloging research software with appropriate citation links in observatories can be explored. Research software observatories need to support metadata for research software classiﬁcation and citation to further empower researchers to ﬁnd, access and reuse relevant and interoperable research software. Recommendations to make Computer Science Research Software FAIR and Open Publishing research software in an archival repository is currently not common in all areas of computer science. Our initial study revealed highly varying publication behavior in different scientiﬁc disciplines. Research software is usually managed in GitHub or similar repositories, where it can be maintained and re-used, but not published for scientiﬁc reward and proper ci- tation. An approach to addressing these issues is by enabling and standardizing citation of software. Software citation brings the effort of software development into the current model oReference implementations have already emerged; e.g. https://github.com/webobservatory/ 14 of academic credit, while simultaneously enhancing the scholarly record by linking together software with publications, datasets, methods, and other research objects. Therefore, our rec- ommendations along to the FAIR principles are the following: For ﬁndability, challenges to be addressed for FAIR publication of research software are methods for software citation and software retrieval.
To support ﬁndability, computer science sub-disciplines may adopt approaches that are currently under exploration for research software in general. However, appropriate software metadata remains a great challenge. Authors sometimes want their users to cite something other than the piece of software directly. Examples include citing a paper that introduces the software, a published soft- ware manual or book, a ‘software paper’ (such as JOSS) created speciﬁcally as a citation target, or a benchmarking paper. However, there exists guidelines for software citation and identiﬁcation [36], and already some metadata standards for software citation exist [26]: • The Citation File Format (CFF) is a human- and machine-readable ﬁle format in YAML which provides citation metadata for software [11]. • A CodeMeta instance ﬁle describes the metadata associated with a software object using JSON’s linked data (JSON-LD) notation [3]. The CiteAs.org online service links between software repositories and their requested citations, exploiting the above standards. What is missing, are search engines that exploit this metadata and, more importantly, widespread annotation of research software with citation information. 15 For accessibility, software artifacts should be published with preservation in mind. GitHub, for example, does not directly support the preservation of software “snapshots” which were used to achieve some research results. This may, for instance be achieved via taking a snapshot from GitHub to be archived on Zenodo.org:p • GitHub serves for use, reuse, and active involvement of researchers. • Zenodo serves for archival and reproducibility of published research results. An open question is whether computer science research needs its own discipline-speciﬁc data repository and whether the combination of GitHub and Zenodo is sufﬁcient. The Software Heritage archive could be another option for software preservation [10]. For interoperability, research software engineers should adhere to established software and data standards allowing for interoperable software components [18]. Proper interface deﬁ- nitions in modular software architectures are essential for interoperable research software components. Artifact evaluation processes may support interoperability, if the reviewers take this con- cern into account. For reusability, artifact evaluation processes review replicability and reproducibility and, if suc- cessful, reusability of research software. This way, the reusability of research software may be improved signiﬁcantly. Software virtualization techniques such as Docker containers and online services help to support portability, and thus reusability across platforms. It may be useful to distinguish between Software-as-Code (e.g., via GitHub) and Software-as-a-Service (e.g., via some online cloud service on which the software is executed, such as BinderHub). From a software engineering point of view, modular software architectures allow for reusing parts of research software systems [19]. So far, many research software systems are not structured in a modular architecture, what should be improved in the future. Domain- speciﬁc languages may also help with the comprehensibility of research software [23]. It is vital for reusability to follow good engineering practices to ensure that the software can be built on by others [8]. Adequate documentation is important, but so are engineering practices such as providing testing frameworks and test data for continuous integration to ensure that future adaptations can be tested to ensure that they work correctly. phttps://guides.github.com/activities/citable-code/ 16 Summary Compared to research data, research software should be both archived for reproducibility and actively maintained for reusability. The combination of Zenodo (for archival and reproducibility) and GitHub (for maintenance and reuse) may be used to achieve this. Furthermore, research software should be open source software. Established open source software licenses [2] pro- vide adequate licensing options such that there is no need to keep research software closed. In the vast majority of cases some existing license will be appropriate. For research data this is different. Research data may, for instance, be subject to privacy regulations. Thus, the FAIR data principles do not require openness, but accessibility that might include authentication and authorization. However, for research software, openness is to be expected [25]. Only in excep- tional cases and for very good reasons should research software be closed. Reproducibility and reusability are essential for good scientiﬁc practice. Future work should address the deﬁnition and establishment of appropriate metadata for citing both software code and software services. Such metadata could make research software also better searchable and discoverable. Research software observatories may provide such services for software retrieval and analysis. Modularity is essential for maintainability, scalability and agility, but also for reusability. We suggest to further establish the concept of artifact evaluation to ensure the quality of published artifacts for better reusability. Proper research software engineering enables reproducibility and reusability of research soft- ware in computational science. However, software engineering should also help software en- gineering and computer science research itself to support replicability and reproducibility of research software that is used in computer science experiments. This way, we may achieve FAIR and open computer science and software engineering research.",0
"The authors analyze the link between standardization and economic growth by systematically reviewing 
leading economics journals, leading economic growth researchers’ articles, and economic growth-
related books. They make the following observations: 1) No article has analyzed the link between 
standardization and economic growth in top 5 economics journals between 1996 and 2018. 2) A 
representative sample of the leading researchers of economic growth has allocated little attention to 
the link between standardization and economic growth. 3) Typically, economic growth textbooks do 
not contain “standards” or “standardization” in their word indices. These findings suggest that the 
economic growth theory has neglected the role of standardization. Keywords Allocation of Attention, Bibliometric Analysis, Economic Growth, Standardization 1 INTRODUCTION “The lack of cooperative standardization in British industry is conspicuous in regard to locomotives. 
Every considerable railway has its own models, though the materials are to some extent standardized” Alfred Marshall (1919, p.591) “Standardisation and connection standards may seem purely technical details to the casual observer, 
but in fact they reflect the importance of achieving economies of scale.” Nathan Rosenberg (1983, p.183) “Perhaps because these standards are so taken for granted, they are rarely the subject of discussion in 
circles beyond those in which they are formulated. They are even more rarely the subject of discussion 
in the public square in democratic institutions of government, or among friends. Indeed, standards 
are so taken for granted, so mundane, so ubiquitous, that they are extremely difficult to write about. 
They are usually noticed only when they fail to work.” International Journal of Standardization Research
Volume 19 • Issue 1 2 Lawrence Busch (2011, p.2)
Standards can be defined as rules, guidelines, or characteristics established by consensus and 
approved by a recognized body (see ISO/IEC, 2004). According to ISO/IEC (2004), standardization 
is “the process of development and application of standards” (see Choi et al., 2011). Standards are 
ubiquitous, and every one of us is exposed to several standards every day (Kindleberger, 1983; Busch, 
2011). Consider, for instance, the measurement of time, metric systems, various safety standards, 
electricity standards, including plugs and sockets, data, image, video and audio compression 
technologies (codecs), Internet protocols, connectivity of devices via cellular networks, Wi-Fi or 
Bluetooth, etc. Standards have obvious public good characteristics (Kindleberger, 1983; David & 
Greenstein, 1990; Swann, 2000; Blind & Jungmittag, 2008) and, generally, the promotion of standards 
is considered beneficial, as reflected, for instance, by the increasing number of national and voluntary 
standards organizations and their expressed missions. For example, the International Organization for 
Standardization (ISO) has more than 164 national standards organizations as members that promote 
standardization nationally.1 It has been documented that societies underinvest in R&D (Jones & Williams, 2000; Lucking 
et al., 2018). Much less empirical evidence exists on whether societies under- or overinvest in 
standardization. According to Rysman and Simcoe (2009, p.1932): “the importance of SSOs has 
been widely discussed, yet there have been no attempts to systematically measure the effects of these 
institutions.” Standards can be national, international, or global by their geographical dimension 
(Swann et al., 1996; Nadvi, 2008; Blind et al., 2018). Scale and network effects are typically greater 
the more international the scope of a standard is.2 Standards have played an indispensable role, 
for instance, in creating and maintaining the proper functioning of the European Single Market 
(Pelkmans, 1987; David & Steinmueller, 1994; EC, 2018; Blind et al., 2018). Economists agree that 
institutions matter for economic growth (North, 1991; Mokyr, 2002; Acemoglu & Robinson, 2012). 
Blind and Jungmittag (2008) noted that “standards can also be interpreted as institutions. Institutional 
economists postulate a close relationship between institutional development and economic growth.” 
This is also an important premise of the current article: Standards are important institutions that 
matter for technological progress, innovation, and, therefore, for economic growth and development.
While there exists a variety of different types of standards, we focus here on standards that are 
a result of open and voluntary standard development or setting and are related to technologies. We 
also note that the dimension of feedback processes classifies standardization organizations. Their 
operational mode is either one-shot standard setting or dynamic standard development (Teece, 
2018).3 Economists share the belief that innovations and technological progress are the key drivers of 
economic growth in the long run (Aghion & Howitt, 2009) and, presumably, technology standardization 
impacts the rate and direction of technological change.4 While researchers of network economics and 
industrial organization economists have extensively studied standardization (e.g., Farrell & Saloner, 
1985; Katz & Shapiro, 1986) and the role of patents in standard development (e.g., Lerner & Tirole, 
2015), it appears that economic growth theory is almost silent about the macroeconomic impacts of 
standardization (e.g., Blind & Jungmittag, 2008; Swann, 2010; Baron & Schmidt, 2017). Consequently, 
we know much more about the microdynamics than the macrodynamics of standardization.
It is interesting that the macroeconomic impacts of technology standardization have received little 
attention among economists, particularly as standardization organizations have existed for more than 
a century.5 Examples of standards that have had a substantial global impact include, among others, 
freight container standards (ISO/TC 104 Freight containers; Levinson, 2006; Bernhofen et al., 2016), 
Internet standards (IETF, W3C, Simcoe, 2015) and telecommunication standards (ETSI, ITU, Röller 
& Waverman, 2001; Teece, 2018). These standards have significantly promoted globalization and 
technological change. The heterogeneous nature of different standards and standardization processes 
makes it challenging to analyze the aggregate macroeconomic impacts of standardization. Presumably, 
this is a major factor explaining the dearth of research on the topic. The goal of this article is to 
shed more light on this research gap and the link between standardization and economic growth. We International Journal of Standardization Research
Volume 19 • Issue 1 3 contribute to the existing literature by providing a systematic bibliometric analysis on this link and 
by reviewing the role of technology standardization in leading economics journals and particularly 
in economic growth theory.
The rest of the paper is structured as follows. Section 2 discusses the theoretical and empirical 
links between standardization and economic growth. Section 3 describes the theoretical framework. 
Methods and data are presented in Section 4, and the findings are report in Section 5. Section 6 
concludes the paper. 2 STANDARDIZATION, TECHNOLOGICAL 
PROGRESS, AND ECONOMIC GROWTH 2.1 Theoretical Link Between Standardization and Economic Growth The leading researchers of economic growth have analyzed a variety of factors that are associated 
with economic growth.6 Economists and economic historians share the belief that technological 
progress is the key driver of economic growth in the long run (Solow, 1956; Mokyr, 2002; Aghion 
& Howitt, 2009). However, often, economists do not dig deeper into the details, or “the black box” 
(Rosenberg, 1983), of technological progress. Standardization has been discussed and analyzed for 
more than a century by economists, including Thorstein Veblen (1904) and Alfred Marshall (1919).7 
As early as 1919, Marshall discussed standardization extensively in his 1919 book, “Industry and 
Trade,” and emphasized the important role that “multiform standardization” has had in promoting 
mass production in the U.S. (Langlois, 2001).8 Yet, the paradigm of economic growth theory seems to 
have neglected the role of standardization in technological progress and in our increasing wellbeing.
To a large extent, the rate and direction of technological change are determined by the allocation 
of R&D investments, and there are several institutions that determine the incentives to invest in R&D 
(Arrow, 1962; Scotchmer, 2004). As mentioned, the role of institutions as determinants of economic 
growth has received increasing attention (North, 1990; Acemoglu & Robinson, 2012). According 
to North (1991), “institutions are the humanly devised constraints that structure political, economic 
and social interaction.” By this definition, technical standards are also “institutions” (cf. Blind & 
Jungmittag, 2008; Featherston et al., 2016; Maze, 2017). The key message of new institutional 
economics is that “institutions matter” for economic growth. From the perspective of technological 
progress, the institutions that promote the creation and diffusion of knowledge, i.e., “efficient 
functioning of the knowledge infrastructure” (cf. Edquist, 1997), are the key factors. Standards are 
an important institution that can promote the diffusion of technologies (Blind & Jungmittag, 2008) 
and foster competition (Koski & Kretschmer, 2005). Standards promote interoperability, adoption 
of technologies, and network effects (Matutes & Regibeau, 1996; Shapiro & Varian, 1999; Swann, 
2000). Thus, standards also matter for economic growth. First generation endogenous technological 
change models (Romer, 1990; Grossman & Helpman, 1991; Aghion & Howitt, 1992) added realism 
to earlier models of economic growth by treating innovation processes as endogenous. Similarly, 
standardization is an endogenous process that shapes technological progress.
R&D investments create positive externalities, and important progress has been made in the 
analysis of these knowledge spillovers (Lucking et al., 2018). Still, the quantification of externalities 
remains a great challenge to economists (Jones, 2016). For instance, Jones (2016) notes that there 
is a need for economic growth researchers to learn more about “the extent of knowledge spillovers 
across countries” as “each country benefits from knowledge created elsewhere in the world.” 
Similarly, spillovers from standardization are not easily quantifiable. According to Leiponen (2008): 
“Opportunities to learn and accumulate social and political capabilities are thus of the essence in the 
creation of new standards. Firms are advised to engage in a broad cooperative approach if they wish 
to influence the evolution of standards.” On the other hand, Blind and Mangelsdorf (2016) note: 
“Significant knowledge flows are apparent within standardization processes, especially from larger 
to the smaller German companies opposite of that seen in other types of strategic alliances. SDOs International Journal of Standardization Research
Volume 19 • Issue 1 4 are – as shown in Sherif (2015) for Chinese companies – interactive learning spaces. Consequently, 
companies’ knowledge management, including their open innovation strategies, must take these 
opportunities into account when considering entrance into standardization.”
A particular type of technology that creates large amounts of positive externalities is a “general 
purpose technology” (GPT). GPTs include electricity, steam, semiconductors, and the Internet 
(Bresnahan & Trajtenberg, 1995; Simcoe, 2015, p.16).9 From the economic perspective, standards 
that promote interoperability are also particularly important (cf. Simcoe, 2015), as interoperability 
promotes efficiency. Standardization of technologies is closely linked to the development of general 
purpose technologies (Simcoe, 2015, p.26).10 Standardized technologies, such as cellular connectivity, 
can be viewed as a general purpose technology (Teece, 2018) that enables downstream technological 
trajectories (Dosi, 1982)11 in multiple industries (Kim et al., 2017). Standards are a heterogeneous 
set (Tassey, 2000; Simcoe et al., 2009; Wiegmann et al., 2017; Teece, 2018; Baron & Spulber, 2018; 
Baron et al., 2019), and some “upstream” standards enable a larger variety of downstream products and 
innovations. Swann (2000) notes: “The ultimate measure of how a standards infrastructure contributes 
to the economy is the sum of additional innovative products and services (and any attendant cost 
reductions) that grow on the back of that standards infrastructure.”
Endogenous models of economic growth have incorporated competition and innovation into the 
analysis of economic growth, and researchers have derived stylized facts and predictions that can be 
explained by these dynamics (Aghion et al., 2015). However, coordination in innovation activity and 
how this affects the sequence of knowledge accumulation, i.e., direction of innovation, has thus far 
received less attention. The patent system in itself is a decentralized coordination mechanism that 
allocates the attention and investments of profit-maximizing agents to research projects that have the 
highest expected returns (Scotchmer, 2004). The public sector can also affect the rate and direction 
of technological change by using other “innovation policy instruments” (Takalo, 2013) or “policy 
toolkits” (Bloom et al., 2019), including, for example, R&D subsidies, R&D tax exemptions, and 
IPR systems.
Economists highlight the role of the combinatorial growth of ideas and related increasing returns 
(Romer, 1993; Weitzman, 1998; Jones, 2005). However, ideas-based growth cannot be only about 
increasing the variety of ideas because human attention and resources are limited. In a world where 
attention and resources are scarce, there needs to be some mechanisms that define which ideas to 
pursue and, also, in which sequence. The allocation of R&D investments crucially impacts the rate 
and direction of technological change and technological trajectories. Economic growth researchers 
often focus on innovation incentives but neglect incentives to create compatible and interoperable 
products in the value chain.
The abovementioned institutions all affect the rate and direction of technological change, but 
they rely mainly on competition of ideas and products in the market. Standardization, on the other 
hand, is based on a balance between competition and collaboration (i.e., “coopetition”) where the 
aim is to achieve consensus (Schmidt & Werle 1998; Egyedi, 2000; Simcoe, 2015). Standardization 
can be understood as a coordination mechanism (Maze, 2017; Wiegmann et al., 2017) by which 
economic actors can collaboratively decide the technical specifications for products, etc. (Simcoe, 
2015; Wiegmann et al., 2017) based on specific criteria and through either consensus or majority vote 
(Baron et al., 2019).12 The focus is not on increasing the variety of ideas but, in contrast, reducing the 
number of ideas and amount of variety. Standardization is, in essence, variety reduction (Farrell & 
Saloner, 1985; Tassey, 2000). This process leads to focused technological trajectories with reduced 
market uncertainty (Gaynor, 2001). Sometimes, there are also competing standards (Wiegmann et 
al., 2017) that can co-exist and compete on the market. It is not hard to see why increasing variety 
of incompatible or non-interoperable product interfaces is anything but welfare increasing. In the 
standardization process, decision makers collectively select technical solutions on the basis of a 
consensus, and there is thus no need for all of the competing standards to enter the market when 
competition and selection of technical solutions occur within standardization process instead. International Journal of Standardization Research
Volume 19 • Issue 1 5 Standards can be concurrently defined as either knowledge, ideas, meta-ideas, technology, 
recipes, or institutions. Standardization is a process in which the technical specifications of certain 
aspects of technologies are codified, and thus the standard is codified knowledge. “Idea” is a very 
broad concept that forms the basic unit of “ideas-based growth” (Romer, 1993; Jones, 2005) and 
can be defined as “instructions or recipes, things that can be codified in a bitstring as a sequence of 
ones and zeros” (Jones, 2005). According to Romer (1993), “Perhaps the most important ideas of 
all are meta-ideas—ideas about how to support the production and transmission of other ideas.”13 
Standards fit this definition. As already mentioned, standards are also institutions, i.e., rules of the 
game, as they constrain future technological development by reducing variety (cf. North, 1991). 
Moreover, the standardization process is a meta-idea or institution, as it is an idea about how to 
produce efficiently better ideas. Standards are technologies or recipes since they specify how one 
gets a specific output from a set of inputs (Romer, 1990). In addition, standards can be regarded as 
“technological trajectories” (Dosi, 1982, 1988; Cozzi, 1997; Kim et al., 2017). By reducing the variety 
of technologies, standardization promotes the more efficient allocation of resources as economic 
agents abandon some technological trajectories.
According to Grossman and Helpman (1994), “profit-seeking investments in knowledge play 
a critical role in the long-run growth process.” Knowledge, technology, and innovations are not 
“manna from heaven” (cf. Audretsch, 2007). Similarly, standards are not manna from heaven. In 
Romer’s (1990) endogenous growth model, growth “is driven by technological change that arises 
from intentional investment decisions made by profit-maximizing agents.” Economic agents require 
appropriate incentives to invest in standardization activities and adopt standardized technologies. 
Thus, the observation that companies participate in technology standardization and adopt standardized 
technologies indicates that companies consider standardization to have positive expected returns. 2.2 Empirical Link Between Standardization and Economic Growth The importance of standards has increased over the past decades (Lerner & Tirole, 2015), particularly 
in the ICT sector (Shapiro & Varian, 1999; Baron et al., 2019). According to Baron et al. (2019), 
standardization has long been recognized as playing an important role in technological innovation, the 
diffusion of new technologies, and economic growth. Therefore, it is expected that standards’ impact 
on the rate and direction of technological progress and economic growth has similarly increased. 
Standards are a heterogenous set, as are standard development organizations (SDOs, Wiegmann et 
al., 2017; Teece, 2018; Baron & Spulber, 2018; Baron et al., 2019). Here, we briefly review whether 
there is any empirical link between standardization and innovation and technological change.
Schumpeterian growth theory (Aghion et al., 2015) predicts that incumbents do the most R&D. 
Incumbents also do the most standards development, according to empirical evidence (Larouche & 
Schuett, 2019). Existing empirical evidence on the impact of standards is scant (Blind & Jungmittag, 
2008; Rysman & Simcoe, 2008; Baron & Schmidt, 2017). The typical challenge is the lack of data 
on counterfactual worlds. However, empirical evidence suggests that standardization has important 
impacts in multiple fields.14 Whereas a large share of the standardization literature has discussed the possibility that standards 
may lead to a lock-in to inferior technologies (David, 1985; Maze, 2017), empirical evidence suggests 
that SDOs perform well in selecting important technologies (Rysman & Simcoe, 2008).15 Kim et al. 
(2017) find that standards are a driving force of technological convergence. Interestingly, it seems 
that in the economics literature, the potential negative impacts of standards seem to be sometimes 
more frequently cited than the benefits of standards. The anecdote of the QWERTY keyboard as 
an inferior de facto standard is a very popular example used to illustrate how path-dependence and 
standardization can lead to a lock-in to an inefficient equilibrium outcome (David, 1985).
Our understanding of the impacts of standards has expanded over time. While David (1987) 
identified three different purposes of the standards – (1) compatibility or interoperability, (2) minimum 
quality or safety, and (3) variety reduction - Swann (2010) listed eight: (1) variety reduction, (2) quality International Journal of Standardization Research
Volume 19 • Issue 1 6 and performance, (3) measurement, (4) codified knowledge, (5) compatibility, (6) vision, (7) health 
and safety, and (8) environmental (see also Swann, 2015). For more extensive reviews of the impacts 
of standards, see Swann (2000, 2010). Network infrastructures particularly require compatibility 
standards (Farrell & Saloner, 1985). Telecommunication and broadband infrastructures have been 
found to positively affect economic growth (Röller & Waverman, 2001; Czernich et al., 2011), and 
telecommunication is an archetypal example of a network industry in which standards have been and 
continue to be subject to extensive IO analysis (Leiponen, 2008).
An important aspect of standards and standardization is geography. Standards are often classified 
as national, international, or global (Swann et al., 1996; Nadvi, 2008). The geographical dimension 
affects particularly strongly the scale effects and, in the case of compatibility and interoperability 
standards, the network effects of standardization. Researchers of economic growth have allocated a 
significant amount of their attention to country comparisons and cross-country knowledge spillovers. 
International trade promotes the efficient division of labor, fosters the diffusion of ideas, and causes 
economic growth (Frankel & Romer, 1999). Empirical evidence suggests that certain standards 
promote trade (Swann et al., 1996; Levinson, 2006; Bernhofen et al., 2016). Standardization increases 
the size of the markets and promotes economies of scale by enabling compatibility. When standards are 
national and differ across countries, domestic companies may face entry barriers to foreign markets due 
to incompatible standards. International standardization promotes, therefore, international competition, 
and competition puts pressure on companies to innovate. Table 1 summarizes the economic impacts 
of standards discussed in prior literature.
To summarize, standards are ubiquitous and their economic impact is mainly positive according to 
existing empirical studies. The extensive use, support, and development of standards that we observe 
in practice signal that standards are welfare increasing and economic growth promoting. The research 
question of this article is, thus, what is the role of technology standardization in leading economics 
journals and particularly in economic growth theory? Do leading economic growth researchers 
acknowledge the link between standardization and economic growth? Do books on economic growth 
discuss standardization? In the next section, we provide a theoretical framework for analyzing these 
questions. 3 ALLOCATION OF ATTENTION IN SCIENTIFIC RESEARCH AND EDUCATION Why do certain research topics receive significant attention while others receive negligible attention 
or no attention at all? Which factors determine the set of research trajectories that we observe, and 
which factors determine the division of labor across scientists and the distribution of attention over the 
(in)finite set of research topics? How does the famous “invisible hand” generate, via a decentralized 
process, an equilibrium of supply and demand of scientific knowledge and enable self-interested 
researchers to produce scientific knowledge that meets the needs of society?16 In this section, we 
present a simple framework to analyze the allocation of attention among researchers.17 We move from 
general scientific progress to the specificities of economics. 3.1 General Framework Science is a social institution, and researchers who produce scientific knowledge are, of course, 
also human beings possessing human limitations, interests, and intrinsic and extrinsic motives18 
(Goldman & Shaked, 1991; Leonard, 2002). The attention of scientists is a scarce resource and a 
coordination device in the production of scientific knowledge and in the development of new ideas 
(Klamer & van Dalen, 2001; Simcoe & Waguespack, 2011). Hence, the allocation or distribution of 
attention is typically skewed (Klamer & van Dalen, 2001): Some pieces of knowledge attract more 
cumulative attention than others. Researchers are necessarily boundedly rational decision makers, as 
their time and capacity to review all the existing scientific literature are limited. As a consequence, International Journal of Standardization Research
Volume 19 • Issue 1 7 we understand more about those structures and behaviors of the physical world to which researchers 
allocate their attention.
According to Klamer and van Dalen (2001), researchers tend to cluster with likeminded 
researchers. There exists empirical evidence that status helps draw critical attention to a new idea 
(Merton, 1968; Simcoe & Waguespack, 2011). This “Matthew effect” may lead to herding and the 
path-dependent accumulation of scientific knowledge (Merton, 1968). There are also pecuniary 
rewards to received attention, and studies report a positive association between citations and salary 
(see Hamermesh, 2018, Table 8 for a summary).
Technological progress and standard development, as well as scientific progress and allocation of 
attention among researchers, are endogenous processes - that is, they are defined “within the system.” 
The dynamics of these processes could be analyzed using rational choice theory, as would any other 
decision-making situation in which an optimizing decision maker with certain preferences makes 
choices (cf. Diamond, 1988; Brock & Durlauf, 1999). Suppose there is a finite set of researchers, an 
infinite set of research topics, and finite time. Researchers aim to produce scientific knowledge and 
compete for priority (Dasgupta & David, 1994; Strevens, 2013). Researchers’ “return” would thus 
be conditional on being the first to create new ideas and new research results. Thus, the optimization 
problem faced by a researcher at a specific point in time could be characterized as a choice of 
distributing attention and research effort over a menu of research topics that maximizes expected utility 
over the researcher’s career given her or his preferences, existing scientific knowledge, institutions, 
and beliefs over the choices of other researchers.
Irrespective of the exact form of researchers’ heterogenous objective functions, preferences, 
and incentive systems, it is clear that researchers face tradeoffs and must make choices about how to 
allocate their scarce attention during the finite time that they have. The limited attention of academic 
researchers is necessarily focused on a specific set of topics during the finite research career.19 
Researchers must prioritize given their preferences over the set of possible research topics and also 
concurrently take into account and have beliefs and second-order beliefs about the choices of other Table 1. Economic impacts of standards, examples Standards by 
purpose
Potential positive impacts
Potential negative impacts Compatibility/
Interface Network externalities
Lock-in in old technologies if strong network 
externalities Avoids lock-in in old technologies
Risk of monopolization Increases variety of system products Efficiency in supply chains Minimum Quality/
Safety Avoids adverse selection (Greshman´s Law)
Risk of regulatory capture; Raising rivals’ costs Creates trust 
Barriers to entry Reduces transaction costs Variety Reducing 
standards Economics of scale 
Reduces choice Focus and critical mass in emerging 
technologies and industries
Risk of premature selection of technologies Reduces transaction costs Information/
Measurement Facilitates trade Provides codified knowledge Reduces transaction costs Notes: Summary based on Swann (2000, 2010) and Blind (2013). International Journal of Standardization Research
Volume 19 • Issue 1 8 competing researchers. The outcome of these correlated research topic choices and allocation of 
attention is a set of research paradigms and trajectories that we observe. In other words, the observed 
research outputs reveal researchers’ preferences and beliefs about what they have considered important 
and worth researching.20 These choices may have path-dependent consequences on the careers of 
researchers (cf. Jensen, 2013).
Weitzman (1998) notes that several authors have argued that invention or discovery in any 
sector takes place by combining ideas. Since science is cumulative by nature, there is a tendency of 
path-dependence and lock-in on specific topics, theories, and/or methodological choices (Akerlof, 
2019). Researchers stand on the shoulders of giants and build their studies on prior existing studies 
and research trajectories (Kuhn, 1962; Furman & Stern, 2006). Kuhn (1962) highlighted the role of 
books in establishing research paradigms. He writes that “textbooks expound the body of accepted 
theory, illustrate many or all of its successful applications, and compare these applications with 
exemplary observations and experiments”, and that “from textbooks each new scientific generation 
learns to practice its trade” (p.10). The content of textbooks and existing research articles have an 
impact on the framing of certain topics. Figure 1 illustrates that researchers stand on the shoulders 
of giants and that the existing knowledge stock defines the research topics and syllabi of courses 
based on prior research.
Notes: Authors’ illustration.
Systematic reviews and bibliometric analyses can reveal research gaps or under-researched topics. 
Romer (1990) describes ideas as recipes that can be combined, and Weitzman (1998) formalized an 
idea-based growth model by “introducing a production function for new knowledge that depends 
on new recombinations of old knowledge” (see also Olsson, 2000, 2005). Research gaps can be 
understood as combinations of ideas that have not yet been investigated in the existing literature. 
Researchers have not even considered these idea combinations, or they have considered them but have 
instead allocated their limited attention to different, more promising topics or combinations of ideas. Figure 1. Allocation of attention or division of cognitive labor of researchers International Journal of Standardization Research
Volume 19 • Issue 1 9 3.2 Economics Framework Which factors determine the set of research trajectories that we observe in the field of economics? 
How do researchers choose their research topics in the field of economics? Which research articles 
are accepted by editors to be published in leading economics journals? The sociology of economics 
research that focuses on these types of questions has a long tradition (e.g., Samuelson, 1962; Stigler 
& Friedman, 1975; Colander, 1989; Coupé, 2004; Hamermesh, 2018).
Empirical evidence indicates that publications and citations matter for labor market outcomes and 
for the salaries of economists (Coupé, 2004; Hamermesh, 2018). Therefore, economists as economic 
agents may focus on topics that fit well into the orthodox views of the research community and 
are therefore easier to publish in journals (cf. Brock & Durlauf, 1999; Akerlof, 2019; Heckman & 
Moktan, 2019). There can be a conflict between societal preferences and the individual preferences 
of economics researchers in the allocation of researchers’ attention (i.e., allocative inefficiency). Even 
if the society might benefit from increased useful knowledge in one topic, economics researchers 
might still choose to focus on another topic.
Moreover, it has been documented that most novel and innovative ideas may face harsh resistance 
before they are accepted (Gans & Shephard, 1994), and there exists evidence of bias against novelty 
in science (Wang et al., 2017; Akerlof, 2019). For instance, W. Brian Arthur’s seminal article (1989) 
was published only after multiple rejections by leading economics journals over a six-year period 
(Gans & Shephard, 1994). Sometimes, important new ideas are only discovered years after they are 
published.21 Hodgson and Rothman (1999), among others, have documented the dominance of a few U.S. 
institutions in published journal articles and journals’ editorial boards. They raise the concern that 
such “institutional and geographical concentration of editors and authors may be unhealthy for 
innovative research in economics.” Similarly, Coupé (2003) reports that U.S. universities and American 
economists dominated in the production of economics literature during 1990-2000, although the 
extent of their dominance did decrease over this period. Drèze and Estevan (2007, see Table 11) 
provide further evidence of this U.S.-oriented concentration by reporting that authors based in U.S. 
institutions dominate publishing in top economics journals. According to Frey and Eichenberger 
(1993), American economists tend to specialize in theory but neglect local institutions, whereas 
European economists are theoretically broad and institutionally specialized. Institutional differences 
between U.S. and Europe can play a role in the context of standardization research.
Presumably, researchers who focus on the theory of economic growth are no different from 
other researchers. Also, they build their new ideas and theories by combining ideas generated by past 
economic growth theory researchers. In the context of economic growth theory (and economics more 
generally), recipients of the Nobel Memorial Prize in Economics, Robert Solow and Paul Romer, 
among others, could be described as “giants” upon whose shoulders current and future growth 
theorists stand. The high number of forward citations received by their research articles is a clear 
indication of this (see Table 4).
As the curricula of economics courses themselves comprise an institution that directs the attention 
of economics students, it is important to analyze their content. A conscious or unconscious choice to 
leave some important topics, such as the economic impact of standards, out of economics curricula is 
likely to have certain outcomes. Students probably less often write their theses about those topics that 
are not discussed in their courses. The process is endogenous and incremental: Economics students 
become experts mainly in the fields in which they receive their education.
To our knowledge, there are no bibliometric analyses on how economists have allocated their 
attention to standardization and particularly on the role of standardization in the context of economic 
growth. Narayanan and Chen (2012) summarized the primary research streams and key arguments 
of technology standards research but did not focus on the perspectives of economics literature. They 
concluded that “the greatest opportunity lies in integrative works that will take us one step closer to 
a comprehensive view of technology standards.” Choi et al. (2011) documented, using a bibliometric International Journal of Standardization Research
Volume 19 • Issue 1 10 analysis, that standardization and innovation research has continuously increased over time: In 1995, 
there were 13 published articles focusing on these topics; in 2008, there were 68 articles on these topics 
(altogether, 528 articles in Web of Science). Choi et al. (2011) identified six subject-group domains 
of management, economics, environment, chemistry, computer science, and telecommunications, and 
suggested that future studies could more deeply analyze the details of these subject-group domains. 
This article focuses on the economics domain and seeks to offer one integrative view by analyzing 
the link between standardization and economic growth. 4 METHODOLOGY AND DATA In order to analyze the attention allocated to the link between standardization and economic growth 
in the economics literature, we conduct a bibliometric analysis that uses a variety of different 
search techniques and methods. The analysis is purely descriptive and comprises three sections: 
(1) Standardization in leading economics journals, (2) Standardization in the peer-reviewed articles 
of leading researchers of economic growth, and (3) Standardization in a set of economic growth-
related books. Scientific articles and books comprise the core of the knowledge base that impacts the 
allocation of attention by future researchers and teachers (see Figure 1). In the following sections, 
we transparently explain the data-gathering process in detail so that other researchers can replicate 
the analyses in the future. 4.1 Articles in Leading Economics Journals Leading academic journals in economics enjoy authoritative positions. Following prior studies 
(e.g., Kalaitzidakis et al., 2003; Heckman & Moktan, 2018; Hamermesh 2013, 2018), we focus on 
the so-called “top5” economics journals: American Economic Review (AER), Quarterly Journal 
of Economics (QJE), Econometrica, Journal of Political Economy (JPE), and Review of Economic 
Studies (RES). It is justifiable to say that they form “the core” of the scientific knowledge stock in 
economics. The majority of the most cited economics papers have been published in these journals.22 
Heckman and Moktan (2018) report that the top5 publications “have a powerful influence on tenure 
decisions and rates of transition to tenure,” and that the “pursuit of T5 publications has become the 
obsession of the next generation of economists.” They also show, using a survey, that the perceptions 
of young economists are consistent with this view.
Furthermore, several of the most important articles in the field of economic growth have been 
published in top5 journals. These include Romer (1990), published in JPE; Solow (1956), published in 
QJE; Grossman and Helpman (1991) and Acemoglu (2002), published in REStud; Aghion & Howitt 
(1992), published in Econometrica; and Kuznets (1955) and Nelson and Phelps (1966), published 
in AER. Of these authors Kuznets, Solow, Phelps, and Romer received Nobel Memorial Prizes in 
Economic Sciences, in 1971, 1987, 2006, and 2018, respectively.
As we focus on analyzing the association between standardization and economic growth, we 
add the Journal of Economic Growth to complement top5 journals in the sample of economics 
journals. The Journal of Economic Growth is the leading special journal in the field. Here, the unit 
of observation is an article in the sample of the mentioned leading economics journals. The search 
query that we apply in the Scopus database is ALL(“standardization” OR “standardisation”) AND 
ALL(technology OR technologies OR technological OR technical) AND ALL(“economic growth”). 4.2 Articles by Leading Researchers of Economic Growth Leading researchers of economic growth are the key decision makers who make important choices 
about how the paradigms and trajectories of economic growth research evolve over time (cf. Figure 
1). Their allocation of attention directs, in a path-dependent manner, the attention of other researchers.
We analyze whether leading researchers of economic growth have studied the link between 
standardization and economic growth. We acknowledge that there are various alternative ways to International Journal of Standardization Research
Volume 19 • Issue 1 11 define the leading researchers of economic growth. We limited the sample to editors of Journal of 
Economic Growth (as of June 2019), which is one of the leading journals in the field, and which has 
been published since 1996. Its editorial board includes Paul Romer and Paul Krugman, both recipients 
of the Nobel Memorial Prize in Economics. In addition, most of the editors have written articles to 
the “Handbook of Economic Growth,” which was also edited by Philippe Aghion and Steven Durlauf, 
editors of the Journal of Economic Growth. There were 34 editors as of June 2019, and Scopus found 
2212 documents (incl. articles and books) published by them as of 14 August 2019. The number of 
documents is biased downward as the Scopus database does not include all older articles (pre-1996). 
In order to identify documents that focus on standardization and economic growth, we conducted 
keyword searches in the Scopus database for each author’s documents separately using the following 
search query: ALL(“standardization” OR “standardisation”) AND ALL(technology OR technologies 
OR technological OR technical) AND ALL(“economic growth”). Each author had on average 65 
publications (median: 53) in the Scopus database. Thus, the unit of observation here is an article 
written by an editor of the Journal of Economic Growth. 4.3 Books Related to Economic Growth Although books are not the major vehicle of scholarly communication in economics (Hamermesh, 
2018), they are still often used in teaching as textbooks. Presumably, books on economic growth are 
an important knowledge source for students of economic growth (cf. Figure 1) since they are often 
used as coursebooks and enter the syllabi of university courses that focus on economic growth theory.23 
Books may therefore frame the thinking of future economic growth researchers (cf. Kuhn, 1962).
We limit our attention to books published by editors of the Journal of Economic Growth. We 
inquired whether “standardization” or “standards” occurred in their indexes. The unit of observation is 
an index of an economic-growth-related book written by an editor of the Journal of Economic Growth. 5 FINDINGS 5.1 Top Economics Journals Table 2 reports the numbers of articles published in the leading economics journals that are captured 
using the specific search terms. The table indicates that there are only a few articles published in 
top5 journals related to standardization and economic growth. Table 3 lists the articles that are found 
using the search terms in column 3 of Table 2.
A manual check of the articles in Table 3 reveals that they in fact do not focus on standardization 
in the sense discussed in this article (i.e., standards development). Articles by Adserà and Ray (1998) 
and Sákovics and Steiner (2012) are captured by the keyword search because they cite Farrell and 
Saloner’s (1985) article, which has the word “standardization” in its title, instead of actually analyzing 
the association between standardization and economic growth. Similarly, Acemoglu (2007) cites Farrell 
and Saloner (1985) but mentions “standardization” also on page 1378 in a footnote: “I assume that the 
research firm can only choose one technology, which might be, for example, because of the necessity 
of standardization across firms.” Also, Jovanovic and Rousseau (2014) does not focus on standards 
development, although they write on page 864, “Our distinction between extensive investment in new 
projects and intensive investment in continuing projects is related to one made by Acemoglu, Gancia, 
and Zilibotti 2012 between innovation and standardization costs.” Acemoglu and Restrepo (2018) 
have a labor economics perspective on standardization, as they focus on “standardization of tasks” 
instead of product standardization and standards development. Finally, Acemoglu et al. (2018) is 
captured because it refers to Acemoglu et al. (2012), which has the word “standardization” in its title.
To summarize, we did not find one single article published in top5 economics journals that 
analyses the link between standardization and economic growth. It is worth noting that in the context International Journal of Standardization Research
Volume 19 • Issue 1 12 Table 2. Leading economics journals Scopus search queries Journal
Time 
window ALL(“standardization” 
OR “standardisation”) 
AND ALL(technology 
OR technologies 
OR technological 
OR technical) 
AND SRCTITLE 
(“*Journal*”) ALL(“standardization” 
OR “standardisation”) 
AND ALL(“economic 
growth”) AND 
SRCTITLE(“*Journal*”) ALL(“standardization” 
OR “standardisation”) 
AND ALL(technology 
OR technologies OR 
technological OR technical) 
AND ALL(“economic 
growth”) AND 
SRCTITLE(“*Journal*”) AER
1996-
2018
12
3
3 QJE
1996-
2018
3
0
0 JPE
1996-
2018
1
1
1 Econometrica
1996-
2018
1
1
1 REStud
1996-
2018
5
1
0 J Econ 
Growth
1996-
2018
1
1
1 Notes: Time window is limited to 1996-2018. Table 3. Articles related to standardization and economic growth Article
Author(s)
Year
Journal Issue
Scopus 
citations 1
History and coordination failure
Adserà, A. & Ray, D.
1998
Journal of Economic Growth 3(3), 
267-276
26 2
Equilibrium bias of technology
Acemoglu, D.
2007
Econometrica 75(5), 1371-1409
80 3
Who matters in coordination problems?
Sákovics, J. 
& Steiner, J.
2012
American Economic Review 102(7), 
3439-3461
22 4
Extensive and intensive investment over 
the business cycle
Jovanovic, B. 
& Rousseau, P.L.
2014
Journal of Political Economy 
122(4), 863-908
7 5
The race between man and machine: 
Implications of technology for growth, 
factor shares, and employment Acemoglu, D. 
& Restrepo, P.
2018
American Economic Review 108(6), 
1488-1542
16 6
Innovation, reallocation, and growth Acemoglu, 
D., Akcigit, U., Alp, 
H., Bloom, N. 
& Kerr, W. 2018
American Economic Review 
108(11), 3450-3491
13 Notes: Based on Scopus results. As of 14 Aug 2019. International Journal of Standardization Research
Volume 19 • Issue 1 13 of labor economics, the concept of standardization is more related to “standardization of tasks” (e.g., 
Acemoglu et al., 2012, 2018) than product or system standardization or technical specifications. 5.2 Leading Researchers of Economic Growth As of August 2019, the Journal of Economic Growth had published no articles that (1) focus on 
technology standardization (see Section 4.1), (2) list “standard,” “standards,” or “standardization” 
as keywords, or (3) list “L15” as a JEL classification code.24 Using economics jargon: It seems that 
the existing research on standardization has not been used as an input in the production of economic 
growth theory. Therefore, we focus in this and the next section on more deeply analyzing the attention 
allocated to standardization by the editors of the Journal of Economic Growth.
Table 4 reports the number of articles by leading economic growth researchers25 that mention 
specific keywords related to standardization and economic growth. It is notable that only six out of 
34 editors (~18%) are affiliated with European universities, while 26 are affiliated with American 
universities, and three with an Israeli university.
Table 5 lists the articles that are authored by editors of the Journal of Economic Growth and are 
captured by the specific search terms related to standardization and economic growth presented in 
Section 4.2. Note that there is a significant overlap between articles in Table 3 and Table 5, as four 
articles can be found in both tables. Notably, Acemoglu is an author in seven of the 11 identified 
articles. We explained already above why Adserà and Ray (1998), Acemoglu (2007), Acemoglu 
(2018), and Acemoglu and Restrepo (2018) are captured. A closer look at the contents of the seven 
other articles reveals why the search query captures them. Durlauf (2005) is captured because its 
reference list includes Farrell and Saloner (1985), and Acemoglu et al. (2015) and Zilibotti (2017) 
are captured because they cite Acemoglu et al. (2012). Acemoglu and Akcigit (2012) is captured as 
it cites Acemoglu et al. (2012) in footnote 7, and Acemoglu (2012) is an introduction to the special 
issue of the Journal of Economic Theory which includes and introduces Acemoglu et al. (2012). 
Acemoglu et al. (2012) focuses on standardization but from a slightly differing theoretical “labor 
economics” perspective. In other words, Aghion et al. (2009) is the only article by an editor of the 
Journal of Economic Growth that actually discusses standardization from the perspective of this 
article. Interestingly, their perspective on standardization seems to be relatively negative, as they 
write (p.689): “In network industries, and in product markets characterized by network externality 
effects, a policy stance of avoiding deliberate standard-setting is not a strategy sufficient to prevent 
regrettable standardization outcomes, in which industries are ‘locked in’ to an inferior technical system 
that proves costly to abandon.” They also highlight on page 689 that “Perhaps the most productive 
question to ask is how we can identify situations in which, at some future time, most technology users 
would look back and agree that they would have been better off had they converged on the adoption 
of an alternative technical option (David, 1987).” 5.3 Books Related to Economic Growth Table 6 shows that most economic growth theory textbooks do not mention “standardization” in their 
indexes. Most prominently, the authoritative “Handbook of Economic Growth” (Aghion & Durlauf, 
2006, 2014) is among that majority. Yet, a few exceptions are found. David Weil (2012) mentions 
standards briefly but focuses on government-imposed standards on page 305: “Excessive standards – Governments impose standards on all sorts of goods that are sold in their 
countries, ranging from regulations designed to protect public health (e.g., pure-food standards) to 
requirements that enable different pieces of equipment to work together. Often, however, standards 
are used to keep foreign products out of the market. For example, Israel, with a population of only 
6 million, requires the use of an electrical plug that is unique in the world, to give an advantage to 
local manufacturers of electrical equipment.” International Journal of Standardization Research
Volume 19 • Issue 1 14 Table 4. Leading researchers of economic growth Editor
Affiliations
Number of 
documents in 
Scopus** Number 
of articles 
published in 
Top5** Share 
of Top5 
articles 
of all 
documents 
in Scopus** Coauthors**
Number 
of citing 
documents Articles 
focusing on 
standardization 
and economic 
growth*** Oded Galor*
Brown University; 
Hebrew University
52
17
32.69%
21
4644
0 Daron Acemoglu
MIT
238
74
31.09%
126
20408
7 Philippe Aghion
Harvard University
148
27
18.24%
146
10821
1 Ufuk Akcigit
University of Chicago
19
11
57.89%
20
295
2 Alberto Alesina
Harvard University
121
28
23.14%
86
15857
0 Quamrul Ashraf
Williams College
12
3
25.00%
8
457
0 Roland Benabou
Princeton University
15
10
66.67%
7
3442
0 Jess Benhabib
New York University
89
9
10.11%
44
4194
0 Jagdish Bhagwati
Columbia University
140
13
9.29%
48
3780
0 Francesco Caselli
Harvard University
29
10
34.48%
20
2810
0 Carl-Johan 
Dalgaard
University of 
Copenhagen
29
1
3.45%
16
817
0 Matthias Doepke
UCLA
26
11
42.31%
12
976
0 Steven Durlauf
University of Wisconsin
102
11
10.78%
64
5389
2 William Easterly
New York University
99
7
7.07%
47
10295
0 James Fenske
University of Warwick 
22
2
9.09%
11
232
0 Gene Grossman
Princeton University
89
32
35.96%
23
8719
0 Vernon 
Henderson
Brown University
99
15
15.15%
56
5730
0 Peter Howitt
Brown University
66
14
21.21%
41
3768
0 Charles Jones
Stanford University
31
16
51.61%
8
6332
0 Paul Krugman
Princeton University
106
9
8.49%
27
12972
0 Ross Levine
Brown University
84
6
7.14%
39
16628
0 Stelios 
Michalopoulos
Brown University
15
5
33.33%
9
517
0 Omer Moav
Hebrew University
21
6
28.57%
13
1311
0 Joel Mokyr
Northwestern University
84
3
3.57%
40
2977
1 Torsten Persson
IIES, Stockholm 
University 
77
22
28.57%
33
5352
0 Debraj Ray
New York University
96
27
28.13%
107
3084
1 Paul Romer
Stanford University
25
9
36.00%
10
8993
0 Nancy Stokey
University of Chicago
29
5
17.24%
16
2938
0 Jonathan Temple
Bristol University
39
1
2.56%
18
2518
0 Hans-Joachim 
Voth
Pompeu Fabra 
University 
54
12
22.22%
21
1020
0 Romain Wacziarg
UCLA
25
5
20.00%
16
4322
0 David Weil
Brown University
62
17
27.42%
40
9334
0 Joseph Zeira
Hebrew University
19
4
21.05%
12
1689
0 Fabrizio Zilibotti
Stockholm University
50
12
24.00%
32
2877
4 Notes: *Editor in chief ** As of 14th August 2019 ***Search in Scopus: Among the documents published by the selected author, ALL(“standardization” OR 
“standardisation”) AND ALL(technology OR technologies OR technological OR technical) AND ALL(“economic growth”) International Journal of Standardization Research
Volume 19 • Issue 1 15 Mokyr (2002) mentions “standardization” a few times, for instance, on page 111: “Modularization was closely related to standardization, making all products of particular type 
conform to a uniform standard. Standardization, much like modularization, helped not just during 
the production stage of output but also in the maintenance of durable equipment. Whoever could 
repair one Model T could repair any Model T.” These observations suggest that standardization plays no focal role in economic growth books. 
This indicates that researchers of economic growth do not generally consider standardization to be 
an important factor affecting technological progress and economic growth.
A comprehensive keyword search within the content of these books is left for future research. 
However, we conducted a keyword search using the search term “standardization” among the articles 
that are included in volumes 1 and 2 of the Handbook of Economic Growth (Aghion & Durlauf, 
2006, 2014). The “Handbook of Economic Growth” which is edited by world-leading researchers of 
economic growth, could be considered to represent the stage of the current scientific paradigm. We 
found one article that included the word “standardization”: Ventura (2005) mentions “Advances in 
telecommunications technology and the standardization of software allow producers around the world 
to combine physical and human capital located in different regions in a single production process,” Table 5. Articles by editors of the Journal of Economic Growth Article
Authors*
Year
Journal Citations 
in 
Scopus** History and coordination failure
Adserà, A., Ray, D.
1998
Journal of Economic 
Growth
26 Complexity and empirical economics
Durlauf, S.N.
2005
Economic Journal 115(504), 
pp. F225-F243
80 Equilibrium bias of technology
Acemoglu, D.
2007
Econometrica 75(5), pp. 
1371-1409
80 Science, technology and innovation for economic growth: 
Linking policy research and practice in ‘STIG Systems’
Aghion, P., David, P., Foray, D.
2009
Research Policy 38(4), 
681-693
105 Intellectual property rights policy, competition and 
innovation
Acemoglu, D., Akcigit, U.
2012
Journal of the European 
Economic Association 
10(1), pp. 1-42
70 Introduction to economic growth
Acemoglu, D.
2012
Journal of Economic 
Theory 147(2), pp. 545-550
7 Competing engines of growth: Innovation and 
standardization
Acemoglu, D., Gancia, 
G., Zilibotti, F.
2012
Journal of Economic 
Theory 147(2), pp. 570-
601.e3
48 Offshoring and directed technical change
Acemoglu, D., Gancia, 
G., Zilibotti, F.
2015
American Economic 
Journal: Macroeconomics 
7(3), pp. 84-122
31 Growing and slowing down like China
Zilibotti, F.
2017
Journal of the European 
Economic Association 
15(5), pp. 943-988
2 Innovation, reallocation, and growth
Acemoglu, D., Akcigit, U., Alp, 
H., Bloom, N., Kerr, W.
2018
American Economic 
Review 108(11), pp. 
3450-3491
13 The race between man and machine: Implications of 
technology for growth, factor shares, and employment
Acemoglu, D., Restrepo, P.
2018
American Economic 
Review 108(6), pp. 1488-
1542
16 Notes: *Editors of J of Econ Growth bolded. **As of 14 Aug 2019 International Journal of Standardization Research
Volume 19 • Issue 1 16 but it does not discuss standardization more extensively. This observation further corroborates the 
currently missing link between standardization and economic growth theory.
To summarize, these findings suggest that there is a research gap regarding the association 
between standardization and economic growth. The current paradigm of economic growth theory 
does not incorporate standardization. Top economics journals published no articles related to the 
topic between 1996 and 2018. The leading journal in its field, the Journal of Economic Growth, has 
not yet touched upon standardization, and most researchers on its editorial board have allocated only 
little attention to standardization. Finally, the concept of standardization is not well-specified and, 
recently, seems to be more often related to “standardization of tasks.” 5.4 Limitations and Avenues for Future Research The current analysis has several limitations. For instance, it includes only a small portion of leading 
economics journals and excludes several journals that have published important articles related 
to standardization (e.g., Journal of Economics and Management Strategy, The Economic Journal, 
European Journal of Political Economy). Second, the applied keyword search methodology is not 
necessarily the most accurate one, and additional robustness checks could be conducted. Finally, the 
current analysis is just a snapshot of a specific research gap at a specific point in time and thus it 
becomes obsolete quickly as the literature on standardization continue to grow.
Initially, we planned to use the Journal of Economic Literature (JEL) classifications codes 
in identifying standardization-related articles (cf. Cherrier, 2017). JEL classification code L15 is 
“Information and Product Quality: Standardization and Compatibility” and, according to JEL guidelines 
it “includes studies on standardization and on compatibility, which reduces the problems associated 
with the information-product-quality nexus.”27 The JEL classification also lists “compatibility,” 
“standardization,” and “ISO” as keywords belonging to L15. However, the use of JEL classification 
codes is not always consistent. As an example, whereas Baron and Schmidt (2017) is classified into 
categories E32, E22, O33, O47, and L15, Blind and Jungmittag (2008) is classified into O41, O52, 
O11, and E13. There is no overlap despite the fact that both articles analyze the macroeconomic 
impact of standards. When looking at keywords, there is a similar lack of overlap, but both articles 
have “standards” or “standardization” as keywords.28 Due to these inconsistencies, this alternative 
JEL-classification search option is left for future studies and possible replications and updates.
As illustrated in Section 3.2, accumulated economics research and textbooks affect the 
curricula of economic growth courses. Future studies could extend the analysis to reviewing the 
role of standardization in economics courses. Acemoglu (2013) has recommended that economics 
instructors should spend more time on teaching economic growth at the undergraduate level and also 
on emphasizing the importance of technology as the key determinant of economic growth. The same 
growth course could be accompanied by a brief review of standardization so that economics students 
can acknowledge the importance of standards. There already exist multiple initiatives to increase 
awareness of the importance of standards (de Vries & Egyedi, 2007; Choi & de Vries, 2011; Blind 
& Dreschler, 2017).
Economics has shifted over time, increasingly from theoretical modelling to empirical analysis 
(Hamermesh, 2013). As there are ever-increasing data on standardization available for researchers (e.g., 
Baron & Spulber, 2018; Baron & Gupta, 2018), it is expected that there will also be more publications 
on the topic and empirical economists will begin to allocate more attention to standards. Future 
research could focus on analyzing and quantifying the macro-level economic impact of standards. 
Evidence-based policies require rigorous empirical analysis and, presumably, the welfare effects of 
standardization will receive more attention in the future. International Journal of Standardization Research
Volume 19 • Issue 1 17 Table 6. Books related to economic growth Authors
Book
Publisher
Year
Index includes 
“standards” or 
“standardization” Joel Mokyr
The Lever of Riches: Technological Creativity and 
Economic Progress
Oxford University 
Press 
1990
- Gene Grossman and Elhanan 
Helpman
Innovation and Growth in the Global Economy
MIT Press
1991
- Gene Grossman
Economic Growth: Theory and Evidence
Edward Elgar 
Publishing
1996
No access Philippe Aghion and Peter 
Howitt
Endogenous Growth Theory 
MIT Press
1997
- Philippe Aghion and Jeffrey 
Williamson
Growth, Inequality, and Globalization: Theory, 
History, and Policy
Cambridge 
University Press
1999
- William Easterly
The Elusive Quest for Growth: Economists’ 
Adventures and Misadventures in the Tropics (The 
MIT Press) MIT Press
2002
- Philippe Aghion and Rachel 
Griffith
Competition And Growth: Reconciling Theory 
And Evidence
MIT Press
2005
- Philippe Aghion and Abhijit 
Banerjee
Volatility and Growth
Oxford University 
Press
2005
- Asli Demirgüç-Kunt and Ross 
Levine
Financial Structure and Economic Growth
MIT Press
2005
- Philippe Aghion and Steven 
Durlauf (eds.)
Handbook of Economic Growth 1A
Elsevier/North 
Holland
2006
- Philippe Aghion and Steven 
Durlauf (eds.)
Handbook of Economic Growth 1B
Elsevier/North 
Holland
2006
- Daron Acemoglu
Introduction to Modern Economic Growth
Princeton University 
Press
2008
- Philippe Aghion and Peter 
Howitt
Economics of growth
MIT Press
2008
- Oded Galor
Unified Growth Theory
Princeton University 
Press
2011
- Joel Mokyr
The Gifts of Athena: Historical Origins of the 
Knowledge Economy
Princeton University 
Press
2011
standardization, 58, 60, 63, 
111, 229, 257 David Weil
Economic growth (3rd ed.)
Routledge
2012
Standards, as trade barriers, 
305 Charles I. Jones and Dietrich 
Vollrath
Introduction to Economic Growth (3rd ed.)
W. W. Norton & 
Company
2013
- Jagdish Bhagwati and Arvind 
Panagariya
Why Growth Matters: How Economic Growth in 
India Reduced Poverty and the Lessons for Other 
Developing Countries PublicAffairs
2014
- Philippe Aghion and Steven 
Durlauf (eds.)
Handbook of Economic Growth 2A
Elsevier/North 
Holland
2014
- Philippe Aghion and Steven 
Durlauf (eds.)
Handbook of Economic Growth 2B
Elsevier/North 
Holland
2014
- Francesco Caselli
Technology Differences over Space and Time
Princeton University 
Press
2016
- Joel Mokyr
A Culture of Growth: The Origins of the Modern 
Economy
Princeton University 
Press
2016
- Asli Demirgüç-Kunt and Ross 
Levine
Finance and Growth
International Library 
of Critical Writings 
in Economics N/A
No access Notes: Economic-growth-related books published by the editors of the Journal of Economic Growth as of August 2019. International Journal of Standardization Research
Volume 19 • Issue 1 18 6 CONCLUDING REMARKS The main findings of this article are the following. No article has analyzed the link between 
standardization and economic growth in top5 economics journals and the Journal of Economic 
Growth. A representative sample of leading researchers of economic growth has allocated only 
negligible attention to the link between standardization and economic growth. Economic growth 
theory textbooks and closely related books only occasionally mention standardization. Based on these 
findings, it is plausible to conclude that the current paradigm of economic growth theory neglects 
standardization. We confirm the observation that there are very few academic studies in the field of 
economics that analyze the contribution of standardization to economic growth (Blind et al., 2005; 
Blind & Jungmittag, 2008; Baron & Schmidt, 2017).
Existing empirical evidence suggests that standards may have significant economic impacts. 
Yet, the role of standardization as a factor in economic growth and development has, thus far, been 
neglected. These observations indicate that mainstream economic growth researchers have not 
considered standardization to be an important determinant of economic growth and prosperity. This 
lack of attention may have significant implications. Since university teaching is research-based, with 
the limited accumulated research on the role of standards in technological change there exists a risk 
that standardization will receive little attention in the future as well. Standards matter for technological 
progress, productivity, and economic growth. Economic growth researchers could further shed light 
on the black box of technological progress by allocating more attention to standardization. ACKNOWLEDGEMENTS Earlier versions of this paper have been presented at the XXXVI Summer Seminar of Finnish 
Economists, JSBE Research Seminar, ETLA brownbag seminar, the Finnish Economic Association’s 
42nd Annual Meeting and it was accepted for presentation at the European Academy for Standardisation 
(EURAS) conference in Glasgow (postponed due to COVID-19). We thank Samira Ranaei, Johan 
Willner, Peter Swann and several anonymous reviewers for helpful comments. Joakim Wikström 
provided excellent research assistance. The views expressed herein are those of the authors and do 
not necessarily reflect the views of their employers. All errors are our own. International Journal of Standardization Research
Volume 19 • Issue 1 19",0
"A brief review of some selected topics in p-adic mathematical
physics is presented. 1
Numbers: Rational, Real, p-Adic We present a brief review of some selected topics in p-adic mathematical
physics. More details can be found in the references below and the other
references are mainly contained therein. We hope that this brief introduc-
tion to some aspects of p-adic mathematical physics could be helpful for the
readers of the ﬁrst issue of the journal p-Adic Numbers, Ultrametric Analysis
and Applications.
The notion of numbers is basic not only in mathematics but also in physics
and entire science. Most of modern science is based on mathematical anal-
ysis over real and complex numbers.
However, it is turned out that for
exploring complex hierarchical systems it is sometimes more fruitful to use
analysis over p-adic numbers and ultrametric spaces. p-Adic numbers (see,
e.g. [1]), introduced by Hensel, are widely used in mathematics: in number
theory, algebraic geometry, representation theory, algebraic and arithmetical
dynamics, and cryptography.
The following view how to do science with numbers has been put forward
by Volovich in [2, 3]. Suppose we have a physical or any other system and we 1 make measurements. To describe results of the measurements, we can always
use rationals. To do mathematical analysis, one needs a completion of the
ﬁeld Q of the rational numbers. According to the Ostrowski theorem there
are only two kinds of completions of the rationals. They give real R or p-adic
Qp number ﬁelds, where p is any prime number with corresponding p-adic
norm |x|p, which is non-Archimedean. There is an adelic formula Q p |x|p = 1
valid for rational x which expresses the real norm |x|∞in terms of p-adic ones.
Any p-adic number x can be represented as a series x = P∞
i=k aipi, where k
is an integer and ai ∈{0, 1, 2, ..., p −1} are digits. To build a mathematical
model of the system we use real or p-adic numbers or both, depending on
the properties of the system [2, 3].
Superanalysis over real and p-adic numbers has been considered by Vladimirov
and Volovich [4, 5]. An adelic approach was emphasized by Manin [6].
One can argue that at the very small (Planck) scale the geometry of the
spacetime should be non-Archimedean [2, 3, 7]. There should be quantum
ﬂuctuations not only of metrics and geometry but even of the number ﬁeld.
Therefore, it was suggested [2] the following number ﬁeld invariance princi-
ple: Fundamental physical laws should be invariant under the change of the
number ﬁeld.
One could start from the ring of integers or the Grothendieck schemes.
Then rational, real or p-adic numbers should appear through a mechanism
of number ﬁeld symmetry breaking, similar to the Higgs mechanism [8, 9].
Recently (for a review, see [10, 11, 12, 13, 14]) there have been exciting
achievements exploring p-adic, adelic and ultrametric structures in various
models of physics: from the spacetime geometry at small scale and strings,
via spin glasses and other complex systems, to the universe as a whole. There
has been also signiﬁcant progress in non-Archimedean modeling of some bi-
ological, cognitive, information and stochastic phenomena.
Ultrametricity seems to be a generic property of complex systems which
contain hierarchy.
Moreover, there is some evidence towards much more
wide applicability of p-adic and non-Archimedean methods to various ﬁelds
of knowledge. To extend p-adic methods into actual problems in diverse ﬁelds
of economics, medicine, psychology, sociology, control theory, as well as to
many other branches of sciences, is a great challenge and a great opportunity. 2 2
p-Adic Strings String theory is a modern uniﬁed theory of elementary particles [15].
p-Adic string theories with p-adic valued and also with complex valued
amplitudes were suggested by Volovich in [2, 3]. These two possibilities are
in correspondence with two forms of the dual string amplitude mentioned in
[2]. The ﬁrst one uses the Veneziano amplitude, which describes scattering
of elementary particles, in the form A(a, b) = Γ(a) Γ(b) Γ(a + b) , where a and b are parameters depending on momenta of colliding particles.
As a p-adic Veneziano amplitude it was suggested Ap(a, b) = Γp(a) Γp(b) Γp(a + b) , where Γp is the p-adic valued Morita gamma function. The second one uses
the following form of the Veneziano amplitude A(a, b) =
Z 1 0
xa−1 (1 −x)b−1 dx. Since the function xa is a multiplicative character on the real axis we can
interpret the Veneziano amplitude as the convolution of two characters. In
[2] an analogue of the crossing symmetric Veneziano amplitude on the Galois
ﬁeld Fp is also introduced as the convolution of the corresponding complex-
valued characters, which is the Jacobi sum, A(a, b) =
X x∈Fp
χa(x) χb(1 −x). Freund and Olson [16] introduced an analogue of the crossing symmet-
ric Veneziano amplitude on the Qp as the convolution of the corresponding
complex-valued characters, which is the Gel’fand-Graev beta function [17], A(a, b) =
Z Qp
χa(x) χb(1 −x) dx. Frampton and Okada [18], and Brekke, Freund, Olson and Witten [19]
have discovered that string amplitudes given by the above formula and its 3 generalization can be described by nonlocal eﬀective ﬁeld theories. Important
adelic formulas were considered by Freund and Witten [20], see also [21].
Vladimirov found a uniﬁed approach to adelic formulas for the superstring
amplitudes using algebraic extensions [22] of the number ﬁelds [23] (see also
[24, 25, 26]).
Loop corrections to the p-adic string amplitudes are considered in [27, 28]
More information on p-adic string theory see in [29, 30, 12, 10, 31].
Strings, motives and L-functions are discussed by Volovich [32]. String
partition function can be expressed as inverse to the Mellin transform of
L-function of the Deligne motive, L(s) =
X n
τ(n)n−s, where τ(n) is the Ramanujan function.
Motives and quantum ﬁelds are discussed by Connes and Marcolli [33].
Motives, algebraic and noncommutative geometry are explored in [34, 35, 36,
37, 38]. Theory of motives is considered by Voevodsky [39].
p-Adic geometry is dicsussed in [40]. 3
p-Adic Field Theories As a free action in p-adic ﬁeld theory one can take the following functional S(f) =
Z Qp
fDfdx where f = f(x) is a function f : Qp →R, dx is the Haar measure and
D is the Vladimirov operator or its generalizations [13]. A p-adic analog of
the Euclidean quantum ﬁeld theory was introduced by Kochubei and Sait-
Ametov [41].
Renormalizations in p-adic ﬁeld theory are studied by Missarov [42] and
Smirnov [43].
Nonlocal scalar ﬁeld theory [18, 19] for p-adic strings has occurred to be
very instructive toy model [44, 45] for truncated string ﬁeld theory mod-
els [46]. Boundary value problems for homogeneous solutions of nonlinear
equations of motion corresponding to the p-adic string [18, 19], e2Φ = Φp 4 and its generalizations were explored in [48, 49, 50, 51, 52] (here 2 is the
d’Alembert operator, the ﬁeld Φ and its argument are real-valued). A trun-
cated version of the supersting ﬁeld theory describing non-BPS branes [47] (−2 + 1)e2Φ = Φ3 was investigated in [48, 50, 54, 53].
A generalization of these nonlocal models to the case of curved spacetime
has been proposed by Aref’eva [55] and their application to cosmology, in
particular, to the inﬂation and dark energy was initiated. Cosmological ap-
plications have been studied in numerous papers [56, 57, 58, 59, 60, 61, 62,
63, 64, 65, 66, 67].
Quantization of the Riemann zeta-function and applications to cosmology
are considered in [68]. If ζ(s) is the Riemann zeta-function then the quantum
zeta function is the pseudodiﬀerential operator ζ(2), i.e. the Riemann zeta-
function is the symbol of the pseudodiﬀerential operator ζ(2). Quantization
of the Langlands program is also indicated.
There is a recent consideration towards an eﬀective Lagrangian for adelic
strings with the Riemann zeta function nonlocality [69].
Zeta-functions and many other areas of mathematics are considered by
Shai Haran in [70].
A dual connection between p-adic analysis and noncommutative geometry
was pointed out by Aref’eva and Volovich [71]. It was found that the Haar
measure on quantum group SUq(2) is equivalent to the Haar measure on
p-adic line Qp if q = 1/p.
Relation between the space of coherent states for free annihilation oper-
ators and the space of p–adic distributions was found in [72].
Physics, number theory, and noncommutative geometry are discussed by
Connes and Marcolli [73, 74].
The number ﬁeld invariance principle [2] requires consideration of quan-
tum ﬁeld theory on an arbitrary number ﬁeld.
Such consideration is at-
tempted in [75]. 4
p-Adic and Adelic Quantum Mechanics A general modern approach to quantum theory is presented in the Varadara-
jan book [76]. There are several versions of p-adic quantum mechanics. One
formulation with the complex valued wave functions given by Vladimirov, 5 Volovich and Zelenov [77, 78] is based on the triple {L2(Qp), W(z), U(t)},
where W(z) is a unitary representation of the Heisenberg-Weyl group in the
Hilbert space L2(Qp) and U(t) is a unitary dynamics.
Representation of
canonical commutation relations for p-adic quantum mechanics for ﬁnite and
inﬁnite dimensional systems were studied by Zelenov [79]. Representation
theory of p-adic groups is discussed in [17, 80, 81, 82].
An approach to a uniﬁed p-adic and real theory of commutation relations
is developed by Zelenov [83]. It is based on the interpretation of the group
of functions with values in the ﬁeld of rational numbers as the experiment
data space.
The consequences for particle classiﬁcation of the hypothesis that space-
time geometry is non-archimedean at the Planck scale are explored by Varadara-
jan [84]. The multiplier groups and universal topological central extensions
of the p-adic Poincar´
e and Galilean groups are determined.
p-Adic Maslov index was constructed by Zelenov [85].
Another formulation [86] uses pseudodiﬀerential operators and spectral
theory. The free wave function of p-adic quantum mechanics satisﬁes a pseu-
dodiﬀerential equation of Schr¨
odinger type. A theory of the Cauchy problem
for this equation is developed by Kochubei [87, 13] and Zuniga-Galindo [88].
Matrix valued Schr¨
odinger operator on local ﬁelds is considered in [89].
Adelic quantum mechanics, which is a generalization of p-adic and or-
dinary quantum mechanics as well as their uniﬁcation, was introduced by
Dragovich [90]. It was found that adelic harmonic oscillator is related to
the Riemann zeta function [91]. Many adelic physical systems have been
considered (as a review, see [92]). As a result of adelic approach and p-adic
eﬀects, there is some discreteness of space and time in adelic systems [93].
Distributions on adeles are considered by Dragovich [94], E. M. Radyna and
Ya. V. Radyno [95].
p-Adic path (functional) integrals are considered by Parisi [96], Zelenov
[97], Varadarajan [98], Smolyanov and Shamarov [99]. Dragovich [90, 100]
introduced adelic path integral and elaborated it with Djordjevi´
c and Neˇ
si´
c
[101].
Using path integral approach, the probability amplitude for one-
dimensional p-adic quantum-mechanical systems with quadratic Lagrangians
was calculated in the exact form, which is the same as that one in ordinary
quantum mechanics [102]. p-Adic Airy integrals are considered in [103].
p-Adic quantum mechanics with p-adic valued wave functions is reviewed
below. 6 5
p-Adic and Adelic Gravity and Cosmology p-Adic gravity and the wave function of the Universe are considered by
Aref’eva, Dragovich, Frampton and Volovich [104].
In particular, p-adic
Einstein equations Rµν −1 2 Rgµν = κ Tµν −Λ gµν are explored, where gµν is p-adic valued gravitational ﬁeld. Summation on
algebraic varieties and adelic products in quantum gravity are investigated.
Adelic quantum cosmology, as an application of adelic quantum mechan-
ics to minisuperspace cosmological models of the very early universe, is ini-
tiated by Dragovich [100, 105] and developments are presented in [106, 107].
It is illustrated by a few cosmological models, and some discreteness of the
minisuperspace and the cosmological constant are found.
As was mentioned above, nonlocal scalar ﬁeld theories for p-adic strings
have occurred to be interesting in cosmology, in particular, in the context of
their relation with nonlocal string ﬁeld inspired models [55, 56, 57, 58, 59,
60, 61, 62, 63, 64, 65, 66, 67]. 6
p-Adic Stochastic Processes The p-adic diﬀusion (heat) equation ∂f(x, t) ∂t
+ Dα
xf(x, t) = 0 was suggested in [10], and its mathematical properties and properties of the
associated stochastic processes were studied (see also [108, 109, 110, 98]).
Here f = f(x, t) is a real valued function of the real time t and the p–adic
coordinate x. Dα
x is the Vladimirov operator.
p-Adic Brownian motion is explored in [111, 112, 113], see also [114]. Var-
ious classes of p-adic stochastic processes are investigated by Albeverio and
Karwowski [115], Kochubei [13], Yasuda [116], Albeverio and Belopolskaya
[117]; see [13] for further references. For the most recent results on p-adic
stochastic integrals and stochastic diﬀerential equations see [118].
Kaneko [119] showed a relationship between Besov space and potential
space, and pointed out a probabilistic signiﬁcance of the relationship in terms
of fractal analysis. 7 7
Vladimirov operator The Vladimirov operator [120, 10] of p-adic fractional diﬀerentiation is de-
ﬁned as Dαf(x) =
Z Qp
χ(−kx)|k|α
pdk e
f(k)dk,
e
f(k) =
Z Qp
χ(kx)f(x)dx. Here f(x) is a complex-valued function of p-adic argument x and χ is the
additive character: χ(x) = exp (2πi{x}), {x} is the fractional part of x.
For α > 0 the Vladimirov operator has the following integral representa-
tion Dαf(x) =
1 Γp(−α) Z Qp f(x) −f(y) |x −y|1+α
p
dy, with the constant
Γp(−α) =
pα −1 1 −p−1−α. There is a well-developed theory of the Vladimirov operator and related
constructions (spectral properties, operators on bounded regions, analogs of
elliptic and parabolic equations, a wave-type equation etc); see section on
wavelets and [10, 13, 121, 122, 123]. 8
Dynamics and Evolution of Complex Bio-
logical Systems An idea of using ultrametric spaces to describe the states of complex biolog-
ical systems, naturally possess a hierarchical organization, has been sound
more than once as from the middle of the 1980th by G. Frauenfelder, G.
Parisi, D. Stain, and the others, see [124]. In protein physics, it is regarded
as one of the most profound ideas put forward to explain the nature of distinc-
tive life attributes, since it proposes, in a wide extend, the existence of very
peculiar order inherent the information and functional carriers in biology.
In earlier theoretical examination of this idea, some models of ultrametric
random walk were proposed, but they were confronted with diﬃculties in
applications to the protein ﬂuctuation dynamics, in particular, to the large
body of data obtained in the experiments on ligand-rebinding kinetics of
myoglobin and spectral diﬀusion in deeply frozen proteins. 8 Realization of this task has been provided for last years by V. Avetisov
in collaboration with A. Bikulov, S. Kozyrev, V. Osipov, and A. Zubarev
[125, 126, 127, 128, 129]. It was shown that, in spite of extreme complexity
of the protein energy landscape, p-adic diﬀusion equation introduced in [10]
oﬀers a surprisingly simple, accurate, and universal description of the protein
ﬂuctuation dynamics on extremely large range of scales from cryogenic up to
room temperatures: ∂f(x, t) ∂t
+ Dα
xf(x, t) = 0,
α ∼1 T . Here t is the real time and the p-adic coordinate x describes the “tree of
basins” which corresponds to the conformational state of the protein, T is
the temperature. This equation was used to describe two drastically diﬀer-
ent types of experiments — on rebinding of CO to myoglobin and spectral
diﬀusion of proteins.
These applications of p-adic diﬀusion equation to the protein dynamics
highlight very important protein attribute, namely, the fact that protein
dynamic states and protein energy landscape, being both extremely complex,
exhibit the hierarchical self-similarity.
On the opposite side of biological complexity, e.g. in modeling of op-
timization selection of complex biological entities over combinatorial large
evolutionary spaces, the p-adic stochastic processes have recently been rec-
ognized as a useful tool too. Though the fact of natural ultrametric (taxo-
nomic) relationships in biology, the ultrametric representation of biological
realm has not been reﬂected by models.
Such an evolutionary model, based on p-adic diﬀusion equation, has re-
cently been proposed by V. Avetisov and Yu. Zhuravlev [130]. It is inter-
esting, that the model, being suggested to describe the evolution of complex
biological entities, cast light on the basic point of the prebiotic evolutionary
concepts known as the “error catastrophe”. It was found that the prebi-
otic evolution can be getting beyond the continuity principle of Darwinian
evolutionary paradigm. 9 9
Quantization with p-Adic Valued Wave Func-
tions The ﬁrst step toward quantum mechanics with wave functions valued in
non-Archimedean ﬁelds (and even superalgebras) was done by Vladimirov
and Volovich [4, 5], see also [3]. This approach was elaborated by Khren-
nikov in series of papers and books [131, 132, 133, 134], [11] and extended
in collaboration with Cianci and Albeverio [135, 136, 137, 147, 148]. Here
we present essentials of this theory. The basic objects of this theory are the
p-adic Hilbert space and symmetric operators acting in this space [149, 150].
Vectors of the p-adic Hilbert space which are normalized with respect to
inner product represent quantum states. p-Adic valued quantum theory suf-
fers of the absence of a “good spectral theorem”. At the same time this
theory is essentially simpler (mathematically), since operators of position
and momentum are bounded. Thus the canonical commutation relations can
be represented by bounded operators1. It is impossible in the complex case.
Representations of groups in Hilbert spaces are cornerstones of quantum me-
chanics.
In [135] Albeverio and Khrennikov constructed a representation
of the Weyl–Heisenberg group in the p-adic Hilbert space. Spectra of the
position and momentum operators were studied in [136], [137].
Theory of p-adic valued functions is exposed in the Schikhof book [138].
Spectral theory in ultrametric Banach algebras and the ultrametric func-
tional calculus are considered by Escassut [139], analytic functions on in-
fraconnected sets are studied in [140].
Theory of meromorphic functions
over non-Archimedean ﬁelds is presented by Pei-Chu Hu and Chung-Chun
Yang [141]. Diﬀerential equations for p-adic valued functions were studied
by many authors, see [142, 143]. p-Adic summability of perturbation series
is also investigated, see [144, 145, 146]. 10
Qp-valued Probability This quantum model induces Qp-valued “probabilities”. Surprisingly we can
proceed in the rigorous probabilistic framework even in such unusual situa-
tion – including limit theorems and theory of randomness, see Khrennikov 1It was discovered by Albeverio and Khrennikov [135] and later and independently
by Kochubei [151] (with a diﬀerent construction of the representation), see also Keller,
Ochsenius, and Schikhof [152]. 10 [134, 11, 147, 148]. The starting point of such a generalized probabilistic
model was extension of von Mises’ frequency probability theory to p-adic
topologies: frequencies of realizations should stabilize not with respect to
the ordinary real topology, but one of p-adic topologies. We emphasize that
relative frequency is always a rational number: νn(α) = n(α)/N, where N is
the total number measurements and n is the number of realizations favorable
to the ﬁxed result α. In the simplest case α = 0, 1. Thus the p-adic version
of frequency probability theory describes a new class of random sequences, a
new type of randomness. 11
Applications to Cognitive Science and Psy-
chology The idea of encoding mental states by numbers has very long history –
Plato, Aristotle, Leibnitz, and the others. A new realization of this idea
was provided by Khrennikov who used rings of m-adic numbers Zm to en-
code states of the brain, see [147]. This approach was developed in collab-
oration with Albeverio, Kloeden, Tirozzi, Gundlach, Dubischar, Steinkamp
[153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163], [148]. The model of
m-adic (and more general ultrametric) mental space was applied to cognitive
science and psychology, including modeling of coupling between unconscious
and conscious ﬂows of information in the brain. Coupling with neurophysiol-
ogy was studied in [162, 163] – the process of “production” of m-adic mental
space by neuronal trees. This model can be used for description of arbitrary
ﬂows of information on the basis of m-adic information space. The main
distinguishing feature of this approach is a possibility to encode hierarchi-
cal structures which are present in information by using ultrametric topolo-
gies, treelike structures. Ultrametric algorithmic information is considered
by Murtagh [164]. 12
Applications to Image Analysis Practically all images have (often hidden) hierarchical structures.
These
structures can be represented by using m-adic information spaces. In some
cases the m-adic representation of images essentially simpliﬁes analysis, in
particular, image recognition. One can ignore details which belong to lower 11 levels of the m-adic hierarchy of an image. More eﬀective algorithms can be
developed starting with the m-adic encoding of information. This approach
was developed by Benois-Pineau, Khrennikov, Kotovich, and Borzistaya [165,
166, 167]. 13
p-Adic Wavelets p-Adic wavelet theory was initiated by Kozyrev [168]. Since in the p-adic
case it is not possible to use for construction of wavelet basis translations
by elements of Z, it was proposed to use instead translations by elements of
the factor group Qp/Zp. An example of p-adic wavelet was introduced in the
form of the product of the character and the characteristic function Ωof the
unit ball:
ψ(x) = χ(p−1x)Ω(|x|p). The orthonormal basis {ψγnj} of p-adic wavelets in L2(Qp) was constructed
[168] by translations and dilations of ψ: ψγnj(x) = p−γ 2 χ(pγ−1j(x −p−γn))Ω(|pγx −n|p), γ ∈Z,
n = −1
X l=k
nlpl ∈Qp/Zp,
nl = 0, . . ., p −1,
j = 1, . . . , p −1. In p-adic analysis wavelet bases are related to the spectral theory of pseu-
dodiﬀerential operators. In particular, the above p-adic wavelets are eigen-
vectors of the Vladimirov operator: Dαψγnj = pα(1−γ)ψγnj. Spectra of more general operators were studied in [169].
Moreover, the orbit of the wavelet ψ with respect to the action of the
p-adic aﬃne group G(a, b)f(x) = |a|
−1 2
p f
x −b a 
,
a, b ∈Qp,
a ̸= 0, gives the set of products of p-adic wavelets from the basis {ψγnj} and roots
of one of the degree p. In general [170], the orbit of generic complex-valued
locally constant mean zero compactly supported function gives a frame of
p-adic wavelets. 12 Relation between real and p-adic wavelets is given by the p-adic change
of variable: the map ρ : Qp →R+,
ρ : ∞
X i=γ
xipi 7→ ∞
X i=γ
xip−i−1,
xi = 0, . . ., p −1,
γ ∈Z maps (for p = 2) the basis {ψγnj} of p-adic wavelets onto the basis of Haar
wavelets on the positive half–line.
Important contributions in p-adic wavelet theory were done by Albeverio,
Benedetto, Khrennikov, Shelkovich, Skopina [171, 172, 173, 174, 175, 176].
p-Adic Tauberian and Shannon-Kotelnikov theorems were investigated by
Khrennikov and Shelkovich [177]. 14
Analysis on General Ultrametric Spaces The analysis of wavelets and pseudodiﬀerential operators on general locally
compact ultrametric spaces was developed in [178, 179, 180, 14]. A pseu-
dodiﬀerential operator on ultrametric space X is deﬁned by Kozyrev as the
following integral operator: Tf(x) =
Z X
T(sup(x, y))(f(x) −f(y)) dν(y) where ν is a Borel measure and some integrability condition for the integra-
tion kernel is satisﬁed. The sup(x, y) is the minimal ball in X which contains
the both points x and y, i.e. the integration kernel T is a locally constant
(for x ̸= y) function with the described domains of local constancy.
Ultrametric wavelet bases {ΨIj} were introduced and it was found that
ultrametric wavelets are eigenvectors of ultrametric pseudodiﬀerential oper-
ators:
TΨIj = λIΨIj. Let us note that locally compact ultrametric spaces under consideration are
completely general and do not possess any group structure.
Analysis on p-adic inﬁnite-dimensional spaces was developed by Kochubei,
Kaneko and Yasuda [181, 13, 182, 183, 184].
Another important class of ultrametric spaces consists of locally compact
ﬁelds of positive characteristic. Analysis over such ﬁelds has both common
and diﬀerent features with p-adic analysis. See the book of Kochubei [185]
for the details. 13 15
Cascade Models of Turbulence One of the problems of fully developed turbulence is the energy-cascading
process. This process is characterized by large hierarchy of scales involved
and is known as the Richardson cascade. The main idea of the Fischenko
and Zelenov paper [186] was to consider the equation of cascade model not
in coordinate space, but in an ultrametric space connected with hierarchy
of turbulence eddies. This idea leads to a rather simple nonlinear integral
equation for the velocity ﬁeld.
Properties of fully developed turbulence such as multifractal behavior of
energy dissipation or Kolmogorov’s 1/3 behavior are obtained by analysis of
solutions of the nonlinear equation over p-adic numbers. So, p-adic numbers
provide a natural and systematic approach to cascade models of turbulence.
A modiﬁcation of nonlinear ultrametric integral equation of [186] was in-
vestigated in [187] by Kozyrev. It was found [187] that, using the ultrametric
wavelet analysis, we get a family of exact solutions for this modiﬁed nonlin-
ear equation. Moreover, for this equation an exact solution of an arbitrary
Cauchy problem with the initial condition in the space of locally constant
mean zero compactly supported functions was constructed. 16
Disordered Systems and Spin Glasses Spin glasses (disordered magnetics) are typical examples of disordered sys-
tems. Order parameter for a spin glass in the replica symmetry breaking
approach is described by the Parisi matrix — some special hierarchical block
matrix. It was found that [188] the structure of correlation functions for spin
glasses are related to ultrametricity.
Avetisov, Bikulov, Kozyrev [125] and Parisi, Sourlas [189] found that the
Parisi matrix possesses the p-adic parametrization, namely matrix elements
of this matrix after some natural enumeration of the lines and columns of
the matrix can be expressed as the real valued function of the p-adic norm
of diﬀerence of the indices Qab = q(|a −b|p). This allows to express the correlation functions of the spin glass in the state
with broken replica symmetry in the form of some p-adic integrals. 14 Also more general replica solutions related to general locally compact
ultrametric spaces were obtained [190, 191, 192]. The p-adic Potts model is
considered in [193, 194]. 17
p-Adic Dynamical Systems The theory of p-adic dynamical systems [195] is an intensively developing
discipline on the boundary between various mathematical theories – dynam-
ical systems, number theory, algebraic geometry, non-Archimedean analysis
– and having numerous applications – theoretical physics, cognitive science,
cryptography, computer science, automata theory, genetics, numerical anal-
ysis and image analysis.
One of the sources of theory of p-adic dynamical systems was dynamics
in rings of
mod pn-residue classes. We can mention investigations of W.
Narkiewicz, A. Batra, P. Morton and P. Patel, J. Silverman and G. Call,
D.K. Arrowsmith, F. Vivaldi and Hatjispyros, J. Lubin, T. Pezda, H-C. Li,
L. Hsia, see, e.g., [197], [196] and also books [195], [198] for detailed bibli-
ography. Another ﬂow was induced in algebraic geometry. This (algebraic
geometric) dynamical ﬂow began with article of M. Herman and J.C. Yoccoz
[199] on the problem of small divisors in non-Archimedean ﬁelds. It seems
that this was the ﬁrst publication on non-Archimedean dynamics. In further
development of this dynamical ﬂow the crucial role was played by J. Silver-
man, R. Benedetto, J. Rivera-Letelier, C. Favre, J-P. B´
ezivin, see Silverman’s
book [200] for bibliography.
Another ﬂow towards algebraic dynamics has p-adic theoretical physics
as its source. One of the authors of this review used this pathway towards
p-adic dynamical systems, from study of quantum models with Qp-valued
functions. As a result, a research group on non-Archimedean dynamics was
created at the V¨
axj¨
o University, Sweeden: Andrei Khrennikov, Karl-Olof
Lindahl, Marcus Nilsson, Robert Nyqvist, and Per-Anders Svensson [195].
We point out recent publications of V. Arnold, e.g., [201], devoted to
chaotic aspects of arithmetic dynamics closely coupled to the problem of
turbulence. Some adelic aspects of linear fractional dynamical systems are
considered in [202].
Finally, we point out a ﬂow towards algebraic dynamics which is extremely
important for applications to computer science, cryptography, and numerical
analysis, especially in connection with pseudorandom numbers and uniform 15 distribution of sequences.
This ﬂow arose in 1992 starting with works of
Anashin [203, 204], followed by a series of his works on p-adic ergodicity
as well as on above applications. It worth mention here one of the most
recent papers from these, [205] that contains a solution of a problem on
perturbed monomial mappings on p-adic spheres – the problem was put by
A.Khrennikov, e.g., [195], see [198] for general presentation of p-adic ergodic
theory. 18
p-Adic Models of the Genetic Code Recently, a new interesting application of m-adic information space was
found in the domain of genetics, (m = 2, 4, 5 depending on a model); see
B. Dragovich and A. Dragovich [206], A. Khrennikov [207], M. Pitk¨
anen
[208], A. Khrennikov and S. Kozyrev [211].
The relation between 64 codons, which are building blocks of genes, and
20 amino acids, which are building blocks of proteins, is known as the genetic
code, which is degenerate. Since G. Gamow, there has been a problem of
theoretical foundation of experimentally known ( only about 16) codes in all
living organisms. The central point of p-adic approach to the genetic code
is identiﬁcation C=1, A=2, T=3, G= 4, where C, A, T, G are nucleotides
in DNA and 1, 2, 3, 4 are digits in 5-adic representation of codons, which
are trinucleotides. Using p-adic distances between codons, it was shown that
degeneration of the vertebral mitochondrial code has p-adic structure, and
all other codes can be regarded as slight modiﬁcations of this one. Details of
this approach can be found in [209] of the present volume and [210].
The following p-adic model of the genetic (amino acid) code was pro-
posed in [211]. It was shown that after some p-adic parametrization of the
space of codons the degeneracy of the amino acid code is equivalent to the
local constancy of the map of a p-adic argument. In two-dimensional 2-adic
parametrization of [211] we get the following table of amino acids on the 16 2-adic plane of codons: Lys Asn Glu
Asp Ter
Ser
Gly Ter Tyr Gln His Trp
Cys
Arg Met Ile
Val
Thr
Ala Leu
Phe
Leu
Ser
Pro 19
Applications to Economics, Finance, Data
Mining Possible applications of p-adic analysis in economics, ﬁnance, business con-
nections, including a p-adic version of the Black-Scholes equation [213], are
discussed in [212, 213, 214, 215].
Application of p-adic analysis in data analysis and data mining is explored
by Murtagh [216]. Acknowledgments The authors would like to thank many colleagues for kind and fruitful dis-
cussions of various themes of p-adic mathematical physics and related topics,
before and during work on this review article.
The work of B. Dragovich was partially supported by the Ministry of Sci-
ence and Technological Development, Serbia, under contract No 144032D.
I. V. Volovich and S. V. Kozyrev gratefully acknowledge being partially sup-
ported by the grant DFG Project 436 RUS 113/951, the grant RFFI 08-01-
00727-a, the grant of the President of the Russian Federation for the support
of scientiﬁc schools NSh 3224.2008.1, the Program of the Department of
Mathematics of the Russian Academy of Science “Modern problems of the-
oretical mathematics”, and by the program of Ministry of Education and
Science of Russia “Development of the scientiﬁc potential of High School,
years of 2009–2010”, project 3341. S. V. Kozyrev also has been partially
supported by the grants DFG Project 436 RUS 113/809/0-1 and RFFI 05-
01-04002-NNIO-a. Authors of this review were also partially supported by 17 the grants of the Swedish Royal Academy of Science and Proﬁle Mathemat-
ical Modeling of V¨
axj¨
o University.",0
"Mathematical models, calibrated to data, have become ubiquitous
to make key decision processes in modern quantitative finance. In
this work, we propose a novel framework for data-driven model
selection by integrating a classical quantitative setup with a genera-
tive modelling approach. Leveraging the properties of the signature,
a well-known path-transform from stochastic analysis that recently
emerged as leading machine learning technology for learning time-
series data, we develop the Sig-SDE model. Sig-SDE provides a
new perspective on neural SDEs and can be calibrated to exotic
financial products that depend, in a non-linear way, on the whole
trajectory of asset prices. Furthermore, we our approach enables to
consistently calibrate under the pricing measure Q and real-world
measure P. Finally, we demonstrate the ability of Sig-SDE to sim-
ulate future possible market scenarios needed for computing risk
profiles or hedging strategies. Importantly, this new model is under-
pinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the
presented algorithms. CCS CONCEPTS • Mathematics of computing →Probability and statistics; •
Applied computing; • Computing methodologies →Machine
learning; KEYWORDS market simulation, pricing, signatures, rough path theory ACM Reference Format:
Imanol Perez Arribas, Cristopher Salvi, and Lukasz Szpruch. 2018. Sig-SDEs
model for quantitative finance. . In 2020 ACM International Conference on
AI in Finance, October 15–16, 2020, NY. ACM, New York, NY, USA, 8 pages.
https://doi.org/10.1145/1122445.1122456 1
INTRODUCTION The question of finding a parsimonious model that well represents
empirical data has been of paramount importance in quantitative
finance. The modelling choice is dictated by the desire to fit and
explain the available data, but is also subject to computational Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICAIF-2020, October 15–16, 2020, NY
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/10.1145/1122445.1122456 considerations. Inevitably, all models can only provide an approxi-
mation to reality, and the risk of using inadequate ones is hard to
detect. A classical approach consists in fixing a class of parametric
models, with a number of parameters that is significantly smaller
than the number of available data points. Next, in the process
called calibration, the goal is to solve a data-dependent optimiza-
tion problem yielding an optimal choice of model parameters. The
main challenge, of course, is to decide what class of models one
should choose from. The theory of statistical learning [28] tell us
that to simple models cannot fit the data, and to complex one are
not expected to generalise to unseen observations. In modern ma-
chine learning approaches, one usually starts by defining a highly
oveparametrised model from some universality class, exhibiting a
number of parameters often exceeding the number of data points,
and let (stochastic) gradient algorithms find the best configuration
of parameters yielding a calibrated model. In this work, we find
a middle ground between the two approaches. We develop a new
framework for systematic model selection that exhibits universal
approximation properties, and we provide a explicit solution to
the optimization used in its calibration, that completely removes
the need to deploy expensive gradient descent algorithms. Impor-
tantly the class of models that we consider builds upon classical
risk models that are well underpinned by research on quantitative
finance.
The mathematical object at the core of this work is the expected
signature of a path, whose properties are well-understood in the
field of stochastic analysis. It allows to identify a linear structure
underpinning the high non-linearity of the sequential data we work
with. This linear structure leads to a massive speed-up of calibra-
tion, pricing, and generation of future scenarios. Our approach
provides a new systematic model selection mechanism, that can
also be deployed to calibrate classical non-Markovian models in a
computationally efficient way. Signatures have been deployed to
solve various tasks in mathematical finance, such as options pricing
and hedging [22, 23], high frequency optimal execution [4, 14] and
others [12, 24]. They have also been applied in several areas of
machine learning [6, 16, 19, 21, 29–34]. 1.1
Sig-SDE Model Let X : [0,T] →Rd denote the price process of an arbitrary finan-
cial asset under the pricing measure Q. To ensure the no-arbitrage
assumption is not violated, X typically is given by the solution of
the following Stochastic Differential Equation (SDE) dXt = ΣtdWt,
X0 = x ,
(1) where W is a one-dimensional Brownian motion and Σt is an
adapted process (the volatility process). Model (1) accommodates
many standard risk models used e.g: the classical Black–Scholes arXiv:2006.00218v2  [q-fin.CP]  3 Jun 2020 ICAIF-2020, October 15–16, 2020, NY
Perez Arribas, Salvi and Szpruch model assumes that volatility is proportional to the spot price, i.e.
Σt := σXt with σ ∈R constant; the local volatility model assumes
that Σt := σ(t,Xt )Xt , where σ(·, ·) (called local volatility surface)
depends on both time and spot. Hence, it is a generalisation of the
Black–Scholes model; various stochastic volatility model assume
that Σt := σtXt with σ2
t following some diffusion process; the SABR model chooses Σt := σtX β
t , with β ∈[0, 1] and where σt
follows a diffusion process.
A natural question would be whether one can find a model for the
volatility process Σt that is large enough to include all the classical
models such as the ones mentioned above and that would allow for
systematic a data driven model selection. We will require such a
model to satisfy the following requirements: (1) Universality. The model should be able to approximate ar-
bitrarily well the dynamics of classical models. (2) Efficient calibration. Given market prices for a family of
options, it should be possible to efficiently calibrate the model
so that it correctly prices the family of options. (3) Fast pricing. Ideally, it should be possible to quickly price
(potentially exotic) options under the model without using
Monte Carlo techniques. (4) Efficient simulation. Sampling trajectories from the model
should be computationally cheap and efficient.
An example of a model that satisfies point 1. above is a neural
network model, where the volatility process Σt is approximated by
a neural network NNθ (t, (Ws)s ∈[0,t]) with parameters θ. Such a
model would be able to approximate a rich class of classical models.
However, the calibration and pricing of such models would involve
performing multiple Monte Carlo simulations on each epoch, which
might be expensive if done naively. See however, [7, 10].
The aim of this paper is to propose a model for asset price dynam-
ics that, we believe, satisfies all four points above. Our technique
models the volatility process Σt as Σt = ⟨ℓN , b
W0,t ⟩
(2) where ℓN is the model parameters and b
W0,t is the signature (c.f def-
inition 2.6) of the stochastic process b
Wt := (t,Wt ). The motivation
for choosing the signature as the main building block of this paper
is anchored in a very powerful result for universal approximation
of functions based on the celebrated Stone-Weierstrass Theorem that
we present next in an informal manner (for more technical details
see [8, Proposition 3]) Theorem 1.1. Consider a compact set K of continuous Rd-valued
paths. Denote by S the function that maps a path X from K to its
signature X. Let f : K →R be any any continuous functions. Then,
for any ϵ > 0 and any path X ∈K, there exists a linear function l∞ acting on the signature such that ||f (X) −⟨l∞, X⟩||∞< ϵ
(3) In other words, any continuous function on a compact set of paths
can be uniformly well approximated by a linear combination of
terms of the signature. This universal approximation property is
similar to the one provided by Neural Networks (NN). However, as
we will discuss below, NN models depend on a very large collec-
tion of parameters that need to be optimized via expensive back-
propagation-based techniques, whilst the optimization needed in our Sig-SDE model consists of a simple linear regression on the
terms of the signature. In this way, the signature can be thought of
as a feature map for paths that provides a linear basis for the space
of continuous functions on paths. In the setting of SDEs, sample
paths are Brownian and solutions are images of these sample tra-
jectories by a continuous functions that one wishes to approximate
from a set of observations. Our Sig-SDE model will rely upon the
universality of the signature to approximate such functions acting
on Brownian trajectories. Importantly, the signature of a realisa-
tion of a semimartingale provides a unique representation of the
sample trajectory [2, 13]. Similarly, the expected signature – i.e. the
collection of the expectations of the iterated integrals – provides a
unique representation of the law of the semimartingale [5].
Note that model calibration is an example of generative modelling
[11, 18]. Indeed, recall that if one knew prices of traded liquid
derivatives, then one can approximate the pricing measure from
market data [3, 22]. We denote this measure by Qreal.
We know that when equation (1) admits a strong solution then
there exists a measurable map G : R × C([0,T]) →C([0,T]) such
that
X = G(x, (Ws)s ∈[0,T ])
(4)
as shown in [15, Corollary 3.23]. If Gt denotes the projection of
G given by Xt := Gt (ξ, (Ws)s ∈[0,t]), then one can view (1) as a
generative model that maps µ0 supported on Rd into (Gt )#µ0 =
Qθ
t . Note that by construction G is a casual transport map i.e a
transport map that is adapted to the filtration Ft [1]. In practice,
one is interested in finding such a transport map from a family of
parametrised functions Gθ . One then looks for a θ such that Gθ
# µ0
is a good approximation of Qreal with respect to a metric specified
by the user. In this paper the family of transport maps Gθ is given
by linear functions on signatures (or linear functionals below). 2
NOTATION AND PRELIMINARIES We begin by introducing some notation and preliminary results
that are used in this paper. 2.1
Multi-indices Definition 2.1. Let d ∈N. For any n ≥0, we call an n-dimensional
d-multi-index any n-tuple of non-negative integers of the form
K = (k1, . . . ,kn) such that ki ∈{1, . . . ,d} for all i ∈{1, . . . ,n}. We
denote its length by |K| = n. The empty multi-index is denoted by
ø. We denote by I
d the set of all d-multi-indices, and by In
d ⊂I
d
the set of all d-multi-indices of length at most n ∈N. Definition 2.2 (Concatenation of multi-indices). Let I = (i1, . . . ,ip)
and J = (j1, . . . , jq) be any two multi-indices in I
d. Their concate-
nation product ⊗as the multi-index I ⊗J = (i1, . . . ,im, j1, . . . , jn) ∈
I
d. Example 2.3. (1) (1, 3) ⊗(2, 2) = (1, 3, 2, 2).
(2) (2, 1, 3) ⊗(1) = (2, 1, 3, 1).
(3) (2, 2) ⊗ø = (2, 2). 2.2
Linear functionals Definition 2.4 (Linear functional). For a given d ≥1, a linear func-
tional is a (possibly infinite) sequence of real numbers indexed by Sig-SDEs model for quantitative finance.
ICAIF-2020, October 15–16, 2020, NY multi-indices in I
d of the following form F = {F(K) ∈R : K ∈I
d }.
(5) We note that a multi-index K ∈I
d is always a linear functional.
Both concatenation ⊗and x can be extended by linearity to opera-
tions on linear functionals. We will now define two basic operations
on linear functionals that will be used throughout the paper. Definition 2.5. For any two linear functionals F,G and any real
numbers α, β ∈R define αF + βG = {αF(K) + βG(K) ∈R : K ∈I
d }
(6) and
⟨F,G⟩=
Õ K ∈I
d
F(K)G(K) ∈R
(7) 2.3
Signatures Rough paths theory can be briefly described as a non-linear ex-
tension of the classical theory of controlled differential equations
which is robust enough to allow a deterministic treatment of sto-
chastic differential equations controlled by much rougher signals
than semi-martingales [26]. Definition 2.6 (Signature). Let X : [0,T] →Rd be a continuous
semimartingale. The Signature of X over a time interval [s,t] ⊂
[0,T] is the linear functional Xs,t := {X(K)
s,t ∈R : K ∈I
d }, such that X(ø)
s,t = 1 and so that for any n ≥1 and K = b
K ⊗a ∈In
d , with a ∈{1, . . . ,d} and b
K ∈In−1
d
we have X(K)
s,t =
∫t s
X(b
K)
s,u ◦dX(a)
u
(8) where the integral is to be interpreted in the Stratonovich sense. Example 2.7. Let X : [0,T] →R2 be a semimartingale. (1) X(1)
s,t = X (1)
t
−X (1)
s . (2) X(1,2)
s,t
=
∫t
s X (1)
s,u ◦dX (2)
u . (3) X(2,2)
s,t
= 1 2(X (2)
s
−X (2)
t )2. A more detailed overview of signatures is included in Appendix A. 3
SIGNATURE MODEL In this section we define the Signature Model for asset price dy-
namics that we propose in this paper. The goal is to approximate
the volatility process Σt (that is a continuous function on the driv-
ing Brownian path) by a linear functional on the signature of the
Brownian path. Definition 3.1 (Signature Model). LetW be a one-dimensional Brow-
nian motion. Let N ∈N be the order of the Signature Model. The
Signature Model of parameter ℓ= {ℓ(K) : K ∈IN
2 } is given by
Σt := ⟨ℓ, b
W0,t ⟩, where b
W denotes the signature of W add-time. In
other words, the asset price dynamics are given by dXt = ⟨ℓ, b
W0,t ⟩dWt,
X0 = x ∈R.
(9) We note that the Signature Model has two components: the hy-
perparameter N ∈N, and the model parameter ℓ. Intuitively, the
hyperparameter N plays a similar role to the width of a layer in a neural network. The larger this value is, the richer the range of mar-
ket dynamics the Signature Models can generate. Once the value
of N is fixed, the challenge is to find a suitable model parameter
ℓ. Again, in analogy with neural networks, ℓplays the role of the
weights of the network.
The Signature Model possesses the universality property, in the
sense that given a classical model, there exists a Signature Model
that can approximate its dynamics to a given accuracy [20].
We show in the upcoming Sections 5-7 that (a) the Signature Model
is efficient to simulate, (b) it is efficient to calibrate, and (c) exotic
options can be priced fast under the Signature Model. Remark 1. The Signature Model introduced in Definition 3.1 as-
sumes that the source of noise (i.e. the Brownian motion W ) is one-
dimensional. This was done for simplicity, but the authors would
like to emphasise that the model generalises in a straightforward
way to multi-dimensional Brownian motion. 4
NUMERICAL EXPERIMENTS We now demonstrate the feasibility of our methodology as outlined
in Sections 5-7. Throughout this section, we work with the Signature
Model
dXt = ⟨ℓ, b
W0,t ⟩dWt,
X0 = 1 with ℓ= {ℓ(K) : K ∈IN
2 }. We fix N = 4. Therefore, the model has
1 + 2 + 22 + 23 + 24 = 31 parameters that need to be calibrated. We
also fix the terminal maturity T = 1.
In this section we will show experiments for the calibration of the
model, pricing of options under the signature model and simula-
tion. Sections 5-7 will then include the technical details of how
calibration, pricing and simulation of signatures model are done. 4.1
Calibration We assume that the family of options available on the market are a
mixture of vanilla and exotic options, given as follows: • Vanilla call options with strikes K = 0.5, 0.6, . . . , 1, 1. and
maturities t = 0.4, 0.45, 0.5, . . . , 0.9, 0.95, 1: Φ := max(Xt −K, 0). • Variance options with strikes K = 0.01, 0.015, . . . , 0.035, 0.04
and maturities t = 0.4, 0.45, 0.5, . . . , 0.9, 0.95, 1: Φ := max(⟨X⟩t −K, 0). where ⟨X⟩is the quadratic variation of X. • Down-and-Out barrier call options with maturity 1, strikes
K = 0.9, 0.92, 0.94, . . . , 1.01, 1.03 and barrier levels L =
0.6, 0.62, 0.64, . . . , 0.88, 0.9: Φ := (
max(Xt −K, 0)
if mins ∈[0,t] Xs > L
0
else. The option prices are generated from a Black-Scholes model with
volatility σ = 0.2:
dXt = σXtdWt .
The optimisation (14) was then solved to calibrate the model pa-
rameters ℓ= {ℓ(K) : K ∈IN
2 }.
Figure 1 shows the absolute error between the real option prices and
the option prices of the calibrated model, for the different option
types. ICAIF-2020, October 15–16, 2020, NY
Perez Arribas, Salvi and Szpruch Figure 1: Error analysis between the option prices of the real
model and the calibrated Signature Model. 4.2
Simulation Once the Signature Model has been calibrated to the available
option prices, we can use Algorithm 1 to simulate realisations of
the calibrated Signature Model. Figure 2 shows 1,000 realisations
of the Signature Model. 4.3
Pricing We will now use the calibrated Signature Model to price a new set of
options that was not used in the calibration step. This set of option
consists of Down-and-In barrier put options with barriers levels
L = 0.7, 0.71, . . . , 0.81, 0.82 and strikes K = 0.9, 0.92, . . . , 1.01, 1.03: Φ := (
max(K −Xt, 0)
if mins ∈[0,t] Xs < L
0
else. Figure 2: 1,000 realisations of the calibrated Signature
Model. Figure 3: Error analysis between the option prices of the real
model and the calibrated Signature Model. Figure 3 shows the absolute error of the prices under the Signature
Model, compared to the real prices.
As we see, the calibrated model is able to generate accurate prices
for these new exotic options. The error is highest when the barrier
is close to the strike price, as expected. 5
SIMULATION This section will address the question of simulation efficiency of
Signature Models. We begin by stating the following two results.
The first result rewrites the differential equation (9) solely in terms
of the lead-lag signature of the Brownian motion, b
WLL
0,t . Here b
WLL denotes the lead-lag transformation of b
W , see Appendix B. We
use the lead-lag transformation because it allows us to rewrite
Itˆ
o integrals as certain Stratonovich integrals, which in turn can
be written as linear functions on signatures. The second result
guarantees that the computational cost of computing b
WLL
0,t is the same as the cost of computing { b
WLL
0,s ; 0 ≤s ≤t}. These two
results lead to Algorithm 1, which provides an efficient algorithm
to sample from a Signature Model. Sig-SDEs model for quantitative finance.
ICAIF-2020, October 15–16, 2020, NY Algorithm 1: Sampling from a Signature Model. Parameters:D = {ti }n
i=1 with
0 = t0 < t1 < . . . < tn−1 < tn = T : sampling
times.
ℓ= {ℓ(K) : K ∈IN
4
}: Signature Model parameter.
x ∈R: initial spot price. Output: A sample path {Xtk }n
k=0 from the Signature Model. 1 Simulate a one-dimensional Brownian motion at the
sampling times {Wti }n
i=0. 2 Apply the lead-lag transformation (25) to b
W to obtain b
W LL. 3 b
WLL
0,0 ←{F (K) : K ∈IN +1
4
} with F (ø) = 1 and F (K) = 0 for K , ø. 4 X0 ←x. 5 for k = 1, . . . ,n do 6
Compute the signature b
WLL
tk−1,tk = { b
WLL,(K)
tk−1,tk : K ∈IN +1
4
}. 7
Use Chen’s identity (Theorem 5.2) to compute the
signature b
WLL
0,tk ←{ b
WLL,K
0,tk
: K ∈IN +1
4
} 8
Use proposition 5.1 to get Xtk ←⟨x(ø) + ℓ⊗(4), b
WLL
0,tk ⟩. 9 end 10 return {Xtk }n
k=0. Proposition 5.1 ([22, Lemma 3.11]). Let X follow a Signature Model
with parameter ℓ= {ℓ(K) : K ∈IN
2 }. Then, X is given by Xt = ⟨x(ø) + ℓ⊗(4), b
WLL
0,t ⟩
(10) where ℓ⊗(4) = {K ⊗(4) : K ∈ℓ}, x = X0 ∈R, and b
WLL denotes
the lead-lag transformation, introduced in Definition B.1, of the 2-
dimensional process b
W = (t,Wt ). Theorem 5.2 (Chen’s identity, [25, Theorem 2.12]). Let 0 ≤s ≤t.
Then, for each multi-index K ∈I
d we have b
WLL,(K)
0,t
=
Õ I, J ∈I
d
I ⊗J=K b
WLL,(I)
0,t
· b
WLL,(J)
0,t
(11) where for any multi-index K ∈I
d we used the notation b
WLL,(K)
0,t
= ⟨K, b
WLL
0,t ⟩. These two results lead to Algorithm 1. We note there are a number
of publicly available software packages to compute signatures, such
as esig 1, iisignature 2 and signatory 3. 6
PRICING This section will show that exotic options can be priced fast under
a Signature Model. This will be done via a two step procedure.
First, it was shown in [22, 23] that prices of exotic options can be
approximated with arbitrary precision by a special class of payoffs
called signature payoffs, defined below. Hence, we will assume that
the exotic option to be priced is a signature payoff, defined as
follows. 1https://pypi.org/project/esig/
2https://github.com/bottler/iisignature, [27]
3https://github.com/patrick-kidger/signatory, [17] Definition 6.1 (Signature payoffs). A signature payoff of maturity
T > 0 and parameter f = {f (K) : K ∈IN
3 } is a payoff that pays at
time T an amount given by ⟨f , b
X0,T ⟩. Second, the price of a signature payoff is ⟨f , E[b
X0,T ]⟩. To price a
signature payoff, all we need is E [[] b
X0,T ], which doesn’t depend
on the signature payoff itself. In particular, it may be reused to price
other signature payoffs.
We now explicitly derive the expected signature E [[] b
X0,T ] in terms
of the model parameters and the expected signature of the lead-lag
Brownian motion E [[] b
WLL
0,T ]. Proposition 6.2. Let X be a Signature Model of order N ∈N with
parameter ℓ= {ℓ(K) : K ∈IN
2 }. Consider the following linear
functionals P1 = (1) and P2 = ℓ⊗(4). Consider any multi-index
I = (i1, . . . ,in) ∈In
2 such that n ≤N. Then X(I)
s,t = ⟨CI (ℓ), b
WLL
s,t ⟩
(12) where CI (ℓ) is given explicitly in closed-form by CI (ℓ) = (. . . ((Pi1 ≻Pi2) ≻Pi3) ≻. . . ≻Pin )
(13) Proof. By Proposition 5.1 we know that if X follows a Signature
Model with parameter ℓ= {ℓ(K) : K ∈IN
2 } then Xt = ⟨x(ø) + ℓ⊗(4), b
WLL
0,t ⟩ Let I = (i1, . . . ,in) be any multi-index in In
2 such that n ≤N. If
n = 1 then I = (i1) and we necessarily one of the following two
options must hold • If i1 = 1 then X(i1)
s,t = t −s = b
WLL,(1)
s,t
= ⟨P1, b
WLL
s,t ⟩ • If i1 = 2 then X(i1)
s,t = Xt −Xs = ⟨ℓ⊗(4), b
WLL
s,t ⟩= ⟨P2, b
WLL
s,t ⟩. Hence the statement holds for n = 1. Let’s assume by induction that
the statement holds for any 1 ≤n ≤N. We write I = J ⊗(in) with
in ∈{1, 2} and J = (i1, . . . ,in−1) ∈In−1
2
. Clearly |(in)|, |J | < n,
therefore by induction hypothesis X(in)
s,t = ⟨C(in)(ℓ), b
WLL
s,t ⟩= ⟨Pin, b
WLL
s,t ⟩ and X(J)
s,t = ⟨CJ (ℓ), b
WLL
s,t ⟩= ⟨(. . . (Pi1 ≻Pi2) ≻. . . ≻Pin−1), b
WLL
s,t ⟩ By definition of the signature (see 2.6) we know that X(I)
s,t =
∫t s
X(J)
s,u ◦dX(in)
u =
∫t s
⟨CJ (ℓ), b
WLL
s,t ⟩◦d⟨C(in)(ℓ), b
WLL
s,t ⟩ = ⟨CJ (ℓ) ≻C(in)(ℓ), b
WLL
s,t ⟩ = ⟨(. . . (Pi1 ≻Pi2) ≻. . . ≻Pin−1) ≻Pin, b
WLL
s,t ⟩ which concludes the induction.
□ ICAIF-2020, October 15–16, 2020, NY
Perez Arribas, Salvi and Szpruch 7
CALIBRATION We will now address the task of calibrating a Signature Model.
We assume that the market has a family of options {Φi }n
i=1 whose
market prices {pi }n
i=1 are observable. Typically {Φi }n
i=1 will contain
vanilla options, together with some exotic options such as various
variance or barrier products. Fix N ∈N be the order of the Signature
Model. The challenge here is to find the model parameter ℓ= {ℓ(K) :
K ∈IN
2 } that best fits the data, in the sense that the prices of Φi,
under the Signature Model with parameter ℓ, are approximately
given by the observed market prices pi.
Following Section 6, we assume that the options Φi are given by
signature options. Therefore, we assume that we can write Φi by Φi = ⟨φi, b
X0,T ⟩,
φi = {φ(K)
i
: K ∈IN
2 }. The minimisation problem we aim to solve now is the following: min
ℓ={ℓ(K):K ∈IN
2 } n
Õ i=1 
⟨φi, E[b
X0,T ]⟩−pi
2
.
(14) where E[b
X0,T ] is the expected signature of the Signature Model
with parameter ℓ= {ℓ(K) : K ∈IN
2 }.
By Proposition 6.2, the price of Φi, which is given by ⟨φi, E[b
X0,T ]⟩,
can be written as a polynomial on ℓ(K). Hence, the optimisation
(14) is rewritten as a minimisation of a polynomial of variables ℓ(K),
for K ∈IN
2 .
If the number of parameters ℓ(K) is large compared to the number
of available option prices, the optimisation problem might be over-
parametrised and there will be multiple solutions to (14). In this
case, we are in the robust finance setting where there are multiple
equivalent martingale measures that fit to the data. If the number
of parameters ℓ(K) is small, however, we are in the setting of classi-
cal mathematical finance modeling and there will in general be a
unique solution to (14). 8
CONCLUSION In this paper we have proposed a new model for asset price dy-
namics called the signature model. This model was develop with the
objective of satisfying the following properties: (1) Universality.
(2) Efficiency of calibration to vanilla and exotic options.
(3) Fast pricing of vanilla and exotic options.
(4) Efficiency of simulation. Due to the rich properties of signatures, the signature model sat-
isfies all four properties and is, therefore, capable of generating
realistic paths without sacrificing the computational feasibility of
calibration, pricing and simulation.
Although this paper has focused on the risk-neutral measure Q, it
can also be used to learn the real-world measure P. One would first
calibrate to the risk-neutral measure Q and then learn the drift. ACKNOWLEDGMENTS This work was supported by The Alan Turing Institute under the
EPSRC grant EP/N510129/1.",0
"Existence theory in economics is usually in real domains such as
the ﬁndings of chaotic trajectories in models of economic growth,
tˆ
atonnement, or overlapping generations models. Computational ex-
amples, however, sometimes converge rapidly to cyclic orbits when
in theory they should be nonperiodic almost surely. We explain this
anomaly as the result of digital approximation and conclude that both
theoretical and numerical behavior can still illuminate essential fea-
tures of the real data. 1
Introduction Mathematical existence theories in various scientiﬁc ﬁelds are usually de-
veloped in Euclidian or more general topological spaces. The mathematical
entities whose “existences” are thereby established can not (in general) be
represented by ﬁnite numbers. This fact is of particular relevance for eco-
nomics where many variables such as prices or product quantities (cars, etc.)
are naturally expressed as integers or rational fractions.
Further, opera-
tional economic decisions invariably boil down to ﬁnite sequences of binary
comparisons among discrete alternatives. This is the case whenever an op-
timization or equilibrium algorithm is used on a digital computer to solve a
given problem numerically, as in the case of “computable general equilibrium
models.” It is also the case when a person solves a problem by “thinking it
through.” In economic problems, however, so long as they are formulated in
ﬁnite dimensional Euchidian spaces, the mathematically existing, real valued 1 solutions can usually be approximated as closely as one likes by using ap-
propriate numerical techniques. In the rest of this paper such problems are
called computational.
When a given real valued solution that is known to exist cannot be ap-
proximated, we shall say that it is not computational. The (perhaps clumsy)
term “computational” is necessary to distinguish the issue we have in mind
from the closely related but deeper theory of “computability” which would
carry into more diﬃcult issues than need to be dealt with here. Besides, that
has already been done in the context of economics in a coruscating fashion
by Velupillai (2000, forthcoming). The classical example of a computational
problem is the circumference of a circle of diameter, say D. Even if D is
an integer or a rational fraction, the circumference, C = πD, cannot be
computed exactly because π is an irrational number. Yet, because π can be
computed to any desired degree of approximation—and has been to many
thousands of places—the circumference is computational. The same can be
said, of course, for squaring the circle.
However, there are theoretical economic problems that have been solved
in real terms that are not computational in the sense of this paper and that
is what we deal with here. This is the case of chaotic trajectories in dynamic
economic processes. It is well know that many such processes, such as over-
lapping generations models, optimal and adaptive economizing growth mod-
els, tˆ
atonnement, adaptive strategies in oligopolistic settings, and so forth,
exhibit chaos and statistical (ergodic) behavior robustly.1 The theorems that
enable these properties to be established live in the reals. A special case has
arisen in the economic models investigated by Nishimura and Yano (1995),
Nishimura and Sorger (1996), and Hommes (1998). Although the models are
diﬀerent, involving, respectively, growth, business cycle and market mecha-
nisms, in each case the trajectories are generated by iterations of the tent
map.
In this paper we show why for all initial conditions, forward iterations sim-
ulationed on a binary computer using this map must converge to an even,
periodic orbit even though for the same map deﬁned over the reals, such
convergence occurs with zero measure.2
Nonetheless, it is possible to con-
struct ﬁnite segments of trajectories that appear to be chaotic. Moreover,
very small perturbations in the map can produce behavior that is highly
irregular—so that it appears to be chaotic—and also produces histograms
that approximate the real theoretical results. 2 We emphasize that the problem of computationality in our sense is well
known.
Anyone working with computational algorithms is bound to run
into it sooner or later.
For example, Stein and Ulam (1964, p.55) wrote
about “spurious convergence” to cycles in computer experiments with the
sine function, even though there are no attractive ﬁxed points for the map in
the real numbers. Farebrother (1991), Kaplan and Glass (1995) and Hommes
(1998)—and no doubt many others—observed the same phenomenon. Their
results can be duplicated with, for example, the freely available program
Dynamics by Nusse and Yorke (1994). Still, students—even those who have
developed sophisticated programming skills—are usually baﬄed by computa-
tional results that “contradict” the theory. Understanding this contradiction
is crucial to guard against the tendency to reject one or the other of the
contending outcomes for the wrong reason. In short, when the theory is not
conﬁrmed by the computer, both computer and real theory may be correct.
It may just be that the latter is not computational!
Computer experimentation played a seminal role in the development of
chaos theory by Lorenz, whose work motivated the Li–Yorke theorems that
provided constructive conditions for establishing the theoretical existence
and robustness of chaos in iterated maps. Thus, although the computer was
helpful in discovering its existence, the results discussed here explain why
chaos may sometimes not appear in computer experiments when supposedly
it should. 2
The General Problem Let θ : X →X be a continuous map where X is a closed, bounded interval
in the non–negative, real numbers, R. Such a map deﬁnes a discrete time
dynamic process xt+1 = θ(xt), t = 0, 1, . . .
where
x0 = x ∈X.
(1) The trajectory of such a map is deﬁned by τ(x) := {θt(x)}∞
t=0 where θ0(x) :=
x0 and θ1(x) := θ(x) and θt(x) = θ(θt−1(x)). Thus, xt = θt(x0), t = 0, 1, 2, . . . .
(2) Li and Yorke (1975a, 1975b) derived simple, constructive criteria to show that 3 such a map could generate chaotic trajectories (in R) and provided condi-
tions that the relative frequency distribution of values in any such trajectory
converged to a unique, absolute, continuous, invariant measure or density
function. Given both conditions, chaos “occurs almost surely” for any initial
condition drawn at random in X.
To pin down exactly what is at the heart of the issue, a few basic prop-
erties of binary arithmetic need to be reviewed.3
First, every number is
represented by a sequence of zeros and ones, the integer portion by a string
of p zeros or ones and the fractional part by a string of q zeros or ones where
p and q are positive integers. Let m be the maximum number of signiﬁcant
digits that can be represented, a number determined by the word length in
the computer and the precision to be used (single, double, etc.).4 The car-
dinal of the set of numbers, ¯
X, representable on the computer is 2m where
p + q = m.
Consider any element x in X. Deﬁne a round–oﬀmap, ρ : X →¯
X by5 ρ(x) = the binary number given by the ﬁrst m signiﬁcant digits of
X. Returning to the domain of our dynamic process, any x ∈X becomes the
number ¯
x = ρ(x) on the computer. ¯
X = ρ(X) :=

¯
x = ρ(x)
for all
x ∈X
 is the set of 2m rational numbers in X with at most m signiﬁcant digits.
Obviously, any initial condition that must be truncated introduces a round–
oﬀerror at the outset.
For most nonlinear maps the value of θ(x) must be approximated by an
algorithm that terminates after a ﬁnite number of steps with an element in
X. On the computer such an algorithm in eﬀect deﬁnes a map ¯
θ : ¯
X →X
such that for all x ∈¯
X, ¯
θ(¯
x) ∈X. Whenever this occurs, a new round–oﬀ
error is introduced in addition to those that might arise in the algorithmic
steps. The map (ρ · ¯
θ)(x) , therefore, is the computational representation of
equation (1) and the computer analog of the real dynamical process is ¯
xt+1 = (ρ · ¯
θ)(¯
xt), t = 0, 1, . . .
where
¯
x0 = ρ(x).
(3) The sequence of computational iterated maps is 4 ¯
xt = (ρ · ¯
θ)t¯
x0
where
¯
x0 = ρ(x0).
(4) By construction ρ · ¯
θ maps ρ(X) into ρ(X). The immediate implication is
that A. Every computed trajectory is cyclic or converges to a cycle in ﬁnite time.6 The general computational problem is that the repeated truncation in a
given simulation can accumulate error so that a computed sequence, ¯
x0, . . .
can be far removed from its theoretical counterpart, x0, . . .. That is, ET = T
 t=0 

(ρ · ¯
θ)t(ρ(x)) −θt(x)



(5) can grow without bound as T becomes large. This is exactly what happens
in the theoretical examples at issue where the real trajectories generated by
θ are chaotic, and those generated by ρ · ¯
θ are cyclic.
This is almost all there is to the story except to show that the cycles are
even in the case of the tent map. 3
The Tent Map The family of tent maps that underlie the economic models cited above is θ(x) := 
ax
if
0 ≤x < N 2
a(N −x)
if
N 2 ≤x ≤N.
(6) For convenience, we consider examples where a = 2 and N is a positive inte-
ger. Points that satisfy the Li–Yorke (1975) overshoot conditions that imply
the existence of chaotic trajectories and periodic orbits of every periodicity
are easily found. It is also relatively easy to show that the uniform distribu-
tion on [0, N] is the unique absolutely continuous invariant measure for the
map θ : [0, N] →[0, N]. Those facts imply that (in the sense of the uniform
measure) for almost all x ∈[0, N] the real trajectories are nonperiodic and
chaotic. See, for example, Day (1994, p.86). Here, then is an example of
existence theory in the reals.
The ﬁrst iterate of the tent map is just equation (6). The second iterate
is given by 5 θ2(x) = 



 


 4x
if
0 ≤x < 1 4N
4(2 4N −x)
if
1
4N ≤x < 2 4N
4(x −2 4N)
if
2
4N ≤x < 3 4N
4(N −x)
if
3
4N ≤x ≤N the third by θ3(x) = 













 












 8x
if
0 ≤x < N 8
8(N 4 −x)
if
N 8 ≤x < N 4
8(x −N 4 −x)
if
N 4 ≤x < 3 8N
8(N 2 −x)
if
3
8N ≤x < N 2
8(x −N 2 )
if
N 2 ≤x < 5 8N
8(3 4N −x)
if
5
8N ≤x < 3 4N
8(x −3 4N)
if
3
4N ≤x < 7 8N
8(N −x)
if
7
8N ≤x ≤N. The nth iterate can be expressed by θn(x) = 

 
 2nx
if
0 ≤x < N 2n,
2n(2iN 2n −x)
if
N 2n(2i −1) ≤x < N 2n2i, i = 1, . . . , 2n−1,
2n(x −2iN 2n )
if
n 2n2i ≤x < N 2n(2i + 1), i = 1, . . . , 2n−1 −1.
(7)
Notice that as a and N are integers, an algorithmic approximation is not
needed. That is, θ ≡¯
θ.
Nonetheless, round–oﬀerror must appear. Equation (7) indicates that
the nth iterate of x—regardless in which interval the initial condition lies—
contains the term 2nx. Clearly, the fractional part of the initial condition x
becomes shorter by one element with each multiplication by 2.7 Therefore,
given the precision m, 2nx must become an integer for some n ≤m. The
terms 2iN are even integers.
The other term is always an integer.
The
diﬀerence of two integers is an integer and any integer multiplied by 2n is
even, so after some number of iterations, not more than m, θt(x) becomes
an even integer. B. If a = 2 and N is an integer, then, after a ﬁnite number of iterations,
every computed trajectory of (6) will have only even integer values. 6 From A every trajectory converges to a cycle in ﬁnite time. From B every
trajectory converges to even integers. Therefore, we have C. For a = 2 and any integer N and given ﬁnite precision m, every computed
trajectory of the tent map (6) converges in ﬁnite time to a periodic orbit
consisting of even integers. Thus, viewing the computer itself as a dynamical system, the chaos that
occurs almost surely in the reals surely does not occur in the binary computer.
If N = 1, then x0 is always a fraction, zero, or one. Zero is always a
ﬁxed point and θ(1) = 0. Since the fractional part goes to zero, we have the
following corollary. D. If N = 1, every computed trajectory converges to zero in ﬁnite time. This is the result obtained numerically by Kaplan and Glass (1995) and
observed by Hommes. 4
Numerical Examples To better understand the above corollary, let us look at an example. For
simplicity, assume that we work on a computer that stores binary numbers
as a string of ﬁve binary digits and that the computer does not use binary
normalization, that is a binary number can start with a zero. For the initial
decimal value of x0 = 0.4, equation (1) for N = 1 produces the periodic
sequence: 0.4, 0.8, 0.4, 0.8, 0.4,. . . . However, due to the round oﬀerror in
representing the initial value and due to the disappearance of the fractional
part the numerical sequence will converge to zero, as D predicts.8 Rewrite
equation (6) for N = 1 using binary numbers in place of the decimal numbers: θ(x) := 
(10.0)2 × x
if
(0.0)2 ≤x < (0.1)2
(10.0)2 × ((1.0)2 −x)
if
(0.1)2 ≤x ≤(1.0)2
(8) Iterating this binary representation of the tent map on our imaginary com-
puter would produce a binary sequence that is shown below together with
its decimal equivalents. 7 operation (in binary numbers)
binary number
decimal equivalent
binary representation of x0 = 0.4
0.0110
0 × 1 2 + 1 × 1 22 + 1 × 1 23 + 0 × 1 24 = 0.375
0.0110 < 0.1 ⇒10.0 × 0.0110
0.1100
1 × 1 2 + 1 × 1 22 + 0 × 1 23 + 0 × 1 24 = 0.75
0.1100 > 0.1 ⇒10.0 × (1.0 −0.1100)
0.0100
0 × 1 2 + 1 × 1 22 + 0 × 1 23 + 0 × 1 24 = 0.25
0.0100 < 0.1 ⇒10.0 × 0.0100
0.1000
1 × 1 2 + 0 × 1 22 + 0 × 1 23 + 0 × 1 24 = 0.5
0.1000 > 0.1 ⇒10.0 × (1.0 −0.1000)
1.0000
1 × 20 = 1
1.0000 > 0.1 ⇒10.0 × (1.0 −1.0000)
0.0000
0 × 1 2 + 0 × 1 22 + 0 × 1 23 + 0 × 1 24 = 0. Notice that the numerical binary sequence is not equivalent to the periodic
decimal sequence. Thus, instead of the 2–period orbit, {.4, .8}, we get the
sequence .375, .75, .5, 1, 0 converging to the stationary state zero.
Consider what happens when N = 100. We computed trajectories using
a C++ program that was compiled with the Borland C++ compiler installed
on a Pentium computer. It took only 16 iterations for the trajectory starting
at x0 = 67.2 to converge to the integer 19 that then led to the 10-period
orbit {8, 16, 24, 32, 48, 56, 64, 72, 88, 96}. Figure 1 displays this cycle with its
integer preimages. The lowest level of the tree contains odd integers. For the
initial value x0 = 4.23828125 the trajectories converged to the integer 85 in
8 iterations, leading to the 2–period orbit {40, 80}. Figure 2 gives this orbit
with its integer preimages. Figure 1: Ten period cycle and its integer preimages 8 — Figure 2: Two period cycle and its integer preimages Figure 3: Fixed point and its integer preimages Of course, the empirical densities for these orbits are just spikes over
the cyclic points and not integrable. For the initial condition x0 = 12.5 the
trajectory quickly converged to the ﬁxed point of 0. See Figure 3. The integer
preimages of the three orbits and the cyclic points themselves account for all
the integers in [0, N]. Note that the elements of all three orbits are even. 5
Imitating Chaos In spite of these results it is possible to approximate chaos. We consider two
possibilities. 9 — Figure 4: Histogram of Preimages for N = 100, x0 = 67.2 5.1
Constructing Preimages Any point x in [0, N] has two preimages, each of which has two and so on of
the diﬀerence equation deﬁned by equation (6). At each point we can choose
one of the two preimages, ﬁnd its two preimages, choose one, and so on. This
can be done for some number say p times starting for any arbitrary number
¯
x ∈ρ

[0, N]

in this way generating a sequence x0, x1, . . . , xp. If the choice
at each stage is “random” (say, by tossing a coin or using a computer random
number generator), this sequence will look more or less chaotic. Moreover,
the sequence x−p, x−p+1, . . ., x0 will be a segment (possibly approximate) of
the real trajectory of (1) starting at the initial condition x−p. However, if we
begin computing the diﬀerence equation (3) from that same point, (x−p), we
will not get the irregular segment just constructed, but will quickly converge
to an even cycle instead!
For example, we took the initial condition 67.2 and constructed in the
above manner 60,000 preimages to obtain the numerical density shown in
Figure 4. It roughly approximates the uniform density, suggesting that the
absolutely continuous invariant density of the real trajectories is computa-
tional.
In spite of this, “chaotic” trajectories so constructed are not computa-
tional. The 60,000th preimage has a fractional part. Consequently, forward
computation from that point cannot follow the true constructed trajectory but
will quickly converge to an even integer cycle as predicted by the theorem and
shown in our examples of section 5. 10 — Figure 5: Histogram for N = 100.0001, x0 = 67.2 While studying convergence to cycles for the map yt+1 = sin πyt, 0 ≤y ≤
1, Stein and Ulam suggested inverse iterations with random choice between
left and right preimages. They point out that for many other maps, the
probability of convergence to low order cycles is very small. Even for the
tent map, however, a good approximation to chaos can be attained in another
way. 5.2
Noninteger domain By setting N to a non–integer value, we can keep x from converging quickly
to an integer. For example, choosing N = 100.0001, rather than 100, as in
the previous experiment, gives us the distribution of Figure 5 (x0 = 67.2,
60, 000 iterations). It is “close” to the uniform distribution that is predicted
by the theory for the tent map over the reals; and it resembles the distribution
obtained using randomly selected backward iterates. 6
Discussion In spite of the anomalies discussed above, experience shows that for most
maps forward iterations generate trajectories that on a computer appear to
be chaotic when in theory they “should be:” irregular ﬂuctuations are gener-
ated and they seem to obey the usual statistical laws of large numbers, even
though ultimately the computations must be cyclic. Thus, the applied mod-
els can provide potential partial explanations for the ubiquitous irregularity 11 of ﬂuctuation in economic data.9 If we acknowledge that much (if not all)
economic data arise in the ﬁnitely representable rationals (as we must), then
theoretical results derived in the reals must be conceptual approximations of
the real world economic phenomena. Thus, it seems to us that the “anomaly”
analyzed here does not vitiate inferences based on the real theoretical prop-
erties of a given economic model—at least not from a practical point of view.
Real theory can still lead us to a better understanding of actual economic
data—most of which is, indeed, highly irregular. Of course, the theory is an
idealization. But that is a general characteristic of theory, the purpose of
which, after all, is to idealize, that is, to help us understand in the simpliﬁed
terms demanded by logic the unfathomably complex world around us. Less
appreciated is the possibility that a numerical analog of a theory could be
entirely misleading, less appreciated, perhaps, because it seems not to occur
often in practice. 7
Appendix: Binary Numbers We present here a brief synopsis of the binary number system. Every real
number, x ∈R, consists of integer and fractional parts and can be uniquely
represented in a base, r, by an inﬁnite sequence, xr
=
(apap−1, . . . , a1a0 · a1a2, . . . , aq, . . .)r
:=

(ap × rp) + (ap−1 × rp−1) + · · · + (a1 × r1) + (a0 × r0)

+

(a−1 × 1 r) + · · · + (a−q × 1 rq ) + · · ·

(9) where ai ∈{0, . . . , r −1}. When r = 2 and we have the binary numbers D,
ai ∈{0, 1}, i ∈{p, p −1, . . . , 1, 0, −1, −2, . . . , −q}. When r = 10, we have
the commonly used decimal numbers.
The decimal number (67.2)10 = 6 · 101 + 7 · 100 + 2 ·
 1 10 1
,
(10) whereas, (0.4)10 = 4 ·
 1 10 1
.
(11) 12 Notice that in both cases the decimal numbers have ﬁnite representations.
A binary computer converts decimal numbers to binary representation.
The algorithm for doing this is given below. In the preceding examples, (67.2)10
=
26 + 21 + 20 +

1
2 3 +

1
2 4 +

1
2 7 +

1
2 8 + . . .
=
(1000011.00110011)2
≃
1000011.00110011001100110 . . . .
(12) (0.4)10
=
0 +

1
2 2 +

1
2 3 + 0 + 0 +

1
2 6 +

1
2 7 + 0 + . . .
=
(0.0110)2
≃
0.011001100110 . . . .
(13) In each case the binary representation involves an inﬁnite series.
Conse-
quently, even though they are rational, the numbers must be approximated on
any computer with ﬁnite precision. Binary Arithmetic and Computer Representation
Consider a decimal number with integer and fractional parts such as 67.4
as shown in section 5.1. To translate the integer part we divide it by 2,
keeping the remainders as binary digits and adding 1 as the most signiﬁcant
digit: Remainder
67 ÷ 2 = 33
1
=>
a0 = 1
(least signiﬁcant digit)
33 ÷ 2 = 16
1
=>
a1 = 1
16 ÷ 2 = 8
0
=>
a2 = 0
8 ÷ 2 = 4
0
=>
a3 = 0
4 ÷ 2 = 2
0
=>
a4 = 0
2 ÷ 2 = 0
0
=>
a5 = 0
a6 = 1
(most signiﬁcant digit) The decimal part is multiplied by 2: 0.2 × 2 = 0.4
=>
a−1 = 0
0.4 × 2 = 0.8
=>
a−2 = 0
0.8 × 2 = 1.6
=>
a−3 = 1
0.6 × 2 = 1.2
=>
a−4 = 1
0.2 × 2 = 0.4
=>
a−5 = 0
... 13 Then we put the integer and fractional parts together: (67.2)10 =

1000011.00110011 2 . Notes ∗The authors gratefully acknowledge comments of Amy Radunskya and two
referees. ∗∗Department of Economics, University of Southern California, Los Angeles,
CA 90089-0253. ∗∗∗Information Systems Department, School of Management, Boston Univer-
sity, Boston, MA 02215. 1.
Many examples are given, for example, in Grandmont (1987) or Day
(1994, 1999). 2. The concepts of measure theory are fundamental to the present discus-
sion for they are necessary to characterize the statistical behavior of chaotic
trajectories and to specify whether or not real chaos is robust with respect
to initial conditions in the reals. This is the case we investigate. That is
why the innocent might be astonished not to ﬁnd it on the computer. For
the far more diﬃcult issue of extending the concept of measure to sequences
of numbers that may not converge to cycles, see Kolmogorous (1998) and
Martin–L¨
of (1966). 3. See the appendix in §7 for examples. 4. Precision is determined by the number of signiﬁcant digits that can be
carried. For example, 1.0000 and 1.0001 have ﬁve signiﬁcant digits each,
while .0001 has a single signiﬁcant digit. See Lipschultz (pp.59, 60) for a
deﬁnition of “precision.” 5. The mere conversion of a number to a diﬀerent base can introduce round–
oﬀ. See section 7, equations (10)–(13) for examples. For a review of these
topics, see, for example, Aho, et al. (1992). The importance of round oﬀer-
ror in digital computations has recently received close attention in an entirely
diﬀerent context; that of computational algorithms used in econometric esti-
mation. See McCullough and Vinod (1999). Such problems were recognized 14 decades ago by Goldberger and Zellner when their students, using diﬀer-
ent regression packages, arrived at markedly diﬀerent parameters in linear
regressions. 6.
Proof: Pick a point x0 ∈¯
X and compute a sequence (ρ · ¯
θ)(x0), (ρ ·
¯
θ)2(x0), . . . , (ρ· ¯
θ)t(x0) such that (ρ· ¯
θ)t(x0) = (ρ· ¯
θ)s(x0) for some s, 1 ≤x ≤
t ≤m −1. Such a point must exist for otherwise (ρ · ¯
θ)m(x0) ̸∈¯
X contrary
to hypothesis. This implies that points (ρ· ¯
θ)s(x0), . . . , (ρ· ¯
θ)t−1(x0) is a t−s
cycle. Let Si = {x1
0, (ρ · ¯
θ)(x1
0), . . . , (ρ · ¯
θ)t−1(x1
0)} and add to this set all the
preimages of x1
0 in ¯
X. All of these end in the t −s cycle just constructed.
Include them in S1. If S1 = ¯
X, we are done. If not, pick a point in ¯
X \ S1
and repeat the process, in this way constructing a ﬁnite sequence of sets
S1, . . . , Sn, k ≥1, such that ∪iSi = ¯
X, and such that every trajectory beginning in Si ends in a stationary point or
a ﬁnite cycle. 7. Everyone knows that multiplying a decimal number by 10 can be done
merely by shifting the decimal one place to the right. Dividing by 10 involves
shifting the decimal one space to the left. The situation is analogous for
the binaries. Multiplying a binary number by 2 is equivalent to shifting the
binary point one digit to the right. Conversely, division by 2 shifts the binary
point one place to the left. It is this fact that means that the tent map (6)
has an exact binary digital representation for a = 2. 8.
The exact binary representation of the decimal 0.4 is (0.0110)2.
The
bar implies an inﬁnite repetition of the sequence 0110. However, allowing
only ﬁve binary digits, this inﬁnite number will be truncated to 0.0110. Such
truncation introduces a round oﬀerror. This would be true for any precision.
This is analogous to representing a rational number such as 1 3 = 0.3333, . . .
in the decimal system. See equations (11) and (13). 9. For numerous examples in various economic settings, again see Day (1994,
2000). 15",0
"We derive measure change formulae required to price midcurve swaptions in the forward swap annuity measure with stochastic annuities’ ratios.
We construct the corresponding linear and exponential terminal swap rate pricing models and show how they capture the midcurve swaption correlation skew. Introduction An interest rate swap is a ﬁnancial instrument with a triangle property. The value of two swaps St1t2, St2t3 between times t1 and t2 and between times t2 and t3 is equal to the value of the swap St1t3 between times t1 and t3 (we assume that all three swaps have the same ﬁxed leg strike). Equivalently, we may say that the swap St2t3 is the diﬀerence between a long swap St1t3 and a short swap St1t2. To express views on swap rates in the future, the interest rate market actively trades options on swaps, i.e. swaptions. Swaptions are non-linear products. The triangle property of the swap generalises into the property of the swaptions by including the convexity. A portfolio of a vanilla swaption on the short swap St1t2 and an option (midcurve swaption) on the swap St2t3 is more expensive than the price of the long swaption St1t3 (when the strikes are the same, and the exercise time of all of the swaptions is the same, t1 - the start of the short and long swaps). If, in the Black-Scholes world, we assume also that the long and short annuities ratios to the annuity of the midcurve swap are deterministic, then from the swap triangle one can derive a useful relationship for the volatilities of all of the three swap rates. The challenge comes when we look into the relations between the volatility smiles (skews) of those rates. In this paper, we discuss a modelling approach for pricing midcurve swaptions that allows one to take into account the stochasticity of the swaps’ annuities and to generate pronounced correlation skews which are typically observed in the midcurve swaption market. 1 arXiv:1812.07415v2  [q-fin.PR]  22 Jun 2019 A midcurve swaption is an eﬃcient way to trade correlations between the short and long swap rates. Others also used this product to trade on the diﬀerence between levels in the short and long term implied volatilities [1]. Being the simplest product on forward volatility, midcurve swaptions can be used for the calibration of the mean reversion parameters in the one factor short rate models [2]. The rich structure of the interest rate market oﬀers two approaches to modelling the price of a midcurve swaption. The product can be viewed dynamically and be priced by modelling the time evolution of the underlying swap rate, or it can be viewed statically and its price can be derived from prices of closely related products - the long and the short swaptions traded in the market. We shall be looking at the static way of pricing the midcurve swaption using a gener- alisation of the triangle property of the swaps to the case of the swaptions. A midcurve swaption can be priced as an option on a weighted basket of the short and long swap rates with the same ﬁxing date. The weights coeﬃcients are functions of the swap annuities ra- tios. The industry standard is to freeze these ratios to be constants. Taking correlation as an input parameter, the weighted basket can be priced by pairing the short and long swap rate distributions via a copula. Some use more advanced models to account for stochasticity of the annuity ratios. The approach that has been adopted by the larger banks is ﬁrst to move both the short and the long swaps rates distributions to the same terminal (discount bond) measure, and then to approximate each of the short and long annuities by deterministic functions of the corre- sponding (short or long) swap rates. This is an extension of the idea [3] where the authors developed a model that directly links constant maturity swap to volatilities of swaptions of all relevant tenors. Note that midcurve swaptions are not in the scope of [3]. This product is liquidly traded in the US Dollar market where the settlement style is physical. Thus, the natural pricing measure for this product is the annuity measure. While allowing a better risk management of the midcurve correlation skew, the terminal measure approach suﬀers from an inconsistency. In this paper, we show that once you ﬁx the stochastic form of the annuity ratio, the measure change is no longer free. We derive the explicit formulae for the measure change in terms of the functional forms of the annuity ratios. One other deﬁciency of the terminal measure approach are negative ratios of annuities. The exponential terminal swap rate model developed in this paper is free of this problem by 2 construction. We analyse in detail the measure change formulae in the case where the annuity ratio is a linear or an exponential function of the short and the long swap rates. The price of a midcurve swaption is often parameterised by its implied correlation as a function of the strike. Even if we use a model that captures the implied volatility smiles of the long and short swap rates well, the implied correlation is still not a constant function of the strike. The termianl swap rate models with stochastic annuities developed in this paper give a handle to match the implied correlation skew. The eﬀect studied in this paper is applicable in conjunction with any smile model. In particular, it is present in the ﬂat volatility world. We provide numerical results on how our methodology captures the midcurve correlation skew in the case when the underlying swap rates are modelled as standard normal variables with a ﬂat smile (i.e. constant across all strikes’ volatilities). 1
Product valuation A midcurve receiver swaption Wrec = Wrec(Srec, Te(x)piry) on a swap Srec(T(s)tart, T(e)nd, K) with a ﬁxed leg rate K gives the holder an option to enter into a receiver swap Srec at expiry time Tx, where the swap starts on Ts, ends on Te and the holder receives the ﬁxed rate K accrued on a notional N over all periods in the schedule formed by a sequence of dates: T fix
1
,
. . . ,
T fix
n
= Te, with n payment dates in the ﬁxed leg schedule.
In return the holder pays ﬂoating rate payments on the sequence of dates from the ﬂoating rate schedule: T fl
1 ,
. . . ,
T fl
m = Te. We will use short notations for the time intervals between two consecutive payments on each of the swap legs: τ fix
i
= T fix
i
−T fix
i−1, i = 1, . . . n, τ fl
j = T fl
j −T fl
j−1, j = 1, . . . m, T fl
0 = T fix
0
= Ts. In order to price a swaption, one uses the swap ﬁxed leg annuity A(t) (t ≤Ts) as a numeraire: A(t) = A(t, Ts, Te) = n
X i=1
τ fix
i
D(t, T fix
i
),
(1) where D(t, T) is the relevant discount bond from t to T. We write the swap as: Srec
= NA(t)(K −R(t)) = NA(t, Ts, Te)(K −R(t, Ts, Te)),
(2) where R(t) = R(t, Ts, Te) is the forward Ts-to-Te-swap rate as seen at t. Consequently, in 3 order to price the swaption Wrec, we can model the distribution for the R(Tx, Ts, Te) in the annuity measure and calculate the value of the swaption as: Wrec(t)
=
A(t)EA[W(Tx)/A(Tx)] =
A(t, Ts, Te)NEA[[K −R(Tx, Ts, Te)]+],
(3) where the superscript in EA denotes the annuity measure with numeraire A(t). The distributions for R(t0, t1, t2) can be implied from the swaption market whenever t0 = t1. We are primarily interested in the distributions of the following two stochastic variables: Rs = R(Tx, Tx, Ts),
Re = R(Tx, Tx, Te),
(4) where R(Tx, Tx, Ts) and R(Tx, Tx, Te) are the swap rates of the corresponding ”short” and ”long” swaps. Following [4] and using (3) the probability density function, PDF, for the distribution of the swap rate R(Tx, Tx, TJ) with J = s or e in the corresponding annuity measure is given by PDFJ
RJ(r) =
1 A(t0, Tx, TJ) · N
∂2Wrec(t0) ∂K2
|K=r,
(5) where the derivative is taken with respect to the strike K of the swaption Wrec(Srec(Tx, TJ, K), Tx).
(6) Within the pricing approach provided by (3), the distributions for Rs, Re are speciﬁed in the corresponding annuity measures: A(t, Tx, Ts), A(t, Tx, Te). The swap rate R(Tx, Ts, Te), that we are interested in, can be expressed as R(Tx, Ts, Te) = w1 · Re −w2 · Rs,
(7) where w1 = A(Tx, Tx, Te) A(Tx, Ts, Te)
w2 = A(Tx, Tx, Ts) A(Tx, Ts, Te).
(8) Therefore, the stochastic variable R(Tx, Ts, Te) representing the underlying swap rate is a weighted diﬀerence of the stochastic variables representing the long and the short swap rates with stochastic coeﬃcients. Note that the three stochastic variables As = A(Tx, Tx, Ts),
Ae = A(Tx, Tx, Te),
and
Au = A(Tx, Ts, Te)
(9) 4 are related via A(Tx, Ts, Te) = A(Tx, Tx, Te) −A(Tx, Tx, Ts).
(10) We are going to model the distribution of the swap rate R(Tx, Ts, Te) in terms of the distri- butions of R(Tx, Tx, Te), R(Tx, Tx, Ts) and their correlation. In order to do this we need to relate three annuity measures corresponding to A(t, Tx, Ts), A(t, Tx, Te) and A(t, Ts, Te). The Radon-Nikodym derivative for the measure change between A(t, Tx, TJ) measure, J = s, e, and A(t, Ts, Te) measure can be reconstructed using the following identity: V (t0)
=
A(t0, Tx, TJ)EA(Tx,TJ)

V (t) A(t, Tx, TJ)  =
A(t0, Ts, Te)EA(Ts,Te)

V (t) A(t, Ts, Te) 
,
(11) where V (t) is the price of a traded security (which is a stochastic variable at any future time). Under the standard assumptions on attainable claims and measure changes, equation (11) implies that for any stochastic process Xt, which is a function of the swap rate R(Tx, Tx, TJ), EA(Ts,Te) [Xt] = EA(Tx,TJ)

Xt
A(t0, Tx, TJ) A(t, Tx, TJ) · A(t, Ts, Te) A(t0, Ts, Te) 
.
(12) The quantity GJ,Tx = A(Tx, Ts, Te) A(Tx, Tx, TJ)
(13) is itself a stochastic variable. We shall assume that it has a joint distribution with the swap rates R(Tx, Tx, Ts) and R(Tx, Tx, Te) PDFJ
GJ,Rs,Re(gJ, x, y),
(14) where the variable gJ is used to indicate a stochastic value for GJ,Tx, the variable x is used to indicate a stochastic value for R(Tx, Tx, Ts), and the variable y is used to indicate a stochastic value for R(Tx, Tx, Te). Lemma 1. The measure change formulae for the marginals φs(x) and φe(y) of the joint distribution of the short and the long swap rates in the measure (u) associated with the underlying swap annuity Au of the midcurve swaption are: φs(x)
:=
PDFu
Rs(x) =
Z +∞ −∞ Z +∞ −∞
PDFu
Ru,Rs,Re(z, x, y)dzdy = =
PDFs
Rs(x)A(t0, Tx, Ts) A(t0, Ts, Te)EA(Tx,Ts) [Gs,Tx|R(Tx, Tx, Ts) = x]
(15) 5 φe(y)
:=
PDFu
Re(y) =
Z +∞ −∞ Z +∞ −∞
PDFu
Ru,Rs,Re(z, x, y)dzdx = =
PDFe
Re(y)A(t0, Tx, Te) A(t0, Ts, Te)EA(Tx,Te) [Ge,Tx|R(Tx, Tx, Te) = y]
(16) Thus, given the PDFs of Re and Rs in their natural (annuities) measures, we can de- rive the PDFs of Re and Rs in the common A(Ts, Te)-measure as soon as we can evaluate EA(Tx,Te) [Ge,Tx|R(Tx, Tx, Te) = y], and EA(Tx,Ts) [Gs,Tx|R(Tx, Tx, Ts) = x]. To evaluate the payoﬀof the midcurve swaption we will make an assumption that the stochastic variables GJ,Tx, J = e, s from (13) are deterministic functions of the swap rates R(Tx, Tx, Ts) and R(Tx, Tx, Te). The integral formula for the payoﬀis: Wrec(t0) A(t0, Ts, Te) · N = EA(Ts,Te) 
[K −R(Tx, Ts, Te)]+
= = EA(Ts,Te) 
EA(Ts,Te) 
[K −R(t, Ts, Te)]+|Rs = x, Re = y

= = EA(Ts,Te)

EA(Ts,Te)

[K −A(Tx, Tx, Te) A(Tx, Ts, Te)Re + A(Tx, Tx, Ts) A(Tx, Ts, Te)Rs]+|Rs = x, Re = y
 = EA(Ts,Te) 
EA(Ts,Te) 
[K −w1(y, x)y + w2(y, x)x]+|Rs = x, Re = y

= =
Z +∞ −∞ Z +∞ −∞
EA(Ts,Te) 
[K −w1(y, x)y + w2(y, x)x]+|Rs = x, Re = y

× × PDFu
Ru,Rs,Re(z(x, y), x, y)dxdy = =
Z +∞ −∞ Z +∞ −∞
[K −w1(y, x)y + w2(y, x)x]+ PDFu
Rs,Re(x, y)dxdy,
(17) where we omitted the symbol z(x, y) in the last formula because it is fully determined by x and y due to our assumption on GJ,Tx, J = e, s. In order to use (17) we need to specify the weight functions w1(y, x) and w2(y, x) as well as the full joint distribution PDFu
Rs,Re(x, y). The latter can be constructed using the copula technique applied to distributions φs(x) from (15) and φe(y) from (16). A popular choice is the Gaussian copula: GCJoin(x, y) = φ(u, v, ρ)du dx
dv
dy,
(18) where φ(u, v, ρ) is the joint normal PDF of two univariate normal variables with correlation ρ and u = Φ−1 [cd
fs(x)] ,
v = Φ−1 [cd
fe(y)] ,
(19) 6 where Φ is the CDF of a univariate normal variable and cd
fs(x), cd
fe(y) are the CDFs corresponding to pdfs φs(x) from (15) and φe(y) from (16). 2
First order approximations The Radon-Nikodym derivative for measure change in (12) and the payoﬀin (17) depend only on the ratio of the annuities A(t, Tx, Ts) and A(t, Tx, Te). Therefore, to use the copula valuation by means of (17) it is suﬃcient to model dynamics of the ratio of annuities. A convenient way for modelling dynamics of the ratio of annuities is provided by the Terminal Swap Rate Model methodology. It covers the zero-th and the ﬁrst order approximations for the ratio. We discuss the corresponding approximations below. Deterministic Annuity Ratio: Assume that the conditional expectations in
(15-16) are independent from the respective variables x and y (we may think of GJ,Tx, J = e, s from (13), for example, as being deterministic). Then in (15) φs(x) ≡PDFs
Rs(x) and in (16) φe(y) ≡PDFe
Re(y), i.e. no change of the measure is needed and w1(y, x) = G−1
e,Tx = A(t0, Tx, Te) A(t0, Ts, Te),
w2(y, x) = G−1
s,Tx = A(t0, Tx, Ts) A(t0, Ts, Te).
(20) This is exactly the constant annuity ratio assumption used in [5] for pricing midcurve swap- tions by means of the Gaussian copula. Linear Approximation: We can approximate linearly the weights in (17) as w1(y, x) = G−1
e,Tx = = A(t0, Tx, Te) A(t0, Ts, Te)(1 + µe(y −ˆ
R(t0, Tx, Te)) + µs(x −ˆ
R(t0, Tx, Ts))),
(21) and w2(y, x) = G−1
s,Tx = = A(t0, Tx, Ts) A(t0, Ts, Te)(1 + νe(y −ˆ
R(t0, Tx, Te)) + νs(x −ˆ
R(t0, Tx, Ts))),
(22) where ˆ
R is used to underline that a measure change is needed to evaluate the corresponding quantity, so that ˆ
R(t0, Tx, Te)
=
EA(Ts,Te) [R(t, Tx, Te)] , ˆ
R(t0, Tx, Ts)
=
EA(Ts,Te) [R(t, Tx, Ts)] ,
(23) 7 and both w1(y, x), w2(y, x) are A(Ts, Te)-martingales. Equating coeﬃcients under x and y in w1(y, x) −w2(y, x) = 1, we see that the four coeﬃcients of linear expansion in (21) and (22) are actually spanned by two parameters σe and σs as µs = A(t0, Ts, Te) A(t0, Tx, Te)σs,
µe = A(t0, Ts, Te) A(t0, Tx, Te)σe, νs = A(t0, Ts, Te) A(t0, Tx, Ts)σs,
νe = A(t0, Ts, Te) A(t0, Tx, Ts)σe.
(24) We shall approximate linearly Ge,Tx = w1(y, x)−1 and Gs,Tx = w2(y, x)−1: Ge,Tx
≈
A(t0, Ts, Te)
A(t0, Tx, Te)(1 −µe(y −R(t0, Tx, Te)) −µs(x −˜
R(t0, Tx, Ts))),
(25) ˜
R(t0, Tx, Ts)
=
EA(Tx,Te) [R(t, Tx, Ts)] ,
(26) and Gs,Tx
≈
A(t0, Ts, Te)
A(t0, Tx, Ts)(1 −νe(y −˜
R(t0, Tx, Te)) −νs(x −R(t0, Tx, Ts))),
(27) ˜
R(t0, Tx, Te)
=
EA(Tx,Ts) [R(t, Tx, Te)] ,
(28) so that Ge,Tx is A(Tx, Te)-martingale and Gs,Tx is A(Tx, Ts)-martingale. Using Equations (25)-(28) we derive the following Lemma: Lemma 2. Under an assumption that the long and the short swap rates are approximately Gaussian the marginals of the joint distribution of the long and the short swap rates in A(t, Ts, Te)-measure are φs(x)
≈
PDFs
Rs(x)

1 −

νs + νeρΣe Σs 
(x −R(t0, Tx, Ts))

, φe(y)
≈
PDFe
Re(y)

1 −

µe + µsρΣs Σe 
(y −R(t0, Tx, Te))

. (29) where Σe, Σs are the volatilities of the long and the short swap rates: Σ2
e = EA(Tx,Te) 
(y −R(t0, Tx, Te))2
,
Σ2
s = EA(Tx,Ts) 
(x −R(t0, Tx, Ts))2
. 8 Proof: Under an assumption that the long and the short swap rates are approximately Gaussian we can project y on to x as: EA(Tx,Ts)[y|x]
=
EA(Tx,Ts) [R(Tx, Tx, Te)|R(Tx, Tx, Ts) = x] =
EA(Tx,Ts) [R(Tx, Tx, Te)] + ρΣe Σs
(x −R(t0, Tx, Ts)).
(30) We can evaluate EA(Tx,Ts) [Gs,Tx|R(Tx, Tx, Ts) = x]
=
A(t, Ts, Te)
A(t, Tx, Ts) 
1 −

νs + νeρΣe Σs 
(x −R(t, Tx, Ts))

, which leads to the expression for the ﬁrst marginal. Similarly we derive the expression for the second marginal. Integrating R(t, Tx, Te) with respect to φe(y) and R(t, Tx, Ts) with respect to φs(y) we derive Lemma 3. If the long and the short rates are approximately Gaussian then the linear ap- proximation for the weights w1(y, x) and w2(y, x) leads to: ˆ
R(t, Tx, Te)
=
EA(Ts,Te) [R(Tx, Tx, Te)] = R(t0, Tx, Te) −(µeΣe + µsρΣs)Σe, ˆ
R(t, Tx, Ts)
=
EA(Ts,Te) [R(Tx, Tx, Ts)] = R(t0, Tx, Ts) −(νsΣs + νeρΣe)Σs.
(31) In practice, it may be convenient to calcuate ˜
R(t0, Tx, Ts) from (26) and ˜
R(t0, Tx, Ts) from (28). This can be done by matching w1(y, x)Ge,Tx = 1 and w2(y, x)Gs,Tx = 1 in the expectation relative to A(Ts, Te)-measure. Lemma 4. If the long and the short rates are approximately Gaussian then the linear ap- proximation for the weights w1(y, x) and w2(y, x) leads to: ˜
R(t, Tx, Te)
=
EA(Tx,Te) [R(Tx, Tx, Te)] =
R(t0, Tx, Te) −(µeΣe + µsρΣs)Σe + (νeΣe + νsρΣs)Σe, ˜
R(t, Tx, Ts)
=
EA(Tx,Ts) [R(Tx, Tx, Te)] =
R(t0, Tx, Ts) −(νsΣs + νeρΣe)Σs + (µsΣs + µeρΣe)Σs.
(32) Thus, in order to price a midcurve swaption we just need two extra parameters σe and σs from (24).
Together with the swap rates distributions PDFs
Rs(x), PDFe
Re(y) and the correlation between them, σe and σs uniquely determine the midcurve swaption price in the Gaussian copula model via (17),(21),(22), (29) and (31). 9 Log Linear Approximation: The ﬁrst order approximation does not immediately prevent weight coeﬃcients w1(y, x) and w2(y, x) from going negative. This can be addressed by an exponential approximation for w2(y, x): w2(y, x)−1 = Gs,Tx = αse−νe(y−˜
R(t,Tx,Te))−νs(x−R(t,Tx,Ts))),
(33) where ˜
R(t, Tx, Te)
=
EA(Tx,Ts) [R(Tx, Tx, Te)]
(34) is used to underline that Gs,Tx and R(t, Tx, Ts) are A(t, Tx, Ts)-martingales. The relation w1(y, x) −w2(y, x) = 1 allows us to recover w1(y, x) from w2(y, x). We can evaluate the coeﬃcient α if we assume that the long and the short rates are approximately Gaussian. Let’s project y on to x as: EA(Tx,Ts)[y|x]
=
EA(Tx,Ts) [R(Tx, Tx, Te)|R(Tx, Tx, Ts) = x] =
EA(Tx,Ts) [R(Tx, Tx, Te)] + ρΣe Σs
(x −R(t0, Tx, Ts)),
(35) so that y −˜
R(t, Tx, Te)
=
p 1 −ρ2Σez + EA(Tx,Ts)[y −R(t, Tx, Te)|x] =
p 1 −ρ2Σez + ρΣe Σs
(x −R(t, Tx, Ts)).
(36) with z ∼N(0, 1). With this assumption we have EA(Tx,Ts) [Gs,Tx]
=
αseν2
e(1−ρ2)Σ2
e/2e((νsΣs+νeρΣe)2/2) =
αse(ν2
eΣ2
e+2ρνeνsΣeΣs+ν2
sΣ2
s)/2 =
A(t, Ts, Te)
A(t, Tx, Ts). αs
=
A(t, Ts, Te)
A(t, Tx, Ts)e−(ν2
eΣ2
e+2ρνeνsΣeΣs+ν2
sΣ2
s)/2.
(37) We can now evaluate EA(Tx,Ts) [Gs,Tx|R(Tx, Tx, Ts) = x]
=
αsEA(Tx,Ts) h
e−νe√ 1−ρ2Σezi
e−(νs+νeρ Σe Σs)(x−R(t0,Tx,Ts)) =
A(t, Ts, Te)
A(t, Tx, Ts)e−(ρνeΣe+νsΣs)2 2
e−(νs+νeρ Σe Σs)(x−R(t,Tx,Ts)) (38) This leads to the next result: 10 Lemma 5. If the long and the short rates are approximately Gaussian then the log linear approximation for the weights w2(y, x) φs(x)
≈
PDFs
Rs(x)e−(ρνeΣe+νsΣs)2 2
e−(νs+νeρ Σe Σs)(x−R(t0,Tx,Ts)), ˆ
R(t, Tx, Ts)
=
EA(Ts,Te) [R(Tx, Tx, Ts)] = R(t0, Tx, Ts) −(νsΣs + νeρΣe)Σs.
(39) Using the relation w1(y, x) −w2(y, x) = 1 we can reconstruct Ge,Tx by numerical inte- gration. To get less precise but more tractable formulae, instead, we evaluate Ge,Tx as an exponential martingale: w1(y, x)−1 = Ge,Tx = αee−µe(y−R(t,Tx,Te))−µs(x−˜
R(t,Tx,Ts)), where ˜
R(t, Tx, Ts)
=
EA(Tx,Te) [R(Tx, Tx, Ts)] .
(40) Similarly to (37) we obtain αe
=
A(t, Ts, Te)
A(t, Tx, Te)e−(µ2
eΣ2
e+2ρµeµsΣeΣs+µ2
sΣ2
s)/2,
(41) EA(Tx,Te) [Ge,Tx|R(Tx, Tx, Te) = y]
=
αeEA(Tx,Te) h
e−µs√ 1−ρ2Σszi
e−(µe+µsρ Σs Σe)(y−R(t0,Tx,Te)) =
A(t, Ts, Te)
A(t, Tx, Te)e−(ρµsΣs+µeΣe)2 2
e−(µe+µsρ Σs Σe)(y−R(t,Tx,Te)). (42) Similar to Lemma 5 we derive: Lemma 6. If the long and the short rates are approximately Gaussian then the exponential approximation for the weight w1(y, x) leads to: φe(y)
≈
PDFe
Re(y)e−(ρµsΣs+µeΣe)2 2
e−(µe+µsρ Σs Σe)(y−R(t0,Tx,Te)), ˆ
R(t, Tx, Te)
=
EA(Ts,Te) [R(Tx, Tx, Te)] = R(t0, Tx, Te) −(µeΣe + µsρΣs)Σe.
(43) 11 In the log linear approximation for the weights we can explicitly evaluate ˜
R(t, Tx, Ts) from (40) and ˜
R(t, Tx, Te) from (34) by observing that w1(y, x) and w2(y, x) are A(t, Ts, Te)- martingales. We ﬁnd, similarly to the linear case: ˜
R(t, Tx, Ts)
=
R(t0, Tx, Ts) −(νsΣs + νeρΣe)Σs + (µsΣs + µeρΣe)Σs, w1(y, x)
=
A(t, Tx, Te)
A(t, Ts, Te)e−(µ2
eΣ2
e+2ρµeµsΣeΣs+µ2
sΣ2
s)/2 × ×
eµe(y−ˆ
R(t,Tx,Te))+µs(x−ˆ
R(t,Tx,Ts)),
(44) and ˜
R(t, Tx, Te)
=
R(t0, Tx, Te) −(µeΣe + µsρΣs)Σe + (νeΣe + νsρΣs)Σe. w2(y, x)
=
A(t, Tx, Ts)
A(t, Ts, Te)e−(ν2
eΣ2
e+2ρνeνsΣeΣs+ν2
sΣ2
s)/2 × ×
eνe(y−ˆ
R(t,Tx,Te))+νs(x−ˆ
R(t,Tx,Ts)).
(45) With these exponential approximations for both Gs,Tx and Ge,Tx we shall chose parameters νe, νs, µe, and µs to minimise EA(Ts,Te) 
(w1(y, x) −w2(y, x))2 (46) in A(t, Ts, Te)-measure. Expanding up to the second order in volatilities Σe, Σs we see that as soon as parameters µe, µs, νe and νs are related as in (24) via: µs = A(t0, Ts, Te) A(t0, Tx, Te)σs,
µe = A(t0, Ts, Te) A(t0, Tx, Te)σe, νs = A(t0, Ts, Te) A(t0, Tx, Ts)σs,
νe = A(t0, Ts, Te) A(t0, Tx, Ts)σe,
(47) the variance of w1(y, x) −w2(y, x) is zero up to the second order in Σe, Σs, i.e.: w1(y, x) −w2(y, x) ≈1 + o(Σ2
e, Σ2
s).
(48) Again as in the linear case, in order to price a midcurve swaption we just need two extra parameters σe and σs from (47).
Together with the swap rates distributions PDFs
Rs(x) and PDFe
Re(y), the correlation between them, σe and σs uniquely determine the midcurve swaption price in the Gaussian copula model via (17),(44), (45), from Lemma 5 and from Lemma 6. 12 3
Estimating parameters σe and σs and some numerical results. Parameters σe and σs introduced in (24) for the linear approximation and in (47) for the log linear approximation, are related to the covariances between swap annuities’ ratios and the swap rates via CovA(Tx,Te)⟨A(Tx, Ts, Te) A(Tx, Tx, Te), R(Tx, Tx, Te)⟩
=
−A(Tx, Ts, Te)2 A(Tx, Tx, Te)2
",0
"The task we consider is portfolio construction in a speculative market, a fundamental problem in
modern ﬁnance. While various empirical works now exist to explore deep learning in ﬁnance, the theory
side is almost non-existent. In this work, we focus on developing a theoretical framework for under-
standing the use of data augmentation for deep-learning-based approaches to quantitative ﬁnance. The
proposed theory clariﬁes the role and necessity of data augmentation for ﬁnance; moreover, our theory
implies that a simple algorithm of injecting a random noise of strength
√ ∣rt−1∣to the observed return rt is
better than not injecting any noise and a few other ﬁnancially irrelevant data augmentation techniques.1 1
Introduction There is an increasing interest in applying machine learning methods to problems in the ﬁnance industry.
This trend has been expected for almost forty years (Fama, 1970), when well-documented and ﬁne-grained
(minute-level) data of stock market prices became available. In fact, the essence of modern ﬁnance is fast
and accurate large-scale data analysis (Goodhart and O’Hara, 1997), and it is hard to imagine that machine
learning should not play an increasingly crucial role in this ﬁeld. In contemporary research, the central
theme in machine-learning based ﬁnance is to apply existing deep learning models to ﬁnancial time-series
prediction problems (Imajo et al., 2020; Buehler et al., 2019; Jay et al., 2020; Imaki et al., 2021; Jiang et al.,
2017; Fons et al., 2020; Lim et al., 2019; Zhang et al., 2020), which have demonstrated the hypothesized
usefulness of deep learning for the ﬁnancial industry.
However, one major existing gap in this interdisciplinary ﬁeld of deep-learning ﬁnance is the lack of a
theory relevant to justify ﬁnance-oriented algorithm design. The goal of this work is to propose such a
framework, where machine learning practices are analyzed in a traditional ﬁnancial-economic utility theory
setting. Our theory implies that a simple theoretically motivated data augmentation technique is better for
the task portfolio construction than not injecting any noise at all or some naive noise injection methods
that have no theoretical justiﬁcation. To summarize, our main contributions are (1) to demonstrate how
we can use utility theory to analyze practices of deep-learning-based ﬁnance, and (2) to theoretically study
the role of data augmentation in the deep-learning-based portfolio construction problem. Organization:
the next section discusses the main related works; Section 3 provides the requisite ﬁnance background
for understanding this work; Section 4 presents our theoretical contributions, which is a framework for
understanding machine-learning practices in the portfolio construction problem; Section 5 describes how to
practically implement the theoretically motivated algorithm; section 6 validates the theory with experiments
on toy and real data. 1This is the full-length version of our work published at the 3rd ACM International Conference on AI in Finance (ICAIF’22).
See https://doi.org/10.1145/3533271.3561720 for the shorter published version.
The code is available at: https://github.com/pfnet-research/Finance_data_augmentation_ICAIF2022 1 arXiv:2106.04114v3  [cs.LG]  22 Dec 2022 Figure 1: Performance (measured by the Sharpe ratio) of various algorithms on MSFT (Microsoft) from 2018-2020.
Directly applying generic machine learning methods, such as weight decay, fails to improve the vanilla model. The
proposed method show signiﬁcant improvement. 2
Related Works Existing deep learning ﬁnance methods. In recent years, various empirical approaches to apply state-
of-the-art deep learning methods to ﬁnance have been proposed (Imajo et al., 2020; Ito et al., 2020; Buehler
et al., 2019; Jay et al., 2020; Imaki et al., 2021; Jiang et al., 2017; Fons et al., 2020). The interested readers
are referred to (Ozbayoglu et al., 2020) for detailed descriptions of existing works. However, we notice that
one crucial gap is the complete lack of theoretical analysis or motivation in this interdisciplinary ﬁeld of
AI-ﬁnance. This work makes one initial step to bridge this gap. One theme of this work is that ﬁnance-
oriented prior knowledge and inductive bias is required to design the relevant algorithms. For example,
Ziyin et al. (2020) shows that incorporating prior knowledge into architecture design is key to the success of
neural networks and applied neural networks with periodic activation functions to the problem of ﬁnancial
index prediction. Imaki et al. (2021) shows how to incorporate no-transaction prior knowledge into network
architecture design when transaction cost is incorporated.
In fact, most generic and popular machine learning techniques are proposed and have been tested for
standard ML tasks such as image classiﬁcation or language processing. Directly applying the ML methods
that work for image tasks is unlikely to work well for ﬁnancial tasks, where the nature of the data is diﬀerent.
See Figure 1, where we show the performance of a neural network directly trained to maximize wealth return
on MSFT during 2019-2020. Using popular, generic deep learning techniques such as weight decay or dropout
does not result in any improvement over the baseline. In contrast, our theoretically motivated method does.
Combining the proposed method with weight decay has the potential to improve the performance a little
further, but the improvement is much lesser than the improvement of using the proposed method over the
baseline. This implies that a generic machine learning method is unlikely to capture well the inductive
biases required to tackle a ﬁnancial task. The present work proposes to ﬁll this gap by showing how ﬁnance
knowledge can be incorporated into algorithm design.
Data augmentation. Consider a training loss function of the additive form L = 1 N ∑i ℓ(xi,yi) for N pairs
of training data points {(xi,yi)}N
i=1. Data augmentation amounts to deﬁning an underlying data-dependent
distribution and generating new data points stochastically from this underlying distribution. A general way
to deﬁne data augmentation is to start with a datum-level training loss and transform it to an expectation
over an augmentation distribution P (z∣(xi,yi)) (Dao et al., 2019), ℓ(xi,yi) →E(zi,gi)∼P (z,g∣(xi,yi))[ℓ(zi,gi)],
and the total training loss function becomes Laug = 1 N N
∑
i=1
E(zi,gi)∼P (z,g∣(xi,yi))[ℓ(zi,gi)].
(1) One common example of data augmentation is injecting isotropic gaussian noise to the input (Shorten and
Khoshgoftaar, 2019; Fons et al., 2020), which is equivalent to setting P(z,g∣(xi,yi)) ∼δ(g−yi)exp[−(z −xi)T(z −xi)/(2σ2)]
for some speciﬁed strength σ2. Despite the ubiquity of data augmentation in deep learning, existing works
are often empirical in nature (Fons et al., 2020; Zhong et al., 2020; Shorten and Khoshgoftaar, 2019; Antoniou
et al., 2017). For a relevant example, Fons et al. (2020) empirically evaluates the eﬀect of diﬀerent types
of data augmentation in a ﬁnancial series prediction task. Dao et al. (2019) is one major recent theoretical
work that tries to understand modern data augmentation theoretically; it shows that data augmentation is 2 approximately learning in a special kernel. He et al. (2019) argues that data augmentation can be seen as an
eﬀective regularization. However, no theoretically motivated data augmentation method for ﬁnance exists
yet. One major challenge and achievement of this work is to develop a theory that bridges the traditional
ﬁnance theory and machine learning methods. In the next section, we introduce the portfolio theory. 3
Background: Markowitz Portfolio Theory How to make optimal investment in a ﬁnancial market is the central concern of portfolio theory.
One
unfamiliar with the portfolio theory may easily confuse the task the portfolio construction with wealth
maximization trading or future price prediction. Before we introduce the portfolio theory, we ﬁrst stress
that the task of portfolio construction is not equivalent to wealth maximization or accurate price prediction.
One can construct an optimal portfolio without predicting the price or maximizing the wealth increase.
Consider a market with an equity (a stock) and a ﬁxed-interest rate bond (a government bond). We
denote the price of the equity at time step t as St, and the price return is deﬁned as rt = St+1−St St
, which
is a random variable with variance Ct, and the expected return gt ∶= E[rt]. Our wealth at time step t is
Wt = Mt + ntSt, where Mt is the amount of cash we hold, and ni the shares of stock we hold for the i-th
stock. As in the standard ﬁnance literature, we assume that the shares are inﬁnitely divisible. Usually, a
positive n denotes holding (long) and a negative n denotes borrowing (short). The wealth we hold initially
is W0 > 0, and we would like to invest our money on the equity. We denote the relative value of the stock we
hold as πt = ntSt Wt . π is called a portfolio. The central challenge in portfolio theory is to ﬁnd the best π. At
time t, our wealth is Wt; after one time step, our wealth changes due to a change in the price of the stock
(setting the interest rate to be 0): ∆Wt ∶= Wt+1 −Wt = Wtπtrt. The goal is to maximize the wealth return
Gt ∶= πt ⋅rt at every time step while minimizing risk 2. The risk is deﬁned as the variance of the wealth
change:
Rt ∶= R(πt) ∶= Varrt[Gt] = (E[r2
t ] −g2
t )π2
t = π2
t Ct.
(2) The standard way to control risk is to introduce a “risk regularizer” that punishes the portfolios with a large
risk (Markowitz, 1959; Rubinstein, 2002).3 Introducing a parameter λ for the strength of regularization (the
factor of 1/2 appears for convention), we can now write down our objective: π∗
t = arg max
π
U(π) ∶= arg max
π
[πTGt −λ 2 R(π)].
(3) Here, U stands for the utility function; λ can be set to be the desired level of risk-aversion. When gt and Ct is
known, this problem can be explicitly solved. However, one main problem in ﬁnance is that its data is highly
limited and we only observe one particular realized data trajectory, and gt and Ct are hard to estimate.
This fact motivates for the necessity of data augmentation and synthetic data generation in ﬁnance (Assefa,
2020). In this paper, we treat the case where there is only one asset to trade in the market, and the task of
utility maximization amounts to ﬁnding the best balance between cash-holding and investment. The equity
we are treating is allowed to be a weighted combination of multiple stocks (a portfolio of some public fund
manager, for example), and so our formalism is not limited to single-stock situations. In section C.1, we
discuss portfolio theory with multiple stocks. 4
Portfolio Construction as a Training Objective Recent advances have shown that the ﬁnancial objectives can be interpreted as training losses for an appro-
priately inserted neural-network model (Ziyin et al., 2019; Buehler et al., 2019). It should come as no surprise
that the utility function (3) can be interpreted as a loss function. When the goal is portfolio construction,
we parametrize the portfolio πt = πw(xt) by a neural network with weights w, and the utility maximization
problem becomes a maximization problem over the weights of the neural network. The time-dependence is 2It is important to not to confuse the price return rt with the wealth return Gt.
3In principle, any concave function in Gt can be a risk regularizer from classical economic theory (Von Neumann and
Morgenstern, 1947). One common alternative would be R(G) = log(G) (Kelly Jr, 2011), and our framework can be easily
extended to such cases. 3 modeled through the input to the network xt, which possibly consists of the available information at time
t for determining the future price.4
The objective function (to be maximized) plus a pre-speciﬁed data
augmentation transform xt →zt with underlying distribution p(z∣xt) is then π∗
t = arg max
w { 1 T T
∑
t=1
Et [Gt(πw(zt))] −λVart[Gt(πw(zt))]},
(4) where Et ∶= Ezt∼p(z∣xt). In this work, we abstract away the details of the neural network to approximate π.
We instead focus on studying the maximizers of this equation, which is a suitable choice when the underlying
model is a neural network because one primary motivation for using neural networks in ﬁnance is that they
are universal approximators and are often expected to ﬁnd such maximizers (Buehler et al., 2019; Imaki
et al., 2021).
The ultimate ﬁnancial goal is to construct π∗such that the utility function is maximized with respect to
the true underlying distribution of St, which can be used as the generalization loss (to be maximized): π∗
t = arg max
πt {ESt [Gt(π)] −λVarSt[Gt(π)]}.
(5) Note the diﬀerence in taking the expectation between Eq (4) and (5) is that Et is computed with respect to
the training set we hold, while ESt ∶= ESt∼p(St) is computed with respect to the underlying distribution of St
given its previous prices. We used the same short-hands for Vart and VarSt. Technically, the true utility we
deﬁned is an in-sample counterfactual objective, which roughly evaluates the expected utility to be obtained
if we restart from yesterday, which is a relevant measure for ﬁnancial decision making. In Section 4.5, we
also analyze the out-of-sample performance when the portfolio is static. 4.1
Standard Models of Stock Prices The expectations in the true objective Equation (5) need to be taken with respect to the true underlying
distribution of the stock price generation process. In general, the price follows the following stochastic process
∆St = f({Si}t
i=1) + g({Si}t
i=1)ηt for a zero-mean and unit variance random noise ηt; the term f reﬂects the
short-term predictability of the stock price based on past prices, and g reﬂects the extent of unpredictability
in the price. A key observation in ﬁnance is that g is non-stationary (heteroskedastic) and price-dependent
(multiplicative). One model is the geometric Brownian motion (GBM) St+1 = (1 + r)St + σtStηt,
(6) which is taken as the minimal standard model of the motion of stock prices (Mandelbrot, 1997; Black
and Scholes, 1973); this paper also assumes the GBM model as the underlying model.
Here, we note
that the theoretical problem we consider can be seen as a discrete-time version of the classical Merton’s
portfolio problem (Merton, 1969). The more ﬂexible Heston model (Heston, 1993) takes the form dSt =
rStdt + √νtStdWt, where νt is the instantaneous volatility that follows its own random walk, and dWt is
drawn from a Gaussian distribution. Despite the simplicity of these models, the statistical properties of
these models agree well with the known statistical properties of the real ﬁnancial markets (Drˇ
agulescu and
Yakovenko, 2002). The readers are referred to (Karatzas et al., 1998) for a detailed discussion about the
meaning and ﬁnancial signiﬁcance of these models. 4.2
No Data Augmentation In practice, there is no way to observe more than one data point for a given stock at a given time t. This
means that it can be very risky to directly train on the raw observed data since nothing prevents the model
from overﬁtting to the data. Without additional assumptions, the risk is zero because there is no randomness
in the training set conditioning on the time t. To control this risk, we thus need data augmentation. One
can formalize this intuition through the following proposition, whose proof is given in Section C.3. 4It is helpful to imagine xt as, for example, the prices of the stocks in the past 10 days. 4 Proposition 1. (Utility of no-data-augmentation strategy.) Let the price trajectory be generated with GBM
in Eq. (6) with initial price S0, then the true utility for the no-data-augmentation strategy is Uno−aug = [1 −2Φ(−r/σ)]r −λ 2 σ2
(7) where U(π) is the utility function deﬁned in Eq. (3); Φ is the c.d.f. of a standard normal distribution. This means that, the larger the volatility σ, the smaller is the utility of the no-data-augmentation strategy.
This is because the model may easily overﬁt to the data when no data augmentation is used. In the next
section, we discuss the case when a simple data augmentation is used. 4.3
Additive Gaussian Noise While it is still far from clear how the stock price is correlated with the past prices, it is now well-recognized
that VarSt[St∣St−1] ≠0 (Mandelbrot, 1997; Cont, 2001). This motivates a simple data augmentation tech-
nique to add some randomness to the ﬁnancial sequence we observe, {S1,...,ST +1}. This section analyzes a
vanilla version of data augmentation of injecting simple Gaussian noise, compared to a more sophisticated
data augmentation method in the next section. Here, we inject random Gaussian noises ϵt ∼N(0,ρ2) to St
during the training process such that zt = St + ϵ. Note that the noisiﬁed return needs to be carefully deﬁned
since noise might also appear in the denominator, which may cause divergence; to avoid this problem, we
deﬁne the noisiﬁed return to be ˜
rt ∶= zt+1−zt St
, i.e., we do not add noise to the denominator. Theoretically, we
can ﬁnd the optimal strength ρ∗of the gaussian data augmentation to be such that the true utility function
is maximized for a ﬁxed training set. The result can be shown to be (ρ∗)2 = σ2 2r
∑t(rtS2
t )2 ∑t rtS2
t
.
(8) The fact the ρ∗depends on the prices of the whole trajectory reﬂects the fact that time-independent data
augmentation is not suitable for a stock price dynamics prescribed by Eq. (6), whose inherent noise σStηt is
time-dependent through the dependence on St. Finally, we can plug in the optimal ρ∗to obtain the optimal
achievable strategy for the additive Gaussian noise augmentation. As before, the above discussion can be
formalized, with the true utility given in the next proposition (proof in Section C.4). Proposition 2. (Utility of additive Gaussian noise strategy.) Under additive Gaussian noise strategy, and
let other conditions the same as in Proposition 1, the true utility is UAdd =
r2 2λσ2T ESt [(∑t rtSt)2 ∑t(rtSt)2 Θ(∑
t
rtS2
t )],
(9) where Θ is the Heaviside step function. 4.4
Multiplicative Gaussian Noise In this section, we derive a general kind of data augmentation for the price trajectories speciﬁed by the GBM
and the Heston model. From the previous discussions, one might expect that a better kind of augmentation
should have ρ = ρ0St, i.e., the injected noise should be multiplicative; however, we do not start from imposing
ρ →ρSt; instead, we consider ρ →ρt, i.e., a general time-dependent noise. In the derivation, one can ﬁnd an
interesting relation for the optimal augmentation strength: (ρ∗
t+1)2 + (ρ∗
t )2 = σ2 2r rtS2
t .
(10) The following proposition gives the true utility of using this data augmentation (derivations in Section C.5). Proposition 3. (Utility of general multiplicative Gaussian noise strategy.) Under general multiplicative
noise augmentation strategy, and let other conditions the same as in Proposition 1, then the true utility is Umult =
r2 2λσ2 [1 −Φ(−r/σ)].
(11) 5 Combining the above propositions, we can prove the main theorem of this work ((Proof in Section C.6)),
which shows that the mean-variance utility of the proposed method is strictly higher than that of no data-
augmention and that of additive Gaussian noise. Theorem 1. If σ ≠0, then Umult > Uadd and Umult > Uno−aug with probability 1. Heston Model and Real Price Augmentation. We also consider the more general Heston model. The
derivation proceeds similarly by replacing σ2 →ν2
t ; one arrives at the relation for optimal augmentation:
(ρ∗
t+1)2 + (ρ∗
t )2 =
1 2rν2
t rtS2
t . One quantity we do not know is the volatility νt, which has to be estimated by
averaging over the neighboring price returns. One central message from the above results is that one should
add noises with variance proportional to rtS2
t to the observed prices for augmenting the training set. 4.5
Stationary Portfolio In the previous sections, we have discussed the case when the portfolio is dynamic (time-dependent). One
slight limitation of the previous theory is that one can only compare the in-sample counterfactual performance
of a dynamic portfolio. Here, we alternatively motivate the proposed data augmentation technique when
the model is a stationary portfolio.
One can show that, for a stationary portfolio, the proposed data
augmentation technique gives the overall optimal performance. Theorem 2. Under the multiplicative data augmentation strategy, the in-sample counterfactual utility and
the out-of-sample utility is optimal among all stationary portfolios. Remark. See Section C.8 for a detailed discussion and the proof. Stationary portfolios are important in
ﬁnancial theory and can be shown to be optimal even among all dynamic portfolios in some situations (Cover
and Thomas, 2006; Merton, 1969). While restricting to stationary portfolios allows us to also compare on
out-of-sample performance, the limitation is that a stationary portfolio is less relevant for a deep learning
model than the dynamical portfolios considered in the previous sections. 4.6
General Framework So far, we have been analyzing the data augmentation for speciﬁc examples of the utility function and the
data augmentation distribution to argue that certain types of data augmentation is preferable. Now we
outline how this formulation can be generalized to deal with a wider range of problems, such as diﬀerent
utility functions and diﬀerent data augmentations. This general framework can be used to derive alternative
data augmentations schemes if one wants to maximize other ﬁnancial metrics other than the Sharpe ratio,
such as the Sortino ratio (Estrada, 2006), or to incorporate regularization eﬀect that into account of the
heavy tails of the prices distribution.
For a general utility function U = U(x,π) for some data point x that describes the current state of the
market, and π that describes our strategy in this market state, we would like to ultimately maximize max
π
V (π),
for V (π) = Ex[U(x,π)]
(12) However, only observing ﬁnitely many data points, we can only optimize the empirical loss with respect to
some θ−parametrized augmentation distribution Pθ: ˆ
π(θ) = arg max
π
1 N N
∑
i
Ezi∼pθ(z∣xi)[U(zi,πi)].
(13) The problem we would like to solve is to ﬁnd the eﬀect of using such data augmentation on the true utility
V , and then, if possible, compare diﬀerent data augmentations and identify the better one. Surprisingly,
this is achievable since V = V (ˆ
π(θ)) is now also dependent on the parameter θ of the data augmentation.
Note that the true utility has to be found with respect to both the sampling over the test points and the
sampling over the N-sized training set: V (ˆ
π(θ)) = Ex∼p(x)E{xi}N∼pN(x)[U(x, ˆ
π(θ))]
(14) 6 Figure 2:
Experiment on geometric brownian motion; S0 = 1, r = 0.005, σ = 0.04.
Left: Examples of prices
trajectories in green; the black line shows the expected value of the price. Right: Comparison with other related
data augmentation techniques. The black dashed line shows the optimal achievable Sharpe ratio. We see that the
proposed method stay close to the optimality across a 600-step trading period as the theory predicts. In principle, this allows one to identify the best data augmentation for the problem at hand: θ∗= arg max
θ
V (ˆ
π(θ))arg max
θ
Ex∼p(x)E{xi}N∼pN(x) [U (x,arg max
π
1 N N
∑
i
Ezi∼pθ(z∣xi)[U(zi,πi)])],
(15) and the analysis we performed in the previous sections is simply a special case of obtaining solutions to this
maximization problem. Moreover, one can also compare two diﬀerent parametric augmentation distributions;
let their parameter be denoted as θα and θβ respectively, then we can say that data augmentation α is better
than β if and only if maxθα V (ˆ
π(θα)) > maxθβ V (ˆ
π(θβ)). This general formulation can also have applicability
outside the ﬁeld of ﬁnance because one can interpret the utility U as a standard machine learning loss function
and π as the model output. This procedure also mimics the procedure of ﬁnding a Bayes estimator in the
statistical decision theory (Wasserman, 2013), with θ being the estimator we want to ﬁnd; we outline an
alternative general formulation to ﬁnd the “minimax” augmentation in Section C.2. 5
Algorithms Our results strongly motivate for a specially designed data augmentation for ﬁnancial data. For a data point
consisting purely of past prices (St,...,St+L,St+L+1) and the associated returns (rt,...,rt+L−1,rt+L), we use
x = (St,...,St+L) as the input for our model f, possibly a neural network, and use St+L+1 as the unseen
future price for computing the training loss. Our results suggests that we should randomly noisify both the
input x and St+L+1 at every training step by
⎧
⎪
⎪
⎨
⎪
⎪
⎩ Si →Si + c
√ ˆ
σ2
i ∣ri∣S2
i ϵi
for Si ∈x;
St+L+1 →St+L+1 + c
√ ˆ
σ2
i ∣rt+L∣S2
t+Lϵt+L+1;
(16) where ϵi are i.i.d. samples from N(0,1), and c is a hyperparameter to be tuned. While the theory suggests
that c should be 1/2, it is better to make it a tunable-parameter in algorithm design for better ﬂexibility; ˆ
σt
is the instantaneous volatility, which can be estimated using standard methods in ﬁnance (Degiannakis and
Floros, 2015). One might also assume ˆ
σ into c. 5.1
Using return as inputs Practically and theoretically, it is better and standard to use the returns x = (rt,...,rt+L−1,rt+L) as the input,
and the algorithm can be applied in a simpler form:
⎧
⎪
⎪
⎨
⎪
⎪
⎩ ri →ri + c
√ ˆ
σ2
i ∣ri∣ϵi
for ri ∈x;
rt+L →rt+L + c
√ ˆ
σ2
i ∣rt+L∣ϵt+L+1.
(17) 7 Table 1: Sharpe ratio on S&P 500 by sectors; the larger the better. Best performances in Bold. Industry Sectors
# Stock
Merton
no aug.
weight decay
additive aug.
naive mult.
proposed Communication Services
9
−0.06±0.04
−0.06±0.04
−0.06±0.27
0.22±0.18
0.20±0.21
0.33±0.16
Consumer Discretionary
39
−0.01±0.03
−0.07±0.03
−0.06±0.10
0.48±0.10
0.41±0.09
0.64±0.08
Consumer Staples
27
0.05±0.03
0.24±0.03
0.23±0.11
0.36±0.08
0.34±0.09
0.35±0.07
Energy
17
0.07±0.03
0.03±0.03
−0.02±0.12
0.70±0.09
0.52±0.10
0.91±0.10
Financials
46
−0.57±0.04
−0.61±0.03
−0.61±0.09
−0.06±0.10
−0.13±0.09
0.18±0.08
Health Care
44
0.23±0.04
0.60±0.04
0.61±0.11
0.86±0.09
0.81±0.09
0.83±0.07
Industrials
44
−0.09±0.03
−0.11±0.03
−0.11±0.08
0.36±0.08
0.28±0.08
0.48±0.08
Information Technology
41
0.41±0.04
0.41±0.04
0.41±0.11
0.67±0.10
0.74±0.11
0.79±0.09
Materials
19
0.07±0.03
0.06±0.03
0.03±0.14
0.47±0.13
0.43±0.13
0.53±0.10
Real Estate
22
−0.14±0.04
−0.39±0.03
−0.40±0.12
0.05±0.10
0.05±0.09
0.19±0.07
Utilities
24
−0.29±0.02
−0.29±0.02
−0.28±0.07
−0.01±0.06
−0.00±0.06
0.15±0.04 S&P500 Avg.
365
−0.02±0.04
−0.00±0.04
−0.01±0.04
0.39±0.03
0.35±0.03
0.51±0.03 5.2
Equivalent Regularization on the output One additional simpliﬁcation can be made by noticing the eﬀect of injecting noise to rt+L on the training loss
is equivalent to a regularization. We show in Section B that, under the GBM model, the training objective
can be written as arg max
bt { 1 T T
∑
t=1
Ez [Gt(π)] −λc2ˆ
σ2
t ∣rt∣π2
t },
(18) where the expectation over x is now only taken with respect to the input.
This means that the noise
injection on the rt+L is equivalent to adding a L2 regularization on the model output πt. This completes
the main proposed algorithm of this work. We discuss a few potential variants in Section B. Also, it is well
known that the magnitude of ∣rt∣has strong time-correlation (i.e., a large ∣rt∣suggests a large ∣rt+1∣) (Lux
and Marchesi, 2000; Cont and Bouchaud, 1997; Cont, 2007), and this suggests that one can also use the
average of the neighboring returns to smooth the ∣rt∣factor in the last term for some time-window of width
τ: ∣rt∣→∣ˆ
rt∣= 1 τ ∑τ
0 ∣rt−τ∣. In our S&P500 experiments, we use this smoothing technique with τ = 20. 6
Experiments We validate our theoretical claim that using multiplicative noise with strength √r is better than not using any
data augmentation or using a data augmentation that is not suitable for the nature of portfolio construction
(such as an additive Gaussian noise). We emphasize that the purpose of this section is for demonstrating
the relevance of our theory to real ﬁnancial problems, not for establishing the proposed method as a strong
competitive method in the industry. We start with a toy dataset that follows the theoretical assumptions and
then move on to real data with S&P500 prices. The detailed experimental settings are given in Section A.
Unless otherwise speciﬁed, we use a feedforward neural network with the number of neurons 10 →64 →64 →1
with ReLU activations. Training proceeds with the Adam optimizer with a minibatch size of 64 for 100 epochs
with the default parameter settings.5 We use the Sharpe ratio as the performance metric (the larger the better). Sharpe ratio is deﬁned as
SRt =
E[∆Wt]
√ Var[∆Wt], which is a measure of the proﬁtability per risk. We choose this metric because, in the framework of portfolio theory, it is the only theoretically motivated metric of success (Sharpe, 1966). In
particular, our theory is based on the maximization of the mean-variance utility in Eq. (3) and it is well-
known that the maximization of the mean-variance utility is equivalent to the maximization of the Sharpe
ratio. In fact, it is a classical result in classical ﬁnancial research that all optimal strategies must have the
same Sharpe ratio (Sharpe, 1964) (also called the eﬃcient capital frontier). For the synthetic tasks, we
can generate arbitrarily many test points to compare the Sharpe ratios unambiguously. We then move to
experiments on real stock price series; the limitation is that the Sharpe ratio needs to be estimated and
involves one additional source of uncertainty.6 5In our initial experiments, we also experimented with diﬀerent architectures (diﬀerent depth or width of the FNN, RNN,
LSTM), and our conclusion that the proposed augmentation outperforms the speciﬁed baselines remain unchanged.
6We caution the readers to not to confuse the problem of portfolio construction with the problem of ﬁnancial price prediction.
Portfolio construction is the primary focus of our work and is fundamentally diﬀerent from the problem of ﬁnancial price
prediction. Our method is not relevant and cannot be used directly for predicting future prices. As in real life, one does not
need to be able to predict prices to decide which stock to purchase. 8 Figure 3: Available portfolios and the market capital line (MCL). The black dots are the return-risk combinations
of the original stocks; the orange dots are the learned portfolios. The MCL of the proposed method is lower than
that of the original stocks, suggesting improved return and lower risk. 6.1
Geometric Brownian Motion We ﬁrst start from experimenting with stock prices generated with a GBM, as speciﬁed in Eq. (6), and we
generate a ﬁxed price trajectory with length T = 400 for training; each training point consists of a sequence
of past prices (St,...,St+9,St+10) where the ﬁrst ten prices are used as the input to the model, and St+10 is
used for computing the loss.
Results and discussion. See Figure 2. The proposed method is plotted in blue. The right ﬁgure compares
the proposed method with the other two baseline data augmentations we studied in this work. As the theory
shows, the proposed method is optimal for this problem, achieving the optimal Sharpe ratio across a 600-step
trading period. This directly conﬁrms our theory. 6.2
S&P 500 Prices This section demonstrates the relevance of the proposed algorithm to real market data. In particular, We
use the data from S&P500 from 2016 to 2020, with 1000 days in total. We test on the 365 stocks that
existed on S&P500 from 2000 to 2020. We use the ﬁrst 800 days as the training set and the last 200 days
for testing. The model and training setting is similar to the previous experiment. We treat each stock as a
single dataset and compare on all of the 365 stocks (namely, the evaluation is performed independently on
365 diﬀerent datasets). Because the full result is too long, We report the average Sharpe ratio per industrial
sector (categorized according to GISC) and the average Sharpe ratio of all 365 datasets. See Section A.1
and A.4 for more detail.
Results and discussion. See Table 1. We see that, without data augmentation, the model works poorly
due to its incapability of assessing the underlying risk. We also notice that weight decay does not improve
the performance (if it is not deteriorating the performance). We hypothesize that this is because weight
decay does not correctly capture the inductive bias that is required to deal with a ﬁnancial series prediction
task. Using any kind of data augmentation seems to improve upon not using data augmentation. Among
these, the proposed method works the best, possibly due to its better capability of risk control. In this
experiment, we did not allow for short selling; when short selling is allowed, the proposed method also works
the best; see Section A.4. In Section A.5.1, we also perform a case study to demonstrate the capability of
the learned portfolio to avoid a market crash in 2020. We also compare with the Merton’s portfolio (Merton,
1969), which is the classical optimal stationary portfolio constructed from the training data; this method
does not perform well either. This is because the market during the time 2019 −2020 is volatile and quite
diﬀerent from the previous years, and a stationary portfolio cannot capture the nuances in the change of the
market condition. This shows that it is also important to leverage the ﬂexibility and generalization property
of the modern neural networks, along side the ﬁnancial prior knowledge. 6.3
Comparison with Data Generation Method One common alternative to direct data augmentation in the ﬁeld is to generate additional realistic syn-
thetic data using a GAN. While it is not the purpose of this work to propose an industrial level method, 9 nor do we claim that the proposed method outperforms previous methods, we provide one experimental
comparison in Section A.5 for the task of portfolio construction. We compare our theoretically motivated
technique with QuantGAN (Wiese et al., 2020), a major and recent technique in the ﬁeld of ﬁnancial data
augmentation/generation. The experiment setting is the same as the S&P500 experiment. The result shows
that directly applying QuantGAN to the portfolio construction problem in our setting does not signiﬁcantly
improve over the baseline without any augmentation and achieves a much lower Sharpe ratio than our sug-
gested method. This underperformance is possibly because QuantGAN is not designed for Sharpe ratio
maximization. 6.4
Market Capital Lines In this section, we link the result we obtained in the previous section with the concept of market capital
line (MCL) in the capital asset pricing model (Sharpe, 1964), a foundational theory in classical ﬁnance. The
MCL of a set of portfolios denotes the line of the best return-risk combinations when these portfolios are
combined with a risk-free asset such as the government bond; an MCL with smaller slope means better return
and lower risk and is considered to be better than an MCL that is to the upper left in the return-risk plane.
See Figure 3. The risk-free rate r0 is set to be 0.01, roughly equal to the average 1-year treasury yield from
2018 to 2020. We see that the learned portfolios achieves a better MCL than the original stocks. The slope
of the SP500 MCL is roughly 0.53, while that of the proposed method is 0.35, i.e., much better return-risk
combinations can be achieved using the proposed method. For example, if we specify the acceptable amount
of risk to be 0.1, then the proposed method can result in roughly 10% more gain in annual return than
investing in the best stock in the market. This example also shows that how tools in classical ﬁnance theory
can be used to visualize and better understand the machine learning methods that are applied to ﬁnance, a
crucial point that many previous works lack. 6.5
Case Study For completeness, we also present the performance of the proposed method during the Market crush in Feb.
2020 for the interested readers. See Section A.5.1. 7
Outlook In this work, we have presented a theoretical framework relevant to ﬁnance and machine learning to un-
derstand and analyze methods related to deep-learning-based ﬁnance.
The result is a machine learning
algorithm incorporating prior knowledge about the underlying ﬁnancial processes. The good performance
of the proposed method agrees with the standard expectation in machine learning that performance can be
improved if the right inductive biases are incorporated. We have thus succeeded in showing that building
machine learning algorithms that is rooted ﬁrmly in ﬁnancial theories can have a considerable and yet-to-be
achieved beneﬁt. We hope that our work can help motivating for more works that approaches the theoretical
aspects of machine learning algorithms that are used for ﬁnance.
The limitation of the present work is obvious; we only considered the kinds of data augmentation that
takes the form of noise injection. Other kinds of data augmentation may also be useful to the ﬁnance;
for example, (Fons et al., 2020) empirically ﬁnds that magnify (Um et al., 2017), time warp (Kamycki
et al., 2020), and SPAWNER (Le Guennec et al., 2016) are helpful for ﬁnancial series prediction, and it is
interesting to apply our theoretical framework to analyze these methdos as well; a correct theoretical analysis
of these methods is likely to advance both the deep-learning based techniques for ﬁnance and our fundamental
understanding of the underlying ﬁnancial and economic mechanisms. Meanwhile, our understanding of the
underlying ﬁnancial dynamics is also rapidly advancing; we foresee better methods to be designed, and it
is likely that the proposed method will be replaced by better algorithms soon. There is potentially positive
social eﬀects of this work because it is widely believed that designing better ﬁnancial prediction methods
can make the economy more eﬃcient by eliminating arbitrage (Fama, 1970); the cautionary note is that
this work is only for the purpose of academic research, and should not be taken as an advice for monetary
investment, and the readers should evaluate their own risk when applying the proposed method. 10",0
"Development and growth are complex and tumultuous processes. Modern economic growth theories
identify some key determinants of economic growth. However, the relative importance of the deter-
minants remains unknown, and additional variables may help clarify the directions and dimensions
of the interactions. The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and structural view of the pro-
ductive possibilities of countries, i.e. their capabilities. Diﬀerent endowments of capabilities are crucial
ingredients in explaining diﬀerences in economic performances. In this paper we employ economic ﬁt-
ness, a measure of productive capabilities obtained through complex network techniques. Focusing on
the combined roles of ﬁtness and some more traditional drivers of growth, we build a bridge between
economic growth theories and the economic complexity literature. Our ﬁndings, in agreement with
other recent empirical studies, show that ﬁtness plays a crucial role in fostering economic growth and,
when it is included in the analysis, can be either complementary to traditional drivers of growth or can
completely overshadow them. Keywords: Economic Fitness; Complexity; Capabilities; Economic Growth. 1. INTRODUCTION Why are some countries wealthier than others? And why do some countries exhibit sustained rates of
growth over long periods, whereas others appear to be stuck in a low-income, low-growth path? These
questions have been central to economics ever since its origin as a science, following Adam Smiths [1]
original enquiry. An understanding of the main determinants of long-run growth arguably remains the
most important issue in economics. The ultimate causes of economic growth are however still not fully understood. In an inﬂuential
study, Helpman [2] examines the recent literature on the subject and acknowledges that the determi-
nants of economic growth remain a mystery. Early models based on capital deepening and exogenous
technical progress have given way to contributions that emphasise the endogenous nature of economic
growth, and that explore the role of factors such as expenditure on education, investment in research
and development, openness to international trade, and the presence of institutions that foster social
and economic inclusion [3]. Whilst all of these factors play a role in aﬀecting economic growth, the relative importance of
each of them is more diﬃcult to establish, especially because they are bound to interact with each arXiv:1808.10428v1  [econ.GN]  30 Aug 2018 other in complex ways. Furthermore, this list of key determinants of economic growth may not be
exhaustive: additional factors which have hitherto been ignored may be just as important, and may
contribute signiﬁcant predictive power to the models that have been studied so far. Exogenous as
well as endogenous growth theories predict that growth can be explained by a set of variables which
capture both the initial conditions of the economy and the rate at which its production inputs are
accumulated. The extent to which these models can predict the growth performance of broad cross-
sections of countries or regions is however limited [4,5]. It is therefore important to look for additional
drivers of growth, which may have been overlooked in the more traditional analyses. The present paper explores one such additional factor: the ﬁtness of the economy, as measured by a
complex-network metric based on the countries revealed comparative advantage [6]. The proﬁle of trade
specialization of a country can in fact be regarded as a reﬂection of its underlying capabilities, which
can be deﬁned as the skills that enable its economy to expand into new production requirements and
to adopt new technologies [7]. Conventional measures of the production possibilities of an economy do
not explicitly consider its degree of ﬂexibility and adaptability: if more complex economies are however
also endowed with a richer set of capabilities, then ignoring them would lead to a misspeciﬁcation of
the underlying models, because relevant explanatory variables would be omitted from the analysis. The structure of this paper is as follows. Section 2 discusses the role of capabilities and complexity
for economic growth. Section 3 deﬁnes the measure of Fitness used in the paper and explains the em-
pirical methodology and the data. Section 4 presents the empirical evidence from both non-parametric
graphical analysis and econometric estimation. Section 5 concludes. 2. ECONOMIC GROWTH, CAPABILITIES AND COMPLEXITY In his encyclopaedic treatment of modern theories of economic growth, Acemoglu [5] oﬀers a thorough
overview of the main approaches that have been set out to explain the dynamic growth paths of the
economies in the long run.
The early analysis by Solow [8] identiﬁed the main sources of growth
in the accumulation of physical capital and in exogenous technical progress, which is responsible for
the upward shift of the production possibility frontier of the economy over time. In the presence of
decreasing returns to capital in the aggregate production function, Solows model predicts that the
rate of growth will eventually tend to peter out, as the economy approaches its long-run steady-state
dynamic equilibrium path. An important implication of the exogenous growth model is that the income per capita of low-
income economies should tend to converge to that of high-income ones in the long run, and we should
therefore observe a catching-up of less developed economies to the more developed ones. Empirical
analyses show that there is some evidence of convergence of low income to high income economies,
albeit at a very slow pace [4]. More recent analyses of growth have sought to endogenize the rate of technical progress as the
outcome of explicit decisions made by economic agents. These models overcome the assumption of
decreasing returns with respect to the factor that is accumulated by acknowledging to the existence
of externalities among ﬁrms. This view was ﬁrst explored by Arrow [9], and is captured in his notion
of learning-by-doing. New technologies are incorporated in new investments, and therefore the rate at
which aggregate productivity increases is directly related to the rate of new investment in the economy.
As a result, the productivity of individual ﬁrms is an increasing function of the aggregate capital
stock in the economy. This way, it is possible to have constant or even increasing returns to capital
accumulation at the aggregate level, even if the technology at the ﬁrm level still exhibits conventional
decreasing returns to its own production inputs. Later developments in endogenous growth theories identify the main drivers of long-run growth
in investment in education in the presence of externalities from human capital accumulation [10], in
expenditure in Research and Development in a stochastic Schumpeterian model of creative destruction
[11], or in the openness of economy to international trade through learning-by-exporting [12,13]. These
drivers may interact with each other, as in the model by Lucas [14] where a high level of human capital
in the economy facilitates investment in new technologies, and in turn enhances their growth-improving
eﬀects. The recent literature on growth has also explored the role of social and political institutions.
In particular, Acemoglu and Robinson [15] distinguish between inclusive institutions, which stimulate
entrepreneurship and innovations, and extractive institutions, which are instead responsible for creating
incentives to exploit rents and which have a dampening eﬀect on growth. Endogenous growth theories are able to explain the lack of convergence of poorer countries and
regions to the richer ones: since the returns to the factors which are accumulated are now constant or
even increasing, the rate of growth of the economy is not necessarily predicted to slow down along its
long-run equilibrium path. An important extension of growth models further considers the possibility
of multiple steady state equilibria. Murphy, Shleifer and Vishny [16] formalise the notion of a big
push, according to which a discrete development eﬀort is required for a low-income economy to escape
its poverty trap and to set in motion a process of self-sustaining growth. The rationale for multiple
equilibria rests on the assumptions of increasing returns in the scale of production and of non-pecuniary
externalities among sectors. Multiple equilibria can provide a justiﬁcation for targeted development
policies, for instance in the form of government support to industrialisation in a critical mass of in-
dustrial sectors. Models with multiple steady-state equilibria have been generalised to models with
multiple dynamic steady states (e.g. Krugman, [17], and Matsuyama, [18]) where initial conditions can
be critical to determine the long-run growth path of the economy. A novel approach to the analysis of economic growth, however, goes beyond aggregate measures
of the production inputs and considers instead a more granular and structural view of the production
possibilities of the economy. This approach examines the possible role of capabilities, which could
be deﬁned as a broad set of skills that could adapt to changing production requirements and which
facilitate the introduction of new technologies. This approach can be traced back to the seminal work
by Hirschman [19], where capabilities make it possible to create backward and forward linkages across
economic sectors, and Penrose [20] with her resource-based theory of the ﬁrm. Teece et al. [21] argue
that capabilities in an organization are both intangible and non-transferable, and lay the foundations
for the modern complex network analysis by showing that they constitute the key factors for a coherent
growth of the ﬁrm consistent with its core competencies. Abramovitz [22] referred to social capabilities
as all those attributes that aﬀect a countrys ability to operate modern and large-scale businesses, and
which include their political and social characteristics: they should therefore not be interpreted in a
narrow individualistic sense (see also Fagerberg and Srholec, [23]). Capabilities can aﬀect economic
growth directly through their impact on the adoption of innovations, as shown by Lall [24] and by
Kremer [25]. In a series of important contributions, Sutton [26, 27], Sutton and Treﬂer, [28]) argues that the
source of the main diﬀerences in output per capita across economies lies not in their accumulation
of physical factors of production, as in the conventional theories of growth, but rather in the set of
capabilities with which their economy is endowed. These capabilities enable ﬁrms to take advantage of
investment by increasing their labour productivity and also make it possible to further expand their set
of skills, thereby generating a virtuous circle. Suttons analysis combines insights from the Industrial
Organization literature, the Economic Geography literature, and the Trade literature. Central to his results is that, in equilibrium, the market structure of an industry will tend to encompass a variety of
products of diﬀerent quality. The capabilities required to produce high-quality products will always
be scarce. This implies that, on a global scale, in each industry there will only be a limited number of
ﬁrms which hold a dominant position. From Economic Geography, capabilities must be concentrated in
some countries in order to take full advantage of agglomeration economies. Each countrys comparative
advantage will therefore reﬂect what Sutton calls the “economics of quality”. The available capabilities
become a key factor in global trade specialization and in determining the growth prospects of each
country. Capabilities are however unobservable. Hausmann, Hwang and Rodrik [29] argue that they can
be inferred from a countrys export specialization proﬁle. Production of complex goods involves the
joint execution of a large number of highly specialised tasks [25,30]. This requires both the existence
of broad sets of advanced skills, and the ability to combine them eﬀectively. A detailed examination
of the revealed comparative advantage across all countries in the world using the tools of complex
network analysis can identify those countries which specialise in complex products. Their production
could involve a number of tasks by high-skilled workers, who could not be substituted by low-skill
workers without compromising the quality of the ﬁnished product [25]. The sophisticated production
structure of these countries also allows them more easily to introduce innovations and to adopt advanced
technologies consistent with the role of dynamic capabilities in an environment of rapid technological
change analyzed by Teece et. al. [7]. The location of a country in the global product space would
therefore be a signiﬁcant predictor of its growth potential. The usefulness of measures of product complexity for predicting growth has been demonstrated
by Hausmann, Hwang and Rodrik [29], who show that the mix of good that a country produces
can explain its growth rate. Ferrarini and Scaramozzino [31] conﬁrm that production complexity can
help explain diﬀerences in economic performance, in an endogenous growth model with human capital
accumulation. Pugliese et al. [32] analyze the role of ﬁtness during the process of development and
industrialization showing that ﬁtness crucially eases the passage towards sustained growth. Sbardella
et al. [33] empirically demonstrate that ﬁtness is a key variable in explaining the relationship between
wage inequality and economic development. Boltho et al. [34] argue that the more complex production
structure in East Germany relative to that in the Italian Mezzogiorno can contribute to explain the
unsatisfactory performance of the latter, and its lack of convergence to the more prosperous regions in
the Centre-North of the country. Cristelli et al.
[35] develop an innovative approach to predict economic growth, the Selective
Predictability Scheme (SPS), which makes use of ideas from the theory of economic complexity and
which adopts the the ﬁtness metric [6, 36, 37]. SPS is shown to out-perform a number of alternative
forecasting models, thus conﬁrming the key role played by the complexity of the structure of production
for predicting the future economic performance of a country. Tacchella et al. [38] further develop this
approach to growth forecasting and outperform the accuracy of the IMF ﬁve-year forecasts by more
than 25% . 3. MATERIALS AND METHODS 3.1 Measuring Fitness As mentioned above, in this paper we use the ﬁtness [6, 36, 37], a measure of economic complexity
based on cross-country diﬀerences in productive structures. Fitness is a proxy for capabilities, and is
calculated by applying complex network techniques to the analysis of international trade data. In fact, GDP may not be suﬃcient to describe development and growth processes, as two countries with similar
GDP levels may actually possess profoundly diﬀerent endowments of capabilities. Countries that are
below the income expected from their economic performances may have already developed the full
range of products that is within their technological reach, nevertheless this capability level may have
not been yet translated into higher GDP levels. Thus, as explained in Section 2, diﬀerent endowments
of capabilities are the main, albeit not empirically observable, sources in explaining diﬀerent economic
performances and in shaping the export proﬁles of countries. In network theory, the notion of capabilities can be conceptually described as an intermediate net-
work layer that connects countries to their exported products [36,37]. As schematically illustrated in
Figure 1, we can deﬁne a tripartite network whose three classes of nodes are countries (C), capabilities
(K) and products (P). The allowed links, connecting the K nodes to the C or P nodes, describe the
capability owned by a country and the capability required to export a product with a comparative ad-
vantage. Tacchella et al. [36] explore the functioning of this tripartite network and show the high degree
of correlation between a country’s endowment of capabilities and its ﬁtness. However, the capability
layer, despite being conceptually crucial, is intangible and the tripartite network is a purely theoretical
tool to better visualize our theoretical framework. By using international trade data, the information
coded in the hidden capability layer can be gathered by building an empirical bipartite network in
which countries are connected to the products they export [39] with a revealed comparative advantage
(RCA) [40]. As represented in Figure 1, this country-product network is viewed as a contraction of the
tripartite network over the capability dimension. How to extract in an optimal way the informative Figure 1: In our theoretical framework capabilities are crucial in explaining the economic performances of
countries. As can be observed on the left, conceptually we can visualize capabilities as an intermediate level in
a tripartite country-capability-product network. Nevertheless, this is a theoretical model, capabilities in fact are
non-measurable entities. Information on capabilities can be gathered by building an empirical country-product
network through international trade data. As can be seen on the left, such bipartite network can be interpreted
as the projection of the tripartite network. In the country-product network a country-product link is established
if and only if the country has a revealed comparative advantage in exporting that product. [36,37]. content coded in such country-product network? The ﬁtness-complexity metric provides the correct
mathematical speciﬁcation based on the network topology, and, through an iterative algorithm, it de-
ﬁnes a measure of country competitiveness, i.e. ﬁtness, and of product sophistication, i.e. complexity. To understand the rationale beyond the metric it is useful to observe the binary adjacency matrix of
the network,
ˆ
M, whose rows represent countries and columns their exported products (see Fig.3.1).
RCA is used to ﬁlter and digitize the data allowing to focus on qualitative diﬀerences, rather than
quantitative, in the export baskets of countries: which kinds of products are exported competitively,
not in which volumes. Therefore, matrix entries are set equal to 1 when a country exports a product
with RCA ≥1, and 0 in the opposite case: Mcp = (
1
if RCAcp ≥1
0
if RCAcp < 1.
(1) Figure 2: The binary matrix of countries and products built from the worldwide 1998’s export ﬂows of BACI
data-set [41].The rows and columns of the matrix are ranked according to the ﬁtness-Complexity algorithm. The
rows are sorted by increasing country ﬁtness and the columns by increasing product Complexity. In such a way,
the matrix acquires a triangular-like shape: countries with more diversiﬁed export baskets are more competitive,
while countries specialized in few products –which generally are also exported by every other country– are the
less competitive. Source of the ﬁgure: [36]. If suitably ordered, the matrix assumes a quasi-triangular shape. Indeed, by looking at the matrix
triangularity, it is possible to infer that, on the one hand, a largely diversiﬁed country’s ability to export
a product with a comparative advantage gives no clues on its complexity. On the other hand, when a
poorly diversiﬁed country –which has a comparative advantage in exporting few and very ubiquitous
products– is able to export a product with a comparative advantage it is likely that its production
requires a low level of industrial sophistication. This is quite informative: a product is complex if
low-ﬁtness countries do not export it. To make this clearer, consider a straightforward example of a
high complexity product, transistors, and a low complexity product, nails. Only highly industrially and
technologically developed countries are able to export transistors; by contrast, nails are exported by
all sorts of countries, both more and less industrialized. Consequently, the low complexity of nails can
be surmised directly from their presence in the export basket of low ﬁtness countries. This observation
hints at a non-linearity in the relation between product Complexity and country ﬁtness1. Therefore,
from the matrix ˆ
M, it is possible to obtain an intensive metric that measures country ﬁtness (Fc) as
a weighted average of the country export basket’s diversiﬁcation, where the weight is the complexity 1The need for a non-linear coupling between ﬁtness and Complexity was formalized in mathematical terms by Caldarelli
et al. [42]. associated to each product. Product Complexity (Qp), instead, is calculated as the number of countries
that export the product with comparative advantage, bounded by the ﬁtness of the least competitive
exporter of the product. In formula: 





 




 e
F (n)
c
= P
p McpQ(n−1)
p e
Q(n)
p
=
1 P
c Mcp
1 F (n)
c 






 





 F (n)
c
=
e
F (n)
c < e
F (n)
c
>c Q(n)
p
=
e
Q(n)
p < e
Q(n)
p
>p (2) where < · >x denotes the arithmetic mean with respect to the possible values assumed by the variable
dependent on x, with initial condition:
X p
Q(0)
p
= 1 ∀p.
(3) The iteration of the coupled equations leads to a ﬁxed point which has been proved to be stable and
non-dependent on initial conditions [6]. The ﬁxed point deﬁnes the non-monetary metric which actually
quantiﬁes Fc and Qp. The convergence properties of Eq.2 are not trivial and have been extensively
studied by Pugliese et al. [43]. 3.2 Econometric Model and Graphical Representation As we have discussed in the previous sections, the economic complexity approach has a structural
interpretation of growth and development, understood as the outcomes of a learning process through
which new capabilities are added to the existing pool thus opening up new and more complex productive
possibilities which will eventually lead to higher prosperity and faster economic growth. In this section, to partially reconcile this new view on development with the stylized facts of
neoclassical theories of economic growth we integrate the economic complexity discourse with the
analyses of growth determinants popularized in the 1990s by Barro [44], and Mankiw, Romer and Weil
[45], the so-called “growth regression models”. In the latter, GDP per capita growth is decomposed into
contributions associated with changes in factor inputs, production technologies, demographic variables
and so on. We therefore focus on the multifaceted relations between GDP per capita growth, ﬁtness, and
various economic indicators considered crucial drivers for economic growth: Capital Intensity, Capital-
Output Ratio, GDP per capita, Life Expectancy, Secondary Schooling, and Total Factor Productivity.
The choice of such drivers of growth is rooted in the growth regression models; in particular, the income
and capital deepening variables are directly linked to the early analysis of Solow [8], while the inclusion
of data on educational attainment and life expectancy among the explanatory variables is based on the
more recent endogenous growth theories mentioned in Section 2. Following the lines of Cristelli et al [35], we opt for a non-parametric description since the dependen-
cies are fundamentally dynamic and non linear, and diﬀerent types and shapes of relationships between
ﬁtness, the chosen growth determinants and economic growth coexist depending on the phase2 that an 2Where with economic phase we refer to the deﬁnition of Cristelli et al. [46]. These diﬀerent economic regimes are
explicitly visible in the ﬁtness-GDP per capita plane. economy is going through: in each phase the dimensions of interest combine in an a priori unknown
fashion. To provide a basis for comparability with the more traditional analysis on growth determinants, we
oﬀer an additional tool of interpretation and perform a linear parametric regression model. In the latter
various drivers of economic growth ﬁgure as right-hand-side explanatory variables aﬀecting the growth
rate of GDP per capita, which is on the left-hand side of the equation. The idea is to add ﬁtness to
the growth drivers and observe whether and to what extent the results change. As discussed above, a
non-parametric approach is the most appropriate framework to investigate the complex and non-linear
interactions that are at the basis of the process of growth and development. Indeed, a linear parametric
approach, looking at averaged interactions and having strong assumptions on the functional forms of
the relationships, might not recognize the importance of a strong but non-linear signal. Speciﬁcally,
for instance, a discrete element as the presence or absence of an enabling capability necessary to
competitively export a product can change in unpredictable ways the development proﬁle of a country.
These nuances might remain unobserved when using a linear parametric approach. Nevertheless, since
non-parametric analyses can be performed only by looking at the simultaneous eﬀect of ﬁtness and a
single growth driver on GDP growth rate, a linear regression enables us to test the robustness of our
results by observing cross-media eﬀects. Our empirical evidence is obtained by using an unbalanced panel of countries over the time period
1963–2000, the number of countries slightly varying around 144. In order to avoid cyclical short-run
ﬂuctuations, the GDP per capita growth rate is calculated over a ∆t = 5 years. Furthermore, to reduce
the risk of simultaneity bias, a lag of ﬁve years is considered between the explanatory variables and
GDP per capita growth rate. In practice, in our non-parametric analysis, we explore the empirical relations by looking at tridi-
mensional color-maps, smoothed representations of GDP per capita growth rate for diﬀerent values of
ﬁtness on the x-axis and, on the y-axis, of each one of Capital Intensity, Capital-Output Ratio, GDP
per capita, Life Expectancy, Secondary Schooling, and Total Factor Productivity. The color-maps are
realized trough a multivariate Nadaraya-Watson regression [47], a continuous non-parametric kernel
estimator and by pooling all countries and years in the panel. We then perform a more traditional parametric linear regression analysis. We opt for a ﬁxed eﬀect
analysis with robust standard errors. Our explanatory variables are the one listed above, while on the
left hand side of the equation ﬁgures GDP per capita growth rate. 3.3 Sources of data The levels of GDP, Population, Total Factor Productivity, Capital, and Human Capital are taken from
the Penn World Table 9.0 (PWT) produced by the University of Groningen and the University of
Pennsylvania [48]. We also include in our analysis the Life Expectancy measure of the World Development Indicators,
the collection of development statistics of the World Bank [49]. Finally, ﬁtness is taken from the Export ﬁtness data-set developed by the PIL group of the Institute
of Complex Systems in Rome which covers a number of countries varying between 130 and 151, over
the period 1963–2000. As brieﬂy explained in the previous section and shown in detail in Tacchella et
al.[6], Cristelli et al. [37], ﬁtness is obtained trough an empirical algorithm that, for the chosen time
window, employs international trade data from UN-COMTRADE [50]. The choice of the time period
of our analysis was constrained by the availability of ﬁtness data. 4. EMPIRICAL EVIDENCE 4.1 Non-parametric Graphical Analysis Fitness and GDP per capita As follows from the hypothesis of diminishing returns with respect to the factor which is being accumu-
lated, exogenous growth theories predict higher growth in response to lower starting GDP per capita,
since in the long-run all economies should catch-up and converge to similar income levels. Initial GDP
per capita, thus, should have a negative impact on economic growth. Such conditional convergence has
been reported, even if not to the extent predicted by neoclassical models , by Barro [51] and Mankiw
et.
al [45].
However, other important empirical contributions do not ﬁnd evidence of conditional
convergence [52,53]. Our empirical ﬁnding show that ﬁtness is decisive in determining the growth proﬁle of countries
[32, 35, 46]. Looking at the variability of color, in Figure 3 we can distinguish three areas. First, in
the case of countries with low starting ﬁtness (−12 < log(Fc) ⩽−7, intense red), the GDP growth
rate is completely led by ﬁtness. When ﬁtness is too low, independently of the initial income level, the
take-oﬀdoes not occur. A critical degree of complexity is necessary to start the catch-up [32]. Second,
in the portion of the plot with countries which exhibit intermediate starting ﬁtness (−7 < log(Fc) < 0,
orange and yellow), contrary to the predictions of exogenous growth theory, higher starting GDP per
capita results in mildly higher growth rates. Here, for example, we ﬁnd oil exporters whose high income
does not reﬂect the high endowment of capabilities that would be necessary to sustain high economic
growth. Thirdly, for countries with high starting ﬁtness (log(Fc) ⩾0, diﬀerent shades of green), the conver-
gence hypothesis seems to apply only when the initial ﬁtness level is quite high . This brings to light
a complementarity between the role of ﬁtness and GDP per capita that cannot be ignored. Countries
with low initial GDP per capita and high ﬁtness, have large capability portfolios which, after a critical
threshold, enable them to catch-up and achieve very fast growth rates. As has been pointed out in
numerous recent contributions [32,35,46], in this area of the ﬁtness-GDP per capita plane we can ﬁnd,
among others, China and South Korea. In Cristelli et al [46] and in diﬀerent other recent contributions [32, 35, 37], it has been explored
in depth how ﬁtness, when put into relation with GDP per capita, is a very powerful instrument in
assessing the growth potential of countries that is not captured by the sole information contained in the
GDP per capita. Indeed ﬁtness, being a structural measure that quantiﬁes the capability endowment
of countries, is able to proﬁle them in a much more nuanced and coarse-grained way with respect to
the aggregate measures of growth inputs. Fitness and Capital Intensity Capital intensity is deﬁned as the ratio between output and labor, i.e. the number of workers employed
in the economy. Under the neoclassical convergence hypothesis, in the long-run capital intensive so-
cieties should tend to have higher standards of living, with higher GDP per capita levels and lower
growth rates. By contrast, emerging economies should tend to be labour intensive and show higher
growth rates. Accordingly, from the bottom to the top of our color-map we should observe that as
capital intensity increases the color goes from green (higher growth rate) to red (lower growth rate).
However, in Figure 4, when the combined eﬀect of ﬁtness and capital intensity is taken into consider-
ation the contour lines are mostly vertical, and the explanatory power of K/EMP loses importance
relative to ﬁtness. Nevertheless, a partial reconciliation with the neoclassical models is found for very Figure 3: The color-map represents the tridimensional relation between ﬁtness, GDP per capita and subsequent
GDP per capita growth rate, where a ∆t = 5 years is considered. The variation of the growth rate is represented
through color. The color-map is obtained through a non-parametric Nadaraya-Watson estimation. Countries
with low ﬁtness are not able to achieve subsequent high growth rates, irrespective of their initial GDP per capita
level. For countries with intermediate initial ﬁtness, higher starting GDP per capita results in mildly higher
subsequent growth rates. Finally, countries with high initial ﬁtness are able to grow at very high rates, especially
when their starting GDP per capita is low or intermediate. As has been explored in detail [32, 35, 46], ﬁtness,
when put into relation with GDP per capita, is able to suggest future scheme of development not fully captured
by solely monetary metrics. high starting ﬁtness countries. The latter, similarly to what was examined in Figure 3, countries with
low/intermediate starting capital intensity are able to achieve the fastest growth rate. Fitness and Employment Rate The employment rate is deﬁned as the employment-to-population ratio, where employment levels are
include all the persons working within national boundaries. As mentioned in the Introduction, according to endogenous growth models, the employment rate
should have a positive eﬀect on economic growth. In particular, Arrow’s learning-by-doing model [9]
predicts that higher employment rates create positive externalities due to a larger size of the market.
In Figure 5, the movement of color hints at the presence of non-linear dependencies, and diﬀerent
behaviors of GDP per capita growth rate can be identiﬁed. The positive eﬀect of the employment rate
on economic growth is detected only for the highest levels of employment rate, after a critical threshold
of EMP/POP ∼55%, as can be seen from the horizontal variation of color from red to green in
left-upper portion of the plot. For lower values of the employment rate, the dominant variation of the
color is vertical: higher ﬁtness leads to higher GDP per capita growth rates. Figure 4: The color-map represents the tridimensional relation between ﬁtness, capital intensity and subsequent
GDP per capita growth rate, where a ∆t = 5 years is considered. The variation of the growth rate is represented
through color. The color-map is obtained through a non-parametric Nadaraya-Watson estimation. When the
combined eﬀect of ﬁtness and capital intensity is taken into consideration, the latter looses explanatory power
and the growth proﬁle of countries is almost completely explained by their ﬁtness level. Higher ﬁtness leads to
higher growth rates, however, countries with high ﬁtness and intermediate capital intensity are able to achieve
the highest growth rates. Fitness and Life Expectancy Life expectancy is a proxy for healthcare quality. The eﬀect of life expectancy on economic outcomes
is positive and signiﬁcant according to the growth regressions by Barro [4, 44]: better health should
lead to higher economic growth. In Figure 6 such positive eﬀect is only obtained when a critical threshold of circa 73 years is
exceeded. Similarly to Figure 5, also here a non-linear behavior is found, and ﬁtness almost entirely
dominates the dynamics. Except for life expectancy ≳73 years, low ﬁtness countries grow slowly,
whilst the contrary is true for high ﬁtness countries. Nevertheless, for high ﬁtness countries too growth
rates are higher if life expectancy is approximately higher than 60 years. Such threshold falls for very
high ﬁtness countries: lower life expectancy is oﬀset by increasing capabilities. However, when it comes to demographic variables, setting the direction of the arrow of causality
may poses some problems. Life expectancy is listed among the growth determinants, but also the
opposite relation is likely, it could be the case that high growth rates lift up life expectancy rather than
the opposite. Figure 5: The color-map represents the tridimensional relation between ﬁtness, employment rate and subsequent
GDP per capita growth rate, where a ∆t = 5 years is considered. The variation of the growth rate is represented
through color. The color-map is obtained through a non-parametric Nadaraya-Watson estimation. Only the
highest levels of employment rate, after a critical threshold of EMP/POP ∼55%, have a positive eﬀect on
economic growth. This is clearly visible from the horizontal variation of color from red to green in upper portion
of the plot. For lower values of EMP/POP, ﬁtness is the most important variables since the dominant variation
of the color is vertical. The higher the ﬁtnessm the higher the GDP per capita growth rate. Fitness and Human Capital In endogenous growth theories, human capital is an essential ingredient to achieve sustained growth
rates. The accumulation of human capital, through formal training or learning-by-doing, creates pos-
itive externalities through increased productivity and technological innovation originating from the
process [9–11]. In Barro’s growth regressions [4, 44] human capital is positively and signiﬁcantly re-
lated to subsequent GDP per capita growth rates. Here human capital is proxied by a measure of educational attainment at each starting period.
Figure 7 puts into light a positive relation between ﬁtness and human capital (as conﬁrmed by their
signiﬁcant Pearson correlation of 0.30), principally for intermediate and high ﬁtness values. In some
areas of the ﬁgure ﬁtness and human capital are entangled and reinforce each other, while in others
high initial ﬁtness can compensate for a lack of initial human capital. In fact, low and intermediate ﬁtness countries are associated with low human capital (< 2). By
contrast, when log(Fc) > −4, increasing ﬁtness has a positive eﬀect on economic growth even if human
capital is low. For log(Fc) > −2, human capital takes oﬀand the curve representing the ﬁtness-human capital
relation starts to assume an increasing shape. In this portion of the plot, we partially recover the
expected eﬀect of human capital on economic growth, ﬁtness, though, still maintains a leading position. Figure 6: The color-map represents the tridimensional relation between ﬁtness, life expectancy and subsequent
GDP per capita growth rate, where a ∆t = 5 years is considered. The variation of the growth rate is repre-
sented through color. The color-map is obtained through a non-parametric Nadaraya-Watson estimation. Life
expectancy values ≳73 years have a positive eﬀect on growth rates. When life expectancy < 73 years, ﬁtness
determines the color contour: the higher the ﬁtness, the higher the growth rate. However, also high ﬁtness
countries show higher growth rate when life expectancy > 60 years. Notably, two main features of the relationships can be highlighted. First, as expected, countries with
very high human capital levels grow slowly, as can be seen in the upper right corner of the ﬁgure.
Second, in the bottom right corner are placed countries that, having very high ﬁtness, are able to enter
into sustained growth regimes even with low initial human capital levels. In such a way -as can be
appreciated by the vertical passage from red to green, and then back to red- for high and intermediate
ﬁtness countries we observe a sort of inverted U-shaped relationship between human capital and the
growth rate. Low initial human capital predicts low growth rate (lower red-yellow area), intermediate
human capital predicts high growth rates (intense green area), and, ﬁnally, when human capital is high,
the subsequent growth rates are low (upper orange area), but not as in the ﬁrst phase. Fitness and Total Factor Productivity According to exogenous growth theories, total factor productivity (TFP) -or the Solow residual- indi-
rectly captures technological improvements. In the neoclassical production function is identiﬁed as the
growth of output that cannot be accounted for by the growth of the observed inputs, capital and labour.
In this framework, an increase in total factor productivity should shift upwards the growth rate. Note
that being estimated as a residual in the decomposition of the growth rate, all the measurement errors
in capital and labour will be reﬂected in total factor productivity In the upper portion of Figure 8, for log(TFP/GDP) ≳−11, as can be noted by the horizontal Figure 7: The color-map represents the tridimensional relation between ﬁtness, human capital and subsequent
GDP per capita growth rate, where a ∆t = 5 years is considered. The variation of the growth rate is represented
through color. The color-map is obtained through a non-parametric Nadaraya-Watson estimation. Fitness and
human capital appear positively correlated in some zones of the plots, and complementary in others. Low and
intermediate ﬁtness corresponds to low human capital. While, when log(Fc) > −4 increasing initial ﬁtness aﬀects
positively the GDP per capita growth rate, even for low initial human capital. movement of color from red to green, the dynamic is dominated by ﬁtness. Keeping TFP constant,
the higher the initial ﬁtness, the greater the growth rate.
Whereas, for log(TFP/GDP) < −11,
as is apparent from the diagonal variation of color, even if ﬁtness remains the prevailing factor, a
complementarity between the eﬀects of ﬁtness and of total factor productivity slowly emerges. This area
can be divided into three zones according to the corresponding ﬁtness levels. Firstly, for very low levels
of initial ﬁtness (−12 < log(Fc) < −10) we observe low growth rates, irrespective of the level of initial
total factor productivity. Secondly, for low and intermediate levels of ﬁtness (−10 < log(Fc) < −2),
we observe a negative eﬀect of initial total factor productivity on economic growth. This might be
attributed to measurement errors possibly due to an overestimation of the labour and capital factors.
The source of such overestimation might be a misattribution of capabilities to capital or labour. Thirdly,
for log(Fc) > −2, growth rates are generally higher. However, the vertical transition of color from
yellow to green shows that, as suggested by the theory, total factor productivity positively aﬀects the
growth rate. Therefore, in this region of the plane, ﬁtness and total factor productivity are mutually
reinforcing. In the non-parametric kernel regressions, which do not assume any aprioristic functional forms for
the relationships, we have observed the combined eﬀect of ﬁtness, a proxy for the basket of capabilities of
a country, and some key growth determinants. Our ﬁndings are unambiguous: ﬁtness is an important,
if not the most important, player in enhancing economic growth. Depending on the drivers and on Figure 8: The color-map represents the tridimensional relation between ﬁtness, total factor productivity and
subsequent GDP per capita growth rate, where a ∆t = 5 years is considered. The variation of the growth rate is
represented through color. The color-map is obtained through a non-parametric Nadaraya-Watson estimation.
For log(TFP/GDP) ≳−11, as can be noted by the horizontal movement of color from red to green, ﬁtness
is the prevaling factor. Keeping TFP constant, high initial ﬁtness corresponds to greater subsequent growth
rates. For log(TFP/GDP) < −11 from the diagonal variation of color we can deduce that ﬁtness and total
factor productivity are complementary in aﬀecting future growth rates. In this area, very low initial ﬁtness
brings low growth, independently from total factor productivity. For low and intermediate ﬁtness countries,
starting total factor productivity has a negative impact on economic growth. This could be due to total factor
productivity measurement errors. Finally, high starting ﬁtness leads to high growth rates, especially when total
factor productivity is high. This highlights a complementarity of ﬁtness and total factor productivity for highly
competitive economies. the diﬀerent phases that an economy is going through, when ﬁtness is included in the analysis it can
be either complementary to traditional drivers of growth or can completely overshadow them. The
complementarity emerges in particular for high ﬁtness countries, which have already a rich basket of
capabilities. This suggests that, once it becomes possible to account for capabilities, some other factors,
otherwise relevant in determining future growth rates, lose importance. Not including ﬁtness among
explanatory variables in growth regressions may signiﬁcantly diminish the overall explanatory power
of the model. Additionally it may cause an omitted variable bias and thus produce misleading results;
the signiﬁcance of some regression coeﬃcients may in fact be spurious and be lost when also the eﬀect
of ﬁtness is considered. Diﬀerent endowment of underlying capabilities explain diﬀerences in economic performances, greater
complexity enable countries to use factors more eﬃciently, be more competitive and catch-up more
rapidly [26–28,32,35]. Variable
Coeﬃcient
Standard Error Constant
-1.44
0.35
log(GDPpc)
-0.084
0.028
log(K/EMP)
-0.025
0.016
log(EMP)
-0.051
0.027
log(TFP/GDP)
-0.048
0.028
log(1/LifeExp)
-0.49
0.11
log(School)
-0.042
0.020
Fitness Rank
0.255
0.048 Variable
Coeﬃcient
Standard Error Constant
-2.36
0.38
log(GDPpc)
-0.069
0.032
log(K/EMP)
-0.025
0.019
log(EMP)
-0.020
0.029
log(TFP/GDP)
-0.035
0.031
log(1/LifeExp)
-0.73
0.12
log(School)
-0.036
0.020 Table 1: Parametric regression results, including or not the Fitness Rank as an explanatory variable
(left and right table, respectively). 4.2 Econometric Analysis Now we perform a parametric linear regression using as explanatory variables GDP per capita log(GDPpc),
Capital Intensity log(K/EMP), number of employees log(EMP), the inverse of Life Expectancy log(1/LifeExp),
Secondary Schooling log(School), and Total Factor Productivity log(TFP/GDP). The model is there-
fore log(GDPpc) ∼const. + β1 log(K/EMP) + β2 log(EMP) + β3 log(TFP/GDP)+ β4 log(1/LifeExp) + β5 log(School) + β5FitnessRank
(4) Various drivers of economic growth ﬁgure as right-hand-side explanatory variables aﬀecting the
growth rate of GDP per capita, on the left-hand side of the equation. The idea is to add ﬁtness to
the drivers and observe whether and to which extent the results change. The results of our analy-
sis are shown in Table 1. The Fitness Rank is highly signiﬁcant. The eﬀect of Life Expectancy is
positive and signiﬁcant, while the coeﬃcient of the GDP per capita is negative. Both results are in
agreement with the literature; in particular, the second one is coherent with the convergence scheme
reported by Barro [51] and Mankiw et. al [45]. The other variables display an unexpected behaviour,
probably attributable to measurament errors (Schooling, TFP), low input substitution (Schooling) or
multicollinearity (number of employees, Schooling, Capital Intensity). We are planning to investigate
further this parametric approach, possibly taking into account also ﬁxed eﬀects in the regression model. 5. CONCLUSION This paper has examined how complex network analysis can be a powerful tool to understand eco-
nomic growth. The traditional literature on economic growth does not fully consider the role of the
underlying capabilities, which are however crucial for the adoption of advanced technologies and for the
introduction of innovations. The Fitness metric used in the paper, which captures the global export
specialization proﬁle of countries, is shown to be a signiﬁcant variable for predicting growth. Economic
models of growth that overlook the role of complexity therefore run the risk of being misspeciﬁed. Acknowledgements and funding We would like to thank Luciano Pietronero for comments and discussions. A.Z. acknowledges funding
from the ’PNR Progetto di Interesse’ CRISIS LAB. The founding sponsors had no role in the design of the study; in the collection, analyses, or inter-
pretation of data; in the writing of the manuscript, and in the decision to publish the results. The
authors declare no conﬂict of interest.",0
"Any ﬁnite conversation can be rationalized. Key words: dialogue, rationality. JEL classiﬁcation:
D83. In the summer of 1977 Herakles came to John very excited about a paper
of Bob Aumann’s on common knowledge. We couldn’t believe the paper,
much less ﬁgure it out. Our adviser Kenneth Arrow couldn’t either. Even
the title “Agreeing to Disagree” seemed to say the opposite of the paper’s
conclusion.
There is nothing more tantalizing than a paradox. As Aumann has man-
aged with other young students time and again, we were hooked. His teaching
style builds on paradoxes. He once described capitalism through a letter he
had gotten from his son about life in the Kibbutz. In the morning, the son
had written, I do something for my community and in the afternoon some-
thing for myself. Aumann said, wouldn’t it be better if by doing something
for himself he was at the same time doing something for his community? He
described integration and the fundamental theorem of calculus as showing
that it is easier to solve many hard problems than a single hard problem. At
a conference in India, he was asked by a reporter to say a word explaining
Game Theory. Aumann replied that the question reminded him of Nikita
Khruschev’s ﬁrst press conference in front of foreign journalists. A reporter
asked Khrushchev to say a word about the health of the Russian economy.
Khrushchev said “Good.” The reporter said he didn’t literally mean one
word, could Khruschev say two words about the health of the Russian econ-
omy? Khrushchev replied “Not Good.” Aumann continued by saying that
in one word game theory is about “Interaction.” In two words, it is about
“Rational Interaction.”
A parodox is something that sounds crazy, but looked at the right way
makes sense. For Aumann, paradoxes abound. In honor of Bob Aumann, we
prove here that paradoxes are ubiquitous. We show that any conversation,
no matter how crazy the opinions and the rejoinders sound, can be explained
as the ﬁrst part of a dialogue between two perfectly rational interlocutors.
Dialogues are interactions. And they might all be rational interactions.
Turing famously suggested that one could distinguish a (nonrational) ma-
chine from a man by engaging it in conversation and then letting a panel of
judges review the transcript and vote man or machine. As is becoming clearer
today with ChatGPT, and as our theorem suggests, it may not be as easy
as Turing hoped.
Bob Aumann himself has often written that what is called irrational be-
havior by behavioral economists might one day be better understood as ra-
tional behavior in a complicated environment with constraints. Our theorem
has a similar ﬂavor. Perhaps the most comforting aspect of our theorem is 1 that it provides some hope for our current troubled and polarized discourse. Aumann (1976) deﬁned common knowledge and proved that consensus
is a necessary condition for common knowledge, that is, that people cannot
agree to disagree about the probability of an event. A Bayesian dialogue is
a sequential exchange of beliefs about the probability of an event. It is the
prototype of a rational dialogue. One of two interlocutors states his belief,
then the other responds with her belief, perhaps informed or inﬂuenced by
his stated belief.
He then responds, perhaps with a revision of his prior
opinion (in view of her opinion), and then she responds again, and so on.
The dialogue is said to terminate at a time T if neither agent changes his or
her mind thereafter. In Geanakoplos and Polemarchakis (1982), we proved
that Bayesian dialogues must always terminate, and that when they do, the
agents are in agreement.1 We show here that a third party, with access only to the transcript of
a dialogue, cannot be sure that any arbitrary ﬁnite sequence of alternating
opinions is not part of a Bayesian dialogue. If the transcript were inﬁnitely
long, then it would necessarily terminate in agreement. We show that the
available ﬁnite transcript of opinions can always be continued to reach an
agreement in such a way that the whole dialogue from the beginning is ra-
tional.
Our argument covers the special case of a didactic dialogue, in which an
expert is better informed than his interlocutor. The expert never changes
his opinion, but the interlocutor follows an arbitrary path. Some of Plato’s
dialogues might be considered didactic dialogues in our sense. Socrates knows
the right answer to which he leads his interlocutor. Plato perhaps understood
our theorem in the sense that in some of his dialogues he has an interlocutor
of Socrates, such as Protagoras, appear at ﬁrst to move further away from
the answer until eventually coming back to the right path.
Our theorem relies on one important premise. If an agent expresses abso-
lute certainty in her opinion, then her interlocutor must immediately agree.
Absolute certainty is tantamount to claiming a proof. If the interlocutor does
not agree, then one or the other cannot be rational. She can be 99.9999%
certain of one thing, and then 99.9999% certain of the opposite at the next
stage; as long as neither she nor he is 100% certain, then whatever her inter- 1We allowed for an arbitrary but ﬁnite state space. Bacharach (1979) looked at Bayesian
dialogues when information is normally distributed. Nielsen (1984) considers dialogues
with an uncountable number of states. 2 locutor and she say can be rationalized.
Loosely speaking, one can consider common knowledge and agreement
as an equilibrium, and the dialogue that leads to common knowledge as the
adjustment path. We are arguing that along the adjustment path, anything
goes. This bears an analogy with general competitive analysis. As follows
from Debreu (1974), the Walrasian tˆ
atonnement that leads to equilibrium, if
it does, is arbitrary. The argument Bayesian Dialogues A Bayesian opinion framework is deﬁned by a ﬁnite probability space, a
subset, two partitions, and an agent, (Ω, π, A, P, Q, i), where Ωis a ﬁnite set of states and π is strictly positive probability on Ω, and
A is a subset of Ω. The probability π is the common prior of two agents p, q.
P and Q are partitions of Ω, corresponding to the two agents p, q, deﬁned by
disjoint subsets or cells (Pc) and (Qd), respectively. For any ω ∈Ω, P(ω) is
deﬁned as the unique cell Pc containing, ω, and likewise for Q(ω). Finally,
the agent i ∈{p, q}.
The Bayesian opinion of agent i = p about the likelihood of A, conditional
on what p knows, is deﬁned by the function iA = pA : Ω→[0, 1] iA(ω) = pA(ω) = π(P(ω) ∩A) π(P(ω))
, and, likewise, when i = q, iA(ω) = qA(ω) = π(Q(ω) ∩A) π(Q(ω))
, deﬁnes q’s Bayesian opinion of the likelihood of A conditional on what q
knows.
If i = p, then, after hearing p’s Bayesian opinion pA, q will revise her
understanding of the world, replacing Q with Q′ = Q ∨pA deﬁned by [Q ∨pA](ω) = Q(ω) ∩{ω′ : pA(ω′) = pA(ω)} for all ω ∈Ω. 3 Similarly, if i = q, then after hearing Q’s Bayesian opinion qA, p will revise
his understanding of the world, replacing P with P ′ = P ∨qA deﬁned by [P ∨qA](ω) = P(ω) ∩{ω′ : qA(ω′) = qA(ω)} for all ω ∈Ω. Thus the Bayesian opinion framework (Ω, π, A, P, Q, p) generates a unique
successor (Ω, π, A, P ′, Q′, −p) = (Ω, π, A, P, [Q ∨pA], q) and the Bayesian
opinion framework (Ω, π, A, P, Q, q) generates a unique successor (Ω, π, A, P ′, Q′, −q) = (Ω, π, A, [P ∨qA], Q, p). It follows that any Bayesian opinion framework (Ω, π, A, P, Q, i) gener-
ates a uniquely deﬁned inﬁnite sequence of Bayesian opinion frameworks (Ω, π, A, P, Q, i) = (Ω, π, A, P1, Q1, i1),
(Ω, π, A, P2, Q2, i2 = −i),
(Ω, π, A, P3, Q3, i3 = i),
(Ω, π, A, P4, Q4, i4 = −i),
. . . in which, at each period, one agent i gives his opinion based on his partition
at that time, and then in the next period the other agent −i gives her
opinion based on her previous partition revised in light of the previous opinion
expressed by him. We call this whole inﬁnite sequence a Bayesian Dialogue
(Ω, π, A, P, Q, i)∞. Dialogues and Rational Dialogues Bayesian dialogues contain many counterfactual statements, covering opin-
ions conditional on all possible worlds ω ∈Ω. In reality we typically only hear
about a ﬁnite number of actual opinions. We deﬁne a dialogue as a ﬁnite
sequence of opinions or beliefs (b1, b2, ...., bT) with bt ∈[0, 1] for all t. Once
we specify a ﬁxed state of the world ω∗∈Ω, every Bayesian dialogue gener-
ates an inﬁnite sequence of beliefs (Ω, π, A, P, Q, i, ω∗)∞≡(r1, r2, ...) where
rt = it,A(ω∗) is the opinion expressed by the opining agent at time t for state
ω∗. We call this inﬁnite sequence of opinions a Bayesian dialogue at a ﬁxed
state. A rational dialogue is any ﬁnite sequence of beliefs (r1, r2, ..., rT) that
can be realized as the ﬁrst part of a Bayesian dialogue at a ﬁxed state. Geanakoplos and Polemarchakis (1982) showed that in any Bayesian di-
alogue at a ﬁxed state, (r1, r2, ...), there must be a ﬁnite time T by which 4 consensus is reached, rt = rT for all t ≥T. Moreover, because Bayesian ra-
tional agents believe in each other’s rationality, if for some T, rT ∈{0, 1},
meaning one of the agents is absolutely certain and will never change his/her
mind, then consensus must have already been reached by time T.
We say that the event E ⊂Ωis common knowledge at ω∗∈E if P(ω) ∪
Q(ω) ⊂E for all ω ∈E. It is evident that if E ⊂Ωis common knowledge at
ω∗, then the Bayesian dialogue at a ﬁxed state ω∗(Ω, π, A, P, Q, iA, ω∗)∞≡
(r1, r2, ...) does not depend on any P(ω), Q(ω) or π(ω) for ω /
∈E. Irrational Dialogues? Are there dialogues (b1, ..., bT) that look so crazy that they could not be the
beginnings of a rational dialogue? Geanakoplos and Polemarchakis showed
that given any positive integer n, there is a Bayesian dialogue (c, d, c, d, ...,
c, d, c, c, ...) in which one agent obstinately maintains the opinion c while
the other maintains d ̸= c, and then, suddenly, after n such alternations,
consensus is reached at c.
In an unpublished paper Polemarchakis (2016) showed that any dialogue
could be rational. Di Tillio, Lehrer, and Samet (2022) extended the the-
orem to inﬁnite dialogues. The following theorem gives a similar result to
Polemarchakis (2016) but in a slightly diﬀerent setting and with a diﬀerent
proof.
There is one property that must hold for any rational dialogue, because
rationality presumes both agents are rational and know that both are ratio-
nal. If one of the agents is certain, then the other must immediately agree.
Certainty is tantamount to claiming a proof, and if the other does not agree,
one of the two interlocutors must not be rational. Deﬁnition: The dialogue (b1, ..., bT) violates certainty acquiescence if for
some t < T, bt ∈{0, 1}, yet bt+1 ̸= bt. Needless to say, if in the dialogue (b1, ..., bT) nobody expresses absolute
certainty, then the dialogue does not violate certainty acquiescence.
The
opinions could bounce around arbitrarily, as long as none hit 0 or 1. Theorem
Let
(b1, ..., bT) 5 be an arbitrary dialogue that does not violate certainty acquiescence. Then
(b1, ..., bT) is a rational dialogue generated by some (Ω, π, A, P, Q, pA, ω∗)∞.
Moreover, consensus is reached at time T at bT. Proof
The proof is by backward induction. Suppose T = 1. Suppose 0 <
bT < 1. Let Ω= {y, n}, and let A = {y}. Let π(y) = bT, and π(n) = 1 −bT.
Let P = Q = {{y, n}}. Let ω∗= y. Clearly consensus is reached at bT
because both agents have the same information.
If bT = 1, delete the point n, and continue as above.
If bT = 0, let
Ω= {y, n}, and let A = {y} and let P = Q = {{y}, {ω∗= n}} and let
π(y) = π(n) = 1/2. Clearly in all three cases the rational dialogue reaches
consensus on the ﬁrst step at bT, no matter which agent is the ﬁrst opiner.
Now suppose the theorem has been proved for all T ≤n and let (b1, b2, ...,
bT) be given for T = n + 1. By the induction hypothesis we can ﬁnd a
Bayesian dialogue at a ﬁxed state (Ω, π, A, P, Q, q, ω∗)∞= (b2, ..., bT, ...), with
consensus at bT, in which q is the ﬁrst speaker (with opinion b2). We shall
now deﬁne
(Ω∗, π∗, A∗, P ∗, Q∗, p, ω∗∗)∞≡(r1, ..., rT, ...) with consensus at time T at rT, with rt = bt for t = 1, ..., T, in which p is the
ﬁrst speaker. If b1 = 1, then by certainty acquiescence, bt = 1 for all t and
we can rationalize that with the Bayesian dialogue in the second paragraph,
and similarly if b1 = 0.
So suppose 0 < b1 < 1. Deﬁne Ω∗and P ∗by adding to Ωtwo extra points
yc, nc for each partition cell Pc, so P ∗
c = Pc ∪{yc, nc}. The partition Q∗adds
two cells to those already in Q, namely Q∗
y consisting of all the yc, and the
other Q∗
n consisting of all the nc. A∗extends A by including also all the yc.
This situation is depicted in Diagrams 1 and 2. 6 7 The crucial step is to note that for every partition cell Pc, there exists
numbers 0 < π(yc) < 1 and 0 < π(nc) < 1 such that b1 =
π(A ∩Pc) + π(yc) π(Pc) + π(yc) + π(nc) This extends the probability measure π to a measure on all of Ω∗. Deﬁne
π∗by rescaling the π (over all Ω∗) so that they add to 1. Observe that the
rescaling in numerator and denominator cancel, so for all Pc, b1 =
π(A ∩Pc) + π(yc) π(Pc) + π(yc) + π(nc) = π∗(A∗∩P ∗
c ) π∗(P ∗
c )
. Take ω∗∗= ω∗. This completes the deﬁnition of the Bayesianl dialogue an-
nounced above.
At the ﬁrst step agent p announces pA(ω∗∗) ≡r1 = π∗(A∗∩P ∗(ω∗∗)) π∗(P ∗(ω∗∗))
= b1 and reveals nothing, because, as noted, for every partition cell p would an-
nounce the same, Hence at the next step Q∗′ = Q∗. But ω∗∗= ω∗∈Ω, hence
by construction Q∗(ω∗∗) = Q(ω∗∗) = Q(ω∗) ⊂Ω. Thus q then announces π∗(A∗∩Q∗′(ω∗∗)) π∗(Q∗′(ω∗∗))
= π(A ∩Q(ω∗)) π(Q(ω∗))
= b2, where the last equality follows from the induction hypothesis and the fact
that π∗scales π. If b2 ∈{0, 1}, then this rational dialogue, like all rational
dialogues, repeats b2 thereafter, reproducing the given dialogue which, by
certainty acquiescence would have repeat b2 thereafter.
So suppose 0 < b2 < 1. Then this announcement of b2 makes it common
knowledge that ω∗∗∈Ω, because had q seen partition cell Q∗
y or Q∗
n, she would
have announced 1 or 0 instead of b2. Thus the Bayesian dialogue at a ﬁxed
state (Ω∗, π∗, A∗, P ∗, Q∗, p, ω∗∗) begins with b1 and then from step 2 onwards
proceeds as the Bayesian dialogue with a ﬁxed state (Ω, π, A, P, Q, q, ω∗).■ Remarks An Example of the Construction
The constructive argument above can
generate any dialogue, no matter how curious. For example, the two agents 8 could agree with each other on say the probability 1/4 for many iterations,
and then suddenly jump to consensus at 3/4.
We give the construction for the dialogue (1 4, 1 4, 1 4, 1 4, 3 4, 3 4) in the matrix below, where each y corresponds to a diﬀerent state in A
and each n corresponds to a diﬀerent state in Ω\A, and the numbers in the
brackets are measures for the corresponding states. The probability measure
that is the common prior of the agents is given by normalizing these measures
to add to one, namely the numbers in brackets divided by 132 3. Observe that
conditional probabilities are not aﬀected by replacing a measure with any
scalar multiple of the measure. The partition of agent p consists of the rows
of the matrix, and the partition of agent q corresponds to the columns of the
matrix. The state of nature ω∗is the y in the top left corner. Notice that
the top left cell of the matrix is the only one containing two points. y[ 3 4], n[ 1 4]
y[0]
n[2]
y[0]
n[0]
y[0]
y[ 1 4]
n( 3 4)
y[0]
n[0]
n[2]
y[ 2 3]
n[0]
y[0]
n[0]
y[0]
y[0]
y[1]
y[0]
n[3]
n[0]
n[ 11 4 ]
n[ 1 4]
y[1]
n[0] The reader can check that p will announce 1/4 = (3/4)/(3/4 + 1/4 + 2),
revealing nothing since the conditional probability of A given any row is ex-
actly 1/4. Then q will announce 1/4, since the conditional probability of A
in the left most column is 1/4. That reveals precisely that ω∗is not in one
of the last two columns, since they would have led to the announcements of
1 or 0. With this information, p still says 1/4, since that is the conditional
probability of A given the top row without its last two elements. This an-
nouncement reveals that ω∗is not in the bottom two rows, since they would
have led to the announcements of 1 or 0. Agent q responds to this by still
saying 1/4 since that is the probability of A given the ﬁrst column without
its last two elements. That reveals to p that ω∗is not the second or third
columns, since they would have led to the announcements of 1 or 0. With
this information, p ﬁnally says 3/4. This reveals that ω∗is in the top left
cell, and gets agreement from q at 3/4.
The measure makes clear how the probabilities were constructed by back-
ward induction. The top left cell is ﬁrst in the construction. If that cell were 9 common knowledge, p and q would agree on 3/4, giving the last two opin-
ions (3/4, 3/4) in the dialogue. Next we add the second and third elements
of the ﬁrst column. The measures assigned to y and n induce q to assign
conditional probability of 1/4 to seeing this part of the ﬁrst column. Thus
we can generate the dialogue (1/4, 3/4, 3/4). y[ 3 4], n[ 1 4] →
y[ 3 4], n[ 1 4]
y[0]
n[2]
→ Next we added the second and third columns of the ﬁrst three rows, as-
signing the measures to make sure that player p gives conditional probability
of A of 1/4 to each row so far constructed. y[ 3 4], n[ 1 4]
y[0]
n[2]
y[0]
y[ 1 4]
n( 3 4)
n[2]
y[ 2 3]
n[0]
→ This gives us a dialogue (1/4, 1/4, 3/4, 3/4).Next we move to add the fourth
and ﬁfth rows, as indicated below, so that q assigns the same probability 1/4
to A in each column. This gives us a dialogue (1/4, 1/4, 1/4, 3/4, 3/4). Finally
we add the last two columns so that p gives conditional probability of A of
1/4 to each row, giving us the whole dialogue (1/4, 1/4, 1/4, 1/4, 3/4, 3/4). y[ 3 4], n[ 1 4]
y[0]
n[2]
y[0]
y[ 1 4]
n( 3 4)
n[2]
y[ 2 3]
n[0]
y[0]
y[0]
y[1]
n[0]
n[ 11 4 ]
n[ 1 4] → y[ 3 4], n[ 1 4]
y[0]
n[2]
y[0]
n[0]
y[0]
y[ 1 4]
n( 3 4)
y[0]
n[0]
n[2]
y[ 2 3]
n[0]
y[0]
n[0]
y[0]
y[0]
y[1]
y[0]
n[3]
n[0]
n[ 11 4 ]
n[ 1 4]
y[1]
n[0] Experts
We described a didactic dialogue as one in which an expert leads
a student through a conversation. At a state of the world, ω∗, individual 1
is an expert concerning the event A if no information in the join (coarsest
reﬁnement of the partitions of the individuals) would cause him to alter his
beliefs.
A didactic dialogue is a dialogue (¯
q, q2, . . . , ¯
q, q2t, . . . , ¯
q),
0 ≤¯
q, ..., qT ≤1, where the opinion at t odd is unchanging, q2t+1 = ¯
q, and at t even, q2t, is
arbitrary but never 1 or 0. 10 Corollary. Any didactic dialogue (¯
q, q2, . . . , ¯
q, q2t, . . . , ¯
q) is a rational dia-
logue. The following matrix of states and measures displays a Bayesian opinion
framework, with the row player opining ﬁrst. At the ﬁxed state given by y
in the to left, this gives a Bayesian dialogue with the opinions (3 4, 1 4, 3 4, 1 4, 3 4, 3 4, ....) y[ 3 4], n[ 1 4]
y[0]
n[0]
y[0]
n[0]
y[0]
y[ 3 4]
n( 1 4)
y[0]
n[0]
n[2]
y[6]
n[0]
y[0]
n[0]
y[0]
y[0]
y[ 1 12]
y[0]
n[ 1 36]
n[0]
n[ 81 4 ]
n[0]
y[ 243 4 ]
n[0] Silence
Here, a dialogue is an alternating sequence of opinions. Formally,
an interlocutor cannot remain silent when it is her turn to speak. One can
interpret silence by an interlocutor at t as the repetition of her opinion at
t −2 : that is, bt = bt−2. Thus if our tape contains only the opinion of one
agent that is changing over time, we can interpret it as a conversation with
an expert who constantly repeats the same opinion.",0
"Systems biology models are useful models of
complex biological systems that may require a
large amount of experimental data to fit each
model’s parameters or to approximate a likeli-
hood function. These models range from a few to
thousands of parameters depending on the com-
plexity of the biological system modeled, poten-
tially making the task of fitting parameters to the
model difficult - especially when new experimen-
tal data cannot be gathered. We demonstrate a
method that uses structural biology predictions to
augment systems biology models to improve sys-
tems biology models’ predictions without having
to gather more experimental data. Additionally,
we show how systems biology models’ predic-
tions can help evaluate novel structural biology
hypotheses, which may also be expensive or in-
feasible to validate. 1. Introduction Systems biology models start from basic physical and chem-
ical principles, and gradually build more complicated mod-
els, from cell circuits to organ systems, that in the limit of
complexity can describe human life (Alon, 2019). Despite
modeling from fundamental biological knowledge, these
models may still require a great deal of data to accurately
predict biological responses, reflecting uncertainty in our
biological knowledge. Unfortunately, gathering more data
to increase the confidence in model predictions may be in-
feasible due to high cost. Conversely, when beginning a
scientific study one may have too many options to choose
from to effectively study their system and may want to
use educated priors to begin data collection using princi-
pled methods such as Bayesian optimal experimental design
(BOED) (Lindley, 1956; Rainforth et al., 2024). 1Department of Biomedical Engineering, University of Cali-
fornia, Irvine, United States. Correspondence to: Vincent Zaballa
<vzaballa@uci.edu>. Accepted at the 1st Machine Learning for Life and Material Sci-
ences Workshop at ICML 2024. Copyright 2024 by the author(s). Structural biology in the form of protein structure prediction
has recently made great strides in predicting structures of
single-chain proteins based on years of curated data col-
lected from experiments (Jumper et al., 2021; Baek et al.,
2021). Prediction of multi-chain protein complexes quickly
followed, providing a new depth of understanding to mul-
timeric protein structures (Evans et al., 2021). These new
capabilities provide rich details to help form new biological
hypotheses but are limited in scope to static descriptions of
biological systems. We demonstrate a method that utilizes the extrapolation
capabilities of systems biology models with the structural
accuracy of protein structure predictions. This method can
improve systems biology models’ predictions and refine
hypotheses generated by protein structure prediction tools,
with implications in drug development. We provide an
example using a model of the Bone Morphogenetic Protein
(BMP) pathway with previously-collected experimental data
and the structure prediction of a surface receptor complex
in the BMP pathway. We also show how this method can
help to evaluate new structural biology hypotheses, which
can be expensive to evaluate using X-ray crystallography or
cryo-electron microscopy (cryo-EM) techniques. 2. Background Bone Morphogenetic Protein pathway The BMP pathway
is utilized in cell-cell communication, where homologous
BMP ligands produced by one set of cells can act in com-
binations to elicit a response in another set of cells that
express BMP receptors (Antebi et al., 2017). This context-
dependent messaging means that the same message sent in
the form of a combination of BMP ligands by one cell in the
pathway can be interpreted in different ways by cells with
different sets and concentrations of BMP receptors. This
“promiscuous signaling” can be modeled by mass action ki-
netics, which effectively describes the competitive binding
of BMP ligands to BMP receptor complexes. There are
multiple models to describe the BMP pathway and we focus
on the “onestep” model Ai + Bk + Lj
K
G
GGGGG
B
F GGGGG
G Tijk,
(1) arXiv:2407.08612v1  [q-bio.QM]  11 Jul 2024 Reducing Uncertainty Through Mutual Information in Structural and Systems Biology Complex prediction
Docking prediction Gather experimental
data
Screen Droplet, Update Parameter Fit, & Queue Next Dose Incubation Stage Throughput Screening YFP Count Old
New Posterior Parameters Inference Evolutionary analysis Return improved
prediction <latexit sha1_base64=""SVo8MYPo/SLAdo78gNUDEvyAO4="">ACF3icbVC7TsMwFHXKq5RXgJHFokJioUoqBIwIFsYi0YLURJHjOq1VJ47sG0QV8hcs/AoLAwixwsbf4LYZyuNIlo7PuVf3hOmgmtwnC+rMje/sLhUXa6trK6tb9ibWx0tM0VZm0oh1U1INBM8YW3gINhNqhiJQ8Guw+H52L+ZUpzmVzBKGV+TPoJjzglYKTAbnj32IsJDMIovysCiQ9mv7kH7A5yKXpFgb37oBnYdafhTID/ErckdVSiFdifXk/SLGYJUEG07rpOCn5OFHAqWFHzMs1SQoekz7qGJiRm2s8ndxV4zyg9HElXgJ4os525CTWehSHpnK8s/7tjcX/vG4G0Ymf8yTNgCV0OijKBAaJxyHhHleMghgZQqjiZldMB0QRCibKmgnB/X3yX9JpNtyjxtHlYf30rIyjinbQLtpHLjpGp+gCtVAbUfSAntALerUerWfrzXqflasmcb/YD18Q3lM6Bp</latexit>kxo −xoldk2 <latexit sha1_base64=""3s5Js6g589+gNC54DTcUambS6s="">ACF3icbVDLTsMwEHR4lvIqcORiUSFxoUoQKhwruHAEiVKkpocdwMWjhPZG2iV5i+48CtcOIAQV7jxN7iPA6+RLI1ndrW7E6ZSGHTdT2dqemZ2br60UF5cWl5ZraytX5gk0xyaPJGJvgyZASkUNFGghMtUA4tDCa3w5njot25BG5Goc+yn0InZlRKR4AytFRq/oD6McPrMp7RZDQ3e/f3EfoYa7grioPwj2gkrVrbkj0L/Em5AqmeA0qHz43YRnMSjkhnT9twUOznTKLiEouxnBlLGb9gVtC1VLAbTyUd3FXTbKl0aJdo+hXSkfu/IWxMPw5t5XBn89sbiv957Qyjw04uVJohKD4eFGWSYkKHIdGu0MBR9i1hXAu7K+XTDONsqyDcH7fJfcrFX8+q1+tl+tXE0iaNENskW2SEeOSANckJOSZNwck8eyTN5cR6cJ+fVeRuXTjmTng3yA87F/YjoHQ=</latexit>kxo −xnewk2 Binding affinity prediction <latexit sha1_base64=""W/3iBS9UtszajBs75AE8mdgw9I="">AB+nicbVBNS8NAEN3Ur1q/Uj16WSyCp5KIVI9FL4KXCvYD2hA27dLMJuxO1xPwULx4U8eov8ea/cdvmoK0PBh7vzTAzL4gF1+A431ZhZXVtfaO4Wdra3tnds8v7LR0lirImjUSkOgHRTHDJmsBsE6sGAkDwdrB+Grqt+Z0jySdzCJmReSoeQDTgkYybfLN37aA/YIqQaVUMgy364VWcGvEzcnFRQjoZvf/X6EU1CJoEKonXdWLwUqKAU8GyUi/RLCZ0TIasa6gkIdNeOjs9w8dG6eNBpExJwDP190RKQq0nYWA6QwIjvehNxf+8bgKDCy/lMk6ASTpfNEgEhghPc8B9rhgFMTGEUMXNrZiOiCIUTFolE4K7+PIyaZ1W3Vq1dntWqV/mcRTRITpCJ8hF56iOrlEDNRFD+gZvaI368l6sd6tj3lrwcpnDtAfWJ8/YeOUwA=</latexit>Kstruct Structure prediction Low High
Accuracy Update complex
posterior Approximate
complex posterior Systems biology
model A
B
C
D
E F
G
Fit flow-based likelihood 
to experimental data pφ(x|Keq) = N
Y i=1 pφ(xi|Keq) H
I
J
K P <latexit sha1_base64=""X1WvaPZvUob9HlNRrBUfxvOhtg8="">AB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48tWltoQ9lsJ+3azSbsboQS+gu8eFAQr/4jb/4bt20O2vpg4PHeDPzgkRwbVz32ymsrW9sbhW3Szu7e/sH5cOjBx2nimGLxSJWnYBqFFxiy3AjsJMopFEgsB2Mb2Z+wmV5rG8N5ME/YgOJQ85o8ZKzbt+ueJW3TnIKvFyUoEcjX75qzeIWRqhNExQrbuemxg/o8pwJnBa6qUaE8rGdIhdSyWNUPvZ/NApObPKgISxsiUNmau/JzIaT2JAtsZUTPSy95M/M/rpia8jMuk9SgZItFYSqIicnsazLgCpkRE0soU9zeStiIKsqMzaZkQ/CWX14l7YuqV6t6XrNWqV/neRThBE7hHDy4hDrcQgNawADhGV7hzXl0Xpx352PRWnDymWP4A+fzB0QgjRA=</latexit>S <latexit sha1_base64=""DpmE4rG+rjC9ngP7SATR5fdcvzY="">AB9nicbVBNS8NAEJ3Ur1q/qh69BIvgqSRS0GPRi8cK1hbaWDbSbt2sxt2N0oJ/R9ePCiIV3+LN/+N2zYHbX0w8Hhvhpl5YcKZNp737RWVtfWN4qbpa3tnd298v7BnZapotikvVDolGzgQ2DTMc24lCEocW+Hoauq3HlFpJsWtGScYxGQgWMQoMVa672KiGZeil7GH0aRXrnhVbwZ3mfg5qUCORq/81e1LmsYoDOVE647vJSbIiDKMcpyUuqnGhNARGWDHUkFi1E2u3rinlil70ZS2RLGnam/JzISaz2OQ9sZEzPUi95U/M/rpCa6CDImktSgoPNFUcpdI91pBG6fKaSGjy0hVDF7q0uHRBFqbFAlG4K/+PIyaZ1V/VrV929qlfplnkcRjuAYTsGHc6jDNTSgCRQUPMrvDlPzovz7nzMWwtOPnMIf+B8/gDKkJMq</latexit>✏ijk <latexit sha1_base64=""yTU1/GILW90FsXyamBKqJQeXwQI="">AB8XicbVBNSwMxEJ2tX7V+VT16CRbBU9mVgh6LXgQvFbq20q4lm2bb2CS7JFmhLP0VXjwoiFf/jTf/jWm7B219MPB4b4aZeWHCmTau+0UVlbX1jeKm6Wt7Z3dvfL+wZ2OU0WoT2Ieq3aINeVMUt8w2k7URSLkNWOLqa+q0nqjSLZdOMExoIPJAsYgQbK93f9DL2OJo8NHvlilt1Z0DLxMtJBXI0euWvbj8mqaDSEI617nhuYoIMK8MIp5NSN9U0wWSEB7RjqcSC6iCbHTxBJ1bpoyhWtqRBM/X3RIaF1mMR2k6BzVAvelPxP6+TmugiyJhMUkMlmS+KUo5MjKbfoz5TlBg+tgQTxeytiAyxwsTYjEo2BG/x5WXSOqt6tarn3dYq9cs8jyIcwTGcgfnUIdraIAPBAQ8wyu8Ocp5cd6dj3lrwclnDuEPnM8faLaQnw=</latexit>KT ijk <latexit sha1_base64=""i+LYtdNT6zdP5DUd59ieWMgQmA="">ACNXicbVDLSgMxFM3UV62vqks3wSK0mzIjUl0W3Qi6qGAf0ClDJs20oZlJTDJimfan3PgfrnThQhG3/oJpOwtPRA4nHMPuf4glGlbfvVyiwtr6yuZdzG5tb2zv53b2G4rHEpI4547LlI0UYjUhdU81IS0iCQp+Rpj+4mPjNeyIV5dGtHgrSCVEvogHFSBvJy18LzxV9WrzyEnI3hiPohkj3/SB5GHu8BF0hudAcplO/TiaZUpQpOmSly/YZXsKuEiclBRAipqXf3a7HMchiTRmSKm2YwvdSZDUFDMyzrmxIgLhAeqRtqERConqJNOrx/DIKF0YcGlepOFU/Z1IUKjUMPTN5GRtNe9NxP+8dqyDs05CIxFrEuHZR0HMoOlhUiHsUkmwZkNDEJbU7ApxH0mEtSk6Z0pw5k9eJI3jslMpV25OCtXztI4sOACHoAgcAq4BLUQB1g8AhewDv4sJ6sN+vT+pqNZqw0sw/+wPr+AeyOrCE=</latexit>pφ(Keq|xo) / pφ(xo|Keq)p(Keq) 180º Variable
Conserved <latexit sha1_base64=""I1yxOlF8L21OYx8l1HQRbzio2Y="">ACXicbVHRSuNAFJ3Edbd21a364IMvwxahxbUkIupj0RdBEBe2KjS13Exv28GZJM7ciCXkJ3bfFXTGpYVrsXBs6c8+dmTNhoqQlz/vtuEuflj9/qa3Uv6urX9rbGxe2zg1AnsiVrG5DcGikhH2SJLC28Qg6FDhTXh/Vuo3j2isjKNfNEtwoGESybEUQAU1bNDFMAsInyjDhzngZWat3y+zwNQyRTaPBihImi9b9vnf/eWTCoz9t8LwgqF79sLeg/ytkTDXcH7fqw0fQ63rz4IvAr0GRVXQ0bz8EoFqnGiIQCa/u+l9AgA0NSKMzrQWoxAXEPE+wXMAKNdpDN08n5bsGM+Dg2xYqIz9l/HRloa2c6LDo10NR+1Eryf1o/pfHJIJNRkhJG4u2gcao4xbyMmo+kQUFqVgAQRhZ35WIKBgQVH1KG4H98iK4Puj4R52jn4fN7mkVR43tsO+sxXx2zLrsnF2xHhPsj8OcFafuvLjL7q7/tbqOpVni70rd/sVn4m0OA=</latexit>Keq ⇠(1 −↵)δ(Keq −Kstruct)+ ↵N(Kstruct, σ2) 





             	





          	

  <latexit sha1_base64=""JuL4UKrdpMa6D32uD4rpuHPwfIk="">AB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKRI9BL4KXCOYByRJmJ51kyOwjM73BsOx3ePGgiFc/xpt/4yTZgyYWNBRV3XR3eZEUGm3728qtrW9sbuW3Czu7e/sHxcOjhg5jxaHOQxmqlsc0SBFAHQVKaEUKmO9JaHqj25nfnIDSIgwecRqB67NBIPqCMzSe9NOghPmMA4TbvFkl256CrxMlIiWSodYtfnV7IYx8C5Jp3XbsCN2EKRcQlroxBoixkdsAG1DA+aDdpP50Sk9M0qP9kNlKkA6V39PJMzXeup7ptNnONTL3kz8z2vH2L92ExFEMULAF4v6saQY0lkCtCcUcJRTQxhXwtxK+ZApxtHkVDAhOMsvr5LGRdmplCsPl6XqTRZHnpyQU3JOHJFquSO1EidcDImz+SVvFkT68V6tz4WrTkrmzkmf2B9/gB/DZKY</latexit>Keq <latexit sha1_base64=""JuL4UKrdpMa6D32uD4rpuHPwfIk="">AB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKRI9BL4KXCOYByRJmJ51kyOwjM73BsOx3ePGgiFc/xpt/4yTZgyYWNBRV3XR3eZEUGm3728qtrW9sbuW3Czu7e/sHxcOjhg5jxaHOQxmqlsc0SBFAHQVKaEUKmO9JaHqj25nfnIDSIgwecRqB67NBIPqCMzSe9NOghPmMA4TbvFkl256CrxMlIiWSodYtfnV7IYx8C5Jp3XbsCN2EKRcQlroxBoixkdsAG1DA+aDdpP50Sk9M0qP9kNlKkA6V39PJMzXeup7ptNnONTL3kz8z2vH2L92ExFEMULAF4v6saQY0lkCtCcUcJRTQxhXwtxK+ZApxtHkVDAhOMsvr5LGRdmplCsPl6XqTRZHnpyQU3JOHJFquSO1EidcDImz+SVvFkT68V6tz4WrTkrmzkmf2B9/gB/DZKY</latexit>Keq Figure 1. Flowchart of our method combining structural (Top) and systems biology (Bottom) predictions. Top row shows A) initial
structure prediction or query from a database, B) prediction of the complex if the structure is not available, C) evolutionary analysis
of the sequence, where symmetric proteins such as BMP4 hommodimer ligands only require an analysis from one chain, D) docking
prediction, and E) binding affinity prediction of the docked complex (Kstruct). Bottom row shows F) the initial math model that describes
the biological pathway of interest, G) collection of experimental data, H) fitting a flow-based likelihood to the data, I) returning a posterior
distribution over model parameters (Keq) for the BMP4-BMPR1A-ACVR2A complex, J) adjusting sampling of the posterior by inclusion
of a spike-and-slab distribution of the structurally-predicted binding affinity (Kstruct), and K) the resulting improved prediction of the
systems biology model as measured by median distance from simulated data points using the new posterior. Processes can be run
independently up until K). where the equilibrium constant K represents the steady-state
forward and backward reactions, subscripts i, j, k represent
the ith type A receptor, the jth BMP homodimer ligand com-
plex, and kth type B receptor, and they all combine in one
step to form a trimeric complex T. The complex then phos-
phorylates SMAD1/5/8 to then send a downstream gene
expression signal (Alarc´
on et al., 2009). This model can
effectively describe how approximately ten homodimeric
ligand variants bind with heterotetramer receptor complexes
made up of two type I and two type II receptors, of which
there are four type I and three type type II receptors. Differ-
ent combinations of these components help to explain the
different tissues that arise during embryonic development
(Salazar et al., 2016; Butler & Dodd, 2003) and are impli-
cated in cancer (Bach et al., 2018; Kallioniemi, 2012), mak-
ing proteins in the pathway a potential drug target. However,
even though this model is capable of modeling responses
of the BMP pathway (Su et al., 2022), it lacks an explicit
likelihood function that can be used to describe the proba-
bility of observed data, which is important in uncertainty
quantification and BOED. We follow the work of Klumpe
et al. (2022) and use this onestep model of the BMP pathway
to model 5 BMP ligands, 3 type I, and 2 type II receptors
present. Simulation-based inference Simulation-based inference
(SBI), also known as Likelihood Free Inference (LFI), relies
on simulations from a model x ∼p(x|θ), observed data
xo, and a starting prior over model parameters p(θ), to fit a
probabilistic model of either the likelihood p(x|θ), posterior p(θ|x), or the likelihood-to-evidence ratio p(x, θ)/p(x)p(θ)
(Cranmer et al., 2020). Normalizing flows are commonly
used to approximate a likelihood or posterior given their
direct density estimation capabilities (Papamakarios & Mur-
ray, 2016; Papamakarios et al., 2018; Greenberg et al.,
2019), but diffusion (Sharrock et al., 2022) and flow-
matching (Dax et al., 2023) methods can also be used to
model the posterior, while classifiers are used to model the
likelihood-to-evidence ratio (Gutmann et al., 2018; Brehmer,
2021). SBI methods can be refined over multiple rounds of
Bayesian inference, thus refining the likelihood, posterior, or
likelihood-to-evidence ratio by repeated use of Bayes’ theo-
rem using observed data p(θ|xo) ∝p(xo|θ)p(θ). One sim-
ple validation metric in SBI is the median distance, which
we define as med(∥x0 −x∥2), which is used to measure
performance of SBI-based posterior distributions, where the
samples drawn from a simulator with an updated posterior
should be closer to observed data than samples drawn using
the prior. An intuitive way to view this metric is as a hyper-
sphere over the dimension of the data decreasing with new
information or better inference methods, as determined by
the posterior distribution. Protein structure prediction Protein structure prediction
methods such as AlphaFold2 (Jumper et al., 2021), Al-
phaFold3 (Abramson et al., 2024), and RoseTTAFold (Baek
et al., 2021) are capable of producing accurate protein struc-
ture predictions given a sequence of amino acids. In addition
to predicting single and multi-chain protein structures to
high accuracy on their own, these models have been used in Reducing Uncertainty Through Mutual Information in Structural and Systems Biology BMPR1A ACVR2A
x2 x2 Top View Figure 2. Symmetries present in the predicted BMP complex. Ro-
tating the BMP4 homodimer (green) 180º about the center of the
receptor complex (purple) results in a symmetric binding site as
the BMPR1A and ACVR2A receptors have an identical copy mir-
rored roughly at -45º and 45º about the horizontal and vertical axes,
respectively, for two additional binding positions. Rotating the
BMP4 homodimer 180º about its own center results in two more
positions, for a total of four possible binding positions. integrative modeling with experimental modalities, such as
cryo-EM, to achieve unprecedented accuracy in large-scale
protein structure prediction that provides greater insight as
to how biological complexes like the nuclear pore complex
operate (Fontana et al., 2022). Evolutionary analysis To predict how two protein struc-
tures might dock, evolutionary conservation data offers cru-
cial insights. Proteins that interact typically exhibit con-
served amino acids at their binding sites, as these regions
are crucial for catalysis and interaction (del Sol Mesa et al.,
2003). Evolutionary analysis uses a set of homologous
sequences, their alignment, and statistical methods to de-
termine a degree of conservation of each amino acid in the
sequence of interest. We used the ConfSurf server (Celniker
et al., 2013) to identify which amino acids in the BMP ho-
modimer and receptor complex were likely responsible for
binding in the complex. Protein-protein docking Prediction of how multiple multi-
chain proteins dock with one another is an important step
in understanding how complexes behave, and is a necessary
preconditioning task for predicting protein-protein bind-
ing affinity. There are many tools for docking (Lyskov &
Gray, 2008; Desta et al., 2020) and we used HADDOCK
(Van Zundert et al., 2016), a software tool that is able to
apply constraints as to which amino acids in the sequences
provided are important to docking. Specifically, we apply
the evolutionary conservation data to the Ambiguous In-
teraction Restraints (AIRs) to identify active and passive
residues in the protein complex that facilitate binding. Protein-protein binding affinity prediction Interactions
between proteins play a critical role in almost all forms of
cellular function, from DNA replication to signal transduc-
tion. The binding affinity is the measure of how likely a
complex will form between two or more proteins, and is thus
an important variable in biological systems. The affinity is
typically described through the dissociation constant Kd,
which is related to the Gibbs free energy ∆G = RT ln Kd.
This is a different quantity from the equilibrium constant,
K, of mass action kinetics and shown in Eq. (1). We can
relate the Gibbs free energy to the equilibrium constant of
mass action kinetics by the principle of detailed balance by
−ln K = NG, where N is the stoichiometric matrix of
the elements involved in binding (Dirks et al., 2007). We
are now able to relate posterior parameter predictions of the
mass action chemical equilibrium parameter K with protein
binding affinity dissociation constant Kd. We distinguish
the binding affinity from structural modeling by calling it
Kstruct for structure-based prediction of binding affinity and
Keq for mass action kinetics-based prediction of binding
affinity. We evaluate both results in terms of mass action
kinetics’s binding affinity, which is negative natural loga-
rithm of the dissociation constant Kd, meaning higher K
represents stronger binding and lower represents weaker
binding. We use Prodigy (Vangone & Bonvin, 2015), a
statistical model of binding affinity based on a pre-docked
protein complex, to predict binding affinity from a docked
structure due to its accuracy and ease of use. 3. Related Work Previous work to fit parameters of the BMP pathway relied
on maximum likelihood estimation point estimates (Klumpe
et al., 2022). However this point estimate directly lacks a
probabilistic interpretation. Bootstrap can provide a pseudo-
probability distribution that can be sampled but cannot eval-
uate the probability of a new data point. Alternatively, there
are many methods for predicting protein-protein interactions
using graph neural networks, ranging from contact location
prediction (Yuan et al., 2022; Gainza et al., 2020) to classifi-
cation of the interactions between proteins in a graph (Xu
et al., 2024). These predictions lack a physics-based rela-
tion to the dissociation constant (Kd), unlike mass action
kinetics, making them incomparable to our method. Hence,
we only evaluate how our method increases information in
systems biology using structural biology. 4. Method We combine the structural and systems biology components,
whose general workflow is shown in Fig. 1, to improve
the median distance calculation during SBI. We chose to
model the BMP4 homodimer ligand and a receptor complex
composed of two BMPR1A and two ACVR2A receptors. Reducing Uncertainty Through Mutual Information in Structural and Systems Biology 20.5 21.0 21.5 22.0 22.5 Median Distance 0
1
2
3
4
5
6
7
8
9 SBI Round 0.914 0.916 0.918 PPC 20
30
40
50
60
70
80
90
100 Kstruct Spike Location 20.25 20.30 20.35 20.40 20.45 20.50 20.55 20.60 Median Distance Sigma 1.0
5.0
10.0
15.0
20.0
Posterior 
Median Distance Figure 3. (Left) Median distance and posterior predictive check (PPC) plots of a trained surrogate normalizing flow likelihood of the BMP
model over 9 rounds. Decreasing median distance over multiple rounds indicates improved prediction accuracy and increasing PPC
indicates improved flexibility in modeling observed data xo. (Right) Median distance curves over different spike locations and varied by
the amount of noise in the slab portion of the spike-and-slab distribution showing improvement in the predicted median distance as the
strength of the binding affinity increases. The red horizontal line is the last posterior’s median distance from the initial SBI training (Left). Predicting structures in the BMP pathway & binding
affinity prediction First, we predicted the structure of
BMP4 homodimer and the ACVR2A and BMPR1A recep-
tor complex using AlphaFold2 via ColabFold (Mirdita et al.,
2022). We used AlphaFold2 multimer for prediction of
the BMP4 homodimer and the receptor complex. Once
the multi-chain structures were predicted, we performed an
evolutionary analysis using the ConfSurf server to identify
conserved evolutionary regions that would assist in specify-
ing AIRs during docking. Once we selected the most likely
docked structure based on the Haddock scoring function and
feasibility of the docked structure, we used the docked com-
plex to predict binding affinity, Kstruct, using the Prodigy
server. Posterior prediction of BMP model parameters We used
publicly-available data collected on the BMP pathway to
train a normalizing flow surrogate of a likelihood function,
pϕ(x|θ), where θ represents all sixty parameters in the
BMP onestep model, but we will focus on one parameter
Keq that represents the equilibrium constant of the BMP4-
BMPR1A-ACVR2A complex. The data have 940 dimen-
sions x ∈R940, with each dimension having five condi-
tioning variables in addition to the shared θ ∈R60. This
can be computationally difficult for normalizing flows, but
we circumvent this issue by using the identity of the joint
distribution for i.i.d. data: that the joint likelihood can be
described by the product of likelihoods for each individual
experiment pϕ(x|θ) = QN
i pϕ(xi|θ), which we modeled
using a neural spline flow (Durkan et al., 2019). We discuss
architecture choice and limitations in Appendix B. Our start-
ing prior p(θ) is a log-uniform distribution in the domain
[10−4, 102]. We train the flow by maximum likelihood over
nine rounds of sequential updates of SBI on the observed
data, xo, and evaluate its performance on the median dis- tance metric and its posterior predictive coverage (PPC).
PPC is a percent of the data that is covered by simulations
from the new posterior, rather than a distance, and we want
to cover as much data as possible. We return a posterior
by using a variational inference-based flow network that ap-
proximates the posterior qψ(θ) given its speed over MCMC-
based methods and similar accuracy (Gl¨
ockler et al., 2022).
We provide details about the SBI procedure in Appendix C. Integrating structural & systems biology There are two
straightforward ways to incorporate structural information
into systems biology. First, as a prior of one of the pa-
rameters θ, of the systems biology model, and second as
a distribution that is combined with the posterior distribu-
tion to return samples from weighted samples from both
distributions. We chose the second method as we first fit a
posterior, evaluated it for good structural candidates, such
as whether the posterior has a clear bimodal distribution,
and then updated the posterior distribution with a spike-
and-slab distribution (Lempers, 1971). The spike-and-slab
distribution models the point information of Kstruct as a
dirac delta function and adds Gaussian noise in a prede-
termined amount around it, as well as a mixing parameter
where we used α = 0.1. Sampling from this model be-
comes Keq ∼(1 −α)δ(Keq −Kstruct) + αN(Kstruct, σ2).
We evaluated samples generated with Gaussian noise of
[1., 5., 10., 15., 20.]. Finally, we combine the two by draw-
ing a set of initial samples for Keq from its posterior dis-
tribution, Keq ∼p(Keq | x), then each sampled value of
Keq is then evaluated with the spike-and-slab distribution.
Weights are calculated for each sample based on their eval-
uated probabilities under the spike-and-slab model. These
weights are computed to emphasize samples that align well
with the spike-and-slab distribution, thereby adjusting the
influence of each sample in subsequent analysis. Finally, a Reducing Uncertainty Through Mutual Information in Structural and Systems Biology new set of samples for Keq is drawn based on the normal-
ized weights. This resampling emphasizes values that are
consistent with both the original data and the assumptions
of the spike-and-slab model. 5. Results Predicting the BMP4-BMPR1A-ACVR2A structure
complex We found for the BMP4 homodimer that the
pLDDT (predicted local distance difference test) and PAE
(predicted alignment error) were confident for most of the
structure except for some exterior components, as shown
in Fig. 1 (A). The BMPR1A-ACVR2A receptor complex
demonstrated high pLDDT and PAE for components that
seem highly probable to function at the surface of the cell,
with four alpha helices clearly demonstrating a transmem-
brane portion of the receptor complex. We show more
visualizations of the proteins, show PAE plots, and dis-
cuss each multimeric structure prediction in more detail
Appendix A. We performed six AlphaFold2 recycles for
the BMP4 homodimer and twenty recycles for the recep-
tor complex, and determined the given structures provided
sufficient accuracy to proceed to evolutionary analysis. Evo-
lutionary analysis of each complex corroborated the pLDDT
and PAE scores, showing that high-confidence areas tended
to be conserved while some low-confidence areas that had
high conservation subsequently were important for docking
the two complexes together. Using the highly-conserved
regions of each protein present at the docking interfaces
between the two proteins, we were able to dock the two
in a non-traditional format. That is, the ligand-receptor
complex predicted does not represent the traditional way of
how BMP ligands fit with receptor complexes as a single
“lock and key” format (Katagiri & Watabe, 2016). Instead,
the BMP4 homodimer has a symmetric axis that seems to
allow it to dock in two different ways to the same receptor
complex, while the receptor complex can be rotated twice
to provide a total of four possible binding configurations,
as shown in Fig. 2. While our docking procedure only re-
turned one of the four conformations, we took this result
into consideration in subsequent analysis. Binding affinity
prediction using Prodigy returned a dissociation constant
(Kd) of 5.8×10−12 that translates into a binding affinity of
Keq = 10.24 and when multiplied by four is Keq = 40.97,
which accounts for the four different ways the complex can
be made. We tested this hypothesis in the creation of our
spike-and-slab distributions representing the binding affin-
ity, varying the dirac delta values from 20 to 100, the limit
of our prior distribution. Updating the BMP model posterior with structural in-
formation Fig. 3 shows the result of sampling from the
updated distribution on median distance prediction. We can
see that the median distance improves as we increase the prior assumption of the strength of the binding affinity and
decrease noise, all the way up to the upper limit of 102 of
our prior distribution, p(θ). This is surprising as this indi-
cates that this complex is very tightly binding - more than
what our prior modeling assumptions allowed - and warrant-
ing a review of our prior modeling assumptions. There is
also a discrepancy between the binding affinity produced
by Prodigy and the best-performing spike-and-slab distri-
bution. This could be due to error in the Prodigy binding
affinity prediction, errors in the structure prediction process,
or an error in the systems biology model. However, there
is agreement between the systems biology predictions and
structural biology predictions indicating that the complex
has a strong binding affinity. Regarding the choice of BMP
systems biology model, the current onestep model only pre-
dicts the binding of BMP ligands binding to pre-formed
receptor complexes but BMP ligands are also known to
bind to receptor components to induce complex formation
(Miyazono et al., 2010). This means the onestep model is
conservative in its binding affinity prediction and is a lower
bound of possible binding affinities, which is also supported
by our data. We did not evaluate the lower ranges of binding
affinity as the data suggested a clear worsening trend in the
median distance metric the lower the spike was located. 6. Discussion We demonstrated how to leverage structural biology to im-
prove a systems biology model’s predictions and in the
process supported a new hypothesis of the structure of the
BMP4-BMPR1A-ACVR2A complex operation by agree-
ment with a systems biology model’s predictions. The ben-
efits of our method are twofold. First, improving systems
biology models’ predictions can be helpful in cases when
it is infeasible to gather more data. By improving predic-
tions of systems biology models, we can improve design
of cell circuits in synthetic biology models or development
of therapeutics to treat diseases related to those biological
pathways. Our demonstration on a single complex in the
BMP pathway showed modest improvement in prediction
but a more comprehensive inclusion of structural biology
data will likely help to improve models’ predictions. Sec-
ond, our approach provides a novel method for proposing
and checking structural biology hypotheses by cross valida-
tion with a systems biology model, for example supporting
the structural hypothesis of a strong binding affinity in the
BMP4-BMPR1A-ACVR2A complex, and the structural ba-
sis as to why it might have such a strong affinity. On structural & systems biology mutual information
We now formalize the use of mutual information in our
framework. The mutual information between two random
variables X and Y can be defined as I(X; Y ) = H(X) −H(X|Y ),
(2) Reducing Uncertainty Through Mutual Information in Structural and Systems Biology which is the change in entropy about one random variable
with knowledge of another. Per Theorem 2.6.5 of Cover
(1999) (Information Can’t Hurt), adding relevant condi-
tional information will reduce the entropy of the underlying
data distribution, such that H(X|Y ) ≤H(X). In our case,
let Keq and Kstruct be latent random variables and X be
the random variable of the observed data point, treating the
median distance of simulated points x ∼X as a proxy for
the entropy. As we demonstrated, H(X|Keq, Kstruct) <
H(X|Keq); thus, we are able to increase information gained
I(X; Keq|Kstruct) > I(X; Keq). A natural extension of
information gain in biology is to drug discovery where mod-
els, such as DiffDock (Corso et al., 2022), can be included
as conditional information about the binding of a BMP lig-
and given a certain small molecule drug p(x|θ, Kstruct, β),
where p(β) is the probability of a given drug docking and
disrupting normal binding. Future work A key concern about this method is the in-
troduction of errors from any component of the structure
prediction pipeline, from single-chain structure prediction
to binding affinity prediction. Replacing these steps with
probabilistic structural models such as AlphaFlow (Jing
et al., 2024), protein-protein docking methods, and binding
affinity predictions would provide a way to help reduce un-
certainty in structure prediction using systems biology data.
We leave this for future work. Acknowledgements We would like to thank Heidi Klumpe, Eric Bourgain, and
Pieter Derdeyn for helpful discussions. This research was
funded by the National Institute of General Medical Sci-
ences (NIGMS) of the National Institutes of Health (NIH)
under award number 1F31GM145188-01.",0
"In this paper, we develop an agent-based model which integrates four important elements, 
i.e. organisational energy management policies/regulations, energy management 
technologies, electric appliances and equipment, and human behaviour, to simulate the 
electricity consumption in office buildings.  Based on a case study, we use this model to 
test the effectiveness of different electricity management strategies, and solve practical 
office electricity consumption problems. This paper theoretically contributes to an 
integration of the four elements involved in the complex organisational issue of office 
electricity consumption, and practically contributes to an application of an agent-based 
approach for office building electricity consumption study. Keywords Office electricity consumption, agent-based simulation, electricity management 
technologies, electricity management strategies 2 1. Introduction In the UK and many other industrialised countries, offices, as a basic unit for work buildings, 
are intensively distributed in big cities and urban areas. As climate change becomes a very 
important global issue, the UK government has set a target of cutting CO2 emission by 34% of 
1990 levels by 2020.  In the UK, the energy consumed in the service sector took up 14% of 
overall energy consumption of the whole country in 2001. Most of the energy for the service 
sector is used in various kinds of offices for heating, lighting, computing, catering and hot 
water. Thus, energy consumption in office buildings is one of the research areas which have 
significant importance for meeting the UK government’s 2020 target. Practically, energy consumption in a modern office building is a very complex organisational 
issue involving four important elements:  Energy management policies/regulations made by the energy management division of an organisation  Energy management technologies installed in the office building  (e.g. metering, monitoring, and automation of switch-on/off technologies)  Types and numbers of the electric equipment and appliances in the office building (e.g. lights, computers and heaters)  Energy users’ behaviour of using electric equipment and appliances in the office building. The four elements interact (Figure 1) in the following way: The energy management division 
makes energy management policies/regulations based on the energy management 
technologies installed in the building; energy management technologies monitor and control 
the energy consumed by electric equipment and appliances, and also influence the behaviour 
of energy users in the building; energy users’ behaviour of using electric equipment and 
appliances directly cause energy consumption. Yet in the UK the energy consumption in office buildings has been primarily administered by 
the Energy Performance of Buildings Directive (EPBD), in which the National Calculation 
Method (NCM) is defined. The NCM is a procedure for demonstrating compliance with the 
Building Regulations for buildings other than dwellings. The NCM proposes that by calculating 
the annual energy use for a proposed building and comparing it with the energy use of a 
comparable “notional” building, an “asset rating” can be produced in accordance with the 
EPBD. The calculations in the NCM make use of standard sets of data for different activity 
areas and call on common databases of construction and service elements. The calculations 
are carried out by approved simulation software packages (e.g. Operational Rating Calculation 
(ORCalc), Front-end Interface for the Simplified Building Energy Model engine (FI-SBEM), and 
Dynamic Simulation Modelling (DSM)). 3 Figure 1: The Four Elements in Office Energy Consumption Although the NCM is a powerful tool for office building management, it does not consider the 
areas of organisational energy management policies/regulations and human factors (i.e. 
energy users’ behaviour) which are very important elements influencing office building 
energy consumption. Looking at the literature in building energy research, there has been 
some interest in the behavioural aspects of energy use [e.g. 1, 2, 3, 4].  Generally these 
studies are empirical models based on measurements in practice. Hoes et al. [5] evaluate the 
effect of user behaviour on building energy performance and propose a decision methodology 
to optimise the design of buildings which have a known close interaction with its users. The 
study is a numerical simulation that tends to provide a guideline for building design. It does 
not reveal the dynamic processes of energy user behaviour and their interactions, and how 
these dynamic processes contribute to the overall energy consumption of an office building. 
Motivated by a desire to comprehensively understand the complex organisational issue of 
office energy consumption, the first objective of the research reported in this paper is to 
provide a dynamic computational simulation model which integrates the aforementioned four 
interactive elements involved in office energy consumption. Each organisation faces a dilemma in terms of energy consumption. On the one hand, it has 
to consume energy to satisfactorily meet the energy needs of staff and maintain comfort 
standards in its office buildings. One the other hand, it has to minimise its energy 
consumption through effective organisational energy management policies/regulations, in 
order to reduce energy bills. This dilemma presents a discord between energy users’ 
behaviour and organisational energy management policies. To solve this discord, an 
organisation needs to make a balanced energy management decision between the two. 
Although the methodology proposed by Hoes et al [5] provides a guideline for new building 
design, in terms of aiding the energy management policy/strategy making for existing human-
centric buildings (i.e. buildings which have large number of users) the methodology appears Energy Management Policies Made by the Energy Management Division Energy Management Technologies Office Electric Equipments and Appliances Staff’s behaviour of using energy 4 to be less robust. In recent years, agent-based simulation as a powerful decision support tool 
has been brought into the attention of researchers in building energy research areas [e.g. 6, 
7]. Drawing on the idea of agents, a second objective of the research reported in this paper is 
to develop a multi-agent decision-making framework to help organisations make proper 
energy management policies/regulations to deal with this dilemma: maintaining efficient 
energy consumption to satisfactorily meet staff members’ energy needs, whilst minimizing 
the energy consumption within the whole organisation, without significant investments on or 
changes to the current energy management technologies. Practically, energy consumption in a modern office building has two parts: gas consumption, 
which does not exist in some types of office buildings; and electricity consumption, which 
generally exists in almost all types of office buildings. In this paper, we specifically focus on 
studying electricity consumption in office buildings. The paper is structured as follows. In the 
second section, we briefly introduce agent-based simulation method and the rationale of 
using agent-based modelling in studying office building electricity consumption. In the third 
section, we describe our agent-based model of office building electricity consumption based 
on a case study in an academic building in Jubilee Campus, University of Nottingham, and 
present the questions that we are going to study with the model. In the fourth section, we 
analyse the outputs from the simulation, and draw some electricity management strategy 
implications. In the fifth section, we discuss the model, and in the sixth section, we summarise 
and conclude the study. 2. Agent-Based Simulation: Methodology and Modelling Rationale 2.1 Agent-Based Simulation In complexity science, agents are the constituent units of a complex adaptive system (CAS); 
agents are autonomous, intelligently behave on their own, and interact with each other in a 
CAS [8]. The intelligent behaviour and interactions of agents produce the global behaviour of 
a CAS. However, this type of global behaviour of a CAS cannot be conversely traced back to 
the behaviour and interactions of its constituent agents. Agents can be software programmes, 
machines, human beings, societies, or anything that is capable of intelligent behaviour [8]. 
Agent-based simulation is a computational modelling approach to study CASs. An agent-based 
model is composed of individual agents, commonly implemented in software as objects. 
Agent objects have states and rules of behaviour. Running an agent-based model simply 
amounts to instantiating an agent population, letting the agents behave and interact, and 
observing what happens globally [9]. Thus a unique advantage of agent-based simulation is 
that almost all behavioural attributes of agents can be captured and modelled. Agent-based 
simulation is widely adopted in studying CASs, particularly those with intelligent human 
beings (e.g. markets, societies, and organisations; for further information about agent-based 
simulation and its applications, please see [10, 11]) 2.2 Modelling Rationale 5 The electricity consumption in an office building is caused by the operation of various types of 
electric equipment and appliances (e.g. electric heaters, computing equipment and lights) in 
the office, which in turn are controlled by the electricity users’ behaviour. The electricity users 
interact, and their interactions can influence their behaviour of using electric appliances. The 
electricity users, electric appliances and the office building environment constitute a CAS 
which is well suited to agent-based modelling. Firth et al [12] carry out a study on the types of home electric appliances that people use 
most frequently and how these electric appliances contribute to the overall electricity 
consumption in a household. They classified home electric appliances into four categories 
based on their pattern of use:  Continuous appliances: Refer to electrical appliances such as clocks, burglar alarms and broadband Internet modems which require a constant amount of electricity.  Standby appliances: Refer to electrical appliances such as televisions and game consoles which have three modes of operation: in use, on standby, or switched off; 
standby appliances consume electricity when they are in the modes of “in use” and 
“on standby”, and some time even in the mode of “switched off” (e.g. Nintendo Wii 
game console); the only certain means to prevent them from consuming electricity is 
to disconnect their power supply.  Cold appliances: Refer to electric appliances such as fridges and freezers which are continuously in use but do not consume constant amount of electricity; instead their 
electricity consumption cycles between zero and a preset level.  Active appliances: Refer to electrical appliances such as lights, kettles and electrical cookers which are actively switched on or off by users and are clearly either in use or 
not in use; they do not have a standby mode and when switched off they do not 
consume electricity at all. The electric equipment and appliances in office buildings (computing equipment, lights and 
security devices) have the same patterns of electricity consumption as home electric 
appliances. Drawing on the idea of Firth et al [12], we see the electricity consumed by 
continuous appliances (e.g. security cameras, information displays and computer servers) and 
cold appliances (e.g. refrigerators) as base consumption, because these kinds of electric 
equipment and appliances (we term them base appliances) have to be switched on all the 
time; and we see the electricity consumed by active appliances (e.g. lights) and standby 
appliances (e.g. desktop computers and printers) as flexible consumption, because these 
kinds of electric equipment and appliances (we term them flexible appliances) can be 
switched on/off  at any time, depending on the behaviour of users. Thus, the total electricity 
consumption of an office building in a certain period of time can be formulated as: (1) 6 Where        is the base electricity consumption and relates to the number and types of 
continuous and cold appliances the office has;            is the flexible electricity consumption 
and relates to the number and types of active and standby appliances in the office. We see the electricity users as agents. Considering the individual active and standby 
appliances and the behaviour of their electricity user agents, we can further break down the 
flexible consumption: (2) Where     ,     ,      ,      are the maximum electricity consumption of each flexible appliance; n 
is the number of flexible appliances; and   ,   ,   ,    are the parameters reflecting the 
behaviour of the electricity user agents. We range β from 0 to 1. If β is near 0, it means that 
the electricity user agent of the flexible appliance always switches it off. If β is close to 1, it 
means that the electricity user agent of the flexible appliance always leaves it on. Combining equation 1 and 2, we can derive an equation to explain the electricity consumption 
in an office in a certain period of time: )    (3) Equation 3 explains how the behaviour of electricity user agents can contribute to the overall 
electricity consumption in an office. It is the rationale for developing an agent-based model of 
office building electricity consumption. 
 
3. Agent-Based Simulation of Office Building Electricity Consumption: A Case Study As electricity users’ behaviour is significantly influenced by the electricity management 
technologies and electricity management policies/regulations in an office building, the above 
equation potentially integrates the four elements we mentioned before (i.e. energy 
management 
policies/regulations, 
energy 
management 
technologies, 
electric equipment/appliances and user behaviour) and provides a theoretical basis for developing an 
agent-based simulation model of office building electricity consumption. Here we develop an 
agent-based model of office building electricity consumption based on an academic building 
in the School of Computer Science, located in Jubilee Campus, the University of Nottingham. 
We chose this case because it is very convenient for us to carry out surveys to understand 
users’ behaviour in the school, and also the Estate Office, who is responsible for the energy 
management in the University of Nottingham, kindly provided us with data about electricity 
management technologies and really electricity consumption in the school building. 3.1 Electricity Consumption in the School Building The School of Computer Science Building is situated in Jubilee Campus which was opened in 
1999. Built on a previously industrial site, Jubilee Campus is an exemplar of brownfield 
regeneration and has impeccable green credentials. In terms of energy technologies, one 7 important feature of the campus is the series of lakes which not only is the home of a variety 
of wildlife, but also provide storm water attenuation and cooling for the buildings in summers. 
Less visible, but equally important energy technologies are (1) the roofs of the buildings 
covered by low-growing alpine plants which help insulate and maintain steady temperatures 
within the buildings throughout the year, (2) super-efficient mechanical ventilation systems, 
(3) lighting sensors to reduce electricity consumption, and (4) photovoltaic cells integrated 
into the atrium roofs. Jubilee Campus has received many awards for its environment-friendly 
nature and energy efficiency of its buildings. The School of Computer Science Building occupies a central position in Jubilee Campus, and is 
the academic home of some hundreds of staff and students. The base electricity consumption 
in the school building includes security devices, information displays, computer servers, 
shared printers and ventilation systems. The flexible electricity consumption includes lights 
and office computers. In terms of electricity management technologies, the school building is 
equipped with light sensors and half-hourly metering systems. Based on these electricity 
management technologies, the energy management division (the Estate Office) has made 
automated lights electricity management strategy (lights are automatically switched on when 
staff enter a room and switched off in 20 minutes after staff leave the room), and also select 
two environmental champions to promote energy saving awareness in the school. Currently there are two practical electricity management issues arising in the school. One 
issue is the debate over the automated lighting management strategy. On the one hand, 
many technical people from the Estate Office believe that automated lighting management 
strategy is more energy-efficient than manual lighting management strategy (i.e. installing 
light switches to enable staff to control the lights).  On the other hand, many electricity users 
in the School of Computer Science believe that if they could control the light manually, the 
electricity consumption in the school would be less, as under the automated lighting 
management strategy the lights are off only after 20 minutes of their leave, which causes 
unnecessary consumption of electricity. The other issue is measuring the proportions of 
electricity consumed by lights and computers. Although currently the school is equipped with 
advanced half-hourly electricity meters which can tell us how much electricity is consumed by 
the whole school, they are not able to tell us how much electricity is consumed by computers 
and how much electricity is consumed by lights. Technically speaking, the amount of 
electricity consumed by lights and computers is related to the behaviour of their users. Thus 
we can hardly to measure it in a simply way. Focusing on this two practical electricity 
management issues, in this case study we are targeting two research questions: (1) Is 
automated lighting strategy always energy-efficient than staff-controlled lighting 
management strategy? (2) What are the proportions of electricity consumed by lights and 
computers respectively? 3.2 Agent Based Model of Office Building Electricity Consumption 8 For our case study we have chosen the first floor of the School of Computer Science. This floor 
is populated by academics, research staff, research students, and admin staff. The building 
plan of the floor is shown in Figure 2.  The details of the rooms and electric equipment and 
appliances on the first floor are listed in Table 1. Figure 2: The Building Plan Table 1: Details of Rooms and Electric Equipment and Appliances on the First Floor Item 
Number Rooms 
47 Lights 
239 Computers 
180 Printers 
24 Information Displays 
2 Maximum Number of Energy Users 
213 We designed the model environment based on the office plan on the first floor of the school 
building (Figure 2), and implemented the model in AnyLogic 6.5.0. The base electricity 
consumption of the school building is fixed (and therefore we don’t need to consider it in our 
simulation model; it will simply be added when we do the output analysis), and the flexible 9 consumption of the school building is caused by the interactions between flexible appliances 
(mainly lights and computers) and the electricity users. We therefore focus on the flexible 
consumption, and design three types of agents: electricity user agents, computer agents and 
light agents. These agents have been assigned to different rooms based on the office plan. 3.2.1 Behaviour of Electricity User Agents In order to understand the electricity consumption behaviour of the electricity users, we have 
carried out an extensive school wide empirical survey (questionnaire and observation). We 
deployed an online questionnaire, and emailed 200 staff and PhD students. In total we have 
received 143 valid responses (response rate = 71.5%). Our survey focuses on the electricity 
use behaviour of the electricity users (i.e. staff and PhD students) when they are in the School 
of Computer Science for work. A descriptive statistics of their behaviour patterns is shown in 
Table 2. Our observation shows that during each working day, electricity users gradually enter 
the school building, walk through the corridors, and enter different offices for work. Their 
behaviour in different stages can trigger the electricity consumption of different electric 
appliances. Based on this observation, we develop an electricity user state chart that allows 
us to represent the behaviour of electricity users, as shown in Figure 3. Figure 3: State Chart of Energy User Agents 10 Table 2: Statistics of Electricity User Behaviour Patterns Behaviour 
Pattern 
Frequency 
Remarks Time of Arrival at 
the School Arriving at the school between 
5am and 9am 8% 
These are designed as early bird 
electricity users agents Arriving at the school between 
9am and 10am 53% 
These are designed as timetable 
complier electricity users agents Arriving at the school between 
10am and 1pm 39% 
These are designed as flexible worker  
electricity users agents Frequency 
of switching 
off computers when 
leaving the office Every time 
1% 
These are designed as environment 
champion electricity users agents Most of the time 
8% 
These are designed as energy saver 
electricity users agents Not very often 
31% 
These are designed as regular user 
electricity users agents Never 
60% 
These are designed as big user 
electricity users agents Talking to others 
about 
energy issues 
in 
the school Very often 
8% 
These are used for designing the 
contact frequency between electricity 
user agents 
Sometimes 
31% Only occasionally 
45% Never 
15% Using the kitchen 
Almost every day 
36% 
These are used for designing the 
frequency of using the kitchen in the 
simulation model 
Some times 
14% Occasionally 
27% Never 
23% What can help 
the school reduce 
energy 
consumption? The energy management in the 
school is fully automated 32% 
These 
are 
used 
to 
support 
the simulation experiments The energy management is the 
school is controlled by users 35% Using more advanced energy 
management technologies 19% Giving energy users incentives 
for saving energy 10% Others 4% 11 We consider four different states of an electricity user agent’s behaviour in the model: out of 
school (outOfSchool), in corridor (InCorridor), in its own office (InOwnOffice) and in other 
rooms (InOtherRooms). In the outOfSchool state, the electricity user agent is not at work, thus 
does not trigger any electricity consumption. In the InCorridor state, as there are many lights 
in the corridors, the electricity user agent’s presence in the corridor triggers the lights on, 
which causes electricity consumption. In the InOwnOffice state, the electricity user agent’s 
presence in its own office triggers the office lights on, and its behaviour of using the computer 
in the office enables the computer in one of the following three modes: on, standby and off. 
Analogously, in the InOtherRooms state the electricity user agent’s presence in other rooms 
(e.g. toilet, kitchen and lab) triggers the electricity consumption of lights and computers (if 
any) in these rooms. The transitions between the outOfSchool state and the InCorridor state (both directions) is 
based on working timetable. Based on our empirical survey on working time, we have 
developed three stereotypes of electricity user agents: early birds, timetable compliers, and 
flexible workers. Early birds (mainly cleaners, security staff and some hard working students 
and staff) often come to the school between 5 am and 9 am, and leave the school according 
to their regular working time. Timetable compliers (mainly administrative staff and academic 
staff) often come to the school between 9 am and 10 am, and leave the school building often 
at 5:30 pm. Flexible workers (mainly academic staff, research staff and PhD students) come to 
school at any time between 10 am and 1 pm, and leave the school at any time after their 
arrival. Based on our statistics from our survey, we assign relevant parameters for the 
electricity user agents (Table 3).  We also consider that each electricity user agent has a very 
small probability (p=0.02) to work on Saturdays and Sundays. This consideration reflects the 
reality that a small number of hard working PhD students and research staff come to school 
on Saturdays and Sundays. Table 3: Stereotypes of Electricity User Agents and Parameters (I) Agent Stereotype 
Percentage 
Arrival Time 
Leave Time Early Birds 
8% 
Monday to Friday, between 5am and 9am, 
random uniform distribution Monday to Friday, between 5pm and 6pm, 
random uniform distribution Timetable Compliers 
53% 
Monday to Friday, between 9 am and 10 am, 
random uniform distribution Monday to Friday, between 5pm and 6pm, 
random uniform distribution Flexible Workers 
39% 
Monday to Friday, between 10 am and 1 pm, 
random uniform distribution Monday to Friday, between arrival time 
and 23pm, random uniform distribution The transition from the InCorridor state to the InOwnOffice state is an electricity user agent’s 
behaviour of entering its own offices. In the simulation model we set the transition rule 
timeout = 2, which reflects the reality that normally after about a two-minute walk in the 
corridors, an electricity user can reach his/her own office. In the InOwnOffice state, the 
electricity user agent’s presence triggers the lights in its own office on. The electricity user 
agent can either work with a computer (the sub-state working_with_computer), or work 
without a computer (the sub-state working_without_computer). The transition from the 12 working_without_computer sub-state to the working_with_computer sub-state is the 
electricity user agent’s behaviour of switching on the computer. We set the transition rule 
timeout = 2. This design is based on our empirical observation that normally an electricity 
user switches on his/her computer within 2 minutes after he/she enters his/her office. The 
transitions from the working_with_computer sub-state to the working_without_computer 
sub-state is the electricity user agent’s behaviour of switching off or setting the computer on 
standby. For setting the computer on standby, the transition rule is a probability (p = 0.05) 
derived from our empirical survey. For switching off the computer, the transition rule is a 
threshold control. We assume that each electricity user agent has a personality parameter 
energySavingAwareness, ranging from 0 to 100, to represent its awareness on energy saving. 
If an electricity user agent’s energySavingAwareness is greater than a threshold, it has a large 
probability to switch off the computer when it does not need to use the computer.  In the 
simulation, the threshold is adjustable, with value ranging from 0 to 100. Based on our 
empirical survey on staff’s energy awareness, we create four stereotypes of electricity user 
agent for the simulation model:  Environmental Champion, Energy Saver, Regular User, and 
Big User. Different stereotypes of electricity user agents have different levels of 
energySavingAwareness, and the probabilities for them to switch off the electric appliances 
that they do not have to use are different, as shown in Table 4. In the InOwnOffice state, an 
electricity user agent can also interact with other electricity user agents. Our empirical survey 
shows that in terms of energy issues in the school, the most widely used interacting means is 
using emails in offices. We thus use an internal transition “contact” within the InOwnOffice 
state to reflect the interactions between electricity user agents. The larger the 
energySavingAwareness an electricity user agent has, the larger probability it will send emails 
about energy saving issues to other electricity user agents who have interactions with it. We 
set the interacting social network type as “small world”.  Based on the statistics from our 
empirical survey, we assign relevant parameters for these stereotypes of electricity user 
agents (Table 4). Table 4: Stereotypes of Energy User Agents and Parameters (II) Stereotype of Agent 
Percentage 
energySavingAwareness 
Probability of Switching Off 
Unnecessary Electric Appliances Probability of Sending Email 
about Energy Issues to Others Environment 
Champion 1% 
Between 95 and 100, 
random uniform distribution 0.95 
0.9 Energy Saver 
8% 
Between 70 and 94, random 
uniform distribution 0.7 
0.6 Regular User 
31% 
Between 30 and 69, random 
uniform distribution 0.4 
0.2 Big User 
60% 
Between 0 and 29, random 
uniform distribution 0.2 
0.05 The transition from the InOwnOffice state to the InCorridor state reflects an electricity user 
agent’s behaviour of leaving its own office.  For an electricity user agent, this can happen at 
any time between its arrival time and leave time. Thus the transition rule is a stochastic event 
and the probability for it to happen is determined by its arrival time and leave time. Here we 13 consider two kinds of leaves: temporary leave and long time leave. Temporary leave means 
that the electricity user agent leaves its own office for less than 20 minutes, while long time 
leave means that it leaves its own office for more than 20 minutes. According to our empirical 
observation, people who temporally leave their offices do not usually switch off electric 
appliances, but people who leave their offices for a relatively long time do. The transition from the InCorridor state to the InOtherRooms state reflects the behaviour of 
using other facilities such as kitchens, toilets and labs in the school. For an electricity user 
agent, this behaviour can also happen at any time between its arrival time and leave time. 
Again we consider the transition rule as a stochastic event and the probability for it to happen 
is determined by the agent’s arrival time leave time. The transition from the InOtherRoom state to the InCorridor state reflects an electricity user 
agent’s behaviour of stopping using other facilities and leaving the facility rooms. The 
transition rule is a timeout. Here we set the time range from 1 to 10 (random uniform 
distribution), which reflect the reality that a user usually finishes using facilities such as toilets 
or kitchens within 10 minutes. The state chart, which we have developed based on our empirical survey, reflects the all 
behaviour of a real electricity user with regards to electricity consumption when he/she works 
in the school building. 3.2.2 Behaviour of Light Agents In the simulation model, we treat the lights in the school building as passive agents, which 
means that these agents do not have proactive behaviour. Instead, their behaviour is passive 
reacting to the behaviour of electricity user agents. Lights’ behaviour pattern is relatively 
simple, as they can only be either off or on, as shown in the light state chart (Figure 4) For a light, the transition from the off state to the on state is associated with the presence of 
electricity user agents if the light is automated by light sensors, or with the behaviour of 
switching on if there is a light switch which enables electricity user agents to have control 
over the light. Conversely, the transition from the on state to the off state is associated with 
electricity user agents’ leaving if the light is automated by light sensors, or with the behaviour 
of switching off if there is a light switch. According to the data provided by the Estate Office, 
when a light is on, its power is 60 Watts; when it is off, its power is 0. 14 Figure 4: State Chart of Light Agents 3.2.3 Behaviour of Computer Agents Similar to lights, computers are also treated as passive agents in our simulation model. 
Computers can be on, off, or in standby. Thus in the state chart of a computer, we consider 
these three states, as shown in Figure 5. For a computer, the transitions between off, on and stand_by states are related to electricity 
user agents’ behaviour of using the computer. According to our survey, when a computer is 
off, its overall power is 0; when it is on standby mode, its overall power is 25 Watts; and when 
it is on, its overall power is about 400 Watts. Figure 5: State Chart of Computer Agents 15 3.2.4 Model Implementation The model has been implemented in the simulation package AnyLogic 6.5.0 [13] on a standard 
PC with Windows XP SP3. We set each time step in the simulation model as one minute, and 
simulate the daily work of staff in the School of Computer Science and observe and analyse 
how their behaviour can result in a system level electricity consumption of the whole floor. 
The light agents and computer agents are assigned to each room, based on their real physical 
distribution in the school. The electricity user agents come to the school every morning, walk 
through the corridors and enter their own offices for work. They may also leave their offices, 
walk through the corridors and enter other rooms for using facilities such as toilet and 
kitchens. They interact with each other in terms of energy issues in the school, and this kind 
of interaction can increase their energySavingAwareness. They also interact with passive light 
and computer agents, and this kind of interaction can directly result in the system level 
electricity consumption of the school. An overview of the model is shown in Figure 6. We also 
animate the electricity user agents, and the interface of the model is shown in Figure 7. Figure 6: Overview of the Model 16 Figure 7: Interface of the Model 4. Simulation Experiments With our model, we have carried out three sets of experiments. We use these sets of 
experiments to test the validity of the model, design electricity management strategies for 
the Estate Office and help the Estate Office gain insights about the electricity consumption in 
the school. Experiment 1: Reproduce the current electricity management strategy of the school Currently, the computer science school is equipped with light sensors which automatically 
switch on the lights when they detect the presence of staff, and switch off the lights when 
they detect the absence of staff for 20 minutes. Thus based on the light sensor technology, 
the Estate Office has adopted an automated electricity management strategy in the school of 
computer science. In that sense, staff do not have control over the switch-on/off of the lights, 
and they only have control over the switch-on/off the computers. Our first set of experiments 
focuses on this and aims to reproduce the electricity management strategy. We set the model 
to the “automated” scenario, run the model and plot the system level school electricity 
consumption in Figure 8, from which we can see that the pattern of simulated school 
electricity consumption is quite similar to the real school electricity consumption provided by 
the Estate Office (Figure 8). The similarity from the comparison signifies that we have 
successfully reproduced the current electricity management strategy in the school of 
computer science, and also validates our model. 17 Figure 8: Comparison of Simulation Results and Empirical Results (Experiment 1) Note: In this figure the simulation result is the average of results of 20 replications with different random seeds. Experiment 2: Automated Strategy vs. Staff-Controlled Strategy Automated and manual lighting management each have advantages in office buildings [14]. 
Although the office buildings that have achieved the lowest reported lighting electricity 
consumption have done so with manual electricity management [15, 16], some studies show 
that under manual switching lighting management it was quite common for users to switch 
on lights even when there was adequate glarefree daylight [17]; once switched on, the lights 
were seldom switched off, regardless of the illumination provided by the daylight [16]. Thus 
there is an argument for an automated electricity management strategy which maintains 
efficient electricity consumption to satisfactorily meet electricity users’ electricity needs and 
meanwhile minimizes the cost of electricity without any user intervention in office buildings. 
Bourgeois et al. [18] show that under automated lighting management the electricity 
consumption is much less than that in automated lighting management. In the School of 
Computer Science, there seems to be a debate between the automated strategy and staff-
controlled strategy. On the one hand, anecdotally, many people, particularly technical people 
from the Estate Office, strongly believe that automated lighting is more energy-efficient than 
staff-controlled lighting. On the other hand, our empirical survey shows that some people in 
the School of Computer Science believe that if they could control the light manually, the 
electricity consumption in the school would be less, as under the automated lighting strategy 
the lights are off only after 20 minutes of their leave, which causes unnecessary consumption 
of electricity. Our second set of experiments focuses on this debate: we compare the 
simulation results under the two different lighting management strategies. The rationale for 
the two strategy scenarios is as follows: in the automated lighting management strategy 
scenarios, lights in an office are off 20 minutes after the last occupying electricity user agent’ 
leave, while in the staff-controlled lighting management strategy, lights in an office are 
switched off by the last occupying electricity user agent based on a probability. The 
probability is determined by the energySavingAwareness of the last occupying electricity user 
agent. The larger the energySavingAwareness the last occupying electricity user agent has, 
the larger the probability it will switch off the lights. The probabilities are assigned based on 18 Table 4. This design reflects the reality that under the staff-controlled lighting management 
strategy, staff can switch off the lights when they leave their offices. The more the staff are 
concerned about energy saving, the more possible they will switch off the lights when they 
leave their offices. The comparison of the simulation results of the two scenarios are shown in 
Figure 9, from which we can see that although the peak time electricity consumption is almost 
the same, the electricity consumption in staff-controlled lighting management strategy 
scenario is substantially higher than that in automated lighting management strategy scenario. 
Thus, one electricity management strategy implication we can draw from the simulation is 
that in the current circumstance, the automated lighting management strategy is more 
energy-efficient than staff-controlled electricity management strategy. Figure 9: Simulation Results (Experiment 2A) Note: In this figure the simulation result is the average of results of 20 replications with different random seeds. We note that the probabilities for staff to switch off lights when they leave their offices are 
related to their energySavingAwareness. One question to which we would like to seek answer 
from the model is that: if we increase the electricity user agents’ energySavingAwareness by 
enhance the interactions about energy issues between electricity user agents, is automated 
lighting management strategy still more energy-efficient than staff-controlled lighting 
management strategy? We increase the contact rate (i.e. the frequency of contact in a certain 
simulation period), run the model and gain the simulation results in Figure 10, which shows a 
negative answer to that question: when enhancing the interactions about energy saving 
between electricity user agents, automated strategy is less effective than staff-controlled 
strategy. Increasing electricity user agents’ energySavingAwareness through social 
interactions can significantly reduce the overall electricity consumption of the school. The 
senior management of the university has already realized the importance of increasing staff’s 
energy saving awareness, thus a university-wide campaign called “gogreener” has been 
carried out and two environmental champions have been appointed in each school to 
monitor the energy consumption of the school and enhance the interactions of staff in terms 
of energy issues. 19 Figure 10: Simulation Results (Experiment 2B) Note: This figure shows the simulated electricity consumption in both automated lighting management scenario and staff-
controlled lighting management scenario when the interactions between energy user agents have been enhanced.  In this 
figure the simulation results are the average of results of 20 replications with different random seeds. Experiment 3: Understanding the proportions of electricity consumed by lights and 
computers The Estate Office has installed some half-hourly electricity meters in the school building to 
monitor the electricity consumption in the School of Computer Science. Although these 
meters can tell us how much electricity is consumed by the school, they are not able to tell us 
how much electricity is consumed by computers and how much electricity is consumed by 
lights, which is also a question the Estate Office keen to know. As indicated in the model, the 
amount of electricity consumed by lights and computers is related to behaviour of electricity 
user agents, which makes it hard to be measured in a simply way. With the help of the 
simulation model, we can gain some insights into this issue. We run the model in automated 
scenario (i.e. the current lighting management strategy of the school), and plot the electricity 
consumption by both lights and computers in Figure 11, from which we can see the 
proportions of electricity consumed by computers and lights vary over time.  We also plot one 
week electricity consumption, as shown in Figure 11. 20 Figure 11: Electricity Consumed by Computers, Lights and Based Electric Appliances Note: The figure on the top is the amount of electricity consumed by lights, computers and base electric appliances, while the 
figure on the bottom shows the percentages. From the simulation results we can see that in the evenings and weekends, 
most of the electricity is consumed by computers (65%); in the daytime (Monday to Friday), the electricity consumed by 
computers (about 33%) is much less than that consumed by lights (52%). 5. Discussion Theory Based Agents vs. Empirical Survey Based Agents In social simulation, many agent-based simulation studies develop the agents in their models 
based on well-established social theories. For example, in marketing researchers develop 
consumer agents based on social psychological theories such as social comparison, imitation 
[e.g. 19, 20, 21] and the theory of planned behaviour [e.g. 22, 23]. In energy economics, Bunn 
and Oliveira [24] developed electricity market agents (i.e. electricity generating companies 
and electricity suppliers) based on market bidding theory to simulate the New Electricity 
Trading Arrangements (NETA) of England and Wales. Clearly, the existence of these well-
established social theories significantly facilitates the development of the social agents. In this particular case of modelling office building electricity consumption, however, there are 
no well-established theories that could be used to model staff behaviour towards electricity 
use.  We thus have to conduct time-consuming and costly empirical surveys and observations 
on the behaviour of real world objects (i.e. staff and PhD students in the school) and develop 
agents based on our empirical survey. Our survey covered most of the aspects of staff’s 
electricity use in the school. As a result, the state chart we have developed to represent the 
behaviour of electricity users in our model has a very strong empirical basis. That is also the 
reason why our simulation results are quite similar to the real world observation. Empirical 21 survey based agents are increasingly used in social simulation, particularly in operations 
management [e.g. 25, 26]. Compared to traditional theory based agents, empirical survey 
based agents, although require much more time and work for the development, are easier to 
be calibrated and validated. Limitations Although we have developed a state chart to represent the behaviour of electricity user 
agents based on a comprehensive empirical survey, clearly artificial agents cannot perfectly 
replicate the real-life of electricity users in the school. Therefore, it is important to 
acknowledge the limitations of the model. Firstly, an electricity user agent’s stereotype in the 
model is fixed. In other words, in the simulation there is no way for an electricity user agent 
to switch its stereotype (e.g. from an early bird to a flexible worker). But we note that in the 
real world the switch of stereotypes can happen, although the probability for its happening is 
low. A second limitation is our assumption that enhancing interactions about energy issues 
between staff can increase staff’s energy saving awareness. This assumption is true in the 
situation where electricity users have to bear the cost of electricity, as some research on 
energy efficiency in domestic sector has already proved it [e.g. 27]. However, while working in 
office buildings staff do not have to bear the cost of electricity, which may result in the 
assumption in question. Currently we have not found any sound evidence to support this 
assumption. Further Research The agent-based model of office building electricity consumption described in this paper has 
potential for further development. Theoretically, we can incorporate more flexible electric 
appliances and more complex human-electric appliance interactions into the model, which 
will make the model more applicable. It can then be applied to modelling a large organisation 
which has very complex behaviour of consuming energy, e.g. large number of staff and 
complex energy management strategies/regulations. This type of simulation models can 
potentially be developed as a building energy simulation software package which provides 
human-centric organisations with organisational energy policy making support.  Moreover, 
we can add more psychological factors into the energy user agents, and study how to 
optimize energy consumption for an organisation while maintain its staff’s satisfaction about 
energy use. 22 6. Conclusions This paper has described an agent-based model of office building electricity consumption. We 
began the paper with an argument for an integration of the four elements involved in office 
building energy consumption, and then described the agent-based simulation method and the 
rationale for developing an agent-based model for studying office building electricity 
consumption. We then developed an agent-based model of office building electricity 
consumption based on the case of the School of Computer Science, in Jubilee Campus, the 
University of Nottingham, and presented the simulation results. Along the way, we focused 
on two objectives. One is the integration of the four elements involved in office building 
energy consumption in to one model. The other one is developing a multi-agent framework to 
study practical energy management issues for an organisation. From the research we 
reported in this paper, we conclude that, although it is not possible to perfectly replicate the 
real organisation, agent-based simulation as a novel approach which integrates the four 
elements involved in office building energy consumption, is a very useful tool for office 
building energy management. 23",0
"In the last years many powerful techniques have emerged to measure protein
interactions as well as gene expression. Many progresses have been done since
the introduction of these techniques but not toward quantitative analysis of
data. In this paper we show how to study cellular adaptation and how to detect
cellular subpopulations. Moreover we go deeper in analyzing signal transduction
pathways dynamics. Keywords:
Real Time PCR, Signal transduction pathways dynamics, Cellular
subpopulations 1. Introduction To study protein interaction innovative technique are available as isothermal
titration calorimetry (ITC), diﬀerential scanning calorimetry (DSC) and sur-
face plasmon resonance (SPR). Although western blot remains the more useful
method to access protein levels in the cell. Consider that although many pow-
erful and sensitive techniques we have, at the end usually we reduce to express
the results with arbitrary units making very diﬃcult for example to compare
articles. Moreover if we want to investigate the reaction to a stimulus, in the
data analysis process we should consider some parameters: Michaelis-Menten
constant, antibody aﬃnity and signal transduction pathways ampliﬁcation. In
this paper we show how to study cellular adaptation taking into account all
these parameters.
At the present days in many laboratories it is also possible to get a quantita-
tive analysis on gene expression using Real Time PCR. Real Time PCR permit
to compare diﬀerent articles, moreover its sensitivity is very high but we can
not easily identify small cellular subpopulations. Droplet Digital PCR solves
the problem by single events ampliﬁcation. However in this article we are going
to show that it can be possible to detect a subpopulation also using Real Time
PCR and a small amount of cDNA. Email address: valentina.agoni@unipv.it (V. Agoni) Preprint submitted to Elsevier
July 22, 2013 arXiv:1307.0451v1  [q-bio.OT]  1 Jul 2013 Currently the state of art in biological studies is to just indicate trends
(obtained with quantitative methods of course) with no mention to any attempt
to quantify cellular adaptation or evolution. Nor it is contemplate the option
that some tissues are themselves heterogeneous populations (as in the case of
muscles), in other cases cells react in diﬀerent ways to an injury. Our purpose is
to consider all these tasks to give a quantitative analysis using existing biological
techniques. 2. Quantifying cellular adaptation Since the pioneering work of Michaelis and Menten [10], the understanding
of enzyme kinetics gained an increasing interest from the community. Recent
advances in room-temperature single-molecule ﬂuorescence studies have allowed
very precise measurements [2]. On the other hand, peptide arrays resulted to
be a key technology for deciphering enzyme function [11]. Quantitative analysis
of enzyme kinetics have been developed by Se-Hui Jung [7] using ﬂuorescence-
conjugated peptide arrays, a surface concentration-based assay with solid phase.
This assay was successfully applied for calculating the Michaelis-Menten con-
stant (KM), deﬁned as the substrate concentration at which the enzyme works
at the half of its maximal velocity. In addition, in the last years, many powerful
techniques have emerged to study protein interactions along with typical pa-
rameters involved in these processes. Isothermal Titration Calorimetry (ITC)
is the gold standard for measuring binding constants (KB), reaction stoichiom-
etry (n), enthalpy (∆H) and entropy (∆S), Diﬀerential Scanning Calorimetry
(DSC) is designed to study thermal stability and ﬁnally Surface Plasmon Reso-
nance (SPR) allows the determination of concentration and binding aﬃnity [3].
These techniques could also be applied to antibodies [9, 8].
In order to investigate the reaction to a stimulus, Western Blot is considered
the more useful method to access protein concentration. However, in the data
analysis process usually it is not taken into account that the relative abundance
of an enzyme could aﬀect its kinetic properties.
In a typical situation the variation of concentration of some enzymes with
respect to control is considered to study a particular aspect, e.g.
metabolic
adaptation and degradation systems. We argue that in some cases it is not
possible to coherently interpret the results considering only enzymes variations
without including in the analysis the relation between them and the enzymes
kinetic properties, in particular their Michaelis-Menten constant.
We are going to show that the increment of an enzyme is lower the higher
is its KM (remember that an high KM corresponds to a low-eﬃciency of the
enzyme, see also Fig. 1). Thus we can observe no signiﬁcant change in the
concentration of a particular enzyme if its KM is very low.
If we look for example at glycolytic metabolism, it is possible to observe
no variation in glyceraldehyde 3-phosphate dehydrogenase (GAPDH) concen-
tration while triose-phosphate isomerase (TPI) increment is signiﬁcant. This
could be due to the very low KM of GAPDH with respect to the TPI one.
Accordingly GAPDH can be considered an “housekeeping” enzyme. 2 Fig. 1 shows the Michaelis-Menten curves of 2 diﬀerent enzymes A and B,
respectively a slow and a fast enzyme. At time t Imagine that a particular condition occurs, for example that an increase in metabolic rate is 
required: the increment of an enzyme with an high KM is lower than the increase in a low-efficiency 
enzyme, so that we can observe no significant change in the enzyme concentration not because 
there is not but because the Michaelis-Menten constant is very low. If we look at glycolytic 
metabolism, maybe glyceraldehyde 3-phosphate dehydrogenase (GAPDH) concentration is not 
altered while triose-phosphate isomerase (TPI) increment is significant. In fact, GAPDH because of 
its very low KM can be considered an ‘housekeeping’ enzyme.
Figure 1 shows the Michaelis-Menten curves of 2 enzymes: a slow (A) and a fast enzyme (B). The 
velocity of the substrate (S) to product reaction is directly proportional to the number of enzyme 
copies (E) (although inversely proportional to the enzyme density). Figure 1. Reaction rate vs concentration. [8] Figure 1: Reaction rate vs concentration of two enzymes A and B, respectively the bottom and
the top one, having diﬀerent eﬃciency. In the ﬁgure S denotes the substrate concentration.
One can see that the higher is the KM of the enzyme the lower is its eﬃciency. We want to show that (for example when a metabolic alteration occurs inside
the cell, since an higher ATP concentration is required) the concentration of an
enzyme A with an high KM (less eﬃcient) varies more than the concentration
of a more eﬃcient enzyme B.
The enzyme rate-substrate concentration relationship reported in 1 refers to
a single enzyme. Although we are interested in cellular adaptation in terms of
variations in enzymes concentration. We know that the velocity of conversion
of substrates into products reﬂects the probability to match the substrate inside
the cell Vn = nV
(1) with n the copies of a single enzyme. Imagine to treat a cell with a drug that
inhibits glucose uptake (e. g. glucocorticoids). In this case there is less glucose
in the cytoplasm so that the cell reacts enhancing the production of glycolitic
enzymes in order to increase the probability to match their substrates. Focus on
two enzymes A and B following the Michaelis-Menten kinetics and assume for
simplicity the number of these two enzymes at time t = 0 before the treatment
to be nA
0 = nB
0 = 1. After the treatment at time t we have nA
t
= nA and
nB
t = nB and according to Eq. (1) we have V A
nA = nAV A,
V B
nB = nBV B
(2) It is realistic to suppose that they have diﬀerent kinetics properties and in
particular suppose A to be more eﬃcient than B V A > V B
(3) 3 We are considering the alteration of a pathway with both the enzymes involved
in this speciﬁc pathway, we can say that V A
nA = V B
nB
(4) and from Eqs. (2) and (3) we ﬁnally get nA > nB.
(5) We have shown that the increment of an enzyme with an high KM is lower than
the increase in a low-eﬃciency enzyme.
But thinking about western blots another parameter to consider is the an-
tibody aﬃnity for the target enzyme. Antibody aﬃnity for the ligand can be
expressed in terms of the association constant Y as: Y = [Ab × Lg] [Ab][Lg]
at equilibrium
(6) where [Ab] and [Lg] denote respectively the antibody and the ligand concen-
trations, while [Ab × Lg] denotes the concentration of the antibody bind to its
ligand. The enzyme concentration we observe with western blot analysis Eobs
is related to the real concentration E through this formula: E = 1 Y · Eobs
(7) To compare the expression of an enzyme between control and treated samples,
it is not necessary to worry about Y .
However, if we mind to compare the
modiﬁcations of diﬀerent enzymes we have to consider antibody aﬃnity. Com-
bining the two parameters (KM and Y ) we obtain a more realistic indicator of
adaptation Ξ can be Ξ = 1 Y · ∆E1
obs · KM1 = 1 Y · ∆E2
obs × KM2
(8) where 1 and 2 label two diﬀerent enzymes of the pathway under consideration.
Finally, if we are studying a certain signal transduction pathways component,
we should take into account signal ampliﬁcation too. Signal ampliﬁcation is not
a small phenomenon. On the contrary, one molecule leads to the activation of
106. 3. Game theory and signal transduction pathways dynamics Signal transduction pathways recover a crucial role in cellular processes:
they represent a connection between environmental conditions and cellular re-
actions. It is well known that signals are transduced from the cell surface to
the cell nucleus by a series of protein-protein interactions, phosphorylation re-
actions. Every signal transduction pathway is composed by one receptor and
some kinases that bring the environmental signal to the nucleus. Usually, when 4 the ligand binds the receptor, it activates a kinase by prosphorylation, the signal
travel through the kinase and then the kinase activates the next one in the chain.
Diﬀerent pathways are linked to create biological networks. In some networks
a single protein is linked to many others, as for Akt in Fig. 2. Nevertheless
it is improbable for a protein to be optimized to interact with so many other
proteins of almost the same dimensions. Figure 2: PI3K/Akt Signaling. We can try to explain this considering inhibitions to be indirect inhibitions.
Indeed signal ampliﬁcation leads to 106 molecules per cell, leaving no space for
the ampliﬁcation of a second pathway not-related (with no links) to the ﬁrst as
cells dimension indicates (a cell can contain about 108 proteins). Hence it can
not be active more non-related pathways per time. This suggests that probably
a tissue is composed by a snap of an heterogeneous population. In this scenario
it can also be possible to calculate the duration of a single pathway activation
knowing the lifetime of its components. In [1] is shown how the formalism of
game theory can be used to characterise biological cascades and gene regulation.
In this paper we propose game theory as a tool for studying signal transduction
pathways. Indeed we introduce the replicator dynamics diﬀerential equation
used in evolutionary game theory. 5 ∂ln xi(t) ∂t
= xi  ui(x) − n
X j=1
xjuj(x)  
(9) where xi is the strategy adopted by player i and ui(x) is the utility (or the
ﬁtness), while Pn
j=1 xjuj(x) is the average population ﬁtness.
The replicator dynamics equation allows to calculate the population growing
rate. In our case u(x) =
1 kM
· s
(10) where KM is the Michaelis-Menten constant of the kinase, and is the ampliﬁca-
tion factor so that, when a signal arrives being a ligand, it causes for example
the activation and ampliﬁcation of diﬀerent Akt downstream kinases witch ﬁght
to reach their targets. This competition for the space indirectly inhibits the ac-
tivation of other pathway and it is responsible for the modulation in the cellular
response. 4. Real Time PCR to detect cellular subpopulations Many progresses have been done since the introduction of Polymerase Chain
Reaction (PCR) [5, 4]. Microﬂuidics allows Real Time PCR [12] which is quan-
titative in opposition to classical PCR. The last innovation in this ﬁeld is the
Droplet Digital [6].
In this Section we suggest an innovative application of Real Time PCR in
detecting cellular subpopulations in our sample. Some tissues are themselves
heterogeneous populations (as in the case of muscles), in other cases cells react
in diﬀerent ways to an injury. Think about images for quantitative immunohis-
tochemistry: they do not represent an homogeneous pattern. This application
of Real Time PCR can be useful overall for acute phase studies. Suppose to
amplify one gene of your control and treated sample and to obtain an induction
of the ubiquitin ligases and an induction of PGC-1alpha (oxidative metabolism
regulator). A possible interpretation of this data could be that some cells are
trying to adapt to the new condition while others are dying. Otherwise imagine
to obtain no signiﬁcant increment. At this point one can think that we need to
increase the number of samples or that this gene does not change under these
conditions. We can try to solve the question with no more samples nor cDNA!
We can make serial dilutions of cDNA and if a cut-oﬀappears with a nonlin-
ear relationship, then we can hypothesize the presence of a subpopulation in
your sample. Indeed, diluting cDNA we expect to ﬁnd a progressive decrease
in cDNA concentration (above sensibility threshold), depending on the ampli-
ﬁcation probability inside the well (sample A′ in Fig.
3).
But if there is a
subpopulation inside we also have to consider the probability to pipetting the
fragments to execute the Real Time PCR experiment (sample A′′ in Fig. 3). 6 Dilution factor 1 CT A￿
A￿￿ Sunday, April 28, 2013 Figure 3: In the picture CT is the threshold cycle.
This qualitative plot is to show that
making serial dilution of cDNA extracted from the treated sample we obtain an exponential
curve (the image represents the linear part only A′), we can say that there is no subpopulation
while we can conclude the contrary if we get a Poissonian (scenario A′′).",0
"Recently delivered lectures on Self-Referential Mathematics, [2], at the
Department of Mathematics and Applied Mathematics, University of
Pretoria, are brieﬂy presented. Comments follow on the subject, as
well as on Inconsistent Mathematics. 0. Prologue The basic idea in the Self-Referential Mathematics, [2], is to replace
the Foundation Axiom, (FA), in Set Theory with a suitable Anti-
Foundation Axiom, (AFA), in such a way as to : - keep all the sets in the usual Set Theory while at the same time, to : - allow a large class of new sets, sets given this time by self-referential
deﬁnitions. In other words, one obtains a signiﬁcant extension of usual Set The-
ory, an extension which is proved to be consistent, provided usual Set 1 Theory is consistent. As it happens not seldom in science, the terminology used may turnout
to be rather inappropriate, if not in fact misleading. The same hap-
pens in [2], where the term ”vicious circle” is used instead of ”self-
referential”. A likely reason for that particular terminology in [2], which has a clear
negative connotation, comes from the fact that the 1903 Russell Para-
dox in Set Theory is based on self-reference, being but a reformulation
in mathematical, in particular, set theoretic terms of the ancient Greek
paradox of the liar. On the other hand, when considered in the larger and longer perspec-
tive of human tradition and civilization, self-reference, together with
inﬁnity and change, have since the earliest known, in fact, prehis-
toric times been some of the fundamental ideas preoccupying human
thought, and as such, they have not had any sort of inevitable neg-
ative connotation, see section 5. It follows, therefore, that the term
”vicious” can be seen as an overstatement resulted from a partial view
of what self-reference does in fact encompass and mean in its more full
generality. We can in essence clarify as follows the aims and the means of the Self-
Referential Mathematics in [2]. Let us consider the following three
groups of axioms of Set Theory, see section 6 for all the usual axioms,
in particular, those used in [2] : ZFC−= Zermello - Fraenkel + Choice ZFC = ZFC−+ FA ZFA = ZFC−+ AFA where the AFA axiom with be speciﬁed in section 2. At ﬁrst, it may appear that the Set Theories corresponding to ZFC
and ZFA may be rather diﬀerent, since their common part corresponds 2 only to ZFC−, while the respective additional axioms FA and AFA
seem in fact to be inconsistent with one another.
However, as it turns out this is not the case.
And what happens
instead is that : • The Set Theory based on the ZFA axioms contains all the sets
in the Set Theory based on the ZFC, and in addition, contains
a large class of other sets obtained by self-referential deﬁnitions. • The axioms ZFA are consistent, provided that the axioms ZFC
are consistent. As for the traditional and still exclusively predominant idea of the
absolute necessity of consistency, one should consider the recent emer-
gence of Inconsistent Mathematics, see [11,12]. And in fact, as far as
everyday practice is concerned, we have for more than half a century
by now been basing much of our lives on a speciﬁc form of Inconsistent
Mathematics. Indeed, our ever more pervasive and critically impor-
tant electronic digital computers are - even when only operating on
non-negative integers - functioning according to the Peano Axioms,
plus the Machine Inﬁnity Axiom, namely ∃M >> 1 : M + M = 1 where M is called ”machine inﬁnity”, and typically is larger than
101000.
And obviously, the Peano Axioms are trivially inconsistent
with the Machine Inﬁnity Axiom. 1. Sets, Ur-Elements and Classes We denote by SET the class of all sets, including the unique void set
φ. As is well known, a lot of mathematics can be built up starting
alone with the void set φ. Indeed, as a ﬁrst step, and following von
Neumann, one can deﬁne the non-negative integers by 0 = φ, 1 = {φ}, 2 = {{φ}}, . . . 3 and then, step by step build all the integers, the rational and real num-
bers, and so on. Further, one can deﬁne Cartesian products, binary
relations, functions, etc., and obtain a considerable part of mathemat-
ics in this manner. In the sequel, it will be convenient to allow, in addition to the void
set φ, other such starting entities in the construction of mathematics.
The class of such entities is denoted by U, and any respective element
u ∈U is called an ur-element, assumed to have only one property,
similar to that of the void set φ, namely that the relation a ∈u does not hold for any entity a in the theory. In this way, there will be three types of entities in the theory, namely 1)
SET, which is the class of all sets, 2)
U which is the class of all ur-elements, and 3)
CLASS which denotes all the classes. Here it is understood that any set a ∈SET is a ”small” class, while
SET itself is one of the ”proper” classes, since it is not itself a set. In
other words, SET ∈CLASS, SET /
∈SET, CLASS /
∈SET.
As for ur-elements, it is assumed that U ∈CLASS and it is another
instance of ”proper” class, thus in particular U /
∈SET. Brieﬂy, we have therefore 1)
SET denotes all the sets, and it is a proper class 2)
CLASS denotes all the classes 3)
U denotes all the ur-elements, and it is a proper class 4)
a set is a ”small” class 4 5)
a class which is not a set is ”large”, thus it is a proper class 6)
an ur-element does not have any elements, either sets, ur-elements,
or classes 7)
non-set = proper class W ur-element 8)
non-set V non-class = ur-element 9)
every predicate determines a class 10)
a subclass of a set is a set 11)
sets are closed under a number of operations, among them,
pairing, union, power set, see below 12)
a ∈b ∈CLASS
=
⇒
a ∈SET 13)
the class SET of all sets is ”large”, thus it is a proper class indeed, according to Russell’s Paradox, let SET be a set,
then R = {a ∈SET | a /
∈a} is a set, thus R ∈SET, and
therefore R ∈R
⇐
⇒
R /
∈R, which is absurd As for the binary relation ∈, we have - set ∈set
- set ∈proper class
- set /
∈ur-element
- proper class /
∈set
- proper class /
∈proper class
- proper class /
∈ur-element
- ur-element ∈set
- ur-element ∈proper class
- ur-element /
∈ur-element thus denoting ∈by →, while /
∈by ↛, we have 5 set ❄
set ✲ ✛ proper class ❄ proper class ❩❩❩❩❩❩❩❩❩
❩
⑦ ✓
✓ ❩
❩
❩
❩
❩
❩
❩
❩
❩
❩
⑥ ur −element ur −element ✻ ✚✚✚✚✚✚✚✚✚
✚
❃
✚
✚
✚
✚
✚
✚
✚
✚
✚
✚
❂ ❙
❙ • set
• φ • proper class SET • SET = proper class
✲ • U = Ur −elements = proper class
CLASS Note on ur-elemets Clearly a /
∈φ holds for all entities a in the theory, however, it is nevertheless con-
sidered that φ ∈SET and φ /
∈U 6 Also, it is possible that X ∈SET and X ∩U ̸= φ, or even X ⊆U For instance, if x ∈U, then X = {x} ∈SET,
X ⊆U. Note on SET Here we should clarify that SET denotes, in fact, all the sets which
exist in the Set Theory based on the ZFA axioms. Therefore, let us de-
note by SET0 all the sets in the Set Theory based on the ZFC axioms.
Then as mentioned in section 0, and seen later, we have SET0 ⫋SET,
thus the above diagram can be made more precise as follows • set
• φ • proper class SET • set
• SET = proper class
✲
SET0 • SET0 = proper class
✲ • U = Ur −elements = proper class
CLASS Examples of Sets There are only two kind of sets a ∈SET, namely (1.1)
a = φ, which is equivalent with ¬ ( ∃b ∈SET S U : b ∈a ) (1.2)
∃b ∈SET S U : b ∈a Operations with sets An ordered pair is the set < a, b >
= {{a}, {a, b}}, with a, b ∈SET 7 thus < a, b >
=
< c, d >
⇐
⇒
a = c, b = d A relation R ∈SET has all its elements given by pairs
< a, b >,
where a ∈A, b ∈B, for two suitably given sets A, B ∈SET. Often
for convenience one denotes aRb for < a, b > ∈R. If A ∈SET is such that < a, b > ∈R =
⇒a, b ∈A, then R is a
relation on A. A relational structure is < A, R >, with R relation on A. A function is a relation R such that < a, b >, < a, c > ∈R =
⇒b = c If f is a function, then (1.3)
dom(f) = {a | ∃b : f(a) = b} (1.4)
rng(f) = {b | ∃a : f(a) = b} thus (1.5)
f ∈c →d
⇐
⇒
c = dom(f), rng(f) ⊆d The power set of a ∈SET is (1.6)
P(a) = {b | b ⊆a} Example : if a = {φ, p}, with p ∈SET S U, then P(a) = {φ, {φ}, {p}, a}. Consider the predicate P ( x ) given by (1.7)
x is an ordered pair < a, b > and b = P(a) then this deﬁnes the power set function P : SET −
→SET, and (1.8)
S dom(P) = SET 8 thus it is ”large”, and therefore, a proper class. The natural numbers are (1.9)
0 = φ, 1 = {0} = {φ}, 2 = {0, 1} = {φ, {φ}}, . . . Disjoint union is A + B = ({0} × A) ∪({1} × B) For a ∈SET, we deﬁne (1.10)
S a = { x | ∃y ∈a : x ∈y } = {x ∈y ∈a} 2. Systems of Equations Which Deﬁne Sets In usual, that is, ZFC Set Theory, one way to deﬁne a set X is by an
equation X = {x | P(x)} where P is a suitable predicate. Within ZFC, an essential restriction
on P is that it cannot in any way refer to the set X which it is sup-
posed to deﬁne. This condition is meant to avoid a ”vicious circle”,
or in more proper terms, self-referentiality, an avoidance which has
until recently been universally accepted, and in fact required, since
Russell’s paradox. In particular, one cannot deﬁne any set a ∈SET0, even by such a
simple equation, like (2.1)
a = {a} since obviously, it is a self-referential equation. On the other hand, as
seen in 1), 4), 5) in Examples 2.1. below, this equation can easily be
solved in SET, that is, based on the Anti-Foundation Axiom, (AFA). Here we can note that one cannot deﬁne any set a ∈SET0, or for that
matter, a ∈SET, even by the yet more simple equation 9 (2.2)
a = a since this equation will obviously not give a unique set in SET0, or in
SET. Also, as seen in 3) in Proposition 2.3. below, one cannot deﬁne a set
a ∈SET by the equation (2.3)
a = P(a) Let us consider now the equation x = {a, x} where a ∈SET S U is given. Then x = {a, x} = {a, {a, x}} = {a, {a, {a, x}}} = . . . thus an intuitive solution would be x = {a, {a, {a, . . .}}} which however is not possible within ZFC, since it would obviously
lead to the inﬁnite descending sequence . . . x ∈x ∈x ∈x ∈x thus contradict the Foundation Axiom, (FA), see below. Let us now return to the general situation and enquire what should
the solution given by sets be of a corresponding system of equations.
Let us as an example consider for that purpose the following system
of equations, where p, q ∈SET S U are given, and where we want to
ﬁnd sets x, y, z ∈SET, such that x = {x, y} 10 y = {p, q, y, z} z = {p, x, y} Let e : X = {x, y, z} −
→the right-hand sides of the above equations thus ex = {x, y}, ey = {p, q, y, z}, ez = {p, x, y} What is then a solution s to these equations supposed to be ? One way is given by s : X −
→SET, namely X ∋v 7−
→sv ∈SET,
with sx = {sx, sy}, sy = {p, q, sy, sz}, sz = {p, sx, sy} or equivalently ∀v ∈X : sv = {sw | w ∈ev ∩X} S {w | w ∈ev ∩A} = = s[ev ∩X] S (ev ∩A) where A = {p, q} Returning now to the equation, see (2.2) x = x one way to avoid the inconvenience of non-unique solutions in SET,
is to take, see also (2.10) below X = {x} ⊆U and then the solution s, if it exists, is a function 11 s : X ∋x −
→sx ∈SET This liberty to distinguish between indeterminates, and on the other
hand, the solution is in fact familiar from usual algebra. Indeed, if for
instance we have the system of equations in real numbers 2x + 3y = 7
5x −4y = 1 then the set of indeterminates is X = {x, y}, while a solution s, which
in this case exists, is given by a function s : X ∋v −
→sv ∈R. This
distinction is even more obvious when, as with a system of equations
like 3x + 2y = 5
5x −3y = 2 the indeterminates have the same value x = y = 1, thus the solution
cannot be identiﬁed with the single number 1, but only with the func-
tion s : X ∋v −
→sv ∈R, for which sx = 1, sy = 1. Finally we can note that, as seen in Deﬁnition 2.2. below, the require-
ment X ⊆U can on occasion be done away with. Now, the approach in [2] to Self-Referential Mathematics will be able
to accept for sets in SET such deﬁnitions which are given by certain
systems of equations that can be self-referential. There are in this
regard three kind of systems of equations considered so far. The ﬁrst
kind of systems is given by Deﬁnition 2.1. A structure E =
< X, A, e > is called a ﬂat system of equations, if
and only if (2.4)
X, A ∈SET,
X ⊆U,
X T A = φ with X the set of indeterminates and A the set of atoms, while 12 (2.5)
e : X −
→P(X S A) deﬁnes the right hand terms of the equations of the system, see (2.8)
below, with (2.6)
X ∋v 7−
→bv = ev ∩X being the set of indeterminates on which v immediately depends, and
similarly, with (2.7)
X ∋v 7−
→cv = ev ∩A being the set of atoms on which v immediately depends In this way, the ﬂat system of equations is given by (2.8)
x = ex,
x ∈X which of course can in particular be one single equation, when X = {x}
is a set with one single element. A solution to E is a function (2.9)
X ∋x 7−
→sx = {sy | y ∈bx} S cx ∈SET and we denote solution −set(E) = S {sx | x ∈X} = = {sy | y ∈ex ∩X, x ∈X} S S x∈X(ex ∩A) = s[X] ∈SET as well as V [A] = S 
solution −set(E)
E ﬂat system of equations
with atoms A 
= 13 =

c
∃E = < X, A, e > ﬂat system of equations : c ∈solution −set(E) 
= =

sy
∃E = < X, A, e > ﬂat system of equations : y ∈ex ∩X,
x ∈X  S S 
c
∃E = < X, A, e > ﬂat system of equations : c ∈ex ∩A,
x ∈X 
⊆SET and clearly, V [A] is always a proper class, see 2) in Note 2.1. below. ✷ There are two remarkable facts about the concept of ﬂat systems of
equations given in the above Deﬁnition 2.1., namely • the Anti-Foundation Axiom, (AFA), upon which the whole of
Self-Referential Mathematics in [2] rests, has a most simple for-
mulation in terms of ﬂat systems of equations, as seen next, • the ﬂat systems of equations do in fact give all the additional
new sets in SET \ SET0, that is those which due to their self-
referential deﬁnitions, cannot be obtained by the usual ZFC Set
Theory, see the equivalence Theorem 2.1. below, see also 1) in
Note 2.1. below. ANTI-FOUNDATION AXIOM (AFA) ∀E ﬂat system of equations : ∃! s solution Here, for the sake of further clarity, let us recall that in ZFC we have AXIOM OF FOUNDATION ( FA ) ∀a ∈SET :
< a, ∈> well-founded 14 where the concept of a well-founded relational structure is deﬁned as
follows. A relational structure < S, R > is called well-founded, if and only if it
has no inﬁnite descending sequence . . . an R an−1 R . . . R a2 R a1 R a0 with a0, a1, a2, . . . , an−1, an, . . . ∈S. For a relational structure < S, R >, we denote < S, R >wf
= {a ∈S | no inﬁnite descending sequence in R starting with a} Clearly < S, R > well −founded
⇐
⇒
< S, R >wf
= S For simplicity, we denote Rwf = < S, R >wf Remark 2.1. In view of the above (AFA) axiom, the question arises : • Which ﬂat systems of equations have solutions under the (FA)
axiom ? The answer is obtained based on the following concept. A ﬂat system
of equations E = < X, A, e > is called well-founded, if and only if the
relation < deﬁned on X by x < y
⇐
⇒
y ∈ex is well-founded. And then we have, see [1,2] 15 Mostowski Collapsing Lemma In ZFC−well-founded ﬂat systems of equations have unique solutions. Corollary 2.1. In ZFC−we have the equivalence (FA)
⇐
⇒

only well-founded ﬂat systems
of equations have solutions  Note 2.1. 1) Flat systems of equations can trivially recover all sets E ∈SET.
Indeed, let A = E. Further, let X = {x}, with x ∈U, such that
x /
∈E, which is possible since U "" E, given the fact that U is a
proper class. Then X ⊆U and X ∩A = φ, hence ex = E is obviously a ﬂat system of equations, thus according to (AFA),
it has a unique solution s.
Now in view of (2.6), (2.7), we have
bx = ex ∩X = φ, cx = ex ∩A = E, and then (2.9) gives sx = {sy | y ∈ex ∩X} ∪(ex ∩A) = E In this way, the set E was obtained as the unique solution s : X = {x} ∋x 7−
→sx = E of the above ﬂat system of equations. 2) In view of the above example, each x ∈U leads to a ﬂat system
of equations with the respective unique solution sx. And clearly, if
x, x ′ ∈U, x ̸= x ′, then sx ̸= sx ′. As for U, it is a proper class,
therefore, so is {sx | x ∈U}. 16 Examples 2.1. Let us illustrate the above in the case of the equations (2.1) - (2.3). 1) For (2.1), we can take X = {x} ⊆U,
A = {φ},
ex = {x} ∈P(X S A) therefore, it is a ﬂat system of equations, made up of a single equation.
As for its unique solution sx ∈SET, we shall see the details in 4) and
5) below. 2) For (2.2), we can take X = {x} ⊆U,
A = {φ},
ex = x thus ex ∈X S A,
ex ⊈X S A,
ex /
∈P(X S A) therefore, it is not a ﬂat system of equations. Also, with (2.2), we can immediately note why the condition (2.10)
X ⊆U was requested in Deﬁnition 2.1. Indeed, without that condition, equa-
tion (2.2) is satisﬁed by all sets a ∈SET, thus (2.2) does not have a
unique solution in SET. 3) With the equation (2.3), we can take X = {x} S x ⊆U,
A = φ,
ex = P(x) "" X S A,
ex /
∈P(X S A) which, however, does not turn (2.3) into a ﬂat system of equations.
Also, as seen in 3) in Proposition 2.3. below, equation (2.3) does not
have any solution in SET. 17 4) In ZFA, the equation (2.11)
x = {x} has a unique solution Ω∈SET. Indeed, as note at 1) above, if we take X = {x} ⊆U,
A = φ,
ex = {x} then (2.11) is a ﬂat system of equations, thus in view of (AFA), it has
a unique solution sx ∈SET, and according to (2.9), we have (2.12)
sx = {sy | y ∈bx} ∪cx However, (2.6) gives bx = ex ∩X = {x} ∩X = X = {x}, while (2.7)
implies cx = ex ∩A = φ. Thus (2.12) becomes sx = {sx} 5) The above unique solution Ω∈SET obviously has the property Ω= {Ω} = {{Ω}} = {{{Ω}}} = . . . however, this need not mean that the bracket pairs { } could be in-
ﬁnitely many, namely, that we could have for instance Ω= . . . {{{Ω}}} . . . let alone that the bracket pairs { } could reach to transﬁnite ordinals,
or go through all the ordinals, see Remark 2.3. below. 6) In ZFA, there is a unique set (2.13)
{0, {1, {2, {3, . . .}}}} ∈SET Indeed, we consider the ﬂat system of equations x0 = {0, x1}
x1 = {1, x2} 18 x2 = {2, x3}
x3 = {3, x4}
.
.
. where X = {x0, x1, x2, x3, . . .} ⊆U,
A = {0, 1, 2, 3, . . .}, while exn =
{n, xn+1}, with n ≥0. Then (AFA) gives a unique solution s, and in
view of (2.6), (2.7), (2.9), we obtain the relations sx0 = {sx | x ∈bx0} ∪cx0 = {sx | x ∈ex0 ∩X} ∪(ex0 ∩A) = {0, sx1}
sx1 = {sx | x ∈bx1} ∪cx1 = {sx | x ∈ex1 ∩X} ∪(ex1 ∩A) = {1, sx2}
sx2 = {sx | x ∈bx2} ∪cx2 = {sx | x ∈ex2 ∩X} ∪(ex2 ∩A) = {2, sx3}
.
.
. thus sx0 = {0, sx1} = {0, {1, sx2}} = {0, {1, {2, sx3}}} = . . . 7) Let us consider that ﬂat system of equations without atoms, that
is, with A = φ, namely x0 = {y0, x1}
y0 = φ
x1 = {y1, x2}
y1 = {y0}
x2 = {y2, x3}
y2 = {y0, y1}
x3 = {y3, x4}
y3 = {y0, y1, y2}
.
.
. where X = {x0, y0, x1, y1, x2, y2, . . .} ⊆U,
A = φ, while exn =
{yn, xn+1}, eyn+1 = {y0, . . . , yn}, for n ≥0, and ey0 = φ.
In this
case, (2.6), (2.7) give bxn = exn ∩X = {yn, xn+1},
n ≥0 by0 = ey0 ∩X = φ
byn+1 = eyn+1 ∩X = {y0, . . . , yn},
n ≥0 cxn = cyn = φ,
n ≥0 19 therefore, in view of (2.9), the unique solution s given by (AFA), is
such that sxn = {sx | x ∈bxn} ∪cxn = {syn, sxn+1},
n ≥0 sy0 = {sy | y ∈by0} ∪cy0 = φ
syn+1 = {sy | y ∈byn+1} ∪cyn+1 = {sy0, . . . , syn},
n ≥0 In particular sy0 = φ
sy1 = {sy0} = {φ}
sy2 = {sy0, sy1} = {φ, {φ}}
sy3 = {sy0, sy1, sy2} = {φ, {φ}, {φ, {φ}}}
.
.
. which means that, for n ≥0, we obtain that syn = n in the von Neumann representation Proposition 2.1. Let E = < X, A, e > be a ﬂat system of equations. If A ⊆U, then set
solution −set(E) is transitive, namely b, c ∈SET, c ∈b ∈solution −set(E)
=
⇒
c ∈solution −set(E) Proof. We recall that solution −set(E) = {sy | y ∈ex ∩X, x ∈X} S S x∈X(ex ∩A) Let b ∈solution −set(E), then either b = sy, for some y ∈ex ∩X, with suitable x ∈X, 20 or b ∈ex ∩A, for some x ∈X. In the ﬁrst case, if c ∈b = sy = {sz | z ∈ey ∩X} ∪(ey ∩A), then either c = sz, thus c ∈solution −set(E), or c ∈ey ∩A, thus again c ∈solution −set(E). In the second case, if b ∈ex∩A, then b ∈U, thus there cannot be c ∈b. Proposition 2.2. If A ⊆U, then V [A] ⊆Vafa[A] where the operation Vafa is deﬁned in (3.5) in the next section. Note : Here we make an advance use of some notations and results
in section 3 below. However, placing Proposition 2.2. here in section
2 helps in the better understanding of the concept of ﬂat system of
equations, as well as of its fundamental importance seen in the equiv-
alence Theorem 2.1. below. Proof. Let c ∈V [A], then for some E = < X, A, e > ﬂat system of equations
we have either c = sy, for some y ∈ex ∩X, x ∈X or c ∈ex ∩A, for some x ∈X In the ﬁrst case we also have 21 c ⊆Z = solution −set(E) = = {sy | y ∈ex ∩X, x ∈X} S S x∈X(ex ∩A) since in view of (2.9) c = sy = {sz | z ∈ey ∩X} ∪(ey ∩A) But in view of Proposition 2.1., the set Z = solution −set(E) is tran-
sitive. Therefore, for x ∈X, we have, see (3.3) below sx ⊆TC(sx) ⊆Z which gives TC(sx) ∩U ⊆Z ∩U ⊆A Indeed, if b ∈Z ∩U, then in particular either b = sy, for some y ∈ex ∩X, with suitable x ∈X, or b ∈ex ∩A, for some x ∈X. In the ﬁrst case, b /
∈U, since sy ∈SET. In the second case obviously b ∈A. In conclusion, in view of (3.5), we have c ∈Vafa[A]. Remark 2.1. The following is, of course, a fundamental question : • How many sets a ∈SET can be obtained as solutions of ﬂat
systems of equations ? In 1) in Note 2.1. above, we have seen that ﬂat systems of equations
can trivially recover as solutions all sets in SET. A more precise and
rather natural answer, and as such, best possible answer will be given 22 in Theorem 2.2. below.
Needless to say, this answer highlights the importance of ﬂat systems of
equations. However, in various contexts, other two concepts of systems
of equations will prove to be useful, concepts given in Deﬁnitions 2.2.
and 2.3. below.
✷ Now, the second kind of systems of equations aims to eliminate the
above restriction X ⊆U in (2.10) on the ﬂat systems of equations.
And as we shall see in Theorem 2.1. below, this is in fact possible, in
spite of the above problem with lack of uniqueness of solutions, pro-
vided that the STRONG AXIOM OF PLENITUDE is accepted. Deﬁnition 2.2. A structure E
=
< X, A, e > is called a generalized ﬂat system of
equations, if and only if (2.14)
X, A ∈SET,
X T A = φ with X the set of indeterminates and A the set of atoms, while (2.15)
e : X −
→P(X S A)
✷ There is a close connection between the solutions of ﬂat, and on the
other hand, generalized ﬂat systems of equations, provided that the
following axiom holds STRONG AXIOM OF PLENITUDE There is an operation new(a, b), such that 1)
∀a ∈SET, b ⊂U : new(a, b) ∈U \ a 2)
∀a, a ′ ∈SET, a ̸= a ′, b ⊂U : new(a, b) ̸= new(a ′, b) 23 Theorem 2.1. Assuming the STRONG AXIOM OF PLENITUDE, every general-
ized ﬂat system of equations E =
< X, A, e > has a unique solution
s. Furthermore, there exists an associated ﬂat system of equations
E ′ =
< Y, A, e ′ >, such that solution −set(E) = solution −set(E ′) Proof. We have to replace X by a set Y ⊂U, such that Y ∩A = φ. Thus we
take Y = {yx | yx = new(x, A),
x ∈X} then Y ⊂U,
Y ∩A = φ Let now e ′
yx = {yz | z ∈ex ∩X} S (ex ∩A) Clearly, E ′ =
< Y, A, e ′ > is a ﬂat system of equations, and thus, it
has a unique solutio s ′. Now we get the solution s of E =
< X, A, e > given by sx = s ′
yx,
x ∈X The uniqueness of s follows from the fact that every solution t of
E =
< X, A, e > gives a solution t ′ of E ′ =
< Y, A, e ′ >. And we
must have t ′ = s ′, thus it follows that t = s.
✷ Example 2.2. 24 For every set a ∈SET, we associate the canonical generalized ﬂat
system of equations Ea = < Xa, Aa, ea >, with, see (3.1) Aa = TC(a) T U Xa = TC({a}) \ Aa and Xa ∋x 7−
→(ea)x = x ⊆Xa
S Aa where we have to prove the inclusion in the last relation. We note in this regard that, see (3.2∗), (3.3∗) and 1) in Examples 3.1. Aa = {x ∈U | x ∈an ∈. . . ∈a1 ∈a0 = a,
n ≥0} Xa = ( {a} S TC(a) ) \ ( TC(a) T U ) = = ( {a} S {x | x ∈an ∈. . . ∈a1 ∈a0 = a,
n ≥0} )\ \ {y ∈U | y ∈bm ∈. . . ∈b1 ∈b0 = a,
m ≥0} = = {a} S {x ∈SET | x ∈an ∈. . . ∈a1 ∈a0 = a,
n ≥0} thus x ∈Xa =
⇒x ∈SET Furthermore x ∈Xa =
⇒x ⊆Xa
S Aa Indeed, given x ∈Xa, then there are the two cases (i.1)
x = a 25 (i.2)
x ∈an ∈. . . ∈a1 ∈a0 = a, for some n ≥0 Let now y ∈x.
In case (i.1), we have y ∈a, thus y ∈Xa, pro-
vided that y /
∈U. Otherwise obviously y ∈Aa. In case (i.2), clearly
y ∈(Xa ∪Aa). Assuming now the STRONG AXIOM OF PLENITUDE, we have in
view of Theorem 2.1., a unique solution sa of Ea. And in fact, we have sa : Xa ∋x 7−
→(sa)x = x ∈SET Indeed, (2.9) gives for x ∈Xa (sa)x = {(sa)y | y ∈(ea)x ∩Xa} ∪((ea)x ∩Aa) ∈SET while (ea)x ∩Xa = x ∩Xa,
(ea)x ∩Aa = x ∩Aa thus (sa)x = {(sa)y | y ∈x ∩Xa} ∪(x ∩Aa) ∈SET which in our case becomes the identity x = {y | y ∈x ∩Xa} ∪(x ∩Aa) ∈SET Theorem 2.2. Equivalence Assuming now the STRONG AXIOM OF PLENITUDE, we have for
A ⊆U the equivalence between sets in SET which have support in A,
and sets in SET which are solutions of ﬂat systems of equations with
atoms in A, namely Vafa[A] = V [A] Proof. 26 In view of Proposition 2.2., we have V [A] ⊆Vafa[A] Let now a ∈Vafa[A]. Then by deﬁnition, see (3.5) below TC(a) ∩U ⊆A Now, in view of Example 2.2., we consider the unique solution sa of
the canonical generalized ﬂat system of equations Ea = < Xa, Aa, ea >,
which is sa = idXa. Thus, recalling that a ∈Xa and Aa = TC(a)∩U ⊆
A, we obtain sa = a which gives a ∈solution −set(Ea) On the other hand, in view of Theorem 2.1., there is a ﬂat system of
equations E = < X, Aa, e > with the same atoms Aa, such that solution −set(Ea) = solution −set(E) hence a ∈solution −set(E) ⊆V [A]
✷ Finally, the third kind of systems of equations allows considerably
more general right hand terms ex in (2.5), (2.15), although it has to
accept harder restrictions on X and A, than in Deﬁnition 2.1., that
is, in (2.10). Namely Deﬁnition 2.3. A structure E =
< X, A, e > is called a generalized system of equa-
tions, if and only if (2.16)
X, A ∈SET,
X, A ⊆U,
X T A = φ 27 with X the set of indeterminates and A the set of atoms, while (2.17)
e : X −
→Vafa(X S A) where the operation Vafa is deﬁned in (3.5) in the next section. Remark 2.2. As we shall see in section 3, the range Vafa(X S A) of the mappings e
in (2.17) is considerably larger than P(X S A), which is the range of
the corresponding mappings in (2.5) and (2.15). Therefore, the gen-
eralized systems of equations deﬁned above contain as a rather small
particular case the ﬂat and the generalized ﬂat systems of equations. Examples 2.3. 1) For a given a ∈SET S U, let us consider the equation (2.18)
x = x −
→a hence it is not a ﬂat or generalized ﬂat system of equations, if we take X = {x},
A = {a},
ex = x −
→a ⊆x × a,
ex ⊈X S A ex /
∈P(X S A) although (2.18) has solution in SET, see 5) in Proposition 2.3. below. 2) For given p, q ∈SET S U, the equation (2.19)
x = {{x, q}, p} is not a ﬂat or generalized ﬂat system of equations, if considered with X = {x},
A = {p, q},
ex = {{x, q}, p} ⊈X S A,
ex /
∈P(X S A) although it can be written as a ﬂat system of equations, provided that 28 x ∈U, namely x = {y, p} y = {x, q} hence X = {x, y},
A = {p, q},
ex = {y, p}, ey = {x, q} ⊆X S A, ex, ey ∈P(X S A) Proposition 2.2. Within ZFC we have 1)
∀a ∈SET : a /
∈a 2)
¬∃a1, . . . , an ∈SET : a1 ∈. . . an ∈a1 3)
¬∃a, b ∈SET : a ∈TC(b) ∈a 4)
¬∃a, b ∈SET : a ∈TC(b),
b ∈TC(a) 5)
∀a, b, c ∈SET : c = < a, b >
=
⇒c ̸= a, c ̸= b, c /
∈a, c /
∈b 6)
∀A, X ∈SET : X ̸= φ =
⇒X ̸= A × X 7)
∀X ∈SET : X = X × X =
⇒X = φ 8)
¬∃function f = A −
→B, A, B ∈SET : f ∈dom(f) 9)
¬∃functions f1 : A1 −
→A2, . . . , fn : An −
→A1, A1, . . . , An ∈SET, a1 ∈A1 : fn(. . . f1(a1) . . .) = f1 29 10)
∀A, X ∈SET : X ̸= X −
→A It is important to note that, as seen next, even in ZFC−, that is, with-
out FA, one can obtain impossibilities of self-reference. Proposition 2.3. Within ZFC−we have 1)
∀function F : A −
→B, A, B ∈SET : {x ∈dom(F) | x /
∈F(x)} /
∈rng(F) 2)
∀function F : A −
→P(A), A ∈SET : {x ∈A | x ∈Rwf} /
∈rng(F) where R = {< x, y > | x ∈F(y)} 3)
∀X ∈SET : X ̸= P(X) 4)
∀X ∈SET : X = X −
→φ
=
⇒
X = {φ} 5)
∀A, X ∈SET : X = X −
→A
=
⇒ =
⇒
A = {a}, X = {f}, f(f) = a 6)
∀X ∈SET : X = X −
→X
=
⇒
X = {x}, x = {< x, x >} The proofs of the above two Propositions 2.2. and 2.3. are rather
simple and immediate, and can be found at [1, pp. 25-27] Remark 2.3. Related to 5) and 6) in Examples 2.1. above, let us note the follow-
ing two kind of situations encountered so far with sets which have
inﬁnitely many brackets, namely : 30 (2.20)
Ω
?
= . . . {{{Ω}}} . . . (2.21)
{0, {1, {2, {3, . . .}}}} ∈SET The second one was, in 6) in Examples 2.1. above, proved to exist
uniquely, and be well deﬁned in ZFA, while the ﬁrst one will be con-
sidered in more detail in 2) below. 1) Related to (2.21), we note the following immediate generalization.
Let α be any inﬁnite ordinal number and let us take X = {xβ | β < α} ⊆U,
A = {β | β < α} while exβ = {β, xβ+1},
β < α Then obviously, we obtain a ﬂat system of equations, therefore (AFA)
gives a unique solution s which, in view of (2.9), has the property sxβ = {sx | x ∈bxβ} S cxβ ∈SET,
β < α where according to (2.6), (2.7), we have bxβ = exβ ∩X = {xβ+1},
cxβ = exβ ∩A = φ,
β < α hence sxβ = {sxβ+1} ∈SET,
β < α which gives (2.22)
sx0 = {0, sx1} = {0, {1, sx2}} = {0, {1, {2, sx3}}} = . . . where the pairs of brackets { } occur once for each β < α. Thus (2.21) is the particular case of the above sx0 in (2.22) correspond-
ing to α = ω which is the ﬁrst inﬁnite ordinal. In the general case of an 31 inﬁnite ordinal α, the above sx0 in (2.22) gives instead of (2.21) the set (2.23)
{0, {1, {2, {3, . . .{β, . . .} . . .}}}} ∈SET which contains all β < α. 2) Let us return to (2.20) and consider it as a particular case of the
following general operation : given a ∈SET S U, deﬁne the set (2.24)
. . . {{{a}}} . . . ∈SET with a pair of brackets { } for each n < ∞. For that purpose, let use
the following notation {
0
a}
0
= a,
{
1
a}
1
= {a},
{
2
a}
2
= {{a}}, . . . Thus the problem is : How to deﬁne in SET (2.25)
{
ω
a}
ω
∈SET where ω denotes the ﬁrst inﬁnite ordinal number. Of course, one would want to deﬁne (2.25) as a certain kind of ”limit”
of the sequence of sets {
n
a}
n
∈SET,
n ≥0. One way to do that for an arbitrary set a ∈SET is as follows. Let us
denote 0
{a
0
} = a 1
{a
1
} =
0
{a
0
} ∪{a} = a ∪{a} 2
{a
2
} =
1
{a
1
} ∪{a, {a}} = a ∪{a} ∪{a, {a}} 3
{a
3
} =
2
{a
2
} ∪{a, {a, {a}}} = a ∪{a} ∪{a, {a}} ∪{a, {a, {a}}} 32 4
{a
4
} =
3
{a
3
} ∪{a, {a, {a, {a}}}} = = a ∪{a} ∪{a, {a}} ∪{a, {a, {a}}} ∪{a, {a, {a, {a}}}}
.
.
. It follows that (2.26)
1
{a
1
} ⊆
2
{a
2
} ⊆
3
{a
3
} ⊆
4
{a
4
} . . . Thus one can deﬁne (2.27)
ω
{a
ω
} = S n<ω n
{a
n
} ∈SET Clearly, that procedure can be extended to all ordinal numbers α. Thus the above problem (2.25) got solved in general, although not
along its initial formulation. On the other hand, in the particular case when a = Ω∈SET, then
in view of the fact that Ω= {
n
a}
n
∈SET,
n < ω one may come up with a deﬁnition of (2.25) considered in its initial
formulation, and which hence is simpler than the one given in (2.27),
namely (2.28)
{
ω
a}
ω
= Ω∈SET And again, one may extend that deﬁnition to all ordinal numbers α, by (2.29)
{
α
a}
α
= Ω∈SET 3) The obvious diﬀerence between (2.20) and (2.21) is that in the sec-
ond, there is an outer pair of brackets { }, while in the ﬁrst there is
none. And such an outer pair of brackets does indeed deﬁne a set in
SET, or for that matter, even in SET0, provided that what is within 33 that outer pair of brackets makes sense in the respective version of Set
Theory. And clearly, for (2.21) such is the case within SET, as seen
in 6) in Examples 2.1. above. One can also note that the generalization of (2.21) in (2.23) to arbi-
trary ordinals α always has an outer pair of brackets { }. On the other
hand, in the generalization (2.29) of (2.20), there is an outer pair of
brackets { }, only if α is not a limit ordinal. 4) The ﬂat system of equations in 7) in Examples 2.1., can obviously
be generalized to arbitrary ordinal numbers α, in a way similar to the
generalization in 1) above of 6) in Examples 2.1. 3. Three Basic Operations In order to pursue the theory, the following three operations, seldom
if at all encountered in usual Set Theory, although quite elementary
as such, will be needed. We start with the deﬁnition of a fundamental concept. Deﬁnition 3.1. A set a ∈SET is called transitive, if and only if b ∈a
=
⇒
b ⊆a or equivalently c ∈b ∈a
=
⇒
c ∈a Clearly, usual sets in mathematics are not transitive. For instance,
given a set X of open subsets in a topological space, then the transi-
tivity of X would imply that for every open subset E ∈X , we must
also have E ⊆X . In other words, X must also contain as elements all
the points x ∈E, for every E ∈X . 34 And now, the ﬁrst basic operation. Deﬁnition 3.2. Given a set a ∈SET, its transitive closure is by deﬁnition the smallest
transitive set which contains it, and which is denoted by TC(a). Lemma 3.1. The transitive closure TC(a) exists for every set a ∈SET, and it is
given by (3.1)
TC(a) = S {a, S a, S S a, . . .} ∈SET Further, for a ∈SET, we have (3.2)
TC(a) = {b | b ∈a} S {c ∈b ∈a} S {d ∈c ∈b ∈a} S S {e ∈d ∈c ∈b ∈a} S . . . Here we used the simplifying notation {c ∈b ∈a} = { c | ∃b ∈a : c ∈b } {d ∈c ∈b ∈a} = { d | ∃b ∈a : ∃c ∈b : d ∈c } .
.
. Note. The meaning of TC(a), for a given set a ∈SET, is clear from (3.2)
which, obviously, can be written in the equivalent form (3.2∗)
TC(a) = {x | x ∈an ∈. . . ∈a2 ∈a1 ∈a0 = a,
n ≥0} Proof. 35 We note that S a = S b∈a b = {c ∈b ∈a} thus S S a = S b∈S a b = {c ∈b ∈S a} = {d ∈c ∈b ∈a} and so on . . . Therefore TC(a) = {x ∈y ∈{a, S a, S S a, . . .}} = = {x ∈y = a} S {x ∈y = S a} S S {x ∈y = S S a} S . . . = = a S ( S a ) S ( S S a ) S . . . = = a S ( S b∈a b ) S ( S c∈S a c ) S ( S d∈S S a d ) . . . = = {b ∈a} S {c ∈b ∈a} S {d ∈c ∈b ∈a} . . . Examples 3.1. 1) Given a ∈SET, then TC({a}) is the smallest transitive set which
has a ∈SET as an element, since TC({a}) = {a} S {c ∈b ∈{a}} S {d ∈c ∈b ∈{a}} . . . = = {a} S {c ∈b = a} S {d ∈c ∈b = a} . . . = = {a} S {b ∈a} S {c ∈b ∈a} . . . = {a} S TC(a) 2) TC(φ) = φ 36 3) If a ∈U, then TC({a}) = {a} Note : if a ∈U, then TC(a) is not deﬁned, since a /
∈SET 4) If a ⊆U, a ∈SET, then TC(a) = a 5) If a ∈SET, then TC(a) = φ =
⇒{b ∈a} = φ =
⇒a = φ thus TC(a) = φ ⇐
⇒a = φ 6) If a ⊆U, a ∈SET, then TC(a) = φ =
⇒a = φ ∈SET TC(a) = φ ⇐
⇒a = φ 7) If A, B ∈SET, A, B ̸= φ, a ∈A, b ∈B, then < a, b > = {{a}, {a, b}} ∈A × B ∈SET and TC(< a, b >) = < a, b > S {d ∈c ∈< a, b >} S S {e ∈d ∈c ∈< a, b >} S . . . = = {{a}, {a, b}} S {d ∈c = {a} W d ∈c = {a, b}} S S {e ∈d ∈c = {a} W e ∈d ∈c = {a, b}} S . . . = = {{a}, {a, b}} S {d ∈{a, b}} S {e ∈d ∈{a, b}} S . . . = = {{a}, {a, b}} S {a, b} S {e ∈{a, b}} S . . . = 37 = {{a}, {a, b}} S TC({a, b}) =
< a, b > S TC({a, b}) 8) If A, B ∈SET, A, B ̸= φ, R ⊆A × B, then TC(R) = R S {c ∈< a, b > ∈R} S {d ∈c ∈< a, b > ∈R} S S {e ∈d ∈c ∈< a, b > ∈R} S . . . = = R ∪{c = {a}| < a, b > ∈R} ∪{c = {a, b}| < a, b > ∈R}∪ ∪{d ∈c = {a}| < a, b > ∈R} ∪{d ∈c = {a, b}| < a, b > ∈R}∪ ∪{e ∈d ∈c = {a}| < a, b > ∈R}∪ ∪{e ∈d ∈c = {a, b}| < a, b > ∈R} ∪. . . = = R ∪{{a} | < a, b > ∈R} ∪{{a, b} | < a, b > ∈R}∪ ∪{a | < a, b > ∈R} ∪{b | < a, b > ∈R}∪ ∪{e ∈a | < a, b > ∈R} ∪{e ∈b | < a, b > ∈R} ∪. . . 9) The above goes in particular when R is a function f : A −
→B, or
when R = A × B, and in the last case we obtain TC(A × B) = (A × B) ∪{{a} | a ∈A} ∪{{a, b} | a ∈A, b ∈B}∪ ∪{a | a ∈A} ∪{b | b ∈B}∪ ∪{e ∈a | a ∈A} ∪{e ∈b | b ∈B} ∪. . . = = (A × B) ∪{{a} | a ∈A} ∪{{a, b} | a ∈A, b ∈B} ∪TC(A) ∪TC(B) 10) If X ∈SET, then TC(P(X)) = P(X) S {c | c ∈b ∈P(X)} S 38 S {d | d ∈c ∈b ∈P(X)} S . . . = = P(X) S {c | c ∈Y ⊆X} S {d | d ∈c ∈Y ⊆X} S . . . = = P(X) S {c | c ∈X} S {d | d ∈c ∈X} S . . . = = P(X) S TC(X) 11) If x, y ∈U and a = { x, { y } }, then TC(a) = {x, {y}, y} since b ∈a ⇐
⇒b = x W b = {y}, thus c ∈b ⇐
⇒c = y, hence TC(a) = a S {c | c ∈b ∈a} S {d | d ∈c ∈b ∈a} . . . = = a S {c | c = y} S {d | d ∈c = y} . . . = a S {y}
✷ The second important operation is presented in Deﬁnition 3.3. We deﬁne for sets their support as follows (3.3)
SET ∋a
7−
→
support(a) = TC(a) T U Further, a set a ∈SET is called pure, if and only if (3.4)
support(a) = φ Note. The meaning of support(a), for a set a ∈SET, is easy to see, based
on (3.2∗), namely (3.3∗)
support(a) = {x ∈U | x ∈an ∈. . . ∈a1 ∈a0 = a,
n ≥0} 39 in other words, support(a) is the set of all ur-elements x ∈U, if there
exist any, with which ﬁnite descending sequences x ∈an ∈. . . ∈a1 ∈
a0 = a, with n ≥0, that start with the set a do terminate. Consequently, pure sets a ∈SET do not have such ﬁnite descending
sequences, but only inﬁnite ones, namely . . . ∈an ∈an−1 ∈. . . ∈a2 ∈a1 ∈a0 = a ✷ Finally, the third important operation is presented in Deﬁnition 3.4. (3.5)
U ⊃A
7−
→
Vafa[A] = {a ∈SET | support(a) ⊆A} and clearly, Vafa[A] is always a proper class. We also denote (3.6)
Vafa[φ] = Vafa = {a ∈SET | a is a pure set}
✷ Clearly (3.7)
Vafa[A] ⊆SET therefore (3.8)
Vafa[A] T U = A T Vafa[A] = φ,
A ⊆U Also, if a ∈SET, then we have seen that TC(a) = φ ⇐
⇒a = φ therefore 40 a = φ =
⇒support(a) = φ Also, if a ⊆U, then we have seen that TC(a) = a therefore support(a) = a If x, y ∈U and a = { x, { y } }, then we have seen that TC(a) = {x, {y}, y} thus support(a) = TC(a) T U = {x, {y}, y} TU = {x, y} 4. Graph Formulation In this section we follow the presentation in [1], without however the
proofs. We consider directed graphs (N, E), where N is the set of nodes and
V ⊆N ×N is the set of vertices. A vertex (n, n ′) ∈V can be denoted
by n →n ′. Thus n0 →n1 →n2 →. . . is a ﬁnite or inﬁnite path The graph (N, E) is called a well-founded graph, if and only if it has
no inﬁnite path We also denote 41 N ∋n 7−
→[ n >= { n ′ ∈N | (n, n ′) ∈E } N ∋n 7−
→
< n ] = { n ′ ∈N | (n ′, n) ∈E } Given a directed graph (N, E) and n0 ∈N, we call (n0 ∈N, E) a
pointed graph. A pointed graph (n0 ∈N, E) is called an accessible graph, if and only if ∀n ∈N : ∃path n0 →. . . →n An accessible graph (n0 ∈N, E) is called a tree with root n0, if and
only if ∀n ∈N : ∃! path n0 →. . . →n A decoration of a directed graph (N, E) is any mapping S : N −
→Set,
such that ∀n ∈N : S(n) = { S(n ′) | n ′ ∈[ n > } Example 4.1.
3
•
",0
"This paper outlines ongoing dissertation research
located
in
the
intersection
of
science
ﬁction,
human-computer interaction and computer science.
Through an interdisciplinary perspective, drawing from
ﬁelds such as human-computer interaction, ﬁlm theory
and studies of science and technology, qualitative and
quantitative content analysis techniques are used to
contextually analyze expressions of science ﬁction in
peer-reviewed computer science research repositories,
such as the ACM or IEEE Xplore Digital Libraries.
This paper concisely summarizes and introduces the
relationship of science ﬁction and computer science
research and presents the research questions, aims
and implications in addition to prior work and study
methodology. In the latter part of this work-in-progress
report, preliminary results, current limitations, future
work and a post-dissertation trajectory are outlined. 1.
Introduction How much of a role does science ﬁction1 (SF) play
in ‘real-world’ scientiﬁc innovation [1] and progress?
What are the opportunities and hindrances – if any –
of SF in the context of computer science research, for
example computer science education [2, 3] or artiﬁcial
intelligence [4] (AI) ethics? Should computer scientists
actually study [5] SF?
Despite the fact that discussions of the synergy
effects between SF and computing research are a
regular topic in popular news and tech magazines,
the bi-directional relationship of both domains is in
reality not accurately described, nor fully scrutinized.
In particular, studies which investigate the presence,
nature, and patterns of use of SF in the context of
computer science research are scarce and rely mostly 1‘Science ﬁction’ (SF), as used in this paper, encompasses
science ﬁction literature as well as science ﬁction movies and shows.
Interactive media, such as science ﬁction-based video games are not
considered in the current research framework. scattered oral accounts.
Therefore, evidence-based
investigations are critical in order to better understand
the potential utility and latent shortcomings of SF
for future computing and human-computer interaction
(HCI) research, innovation and education.
In this
context, this dissertation endeavors to create a better
understanding of the relationship between SF and
computer science research. 2.
Research Aims and Objectives This research draws from an interdisciplinary
perspective and distinct bodies of literature from the
following knowledge domains and topical areas: 1. SF, computing visions and design theory
2. SF in contemporary HCI research
3. SF movies and real-world scientists
4. SF in computer science education
5. SF and computer science frameworks and models
6. SF and evolutionary computer science research The principal research objective of the dissertation is the
discovery, description and analysis of the relationship
between SF and computer science research through a
content analysis of scientiﬁc publications, which refer
SF in their full-text and metadata; over time and within
distinct subﬁelds of computer science research.
As introduced earlier, there are scattered indicators
of the introduced relationship which can be found in the
contemporary public news [6, 7]. tech magazines [8]
and expert communities [9, 10]. While present, these
traces are typically of anecdotal nature with no clear
evidence as in how SF is used in, linked with, or has
contributed to, a speciﬁc research project, product, or
process. While SF in computer science research itself
is discussed tangentially on occasion (e.g. [11, 12, 13]),
proper studies with focus on the subject matter are rare
and, if present: • limited on speciﬁc aspects and applications of SF
for computer science research, for example as
data sources [14, 15]; arXiv:1807.09490v1  [cs.HC]  25 Jul 2018 • have a selection bias and lack objective and
longitudinal data to describe the relationship of SF
and HCI over time and across ﬁelds of computer
science research (e.g [12, 16]); • or use a limited sample size and lack focus on SF
and computing research (e.g. [17, 18]). Currently, no meta-study does investigate the presence
and patterns of usage of SF in the context of
scientiﬁc publications. However, such investigations are
important as they can point to missed opportunities and
future potentials of SF for computing research.
For
example, an overview and evolutionary description of
SF in computing research can provide important insights
for future computing research.
Perhaps, data on the
occurrence, frequency, usage and contextual referrals of
SF and its terminology derivatives in computer science
publications can identify emerging computing research
themes (which are more or less prone to use SF; and
therefore, might beneﬁt more or less from assimilating
SF in the future).
In addition, the cultural origin,
purpose, and nature2 of the SF narratives in computer
science publications have not been described as well,
for instance, the patterns of usage of Western and
non-Western SF. Also, the prevalence or abundance of
SF referrals within and between distinct subﬁelds of
computer science research, for instance, the patterns of
usage in HCI, human-robot interaction (HRI) or design
research are not investigated at present.
One might ask why SF is used in a computer
science publication; what kind of SF is referenced in
the respective publication and how does it inform the
research itself? Is there a cultural bias of the referred SF
in computing literature? Did SF referrals evolve from
rhetorical devices into serious research topics for HCI
and computing literature over time? Are there clusters
of computer science areas (e.g. HCI, HRI, AI) which
regularly draw from and refer to SF? These are research
questions this study aims to answer and it is in this niche,
where the contribution of this dissertation is located in. Research assumptions, aims and objectives
The
assumption in this study is that SF is an existing,
yet undiscovered topical domain in computer science
research and its associated subﬁelds. Speciﬁcally, SF
movies, but also general SF authors, technologies,
concepts, characters, and stories will be traced in order
to investigate the prevalence, usage, and appropriation
of SF in scientiﬁc publications. Thus, the primary aims
of this dissertation are to establish a ground truth of the
relationship between HCI and SF. First, the dissertation
aims to quantify, for the ﬁrst time, the described 2For example, dystopian versus utopian SF. relationship between popular SF media depiction of
technology in computer science publications. Second,
the dissertation aims to create and describe measures
of this relationship through a quantitative content
analysis of publications which reference SF, as well as
descriptive, qualitative analysis. Third, the dissertation
aims to inform a more comprehensive framework
to re-conceptualize the bi-directional relationship of
computer science, HCI and SF, outlined through speciﬁc
research objectives (RO) below. RO 1: Identify an interplay between SF and HCI
Research
The primary research objective of this
dissertation is to discover a relationship between SF
and computer science research, with a special focus
on human-computer interaction (HCI) topics.
For
the purposes of this dissertation, RO1 is addressed
through the related and prior work. The related work
provides a thematic literature review and indicates
a mutual, yet not quantiﬁed, relationship of both
ﬁelds.
The prior work introduces three pilot studies
[19, 20, 21] which conﬁrmed the presence and usage
of SF concepts in the ACM Digital Library (DL),
respectively the CHI proceedings. To further investigate
these suggestions and corroborate RO1, an in-depth
study of the occurrence of SF in a larger scientiﬁc
corpus of peer-reviewed publications is conducted in
a comparable collection, that is the IEEE Xplore DL
repository. RO 2:
Determine patterns of SF references in
computer science / HCI research publications
The
second research objective of this dissertation is to
determine a quantiﬁable, that is to say, measurable,
impact SF has had, currently has and potentially will
have on computer science and HCI research.
To do
so, quantitative text mining in combination with a
qualitative content analysis of the referred SF concept(s)
in relationship to the research theme of the retrieved
publications are applied in order to understand the
context of the SF referral. Through such an analysis,
a determination of the purpose and level of inﬂuence
of the referral in speciﬁc subtopics of computing
research and HCI (such as mobile computing, robotics,
wearables, virtual reality and others) can be assessed.
Moreover, patterns across periodicals and collections,
such as the mechanics of SF referrals in certain
subtopics in computing literature could indicate which
ﬁelds of research or authors are more or less prone to
draw from, and integrate, SF in their research. RO3:
Describe and analyze the characteristics,
purpose and cultural origin of the SF referenced in computer science / HCI publications over time
A third research objective in this dissertation is the
identiﬁcation and determination of the cultural origin
of the referenced SF in the retrieved publication(s) in
order to provide an evolutionary overview of the type
and purposes of SF in computer science research over
time. A contextual analysis of SF in computer science
research publications can describe the cultural origins of
the SF material (e.g. authorship, country of production)
as well as the purposes of the SF referral in the context of
the individual publication. Through RO3, the prevalence
and absence of speciﬁc SF from speciﬁc countries and
areas will be determined.
Patterns, for instance, the
type of SF referral and the purpose of the reference in
correlation to the publication year and computer science
subﬁelds over time should enable deductions if SF
precedes, matches or follows real-world technological
development and research. 3.
Prior Work The dissertation topic has been developed since
2015 and preliminary results have been published on
an ongoing basis with collaborators.
In 2015, an
workshop paper [22] was presented at the Workshop on
SF and the Reality of HCI: Inspirations, Achievements
or a Mismatch in the context of the 27th Australian
Conference on Human-Computer Interaction (OzCHI).
In 2016,
a theoretical paper [23] and poster on
the conceptualization and measurement of science
ﬁciton in computer science research was presented at
the 8th International Conference on Human-Computer
Interaction (HCII). As co-author, a short paper [24]
on the relationship of SF and scientiﬁc inspiration
was published at the 13th International Conference
on Advances in Computer Entertainment Technology
(ACE). In 2017, the ﬁrst pilot study on Star Trek in the
ACM DL was published [19]. In 2018, the second pilot
study was published in the context of the 2018 HCII
conference [20]. Awards, Earned Press and Honors
The dissertation
research and preliminary results have gained attention in
the information science and HCI communities. In 2016,
an external research grant [25], through a collaboration
with a senior researcher from the University of Western
Sydney, Australia, of $3,500 was awarded.
In 2018,
the latest paper of the research project [20] was
referenced in the MIT Emerging Technology News
[26].
An ACM Interactions (IX) blog post [27] on
the topic of SF and HCI was published in early
2018. Furthermore, the doctoral candidate was awarded
an Eugene Garﬁeld Dissertation Fellowship [28] in 2018 for the dissertation research, valued at $3000,
which honors doctoral candidates working on their
dissertations in the library and information science,
information studies, informative, or a related ﬁeld . 4.
Methodology Currently, the ﬁnal analysis of one large dataset of
1400 computer science publications, retrieved in the
IEEE Xplore DL, is prepared for analysis. Speciﬁcally,
the corpora metadata characteristics and contextual
uses, purposes, and types of SF references within
each individual publication will be examined.
Based
on the prior work, the proposed methodology will
follow a two-stage,
combined,
qualitative content
analysis (bottom-up) and quantitative, text analysis
(top-down) approach.
Preceding that ﬁnal study and
content analysis, inter-rater agreement and a coding
scheme will be established and deﬁned.
The SF
referrals and contextual characteristics in the individual
publication are will be manually reviewed and coded
in the bottom-up approach. In the top-down approach,
an assisted, quantitative text analysis of the corpora
metadata will be conducted. For the successive analysis,
the results of both approaches will be consolidated and
compared against each other.
Detailed steps of the
research methodology are outlined below. Step 1: Initial searches and candidate sets
In order
to estimate the occurrence of SF and its related concepts,
initial exploratory searches for SF and term derivatives
of the concept in selected computer science collections,
such as the IEEE Xplore DL, have yielded multiple
candidate sets for potential further investigation and
qualitative analyses. Step 2: Selection and acquisition of publications
In
a second step, a ﬁnal candidate dataset3, based on the
feasibility of the methodology, is acquired. The ﬁnal
dataset will be ﬁrst fully downloaded and then imported
into content analysis software for a further qualitative
inspection with regards to the prior outlined research
aims and objectives. Step 3:
Initial assessment of publications
In a
third step, the ﬁnal candidate set is explored through
a qualitative and guided quantitative coding and text
analysis.
The retrieved publications will be assessed
with regards to the focus of the research publication
itself (e.g. AI, augmented reality, gestural interactions)
in order to identify subtopics in computer science 3Currently, this dataset is a set of 1400 publications, retrieved via
full-text search for “science ﬁction” in the IEEE Xplore DL. research which are more predisposed to reference SF
than others. Finally, an evolutionary analysis over time
and across periodicals and proceedings of the retrieved
publications with a focus on the themes discussed in
the year or decade the research was published in should
enable a measurement of the foresight, impact and
overall usefulness of SF in computer science research. Step 4.1:
Qualitative coding
Prior work [20]
proposes ﬁve research themes for a qualitative coding
of SF in conjunction with the main research theme of
the computer science research publication: 1. Theoretical:
Publications on design research,
ideation, and design ﬁction. 2. New
interactions:
Novel
interfaces
and
interaction
modes
(e.g.
gestural,
haptic,
shape-changing). 3. Human-body
modiﬁcation
/
extension:
End-of-life technologies, on-body fabrication of
artifacts, implants and in-body insertables. 4. Human-Robot
Interaction
and
AI:
Human-robot or human-agent interaction and
agency, artiﬁcial intelligence, natural language
interfaces, and ethics. 5. HCI
and
Future
Visions
of
Technology:
Technology in conjunction with agency and
power, utopian versus dystopian dichotomies in
the tension ﬁeld of ubiquitous computing (e.g.
privacy versus security). Step 4.2: Quantitative analysis
Publication metrics
(e.g. citations, venue, downloads) as well as document
metadata analyses (e.g.
term-frequency, co-word or
keyword analysis) of the ﬁnal dataset will be used to
identify which topical domains in computing literature
are more or less connected with SF4 Step 5:
Consolidation of the qualitative and
quantitative analysis
For the successive analysis,
the results of both approaches will be consolidated
and compared against each other in order to generate
best-possible insights on the results, and subsequently
derive conclusions to in relationship to the outlined
research questions and objectives. 4For such a potential content analysis software, Provalis QDA
Miner [29] provides functionalities for bivariate comparisons of
variables, for example, to compare the venue of publication or
publication year with any given coded text, such as the purpose of
the SF referral. 5.
Implications and Future Work Implications
The results of this study are expected to
discover novel links and synergy effects of computer
science and SF; over time and across categorical
subdisciplines of computing research. Furthermore, the
evolutionary analysis of the results of this study are
expected to demonstrate both, that SF has been a topic
of interest since the inception of the ﬁeld of computer
science, and is increasingly applied in present-day,
computer science research.
The implications of this
study are further expected to guide future computer
scientists and educators to consciously utilize SF in their
teaching, research and scholarship and therefore drive
future innovative HCI and computer science research,
application, and education.
This study also aims to
provide a methodological contribution to the ﬁeld of
studies of science, technology and society (STS) in
form of a case study by disentangling the underlying,
ambiguous relationships between SF and computer
science – or in a broader deﬁnition – science and art. Future Work
At the present time,
one journal
paper on the usage of SF robots in computer- and
HRI-research is in review at ACM THRI [21]. A second
manuscript, an analysis of the cultural prevalence
of speciﬁc science ﬁction material in the full CHI
proceedings is close to submission for an HCI journal.
A third paper, which aims to propose science ﬁction
media (especially audio-visual media, such SF movies
and shows) as design ﬁctions (see e.g. [30]) for HCI
and computer science research is in the conceptual phase
with the aim for submission by the end of 2018.
Future studies to extend the presented research
trajectory will aim to conduct qualitative interviews with
HCI researchers in order to investigate and assess the
reasons, and reservations, of researchers and authors
in computer science, who refer sci-ﬁin their research
output.",0
"Detection of non-technical losses (NTL) which include electricity theft, faulty meters or billing errors
has attracted increasing attention from researchers in electrical engineering and computer science. NTLs
cause signiﬁcant harm to the economy, as in some countries they may range up to 40% of the total
electricity distributed. The predominant research direction is employing artiﬁcial intelligence to predict
whether a customer causes NTL. This paper ﬁrst provides an overview of how NTLs are deﬁned and their
impact on economies, which include loss of revenue and proﬁt of electricity providers and decrease of the
stability and reliability of electrical power grids. It then surveys the state-of-the-art research efforts in a
up-to-date and comprehensive review of algorithms, features and data sets used. It ﬁnally identiﬁes the
key scientiﬁc and engineering challenges in NTL detection and suggests how they could be addressed in
the future. Keywords: Covariate shift, electricity theft, expert systems, machine learning, non-technical losses,
stochastic processes. 1.
Introduction Our modern society and daily activities strongly de-
pend on the availability of electricity.
Electrical
power grids allow to distribute and deliver electricity
from generation infrastructure such as power plants
or solar cells to customers such as residences or fac- tories. One frequently appearing problem are losses
in power grids, which can be classiﬁed into two cat-
egories: technical and non-technical losses.
Technical losses occur mostly due to power dis-
sipation. This is naturally caused by internal electri-
cal resistance and the affected components include
generators, transformers and transmission lines. arXiv:1606.00626v3  [cs.AI]  25 Jul 2017 P. Glauner et al. / Challenge NTL Detection Survey The complementary non-technical losses (NTL)
are primarily caused by electricity theft. In most
countries, NTLs account for the predominant part
of the overall losses as discussed in Ref. 1. There-
fore, it is most beneﬁcial to ﬁrst reduce NTLs before
reducing technical losses as proposed in Ref. 2. In
particular, NTLs include, but are not limited to, the
following causes reported in Refs. 3 and 4: • Meter tampering in order to record lower con-
sumptions • Bypassing meters by rigging lines from the power
source • Arranged false meter readings by bribing meter
readers • Faulty or broken meters • Un-metered supply • Technical and human errors in meter readings,
data processing and billing NTLs cause signiﬁcant harm to economies, in-
cluding loss of revenue and proﬁt of electricity
providers, decrease of the stability and reliability of
electrical power grids and extra use of limited natu-
ral resources which in turn increases pollution. For
example, in India, NTLs are estimated at US$ 4.5
billion in Ref. 5. NTLs are simultaneously reported
in Refs. 6 and 7 to range up to 40% of the total elec-
tricity distributed in countries such as Brazil, India,
Malaysia or Lebanon. They are also of relevance in
developed countries, for example estimates of NTLs
in the UK and US that range from US$ 1-6 billion
are reported in Refs. 1 and 8.
We want to highlight that only few works on
NTL detection have been reported in the literature
in the last three to four years. Given that NTL de-
tection is an active ﬁeld in industrial R&D, it is to
our surprise that academic research in this ﬁeld has
dropped in the last few years.
From an electrical engineering perspective, one
method to detect losses is to calculate the energy bal-
ance reported in Ref. 9, which requires topological
information of the network. In emerging economies,
which are of particular interest due to their high NTL
proportion, this is not realistic for the following rea-
sons: (i) network topology undergoes continuous changes in order to satisfy the rapidly growing de-
mand of electricity, (ii) infrastructure may break and
lead to wrong energy balance calculations and (iii) it
requires transformers, feeders and connected meters
to be read at the same time.
A more ﬂexible and adaptable approach is to em-
ploy artiﬁcial intelligence (AI), which is well cov-
ered in Ref. 10. AI allows to analyze customer pro-
ﬁles, their data and known irregular behavior. This
allows to trigger possible inspections of customers
that have abnormal electricity consumption patterns.
Technicians then carry out inspections, which allow
them to remove possible manipulations or malfunc-
tions of the power infrastructure. Furthermore, the
fraudulent customers can be charged for the addi-
tional electricity consumed. However, carrying out
inspections is costly, as it requires physical presence
of technicians.
NTL detection methods reported in the literature
fall into two categories: expert systems and ma-
chine learning. Expert systems incorporate hand-
crafted rules for decision making.
In contrast,
machine learning gives computers the ability to
learn from examples without being explicitly pro-
grammed. Historically, NTL detection systems were
based on domain-speciﬁc rules. However, over the
years, the ﬁeld of machine learning has become
the predominant research direction of NTL detec-
tion. To date, there is no authoritative survey that
compares the various approaches of NTL detection
methods reported in the literature. We are also not
aware of any existing survey that discusses the short-
comings of the state of the art. In order to advance in
NTL detection, the main contributions of this survey
are the following: • We provide a detailed review and critique of state-
of-the-art NTL detection research employing AI
methods in Section 2. • We identify the unsolved key challenges of this
ﬁeld in Section 3. • We describe in detail the proposed methods to
solve the most relevant challenges in the future in
Section 4. • We put these challenges in the context of AI re-
search as a whole as they are of relevance to many
other learning and anomaly detection problems. P. Glauner et al. / Challenge NTL Detection Survey 2.
The State of the Art NTL detection can be treated as a special case of
fraud detection, for which general surveys are pro-
vided in Refs. 11 and 12. Both highlight expert sys-
tems and machine learning as key methods to detect
fraudulent behavior in applications such as credit
card fraud, computer intrusion and telecommunica-
tions fraud. This section is focused on an overview
of the existing AI methods for detecting NTLs. Ex-
isting short surveys of the past efforts in this ﬁeld,
such as in Refs. 3,13,14 and 15 only provide a nar-
row comparison of the entire range of relevant pub-
lications. The novelty of this survey is to not only
review and compare a wide range of results reported
in the literature, but to also derive the unsolved chal-
lenges of NTL detection. 2.1.
Features In this subsection, we summarize and group the fea-
tures reported in the literature. 2.1.1.
Monthly consumption Many works on NTL detection use traditional me-
ters, which are read monthly or annually by meter
readers. Based on this data, average consumption
features are used in Refs. 1,7,16,17 and 18. In those
works, the feature computation used can be summa-
rized as follows: For M customers {0,1,...,M −1}
over the last N months {0,1,...,N −1}, a feature
matrix F is computed, in which element Fm,d is a
daily average kWh consumption feature during that
month: x(m)
d
=
L(m)
d R(m)
d
−R(m)
d−1
,
(1) where for customer m, L(m)
d
is the kWh consumption
increase between the meter reading to date R(m)
d
and
the previous one R(m)
d−1. R(m)
d
−R(m)
d−1 is the number of
days between both meter readings of customer m.
The previous 24 monthly meter readings are used
in Refs.
19 and 20.
The features computed are
the monthly consumption before the inspection, the
consumption in the same month in the year before the consumption in the past three months and the
customer’s consumption over the past 24 months.
Using the previous six monthly meter readings, the
following features are derived in Ref. 21: average
consumption, maximum consumption, standard de-
viation, number of inspections and the average con-
sumption of the residential area. The average con-
sumption is also used as a feature in Refs. 22 and
23. 2.1.2. Smart meter consumption With the increasing availability of smart meter de-
vices, consumption of electric energy in short inter-
vals can be recorded. Consumption features of in-
tervals of 15 minutes are used in Refs. 24 and 25,
whereas intervals of 30 minutes are used in Refs. 26
and 27.
The 4 × 24 = 96 measurements of Ref. 25 are
encoded to a 32-dimensional space in Refs. 6 and
28. Each measurement is 0 or positive. Next, it is
then mapped to 0 or 1, respectively. Last, the 32 fea-
tures are computed. A feature is the weighted sum
of three subsequent values, in which the ﬁrst value
is multiplied by 4, the second by 2 and the third by
1.
The maximum consumption in any 15-minute
period is used as a feature in Refs. 29–31 and 32.
The load factor is computed by dividing the demand
contracted by the maximum consumption.
Features from the consumption time series called
shape factors are derived from the consumption time
series including the impact of lunch times, nights
and weekends in Ref. 33. 2.1.3. Master data Master data represents customer reference data such
as name or address, which typically changes infre-
quently. The work in Ref. 22 uses the following fea-
tures from the master data for classiﬁcation: location
(city and neighborhood), business class (e.g. resi-
dential or industrial), activity type (e.g. residence
or drugstore), voltage (110V or 200V), number of
phases (1, 2 or 3) and meter type. The demand con-
tracted, which is the number of kW of continuous
availability requested from the energy company and P. Glauner et al. / Challenge NTL Detection Survey the total demand in kW of installed equipment of
the customer are used in Refs. 30–32. In addition,
information about the power transformer to which
the customer is connected to is used in Ref.
29.
The town or customer in which the customer is lo-
cated, the type of voltage (low, median or high), the
electricity tariff, the contracted power as well as the
number of phases (1 or 3) are used in Ref. 23. Re-
lated master data features are used in Ref. 33, in-
cluding the type of customer, location, voltage level,
type of climate (rainy or hot), weather conditions
and type of day. 2.1.4.
Credit worthiness The works in Refs. 1,17 and 18 use the credit wor-
thiness ranking (CWR) of each customer as a fea-
ture. It is computed from the electricity provider’s
billing system and reﬂects if a customer delays or
avoids payments of bills. CWR ranges from 0 to
5 where 5 represents the maximum score.
It re-
ﬂects different information about a customer such
as payment performance, income and prosperity of
the neighborhood in a single feature. 2.2.
Expert systems and fuzzy systems An ensemble pre-ﬁlters the customers to select irreg-
ular and regular customers in Ref. 19. These cus-
tomers are then used for training as they represent
well the two different classes. This is done because
of noise in the inspection labels. In the classiﬁcation
step, a neuro-fuzzy hierarchical system is used. In
this setting, a neural network is used to optimize the
fuzzy membership parameters, which is a different
approach to the stochastic gradient descent method
used in Ref. 16. A precision of 0.512 and an accu-
racy of 0.682 on the test set are obtained.
Proﬁles of 80K low-voltage and 6K high-voltage
customers in Malaysia having meter readings every
30 minutes over a period of 30 days are used in Ref.
26 for electricity theft and abnormality detection. A
test recall of 0.55 is reported. This work is related
to features of Ref. 7, however, it uses entirely fuzzy
logic incorporating human expert knowledge for de-
tection. The work in Ref. 1 is combined with a fuzzy
logic expert system postprocessing the output of the
SVM in Ref. 7 for ~100K customers. The motiva-
tion of that work is to integrate human expert knowl-
edge into the decision making process in order to
identify fraudulent behavior. A test recall of 0.72 is
reported.
Five features of customers’ consumption of the
previous six months are derived in Ref. 21: aver-
age consumption, maximum consumption, standard
deviation, number of inspections and the average
consumption of the residential area. These features
are then used in a fuzzy c-means clustering algo-
rithm to group the customers into c classes. Sub-
sequently, the fuzzy membership values are used to
classify customers into NTL and non-NTL using the
Euclidean distance measure. On the test set, an aver-
age precision (called average assertiveness) of 0.745
is reported. 2.3.
Neural networks Neural networks are loosely inspired by how the
human brain works and allow to learn complex
hypotheses from data.
They are well described
for example in Ref.
34.
Extreme learning ma-
chines (ELM) are one-hidden layer neural networks
in which the weights from the inputs to the hidden
layer are randomly set and never updated. Only the
weights from the hidden to output layer are learned.
The ELM algorithm is applied to NTL detection in
meter readings of 30 minutes in Ref. 35, for which
a test accuracy of 0.5461 is reported.
An ensemble of ﬁve neural networks (NN) is
trained on samples of a data set containing ~20K
customers in Ref. 20. Each neural network uses
features calculated from the consumption time se-
ries plus customer-speciﬁc pre-computed attributes.
A precision of 0.626 and an accuracy of 0.686 are
obtained on the test set.
A self-organizing map (SOM) is a type of un-
supervised neural network training algorithm that is
used for clustering. SOMs are applied to weekly
customer data of 2K customers consisting of me-
ter readings of 15 minutes in Ref.
24.
This al-
lows to cluster customers’ behavior into fraud or
non-fraud. Inspections are only carried out if cer- P. Glauner et al. / Challenge NTL Detection Survey tain hand-crafted criteria are satisﬁed including how
well a week ﬁts into a cluster and if no contractual
changes of the customer have taken place. A test ac-
curacy of 0.9267, a test precision of 0.8526, and test
recall of 0.9779 are reported.
A data set of ~22K customers is used in Ref. 22
for training a neural network. It uses the average
consumption of the previous 12 months and other
customer features such as location, type of customer,
voltage and whether there are meter reading notes
during that period. On the test set, an accuracy of
0.8717, a precision of 0.6503 and a recall of 0.2947
are reported. 2.4.
Support vector machines The Support Vector Machines (SVM) introduced in
Ref. 36 is a state-of-the-art classiﬁcation algorithm
that is less prone to overﬁtting. Electricity customer
consumption data of less than 400 highly imbal-
anced out of ~260K customers in Kuala Lumpur,
Malaysia are used in Ref. 17. Each customer has
25 monthly meter readings in the period from June
2006 to June 2008. From these meter readings, daily
average consumption features per month are com-
puted. Those features are then normalized and used
for training in a SVM with a Gaussian kernel. In
addition, credit worthiness ranking (CWR) is used.
It is computed from the electricity provider’s billing
system and reﬂects if a customer delays or avoids
payments of bills. CWR ranges from 0 to 5 where 5
represents the maximum score. It was observed that
CWR is a signiﬁcant indicator of whether customers
commit electricity theft. For this setting, a recall of
0.53 is achieved on the test set. A related setting is
used in Ref. 1, where a test accuracy of 0.86 and a
test recall of 0.77 are reported on a different data set.
SVMs are also applied to 1,350 Indian customer
proﬁles in Ref. 25. They are split into 135 differ-
ent daily average consumption patterns, each having
10 customers. For each customer, meters are read
every 15 minutes. A test accuracy of 0.984 is re-
ported. This work is extended in Ref. 28 by encod-
ing the 4×24 = 96-dimensional input in a lower di-
mension indicating possible irregularities. This en-
coding technique results in a simpler model that is
faster to train while not losing the expressiveness of the data and results in a test accuracy of 0.92.
Consumption proﬁles of 5K Brazilian industrial
customer proﬁles are analyzed in Ref. 29. Each
customer proﬁle contains 10 features including the
demand billed, maximum demand, installed power,
etc. In this setting, a SVM slightly outperforms K-
nearest neighbors (KNN) and a neural network, for
which test accuracies of 0.9628, 0.9620 and 0.9448,
respectively, are reported.
The work of Ref. 28 is extended in Ref. 6 by in-
troducing high performance computing algorithms
in order to enhance the performance of the previ-
ously developed algorithms. This faster model has a
test accuracy of 0.89.
A data set of ~700K Brazilian customers, ~31M
monthly meter readings from January 2011 to Jan-
uary 2015 and ~400K inspection data is used in Ref.
16. It employs an industrial Boolean expert system,
its fuzziﬁed extension and optimizes the fuzzy sys-
tem parameters using stochastic gradient descent de-
scribed in Ref. 37 to that data set. This fuzzy system
outperforms the Boolean system. Inspired by Ref.
17, a SVM using daily average consumption features
of the last 12 months performs better than the expert
systems, too. The three algorithms are compared to
each other on samples of varying fraud proportion
containing ~100K customers. It uses the area under
the (receiver operating characteristic) curve (AUC),
which is discussed in Section 3.1. For a NTL propor-
tion of 5%, it reports AUC test scores of 0.465, 0.55
and 0.55 for the Boolean system, optimized fuzzy
system and SVM, respectively. For a NTL propor-
tion of 20%, it reports AUC test scores of 0.475,
0.545 and 0.55 for the Boolean system, optimized
fuzzy system and SVM, respectively. 2.5.
Genetic algorithms The work in Refs. 1 and 17 is extended by using a
genetic SVM for 1,171 customers in Ref. 18. It uses
a genetic algorithm in order to globally optimize
the hyperparameters of the SVM. Each chromosome
contains the Lagrangian multipliers (α1,...,αi), reg-
ularization factor C and Gaussian kernel parameter
γ. This model achieves a test recall of 0.62.
A data set of ~1.1M customers is used in Ref.
38.
The paper identiﬁes the much smaller class P. Glauner et al. / Challenge NTL Detection Survey of inspected customers as the main challenge in
NTL detection. It then proposes stratiﬁed sampling
in order to increase the number of inspections and
to minimize the statistical variance between them.
The stratiﬁed sampling procedure is deﬁned as a
non-linear restricted optimization problem of min-
imizing the overall energy loss due to electricity
theft. This minimization problem is solved using
two methods: (1) genetic algorithm and (2) simu-
lated annealing. The ﬁrst approach outperforms the
other one. Only the reduced variance is reported,
which is not comparable to the other research and
therefore left out of this survey. 2.6.
Rough sets Rough sets give lower and upper approximations of
an original conventional or crisp set. The ﬁrst ap-
plication of rough set analysis applied to NTL de-
tection is described in Ref. 39 on 40K customers,
but lacks details on the attributes used per customer,
for which a test accuracy of 0.2 is achieved. Rough
set analysis is also applied to NTL detection in Ref.
23 on features related to Ref. 22. This supervised
learning technique allows to approximate concepts
that describe fraud and regular use. A test accuracy
of 0.9322 is reported. 2.7.
Other methods Different feature selection techniques for customer
master data and consumption data are assessed in
Ref. 33. Those methods include complete search,
best-ﬁrst search, genetic search and greedy search
algorithms for the master data. Other features called
shape factors are derived from the consumption time
series including the impact of lunch times, nights
and weekends on the consumption. These features
are used in K-means for clustering similar consump-
tion time series. In the classiﬁcation step, a decision
tree is used to predict whether a customer causes
NTLs or not. An overall test accuracy of 0.9997 is
reported.
Optimum path forests (OPF), a graph-based clas-
siﬁer, is used in Ref.
31.
It builds a graph in
the feature space and uses so-called “prototypes”
or training samples. Those become roots of their optimum-path tree node. Each graph node is classi-
ﬁed based on its most strongly connected prototype.
This approach is fundamentally different to most
other learning algorithms such as SVMs or neural
networks which learn hyperplanes. Optimum path
forests do not learn parameters, thus making training
faster, but predicting slower compared to parametric
methods. They are used in Ref. 30 for 736 cus-
tomers and achieved a test accuracy of 0.9021, out-
performing SMVs with Gaussian and linear kernels
and a neural network which achieved test accuracies
of 0.8893, 0.4540 and 0.5301, respectively. Related
results and differences between these classiﬁers are
also reported in Ref. 32.
A different method is to estimate NTLs by sub-
tracting an estimate of the technical losses from
the overall losses reported in Ref. 27. It models
the resistance of the infrastructure in a temperature-
dependent model using regression which approxi-
mates the technical losses. It applies the model to
a data set of 30 customers for which the consump-
tion was recorded for six days with meter readings
every 30 minutes for theft levels of 1, 2, 3, 4, 6, 8 and
10%. The respective test recalls in linear circuits are
0.2211, 0.7789, 0.9789, 1, 1, 1 and 1, respectively. 2.8.
Summary A summary and comparison of models, data sets and
performance measures of selected work discussed in
this review is reported in Table 1. The most com-
monly used models comprise Boolean and fuzzy ex-
pert systems, SVMs and neural networks. In addi-
tion, genetic methods, OPF and regression methods
are used. Data set sizes have a wide range from 30
up to 700K customers. However, the largest data set
of 1.1M customers in Ref. 38 is not included in the
table because only the variance is reduced and no
other performance measure is provided. Accuracy
and recall are the most popular performance mea-
sures in the literature, ranging from 0.45 to 0.99 and
from 0.29 to 1, respectively. Only very few publi-
cations report the precision of their models, ranging
from 0.51 to 0.85. The AUC is only reported in one
publication. The challenges of ﬁnding representa-
tive performance measures and how to compare in-
dividual contributions are discussed in Sections 3.1 P. Glauner et al. / Challenge NTL Detection Survey Table 1. Summary of models, data sets and performance mea-
sures (two-decimal precision). Ref.
Model
#Customers
Accuracy
Precision
Recall
AUC
NTL/theft proportion 1
SVM (Gauss)
< 400
0.86
-
0.77
-
- 7
SVM + fuzzy
100K
-
-
0.72
-
- 16
Bool rules
700K
-
-
-
0.47
5% 16
Fuzzy rules
700K
-
-
-
0.55
5% 16
SVM (linear)
700K
-
-
-
0.55
5% 16
Bool rules
700K
-
-
-
0.48
20% 16
Fuzzy rules
700K
-
-
-
0.55
20% 16
SVM (linear)
700K
-
-
-
0.55
20% 17
SVM
< 400
-
-
0.53
-
- 18
Genetic SVM
1,171
-
-
0.62
-
- 19
Neuro-fuzzy
20K
0.68
0.51
-
-
- 22
NN
22K
0.87
0.65
0.29
-
- 23
Rough sets
N/A
0.93
-
-
-
- 24
SOM
2K
0.93
0.85
0.98
-
- 25
SVM (Gauss)
1,350
0.98
-
-
-
- 27
Regression
30
-
-
0.22
-
1% 27
Regression
30
-
-
0.78
-
2% 27
Regression
30
-
-
0.98
-
3% 27
Regression
30
-
-
1
-
4-10% 29
SVM
5K
0.96
-
-
-
- 29
KNN
5K
0.96
-
-
-
- 29
NN
5K
0.94
-
-
-
- 30
OPF
736
0.90
-
-
-
- 30
SVM (Gauss)
736
0.89
-
-
-
-
30
SVM (linear)
736
0.45
-
-
-
- 30
NN
736
0.53
-
-
-
- 33
Decision tree
N/A
0.99
-
-
-
- P. Glauner et al. / Challenge NTL Detection Survey and 3.6, respectively. 3.
Challenges The research reviewed in the previous section indi-
cates multiple open challenges. These challenges do
not apply to single contributions, rather they spread
across different ones.
In this section, we discuss
these challenges, which must be addressed in order
to advance in NTL detection. Concretely, we discuss
common topics that have not yet received the neces-
sary attention in previous research and put them in
the context of AI research as a whole. 3.1.
Class imbalance and evaluation metric Imbalanced classes appear frequently in machine
learning, which also affects the choice of evalua-
tion metrics as discussed in Refs. 40 and 41. Most
NTL detection research do not address this property.
Therefore, in many cases, high accuracies or high re-
calls are reported, such as in Refs. 17,22,23,31 and
38. The following examples demonstrate why those
performance measures are not suitable for NTL de-
tection in imbalanced data sets: for a test set contain-
ing 1K customers of which 999 have regular use, (1)
a classiﬁer always predicting non-NTL has an accu-
racy of 99.9%, whereas in contrast, (2) a classiﬁer
always predicting NTL has a recall of 100%. While
the classiﬁer of the ﬁrst example has a very high ac-
curacy and intuitively seems to perform very well, it
will never predict any NTL. In contrast, the classiﬁer
of the second example will ﬁnd all NTL, but triggers
many costly and unnecessary physical inspections
by inspecting all customers. This topic is addressed
rarely in NTL literature, such as in Refs. 20 and 42,
and these contributions do not use a proper single
measure of performance of a classiﬁer when applied
to an imbalanced data set. 3.2.
Feature description Generally, hand-crafting features from raw data is a
long-standing issue in machine learning having sig-
niﬁcant impact on the performance of a classiﬁer,
as discussed in Ref. 43. Different feature descrip-
tion methods have been reviewed in the previous section. They fall into two main categories: fea-
tures computed from the consumption proﬁle of cus-
tomers, which are from monthly meter readings, for
example in Refs. 1, 7, 16–22 and 23, or smart me-
ter readings, for example in Refs. 6,24–32, and 33,
and features from the customer master data in Refs.
22, 23, 29–32 and 33. The features computed from
the time series are very different for monthly meter
readings and smart meter readings. The results of
those works are not easily interchangeable. While
electricity providers continuously upgrade their in-
frastructure to smart metering, there will be many
remaining traditional meters. In particular, this ap-
plies to emerging countries.
There are only few works on assessing the statis-
tical usefulness of features for NTL detection, such
as in Ref. 44. Almost all works on NTL detection
deﬁne features and subsequently report improved
models that were mostly found experimentally with-
out having a strong theoretical foundation. 3.3.
Data quality In the preliminary work of Ref.
16, we noticed
that the inspection result labels in the training set
are not always correct and that some fraudsters may
be labelled as non-fraudulent. The reasons for this
may include bribing, blackmailing or threatening of
the technician performing the inspection. Also, the
fraud may be done too well and is therefore not
observable by technicians. Another reason may be
incorrect processing of the data. It must be noted
that the latter reason may, however, also label non-
fraudulent behavior as fraudulent. Handling noise is
a common challenge in machine learning. In super-
vised machine learning settings, most existing meth-
ods address handling noise in the input data. There
are different regularization methods such as L1 or
L2 regularization discussed in Ref. 45 or learning
of invariances allowing learning algorithms to bet-
ter handle noise in the input data discussed in Refs.
46 and 47. However, handling noise in the training
labels is less commonly addressed in the machine
learning literature. Most NTL detection research use
supervised methods. This shortcoming of the train-
ing data and potential wrong labels in particular are
only rarely reported in the literature, such as in Ref. P. Glauner et al. / Challenge NTL Detection Survey 19, which uses an ensemble to pre-ﬁlter the training
data. 3.4.
Covariate shift Covariate shift refers to the problem of training data
(i.e. the set of inspection results) and production
data (i.e. the set of customers to generate inspections
for) having different distributions. This fact leads to
unreliable NTL predictors when learning from this
training data. Historically, covariate shift has been a
long-standing issue in statistics, as surveyed in Ref.
48. For example, The Literary Digest sent out 10M
questionnaires in order to predict the outcome of the
1936 US Presidential election. They received 2.4M
returns. Nonetheless, the predicted result proved to
be wrong. The reason for this was that they used
car registrations and phone directories to compile a
list of recipients. In that time, the households that
had a phone or a car represented a biased sample of
the overall population. In contrast, George Gallup
only interviewed 3K handpicked people, which were
an unbiased sample of the population. As a con-
sequence, Gallup could predict the outcome of the
election very well.
For about the last ﬁfteen years, the Big Data
paradigm followed in machine learning has been
to gather more data rather than improving models.
Hence, one may assume that having simply more
customer and inspection data would help to detect
NTL more accurately. However, in many cases, the
data may be biased as depicted in Fig. 1. Fig. 1. Example of spatial bias: The large city is close to the
sea, whereas the small city is located in the interior of the
country. The weather in the small city undergoes stronger
changes during the year. The subsequent change of electric-
ity consumption during the year triggers many inspections.
As a consequence, most inspections are carried out in the
small city. Therefore, the sample of customers inspected
does not represent the overall population of customers. One reason is, for example, that electricity sup-
pliers previously focused on certain neighborhoods
for inspections. Concretely, the customers inspected
are a sample of the overall population of customers.
In this example, there is a spatial bias. Hence, the in-
spections do not represent the overall population of
customers. As a consequence, when learning from
the inspection results, a bias is learned, making pre-
dictions less reliable. Aside from spatial covariate
shift, there may be other types of covariate shift in
the data, such as the meter type, connection type,
etc.
To the best of our knowledge, the issue of covari-
ate change has not been addressed in the literature on
NTL detection. However, in many cases it may lead
to unreliable NTL detection models. Therefore, we
consider it important to derive methods for quanti-
fying and reducing the covariate shift in data sets
relevant to NTL detection. This will allow to build
more reliable NTL detection models. 3.5.
Scalability The number of customers used throughout the re-
search reviewed signiﬁcantly varies. For example,
Refs. 17 and 27 only use less than a few hundred
customers in the training. A SVM with a Gaussian
kernel is used in Ref. 17. In that setting, training is
only feasible in a realistic amount of time for up to a
couple of tens of thousands of customers in current
implementations as discussed in Ref. 49. A regres-
sion model using the Moore-Penrose pseudoinverse
introduced in Ref. 50 is used in Ref. 27. This model
is also only able to scale to up to a couple of tens of
thousands of customers. Neural networks are trained
on up to a couple of tens of thousands of customers
in Refs. 20 and 22. The training methods used in
prior work usually do not scale to signiﬁcantly larger
customer data sets. Larger data sets using up to hun-
dreds of thousands or millions of customers are used
in Refs. 16 and 38 using a SVM with linear kernel or
genetic algorithms, respectively. An important prop-
erty of NTL detection methods is that their computa-
tional time must scale to large data sets of hundreds
of thousands or millions of customers. Most works
reported in the literature do not satisfy this require-
ment. P. Glauner et al. / Challenge NTL Detection Survey 3.6.
Comparison of different methods Comparing the different methods reviewed in this
paper is challenging because they are tested on dif-
ferent data sets, as summarized in Table 1. In many
cases, the description of the data lacks fundamental
properties such as the number of meter readings per
customer, NTL proportion, etc. In order to increase
the reliability of a comparison, joint efforts of dif-
ferent research groups are necessary. These efforts
need to address the benchmarking and comparability
of NTL detection systems based on a comprehensive
freely available data set. 4.
Suggested Methodology We have reviewed state-of-the-art research in ma-
chine learning and identiﬁed the following sug-
gested methodology for solving the main research
challenges in NTL detection: 4.1.
Handling class imbalance and evaluation
metric How can we handle the imbalance of classes and
assess the outcome of classiﬁcations using accurate
metrics?
Anomaly detection problems are particularly im-
balanced, meaning that there are much more train-
ing examples of the regular class compared to the
anomaly class. Most works on NTL detection do
not reﬂect the imbalance and simply report accura-
cies or recalls, for example in Refs. 17, 22, 23, 31
and 38. This is also depicted in Table 1. For NTL
detection, the goal is to reduce the false positive rate
(FPR) to decrease the number of costly inspections,
while increasing the true positive rate (TPR) to ﬁnd
as many NTL occurrences as possible. In Ref. 16,
we propose to use a receiver operating characteristic
(ROC) curve, which plots the TPR against the FPR.
The area under the curve (AUC) is a performance
measure between 0 and 1, where any binary classi-
ﬁer with an AUC > 0.5 performs better than random
guessing. In order to assess a NTL prediction model
using a single performance measure, the AUC was
picked as the most suitable one in Ref. 16. All works in the literature only use a ﬁxed NTL
proportion in the data set, for example in Refs.
17, 20, 22, 23, 31, 38 and 42.
We think that it is
necessary to investigate more into this topic in order
to report reliable and imbalance-independent results
that are valid for different levels of imbalance. This
will allow to build models that work in different re-
gions, such as in regions with a high NTL ratio as
well as in regions with a low occurrence of NTLs.
Therefore, we suggest to create samples of different
NTL proportions and assess the models on the entire
range of these samples. In the preliminary work of
Ref. 16, we also noticed that the precision usually
grows linearly with the NTL proportion in the data
set. It is therefore not suitable for low NTL propor-
tions. However, we did not notice this for the recall
and made observations of non-linearity similar to re-
lated work in Ref. 27, as depicted in Table 1. With
the limitations of precision and recall, the F1 score
did not prove to work as a reliable performance mea-
sure.
Furthermore, we suggest to derive multi-criteria
evaluation metrics for NTL detection and rank cus-
tomers that cause a NTL with a conﬁdence level, for
example models related to the ones in introduced in
Ref. 51. For example, the criteria we suggest to
include are the costs of inspections and possible in-
creases in revenue. 4.2.
Feature description and modeling temporal
behavior How can we describe features that accurately reﬂect
NTL occurrence and can we self-learn these features
from data? NTL of customers is a set of inherently
temporal events where for example a fraud of cus-
tomers excites themselves or other related customers
to commit fraud as well. How can we extend tempo-
ral processes to model the characteristics of NTL?
Most research on NTL uses primarily informa-
tion from the consumption time series. The con-
sumption is from traditional meters, such as in Refs.
1, 16, 17, 19 and 20, or smart meters, such as in
Refs. 6,25–27,30,32 and 33. Both meter types will
co-exist in the next decade and the results of those
works are not easily interchangeable. Therefore, we
suggest to shift to self-learning of features from the P. Glauner et al. / Challenge NTL Detection Survey consumption time series. This topic has not been ex-
plored in the literature on NTL detection yet. Deep
learning allows to self-learn hidden correlations and
increasingly more complex feature hierarchies from
the raw data input as discussed in Ref. 52. This
approach has lead to breakthroughs in image analy-
sis and speech recognition as presented in Ref. 53.
One possible method to overcome the challenge of
feature description for NTL detection is by ﬁnding a
way to apply deep learning to it. In a different vein, we believe that the neigh-
borhood of customers contains information about
whether a customer may cause a NTL or not. Our
hypothesis is conﬁrmed by initial work described in
Ref. 21, in which also the average consumption of
the residential neighborhood is used for classiﬁca-
tion of NTL. We have shown in Ref. 44 that features
derived from the inspection ratio and NTL ratio in a
neighborhood help to detect NTL. A temporal process, such as a Hawkes process
described in Ref. 54, models the occurrence of an
event that depends on previous events. Hawkes pro-
cesses include self-excitement, meaning that once an
event happens, that event is more likely to happen
in the near future again and decays over time. In
other words, the further back the event in the pro-
cess, the less impact it has on future events. The
dynamics of Hawkes processes look promising for
modeling NTL: Our ﬁrst hypothesis is that once cus-
tomers were found to steal electricity, ﬁnding them
or their neighbors to commit theft again is more
likely in the near future again and decays over time.
A Hawkes process allows to model this ﬁrst hypoth-
esis. Our second hypothesis is that once customers
were found to steal electricity, they are aware of in-
spections and subsequently are less likely to commit
further electricity theft. Therefore, ﬁnding them or
their neighbors to commit theft again is more likely
in the far future and increases over time as they be-
come less risk-aware. As a consequence, we need to
extend the Hawkes process by incorporating both,
self-excitement in order to model the ﬁrst hypothe-
sis, as well as self-regulation in order to model the
second hypothesis. Only few works have been re-
ported on modeling anomaly detection using self-
excitement and self-regulation, such as faulty elec- trical equipment in subway systems reported in Ref.
55.
The neighborhood is essential from our point of
view as neighbors are likely to share their knowl-
edge of electricity theft as well as the outcome of
inspections with their neighbors. We therefore want
to extend this model by optimizing the number of
temporal processes to be used.
In the most triv-
ial case, one temporal process could be used for
all customers combined. However, this would lead
to a model that underﬁts, meaning it would not be
able to distinguish among the different fraudulent
behaviors. In contrast, each customer could be mod-
eled by a dedicated temporal process. However, this
would not allow to catch the relevant dynamics, as
most fraudulent customers were only found to steal
once. Furthermore, the computational costs of this
approach would not be feasible. Therefore, we sug-
gest to cluster customers based on their location and
then to train one temporal process on the customers
of each cluster. Finally, for each cluster, the con-
ditional intensity of its temporal process at a given
time can then be used as a feature for the respec-
tive customers. In order to ﬁnd reasonable clusters,
we suggest to solve an optimization problem which
includes the number of clusters, i.e. the number of
temporal processes to train, as well as the sum of
prediction errors of all customers. 4.3.
Correction of spatial bias Previous inspections may have focused on certain
neighborhoods. How can we reduce the covariate
shift in our training set?
The customers inspected are a sample of the
overall population of customers. However, that sam-
ple may be biased, meaning it is not representa-
tive for the population of all customers. A reason
for this is that previous inspections were largely fo-
cused on certain neighborhoods and were not suf-
ﬁciently spread among the population. This issue
has not been addressed in the literature on NTL
yet.
All works on NTL detection, such as Refs.
1, 16, 17, 20, 22, 23, 31, 33, 38 and 42, implicitly as-
sume that the customers inspected are from the dis-
tribution of all customers. Overall, we think that the
topic of bias correction is currently not receiving the P. Glauner et al. / Challenge NTL Detection Survey necessary attention in the ﬁeld of machine learning
as a whole. For about the last ten years, the paradigm
followed has been labeled in Ref. 56: “It’s not who
has the best algorithm that wins. It’s who has the
most data.” However, we are conﬁdent to also show
that having more representative data will help rather
than just having a lot of more data for NTL detec-
tion.
Bias correction has initially been addressed in
the ﬁeld of computational learning theory, see Ref.
57, which also calls this problem covariate shift,
sampling bias or sample selection bias in Ref. 58.
For example, one promising approach is resampling
inspection data in order to be representative for the
overall population of customers. This can be done
by learning the hidden selection criteria of the deci-
sion whether to inspect a customer or not. Covariate
shift can be deﬁned in mathematical terms as intro-
duced in Ref. 58: • Assume that all examples are drawn from a distri-
bution D with domain X ×Y ×S, • where X is the feature space, • Y is the label space • and S is {0,1}. Examples (x,y,s) are drawn independently from
D.
s = 1 denotes that an example is selected,
whereas s = 0 does not. The training is performed
on a sample that comprises all examples that have
s = 1. If P(s|x,y) = P(s|x) holds true, we can imply
that s is independent of y given x. In this case, the
selected sample is biased but the bias only depends
on the feature vector x. This bias is called covariate
shift. An unbiased distribution can be computed as
follows: b
D(x,y,s) := P(s = 1) D(x,y,s) P(s = 1|x).
(2) Spatial point processes surveyed in Ref. 59 build
on top of Poisson processes. They allow to exam-
ine a data set of spatial locations and to conclude
whether the locations are randomly distributed in
a space or if they are skewed.
Eq.
(2) requires
P(s = 1|x) > 0 for all possible x. In order to compute
this non-zero probability for spatial locations x, we suggest to use and amend spatial point processes in
order to reduce the spatial covariate shift of inspec-
tion results. This will in turn allow to train more
reliable NTL predictors. 4.4.
Scalability to smart meter proﬁles of
millions of customers How can we efﬁciently implement the models in or-
der to scale to Big Data sets of smart meter readings?
Experiments reported in the literature range from
data sets that have up to a few hundred customers
in Refs. 1, 27 and 30 through data sets that have
thousands of customers in Refs. 24 and 29 to tens
of thousands of customers in Refs.
19 and 22.
The world-wide electricity grid infrastructure is cur-
rently undergoing a transformation to smart grids,
which include smart meter readings every 15 or 30
minutes. The models reported in the literature that
work on smart meter data use only very short periods
of up to a few days for NTL, such as in Refs. 24–26
and 27. Future models must scale to millions of cus-
tomers and billions of smart meter readings. The
focus of this objective is to perform the computa-
tions efﬁciently in a high performance environment.
For this, we suggest to redeﬁne the computations to
be computed on GPUs, as described in Ref. 60, or
using a map-reduce architecture introduced in Ref.
61. 4.5.
Creation of a publicly available real-world
data set How can we compare different models?
The works reported in the literature describe a
wide variety of different approaches for NTL detec-
tion. Most works only use one type of classiﬁer,
such as in Refs. 1, 22, 24 and 27, whereas some
works compare different classiﬁers on the same fea-
tures, such as in Refs. 29, 31 and 35. However, in
many cases, the actual choice of classiﬁcation algo-
rithm is less important. This can also be justiﬁed by
the “no free lunch theorem” introduced in Ref. 62,
which states that no learning algorithm is generally
better than others.
We are interested in not only comparing classiﬁ-
cation algorithms on the same features, but instead P. Glauner et al. / Challenge NTL Detection Survey in comparing totally different NTL detection mod-
els. We suggest to create a publicly available data set
for NTL detection. Generally, the more data, the bet-
ter for this data set. However, acquiring more data
is costly. Therefore, a tradeoff between the amount
of data and the data acquisition costs must be found.
The data set must be based on real-world customer
data, including meter readings and inspection re-
sults. This will allow to compare various models re-
ported in the literature. For these reasons, it should
reﬂect at least the following properties: • Different types of customers: the most common
types are residential and industrial customers.
Both have very different consumption proﬁles.
For example, the consumption of industrial cus-
tomers often peaks during the weekdays, whereas
residential customers consume most electricity on
the weekends. • Number of customers and inspections: the num-
ber of customers and inspections must be in the
hundreds of thousands in order to make sure that
the models assessed scale to Big Data sets. • Spread of customers across geographical area: the
customers of the data set must be spread in order
to reﬂect different levels of prosperity as well as
changes of the climate. Both factors affect elec-
tricity consumption and NTL occurrence. • Sufﬁciently long period of meter readings: due
to seasonality, the data set must contain at least
one year of data. More years are better to reﬂect
changes in the consumption proﬁle as well as to
become less prone to weather anomalies. 5.
Conclusion Non-technical losses (NTL) are the predominant
type of losses in electricity power grids. We have
reviewed their impact on economies and potential
losses of revenue and proﬁt for electricity providers.
In the literature, a vast variety of NTL detection
methods employing artiﬁcial intelligence methods
are reported. Expert systems and fuzzy systems are
traditional detection models. Over the past years,
machine learning methods have become more pop-
ular. The most commonly used methods are sup-
port vector machines and neural networks, which outperform expert systems in most settings. These
models are typically applied to features computed
from customer consumption proﬁles such as average
consumption, maximum consumption and change
of consumption in addition to customer master data
features such as type of customer and connection
type. Sizes of data sets used in the literature have a
large range from less than 100 to more than one mil-
lion. In this survey, we have also identiﬁed the six
main open challenges in NTL detection: handling
imbalanced classes in the training data and choos-
ing appropriate evaluation metrics, describing fea-
tures from the data, handling incorrect inspection
results, correcting the covariate shift in the inspec-
tion results, building models scalable to Big Data
sets and making results obtained through different
methods comparable. We believe that these need to
be accurately addressed in future research in order to
advance in NTL detection methods. This will allow
to share sound, assessable, understandable, replica-
ble and scalable results with the research commu-
nity. In our current research we have started to ad-
dress these challenges with the methodology sug-
gested and we are planning to continue this research.
We are conﬁdent that this comprehensive survey of
challenges will allow other research groups to not
only advance in NTL detection, but in anomaly de-
tection as a whole. Acknowledgments We would like to thank Angelo Migliosi from the
University of Luxembourg and Lautaro Dolberg,
Diogo Duarte and Yves Rangoni from CHOICE
Technologies Holding S`
arl for participating in our
fruitful discussions and for contributing many good
ideas. This work has been partially funded by the
Luxembourg National Research Fund.",0
"The method of optimizing entropy is used to (i) conduct Asymptotic Hypothesis Testing and (ii) determine the energy distribution for which Entropy is maximized. This paper focuses on two related applications of information theory: Statistics and Statistical Mechanics. Introduction Entropy is one measure of uncertainty within a system, and is often used to describe the disorder of sequences of quantized random variables. However, entropy can also be extended to methods within optimization, in which the disorder of a system of interest may be maximized or minimized.
Such methods are prevalent within statistics, the physical sciences, and econometrics. The concerns of a statistician observing a sequence of outcomes include the validity of an explanatory hypothesis, its degree of signiﬁcance, and any assumptions underlying the statistical tests. While linear hypothesis testing is often suﬃcient, larger sequences exhibit large deviations in behavior that should receive separate treatment. Traditional linear hypothesis testing trivially assigns a constant multiple of an explanatory parameter β to an observation when forming a hypothesis (H0 = kβ = 0). Optimal entropy, in which disorder is locally minimized or maximized, can be used to construct asymptotic, non-linear hypothesis tests. Unlike linear hypothesis testing, error probability can be minimized. Optimizing entropy extends to thermodynamic systems. The Third Law of Thermodynamics states that the entropy of a closed system, i.e.
one in which no mass or energy is added or removed, must be bounded from below by zero. Achieving a non-entropic system is nearly impossible, except within a perfect crystal lattice. A more probable state is one for which the entropy of a system is maximized, and observations follow a Boltzmann distribution. In our study of optimal entropy, we will use classical Statistics and Statistical Mechanics as a lens. We will demonstrate the concept of minimal entropy through two statistical tests: the univariate optimality test deﬁned by Stein’s Lemma, and a multivariate optimality test, as deﬁned by the ChernoﬀBounds. We will see that there exists a distribution, the Boltzmann distribution, that approaches maximum entropy as temperature goes to inﬁnity. Finally, we will apply the concept of asymptotic hypothesis testing to Statistical Mechanics. In particular, we will test and observe the evolution of error probability with a growing sample size. We will also compare Q-function error probability with that of the Chernoﬀ ∗Prof. Peter Shor provided generous amounts of feedback
Department of Mathematics
Massachusetts Institute of Technology
18.424: Seminar in Information Theory 1 arXiv:1603.02589v1  [math.ST]  5 Mar 2016 2 bound. The remainder of the paper is organized as follows. To better understand the atypicality of sequences, we will study the method of types. We will then learn how such sequences behave through the large deviation theory. The focus of the paper will then shift to hypothesis testing, in which we will develop tools for recognizing the asymptotic optimality of entropy. Illustrative examples of this will include Stein’s Lemma and Chernoﬀbounds. For the interest of the physical sciences, we will rigorously derive the Boltzmann distribution, for which entropy is nearly maximized. Finally, we will converge the aforementioned topics through simulations of asymptotic statistical testing. Hypothesis Testing Statisticians are often concerned with not just observed data, but the several possible underlying expla- nations. A few examples include: testing for the eﬀectiveness of a drug, determining whether or not a coin is biased, and the eﬀect of gender on wage growth. We begin with a simple case in which we decide between two hypothesis, each of which is represented by an independent and identical distribution, or i.i.d. Let X1, X2, . . . , Xn be i.i.d. ∼Q(x). For an observed outcome, we have two possible explanations: • H1 : Q = P1 • H2 : Q = P2 We now deﬁne a general decision function, whose value reﬂects the acceptance and rejection of the above hypothesis. Namely, for general decision g(x1, x2, . . . , xn), g(x) = i indicates that Hi is accepted. In the binary case, the set A over which g(x) = i is complemented by the set Ac, over which g(x) ̸= i. Quite often, statisticians are concerned with accepting incorrect hypotheses and rejecting correct ones. Such occurrences, recognized as Type I/II errors, often occur when sequences exhibit atypicality and large deviating behavior (See appendix). Error probabilities are reﬂected through the decision function using weights α, β: α = P(g(x) = 2|H1 true) = P n
1 (Ac) β = P(g(x) = 1|H2 true) = P n
1 (A)
(1) Notice that the general decision function takes on values contradicting those implied by the conditional hypothesis. The ﬁrst implies that H2 was accepted even though H1 was true, and the second implies that H1 was accepted even though H2 was true. Type I (reject true) and Type II (accept false) errors similarly prove detrimental to experiments, and so we wish to minimize probabilities α and β. Minimizing α increases β, and minimizing β increases α. We will now explore methodology to minimize the overall probability of error by optimizing entropy as a weighted sum of α and β. Stein’s Lemma We ﬁrst ﬁx either α or β, and manipulate the other to minimize the probability of error. Theorem 1 (Stein’s Lemma). Let X1, X2, . . . , Xn be i.i.d. ∼Q. Further, let D(P1∥P2) represent the Kullback-Leibler distance, or relative entropy between the probability densities. Consider the hypothesis test between two alternatives Q = P1 and Q = P2 where D(P1∥P2) < ∞. Let An ⊆Hn be an acceptance region for hypothesis 1. Let the probabilities of error be αn = P n
1 (Ac
n) βn = P n
2 (An)
(2) 3 and for 0 < ϵ < 1 2, deﬁne βϵ
n = minAn⊆Xnβn
(3) Then, limϵ→0 limn→∞
1
nlogβϵ
n = −D (P1∥P2)
(4) Proof. See Appendix Thus, no sequence of sets Bn has an exponent better than D (P1∥P2). But the sequence An achieves the exponent D (P1∥P2). Thus An is asymptotically optimal, and the best error exponent is D (P1∥P2). ChernoﬀBound Thus far, α and β have been treated separately. The approach underlying Stein’s Lemma was to set one error probability to be inﬁnitesimally small, and measure the eﬀect on the resulting probability. We saw that setting α ≤ϵ achieved βn = 2−nD. However, the distribution of error amongst α and β
may be highly asymmetrical, in which case univariate optimization may not suﬃce. We now explore methodology for a bivariate optimization. An alternative approach is to minimize the weighted sum of α and β. The resulting error exponent is known as the ChernoﬀInformation. Consider a distribution of i.i.d. random variables: X1, X2, . . . , Xn
representative of the decision function.
We assign P1 to Q with probability π1 and P2 to Q with probability π2. Upholding the deﬁnition of α and β, the overall probability of error is P n
ϵ = π1αn + π2βn
(5) Theorem 2 (Chernoﬀ). The best achievable exponent in the Bayesian probability of error is D∗, where D∗= textlimn→∞minAn⊆Hn −1 nlogP n
ϵ = D (Pλ∗∥P1) = D (Pλ∗∥P2)
(6) where Pλ =
P λ
1 (x)P 1−λ
2
(x)
P a∈X P λ
1 (a)P 1−λ
2
(a)
(7) and λ∗the value of λ such that D (Pλ∗∥P1) = D (Pλ∗∥P2) .
(8) Proof. See Appendix Physical Chemistry Claude Shannon ﬁrst proposed that the uncertainty due to possible errors in a message could be encap- sulated by U(W) = logW
(9) where W is the number of possible ways (state space) of encoding random information. Intuitively, the uncertainty increases with increasing W, and is zero if W=1. The concept of entropy provides a deep- rooted link between information theory and statistical mechanics. The state with the least information available, or greatest entropy occurs when the set of all states are equiprobable. This is also the state with maximum uncertainty. An information theoretic perspective dictates that explicit knowledge of various probabilities associated with the system constitutes greater information. Similarly, the thermodynamics 4 of a system of isolated particles indicate that entropy is directly correlated with expected energy level. Below is a molecular orbital diagram that illustrates the possible energy states, all of which depend on the position an electron occupies. Figure 1: The ﬁgure above is a molecular orbital diagram. When two atoms interact, and possibly share
their valence (outermost) electrons, the electrons must occupy a particular orbital. Once occupying an
orbital, the atoms are able to form a bond. Here, we are not concerned with the type of the bond,
but rather, paired occupancy. The empty orbitals are indistinguishable, and will be occupied with equal
probability. Entropy may also be observed in macroscopic states. The second law of thermodynamics states that in equilibrium, changes in entropy are proportional to changes in system heat per unit temperature.
We have dW = dQsys T
. We can better understand this through an illustration. Consider a system of gas particles that may be expanded or compressed. We can study the system under various entropic regimes. The diagram below illustrates how available work decreases (increases) for gaseous expansion (compression) under bivariate states of pressure and volume. Consider a perfectly structured crystal lattice structure, in which the positions of each contributing molecule is ﬁxed. If we observe such a system, depart, and return after n periods, the position of each molecule within the crystal will have remained the same almost surely. If the particles did not displace, then they also carried zero kinetic energy, which is representative of a zero temperature system. This near certainty of a thermodynamic system is an example of an optimization in which entropy is minimized. If
instead, the system consisted of a fair coin toss, with no extra information, entropy would be maximized. The Boltzmann Distribution The distribution that maximizes the state space for a ﬁxed energy level is the Boltzmann distribution. We will now derive such a distribution, and show that it uniquely maximizes entropy on each energy level. Consider a crystal containing Nparticles, each of which has available energy levels, ϵn. The state space, W, is the number of ways the total energy E = P n Nnϵn can be distributed amongst the the particles in each energy level, across all energy levels, N = P n Nn. The expected number of particles in each energy level, Nn, is the product of the probability that a particle is at an energy level, Pn, and the total number of particles, N. The only consideration for such a distribution is the number of 5 Figure 2: The ﬁgure above is a pressure-volume diagram for a system of Argon gas particles. Expanding
or compressing a gas requires energy, the extent of which depends on the state of the system. Notice that
for an adiabatic system (dQsys = 0), compressing the gas requires the least relative energy. That is, when
when the change in entropy in minimized, a system can be most naturally expanded/compressed. To
reach minimum work available, we move down the gradient of steepest descent until entropy is globally
minimized. particles in each energy levels, not necessarily the amount of energy allocated to each particle. Energy is conserved amongst states and across energy levels. While there exist several ways of assigning the number of particles in each energy level ϵn, we wish to ﬁnd the state with the distribution achievable in the most number of ways for ﬁxed energy levels. Our ﬁrst constraint is that the total state space W is the sum of individual states occupied, Wi for all possible distributions W =
X
Wi
(10) Amongst all possible distributions of particles, there exists one that can be achieved in more ways than any other. A distribution that approaches maximum entropy for ﬁxed energy levels is the Boltzmann distribution. logW ∼
= logWB
(11) To ﬁnd the most probable distribution that maximizes W, we ﬁrst note that each particle in the crystal can be distinguished from the others because it occupies a deﬁned position in space. Therefore, such a setting allows us to number the particles 1, 2, . . . , N. We assume a large N, to maintain consistency with
typical non-deﬁcient states. S A particular microstate of the crystal will place particle 1 in energy level ϵi, particle 2 in energy level ϵj, and so on. Initially, we seek the number of Wi microstates in a distribution for which there are N1 particles in ϵ1, N2 particles in ϵ2,, and so on. We choose, at random, particles from the crystal, Ni, and assign them to energy levels, ϵi. The number of ways this can be done is equal to the number of diﬀerent orders in which the particles can be chosen from the crystal. The ﬁrst particle can be chosen from a group of N. With N −1 particles remaining, the second can be chosen in N −1 ways. We see that the number of ways for selecting the ﬁrst two particles is N(N −1). Following this procedure, we then we see that the number of ways for selecting the N particles is N(N −1)(N −2)(N −3) . . . (3)(2)(1), or N!. We have over counted the ways of achieving a given distribution, and have assumed that all states are distinguishable. Consider the placement of the ﬁrst two particles into energy level ϵ1. It makes no 6 diﬀerence whether the ﬁrst particle is placed into ϵ1 prior to or following particle 2. That is, the states are indistinguishable. This relaxes the strictness on order, and so permutation are ignored. Thus, the state space, Wi, for a given distribution, is N! divided by the product of all N! Wi =
N!
Q n Nn!
(12) To ﬁnd the distribution that maximizes Wi, we note Stirling’s approximation for log N! log N! ≈N log N −N
(13) Finding the maximum of Wi is equivalent to ﬁnding the maximum of log Wi, so we combine equations (13) and (14), and re-arrange as follows log Wi ≈log N! −
X n
logNn! = N log N −
X n
Nn logNn = X n
Nn ! log
X n
Nn −
X n
Nn logNn (14) However, the set of particles is conserved. Moreover, the net energy within the system is conserved. This provides the following two constraints X j
Nj = N X j
ϵjNj = E
(15) We now use Lagrange’s method of undetermined multipliers. When Wi is maximized, its diﬀerential log must be zero d log Wi =
X j ∂logWi ∂Nj
j
dNj = 0
(16) We multiply the constraints on particle count and energy by constants α, β, and then take the diﬀerential to obtain α
X j
dNj = 0 β
X j
ϵjdNj = 0
(17) Subtracting these two constraints from the log-entropy, we obtain X j ∂logWi ∂Nj
−α −βϵj 
dNj = 0
(18) Through use of log-properties and algebraic manipulation (See Appendix), the expression above is re- duced to logNj = logN −α −βϵj
(19) which, after exponentiating both sides is Nj = Ne−αe−βϵj
(20) The signiﬁcance of this result is that it shows the occupancy of en energy state ϵj is proportional to e−βϵj. 7 Key Result To account for energy states, thermodynamicists often make use of temperature, an intrinsic quantity. Temperature is equivalent to the average kinetic energy of a system of particles. Because this varies across systems, we normalize. For a temperature T, and the Boltzmann constant, kB, β =
1 kBT .
(21) Inducting on the one particle case, in which, eα = P j e −ϵj kBT , and combining the preceding three expres- sions, providing the desired result Pn = Nn N =
ez P N ez where z = −ϵj kBT (22) We have now found the Boltzmann distribution, for which entropy is maximized. The Boltzmann prob- ability above expresses the fraction of particles placed in each quantum state n to maximize entropy Wi
of the distribution over each energy level, ϵj. Simulation: Asymptotic Hypothesis Testing Thus far, we have studied various methods of statistical testing, highlighting the importance of asymptotic tests such as the ChernoﬀInformation Bound. We have also (brieﬂy) explored Statistical Mechanics, in which we show that entropy is maximized for a Boltzmann distribution. We now demonstrate the importance of our learnings through a representative example. Robust methods of signal interpretation allow for communication, and involve the separation of signal and noise.
A simple signal will follow a Gaussian distribution, for which entropy is maximized.
A hypothesis consists of assigning observations as either signals, or as noise. Such hypotheses carry error probabilities, and should be studied with both linear testing, as well as asymptotic testing. Example: Binary Detection The classical binary detection problem involves the reception of ﬁnite-length signals realized as a random process r[n], n = 1, 2, . . . , [3]. The signal can be attributed to either Gaussian white noise n[i] or a deterministic signal s[i]. Basic studies involve the interpretation of the signal-to-noise ratio, a measure of quality. Consider a binary detection problem: r = si + n, , i ∈{1, 2}
(23) • Detections are composed of signals and noise • n: N-dimensional noise vector • i.i.d. Gaussian random variables and ∼(0, 1) • s1 = (m, m, . . . , m) • s2 = (0, 0, . . . 0) 8 Evaluate the error probability for both N = 1 and N = 4 when m ∈{1, 2, 3, 4, 5, 6}. Traditionally, error probability is evaluated through the Q-function, which represents the probability that a normal random variable will obtain a value larger than x standard deviations above the mean. It can also be thought of as the ""tail"" probability of the standard normal distribution, and is useful for linear hypothesis testing. Q(x) =
1
√ 2π Z ∞ x
exp
−t2 2 
dt = 1 2erf
 x
√ 2 
(24) Given a signal-to-noise Ratio, the Q-function can be used to determine the error probability P e = Q
 ρ σ 
where
ρ2 σ2 = Nm2 4
(25) We also know that any error probability is bounded from above by the ChernoﬀInformation bound. For the Q-function, Q(x) ≤exp

−x2 2 
erf
(26) And so, P e = Q √ Nm 2 ! ≤exp

−m2N 8 
(27) The ﬁgure below illustrates the growth of error in both forms of testing. Figure 3: The ﬁgure above shows the evolution of error probability with an increasing signal length (m).
Error probability decreases as: (i) the number of bits in the signal increases, and (ii) the number of
elements in the noise vector increases. Concluding Remarks Optimizing entropy demonstrates the applicability of information theory beyond computing. Asymp- totic testing captures error probability in atypical sequences, and a Boltzmann distribution of particles approaches maximum entropy, as temperature goes to inﬁnity. 9 Appendix Motivation for asymptotic testing arises from atypicality and large deviations in sequences. We brieﬂy review this, and encourage the ambitious reader to study further. The Method of Types The Asymptotic Equipartition Property formalizes that although there exist several possible outcomes of a stochastic process, there exists a set from which sequences are typical , or most frequently observed. The centric approach underlying the AEP involves deﬁning an almost sure convergence in probability between the expectation of a sequence to its entropy. Similarly, the Method of Types deﬁnes strong bounds on the number of sequences of a particular distribution, as well as the probability of each such sequence being observed. Large Deviation Theory Recall that type of a sequence xn
i ∈An is representative of its empirical distribution ˆ
P = ˆ
Pxn
i where: ˆ
P(a) = |{i : xi = a}| n
, a ∈A.
(28) A distribution P on A is called an n-type if it is the type of some xn
1 ∈An. The set of all xn
1 ∈An of type P is called the type class of the n-type P and is denoted by T n
p . Lemma 3. The number of possible n-types is n + |A|−1
|A|−1 
(29) Proof. T (P) = {x ∈X n : Px = P}
(30) The combinatoric cardinality of T (P) provides the result Lemma 4. For any n-type P, n + |A|−1
|A|−1 −1
2nH(P ) ≤|T n
p |≤2nH(P )
(31) Proof. First, we prove the upper bound using P(T (P) ≤1. 1 ≥P n(T (P)) =
X x inT (P )
P n(x) =
X x inT (P )
2−nH(P ) = ∥T(P)∥2−nH(P )
(32) Consequently, ∥T(P)∥≤2nH(P ). For the lower bound, using the fact that T(P) has the highest proba- bility amongst all type classes in P, we can bound the ratio of probabilities P n(T(P)) P n(T( ˆ
P))
=
∥T(P)∥Q a∈XP (a)nP (a) ∥T( ˆ
P)∥Q a∈XP (a)n ˆ
P (a)
=
Y a∈X (n ˆ
P(a))! (nP(a))!P(a)n(P (a)−ˆ
P (a))
(33) 10 Using the identity m! n! ≥nm−n, we see P n(T (P)) P n(T ( ˆ
P))
≥
Y a∈X
nn(P (a)−ˆ
P (a)) = nn(1−1) = 1
(34) So P n(T(P)) ≥P n(T( ˆ
P)). The lower bound can now be found as 1 =
X Q∈Pn
P n(T(Q)) ≤
X Q∈Pn = maxQP n(T(Q)) =
X Q∈Pn
P n(T(P)) ≤(n + 1)∥X∥P n(T(P)) = (n + 1)∥X∥
X x∈T (P )
P n(x) = (n + 1)∥X∥
X x∈T (P )
2−nH(P ) = (n + 1)∥X∥∥T(P)∥2−nH(P ) (35) To connect the theory of types with general probability theory, we must develop a sense of relative entropy. For any distribution P on A, let P n denote the distribution of n independent drawings from P, that is, P n (xn
1) = Qn
i=1 P (xi) , xn
1 ∈An. Lemma 5. For any distribution P on A and any n-type Q P n (xn
1) Qn (xn
1) = 2−nD(Q∥P ), ifxn
1 ∈T n
Q
n + |A|−1
|A|−1 −1
2−nD(Q∥P ) ≤P
",0
"The coronavirus disease (COVID-19) has caused one of the most serious social and economic losses to countries around the world since the Spanish influenza pandemic of 1918 (during World War I). It has resulted in enormous economic as well as social costs, such as increased deaths from the spread of infection in a region. This is because public regulations imposed by national and local governments to deter the spread of infection inevitably involves a deliberate suppression of the level of economic activity. Given this trade-off between economic activity and epidemic prevention, governments should execute public interventions to minimize social and economic losses from the pandemic. A major problem regarding the resultant economic losses is that it unequally impacts certain strata of the society. This raises an important question on how such economic losses should be shared equally across the society. At the same time, there is some antipathy towards economic compensation by means of public debt, which is likely to increase economic burden in the future. However, as Paul Samuelson once argued, much of the burden, whether due to public debt or otherwise, can only be borne by the present generation, and not by future generations. Keywords:  COVID-19, pandemic, health capital, lockdown, future burden, Lerner-Samuelson proposition JEL Classification Numbers: H12, H30, H51, H63, H75, I18 1 
Introduction The coronavirus disease (COVID-19), which has spread to many countries around the world since the beginning of 2020, has inevitably caused great economic loss at a global scale, unlike any pandemic that has occurred since World War II. It is not simply due to the infectious nature of the disease. The weight of social losses created by the deaths of countless people due to the spread of the infection is enormous in modern society, with the exception of some dictatorships 1 Senshu University, School of Economics, Tokyo, Japan 2 that still exist. On the basis of such modern value judgments, many countries are willing to implement measures such as lockdowns which clearly involve heavy economic losses—in order to save as many lives as possible. The Spanish influenza pandemic, which began in 1918 during World War I, resulted in cumulative deaths in the tens of millions throughout the world. In spite of considerable variation, even the smallest of the estimates was large enough to cause a temporary decline in the global population, which had previously been growing by around 13 million people per year (Roser [2020]). Even at that time, in some countries and regions, measures similar to the current lockdowns had been implemented. However, the immensely high death toll suggests that such conscious measures to prevent the spread of the disease may have been extremely rare. This might indicate not only the low level of medical knowledge, but also the low value attached to human life during that time. In the COVID-19 pandemic, cities across numerous countries faced a rapid spread of infection, and almost without exception, severe restrictions on economic activity were implemented. This was because the only way to prevent further spread of infection was to reduce human contact as much as possible, which implied that economic activity should be kept to a minimum. These stark trade-offs between economic activity and epidemic prevention denote the general characteristics of policies for pandemic deterrence. Another characteristic of modern policies for pandemic deterrence is that they often involve some form of economic compensation by governments and local authorities. This kind of public economic compensation basically has two policy objectives. One of these is the provision of financial incentives for leave of absence. If it were not for such financial incentives, people without sufficient savings would have no choice but to leave home and go to work to earn an income. The other is income compensation (similar to insurance) against the decline in corporate and household incomes that would inevitably result from policies to deter the spread of infection. This enables the economic costs of pandemic deterrence to be re-distributed more equitably across society. Modern pandemic deterrence policies require huge public expenditures including public economic compensation. The relative size of this public spending could swell to levels not seen since the two world wars of the 20th century, in some cases as high as 30-40% of the GDP. In the case of COVID-19, most of the public spending associated with pandemic deterrence policies will probably also be financed by public debt. This is because governments do not want their economies, which have already contracted significantly due to pandemic deterrence policies, to atrophy further by raising taxes on people. Nonetheless, concerns about ballooning public debt are sure to grow. There is a clear tendency among some national policy authorities to keep public economic compensation to a minimum, under the pretext of preventing the expansion of the burden in the future. There are 3 many people who outrightly oppose public economic compensation because of the concern that its further expansion would directly increase the burden on themselves. However, there is already a long history of debate and controversy in the economic science on the relationship between public debt and future burdens. While it was Abba Lerner who sparked the idea (Lerner [1948]), the more general insights were gleaned from his approach and later organized in a textbook by Paul Samuelson (Samuelson [1967]). Their argument conclusively demonstrates the economic truth that much of the economic burden, irrespective of whether the spending is financed by higher taxes or public debt, is ultimately borne by the current generation and not by the future generations. This conclusion is essentially in harmony with respect to the public economic compensation that accompanies pandemic deterrence. In other words, even if the entire economic compensation is financed by public debt, it does not directly lead to a future burden. Economic compensation in the form of public debt is not intended to pass the burden on to future generations, but only to redistribute the burden among the current generation. This paper proceeds as follows. Section 2 uses the concept of health capital to articulate the trade-offs between economic activity and epidemic prevention that societies should be aware of when trying to contain a pandemic. It also identifies the optimal public intervention path (in terms of minimizing the combined loss from pandemics as a weighted sum of social and economic losses) for national and local governments. Section 3 derives the economic losses caused by a pandemic response based on the trade-off relationship between economic activity and epidemic prevention, and subsequently identifies the need to equalize the unequal share of these economic losses across various strata of the society. Section 4 argues, mainly relying on Samuelson’s argument, that the economic burden of pandemic preparedness is essentially borne by the current generation, and that even if the government’s economic compensation was based on public debt, it would not be possible to transfer that burden to the future. Section 5 concludes that the various considerations made in relation to pandemic preparedness are both necessary and unnecessary, referring to the problems of balancing economic activity with epidemic prevention and sharing economic costs and the problems of increasing future burdens due to fiscal deterioration, respectively. 2 
Economic analysis of counter-pandemic measures 2-1 
Trade-off between economic activity and quarantine As noted in Section 1, one distinct feature of pandemic preparedness is that it almost always has a trade-off relationship with economic activity. In peacetime when any threat of epidemic or pandemic is undetected, the policy idea of consciously suppressing economic activity to prevent the spread of infection does not exist. It is conceivable that there is some degree of trade- 4 off between economic activity and disease prevention even in peacetime, although the society is not aware of it. However, once the epidemic or the pandemic becomes a reality, the situation changes drastically. The society is forced to realize that economic activity must be restricted more severely to strongly deter the spread of infection. In the early stages of infection control, wherein a growing number of infections are being gradually reported overseas, but have not been confirmed inland, governments tend to adopt “bolder measures.” These are aimed at strengthening the monitoring the influx of people from abroad, especially from countries and regions where the infection has been confirmed. Such measures would have a decidedly negative effect on domestic economic activity, as it would imply restricting foreign tourists and other people entering the country for business purposes. Nevertheless, the effect of bolder measures on domestic economic activity is likely to be very limited as long as they are successful. This is because there should be no need to strongly regulate economic activity in the country as long as the infection has not yet spread in the country. However, as soon as it is confirmed that the infection has crossed the border into the country, there is a radical progression in phase. Then, the governments are forced to implement some kind of regulation. Typically, the governments provided instructions to the general public to refrain from activities, such as public gatherings and events; shut down various educational institutions, stores, and restaurants, where human contact is unavoidable; and transition to staggered attendance in companies. Some countries and regions that were severely affected by the pandemic went beyond these directives or solely moral imperatives to implement certain legally enforceable public regulations, in the form of a lockdown, to deter the spread of the disease. Restrictions on mobility were imposed, except for the minimum activities necessary for the maintenance of life. Inevitably, all economic activities, other than the production and sale of goods and services that were considered to be basic necessities, were in principle suspended. In modern society, where the value of human life is highly respected, it is considered unavoidable to suffer a possible economic loss in order to save as many lives as possible, once faced with increasing human loss due to the spread of infection. The relationship between economic activity and quarantine is illustrated in Figure 1. The vertical axis of this figure shows “income” or “production of goods and services” and the horizontal axis shows “health capital.” In general, the income or output of goods and services at any given time is representative of the level of economic activity. In order to deter the spread of infection, it is necessary to ensure certain social environments, such as group avoidance through the promotion of absence from work (or remote work) and ensuring sufficient social distance. 5 Health capital comprises precisely these social environments.2 There is clearly a trade-off between the two, since one must abandon some economic activity in order to secure them. Three points are shown on this curve; peacetime, moral imperative, and lockdown. The trade-off between income and health capital does not arise significantly in the vicinity of the peacetime point. This implies that individuals can secure a certain amount of health capital without any significant income loss. An ironic consequence of the COVID-19 infection control policies is that the imposition of lockdowns in large cities in developed countries (where economic activity had previously been very high) has resulted in significant improvements in the living conditions in the form of reduced air pollution. Some experts point out that these environmental improvements may have some effect of improving people’s health. Conversely, this means that the vigorous economic activity that had been the norm prior to this pandemic had been worsening people’s health through air pollution and other forms of environmental degradation, and the consequent social costs had been universally accepted as the price of economic benefit. Such trade-offs between economic activity and health capital, although always existent, are rarely noticed by people in normal times. Most people do not feel any emergent need to 2 Grossman [1972] was the first to use the concept of health capital in economic analysis, in which health capital is grasped a kind of human capital embodied in the individual. Subsequently, however, there has been an emphasis on its public good nature in conjunction with the rise of the concept of “sustainability” in environmental and development economics, which is exemplified by Arrow, Dasgupta, and Mumford [2014]. Health capital Income (production of goods and services) Figure 1  Trade-off between economic activity and quarantine peacetime point
peacetime income moral imperative  point lockdown  point moral imperative  
period income lockdown period 
income 6 secure health capital at the expense of apparent economic loss. However, securing health capital becomes a top priority for the society once a pandemic strikes. This is because the spread of infection in a pandemic would itself cause social losses, such as a rapid increase in the number of deaths due to the pandemic. Governments must then implement policies to curb all external activities of people, including economic activities, by means of public requests or legal regulations, in order to curb the spread of the infection. In other words, the trade-offs between economic activity and health capital become more acute with the external environment, as the pandemic leads to a rapid expansion of socially needed health capital. In some cases, societies are forced to restrict all external activities other than the essential economic activities in order to ensure as much health capital as possible. This corresponds to the lockdown point in Figure 1. It means that society has made a public choice to indulge in the minimum income necessary to ensure the maximum health capital. In contrast, the moral imperative point, is a situation in which the governments restrict business operations and individual mobility, not through legal coercion, but solely through some kind of social imperative. The declaration of a state of emergency issued by the Japanese government to major cities on April 7, 2020 (that was extended nationwide on April 16) is essentially such a measure. There is no legal regulation of people’s behavior, such that health capital secured at that point is lesser than that at the lockdown point. There are also lesser constraints on economic activity and therefore, fewer economic losses. The Japanese government’s decision to avoid the lockdown was likely due to its concerns about the resultant economic losses, as well as its constitutional restrictions. Similarly, Sweden decided to not implement lockdowns or mobility restrictions, but instead to limit to lenient regulations, such as requests for population bans, in order to acquire population immunity. In the trade-off between maintaining economic activity and securing health capital for the protection of human life, there is always an implicit value judgment about how each society weights both the economic costs of deterring the spread of infection and the social costs of the spread. While the value of human life is extremely high in modern society, the economic cost of protecting that value cannot be neglected at all. That is the reason why even the most humanitarian nation will not ban cars, in spite of the fact that car accidents cause deaths around the world. Some progressive intellectuals often argue that once the risk to human life becomes apparent, it should be addressed at any economic cost. Therefore, as regards COVID-19 disease control, all governments that do not impose lockdowns would be subject to criticism for their “disregard for human life.” However, if even a small island nation with no confirmed cases of infection was forced to impose lockdown (unless risk of infection is absolutely zero), the islanders would be indignant at such interfering measure that is sure to disturb their daily living. 7 2-2 
Social loss function of the pandemic While the value judgement over the aforementioned trade-off is not necessarily a determinant, it always exists behind the policy decisions made by governments and local authorities. It is the very risk of infection spread, in an epidemiological sense, that determines how strongly governments should regulate the economy. Strong public regulation is needed in the first place because the spread of the infection would be left unaddressed and the death toll would increase without it. Even if a government does not take any action against a pandemic because it is wary of economic loss, it is possible that the infection eventually subsides, and the number of deaths decreases as the population gains immunity. However, the cumulative death toll would be enormous, as it once was with the Spanish influenza. In contrast, if the government responds in some manner to a pandemic, it can certainly reduce the cumulative death toll, albeit with economic losses. However, the effect of the public intervention, i.e., the reduction in cumulative deaths, depends not only on the intensity of the intervention, such as whether it is requested or enforced, but also on the phase of the infection. In other words, the nature and intensity of public interventions needed to deter the spread of infection clearly depends largely on the phase in which they are undertaken. This is evident by recalling the countries and regions, such as Taiwan and Hong Kong, that have had notable successes in their policies to prevent spread of COVID-19. Taiwan and Hong Kong have avoided serious outbreaks of COVID-19, such as those in the West, despite their deep ties to China, the original source of the outbreak. This is because their policymakers immediately grasped the seriousness of the situation from the information coming in from China and took measures, such as limiting entry at the border, from an early stage. As a result, Taiwan and Hong Kong, in contrast to Western countries, have avoided the implementation of measures, such as lockdowns, that result in massive economic losses. Since the infection had not spread to Taiwan and Hong Kong from the beginning, even if the lockdown had been implemented at this time, there would have been significant economic losses and little additional benefit from it. Perhaps, the same can be said of the diminishing phase of the infection, after the peak of its spread has passed. The governments that have implemented lockdowns will endure the economic losses from restricting economic activity while they are doing it. However, if the additional social losses that would result from relaxing the regulations (i.e., the additional increase in cumulative deaths) were small enough, the governments would prefer to immediately relax the regulations and normalize economic activity. When this situation will occur is determined by health system measures, such as the establishment of effective countermeasures to prevent the spread of infection, the emergence of effective new drugs, and the acquisition of population immunity. If, as a result of such an improved epidemiological situation, the strength or weakness of public regulation to deter the spread of infection becomes 8 almost indiscriminate against social losses, it would mean that the economy was once again in a situation where it should return to the peacetime point in Figure 1. Therefore, the social losses caused by the spread of an epidemic can be expressed as follows: SL = MSL ‒SG (P, I ).  
 
(1) This equation implies that the social loss caused by the pandemic SL is the maximum social loss MSL minus the social gain of the public intervention SG that is a function of two variables: the phase of the infection at the time of the intervention P and the intensity of the public intervention I. Hereafter, we will refer to this equation (1) as the social loss function of an epidemic or pandemic. In general, social loss from the spread of an epidemic refers to all disadvantages other than the economic loss suffered due to the infection, the foremost of which is the loss of human life itself. If the social losses from a pandemic are represented by the cumulative number of deaths, then the value can be derived from the epidemic curve of the infection. This is because the cumulative number of deaths from a pandemic is merely the integral value of the newly infected, as illustrated in the epidemic curve, multiplied by the mortality rate. Suppose that the epidemic curve of an infectious disease is mountain-shaped, as in a normal distribution. Moreover, suppose that there are four phases (P=1 to P=4) ranging from the beginning of the spread of the infection to its convergence. The intensity of public interventions to deter the spread of infection should also be represented by I=0 (no intervention), I=1 (weak intervention), and I=2 (strong intervention). Figure 2 shows the epidemic curve in the absence of any public intervention in all phases, and the phase categories from 1 to 4. The integral of this epidemic curve, i.e., the area of the mountain-shaped curve, represents the Number of new infections Time Figure 2  Epidemic curve and phases of the infection P=1 (phase 1 ) P=2 (phase 2 ) P=3 (phase 3 ) P=4 (phase 4 ) 9 cumulative number of deaths in the absence of any public intervention in all the phases. That is the MSL in equation (1), which denotes the maximum social loss. The shape of the epidemic curve will inevitably change if the government intervenes. However, the degree of change also depends on the phase of the infection. Figure 3 shows that, during the initial phase of the infection (P=1), the epidemic curve does not depend on the intensity of the public intervention. That is, if the intensity of the intervention in each phase is expressed as IP, then the peak in I1=1 is the same as that in I1=2. Thus, even if Taiwan and Hong Kong were to implement lockdown in the early stages, the effect would not be much different from that of the border measures alone. However, the situation changes drastically at P=2, i.e., during the expanding phase of the infection. As Figure 4 shows, the shape of the epidemic curve is very different between a weak and a strong government intervention, i.e., between I2=1 and I2=2. Hence, the government needs to put in place stronger behavioral regulations, such as lockdowns, to curb the cumulative death toll from the infection. Figure 3  Epidemic curve in the case of an earlier intervention Time Number of new infections I=1, 2 P=1 (phase 1 ) P=2 (phase 2 ) P=3 (phase 3 ) P=4 (phase 4 ) Number of new infections Time Figure 4  Epidemic curves in the case of weak and strong interventions I=1 I=2 P=1 (phase 1 ) P=2 (phase 2 ) P=3 (phase 3 ) P=4 (phase 4 ) 10 A similar dilemma could arise even during the phase where the spread of the infection has finally been deterred. Figure 5 depicts how the government’s strong interventions in one stage led to a decline in new infections. If, however, the government does not bear the weight of the economic losses and weakens its intervention prematurely (I3=1), then the spread of the infection may occur again. In such cases, it is likely that governments will be able to safely loosen the restrictions on people’s behavior only after a phase, such as P=4, where the intensity of public intervention will no longer be a major determinant of the epidemic curve. From the above consideration, apart from the social loss SL calculated at a point of public intervention with a certain intensity in a certain phase, the total social loss across all phases of infection TSL can be obtained as follows: TSL = MSL ‒ 
.  
 
(2) 2-3 
Optimal public intervention path As described above, governments generally choose the intensity of public interventions that is optimal for each phase, based on the assumption that the type of public interventions they undertake to deter the spread of infection will affect social losses represented by the cumulative number of deaths. Here, “optimal” does not necessarily mean minimizing social losses per se. In order to reduce social losses, more public intervention is needed to secure higher health capital, which entails greater economic loss in the form of further income loss. This is exactly the trade- off between economic activity and quarantine. What the government is trying to minimize is some combined measure of the economic and social loss. It can be expressed as follows: CPL = EL ‒λSL. 
 
 
 
(3) Number of new infections Time I=1 Figure 5  Epidemic curves in the case of premature weakening of interventions P=1 (phase 1 ) P=2 (phase 2 ) P=3 (phase 3 ) P=4 (phase 4 ) 11 This combined pandemic loss CPL is the weighted sum of the economic loss EL and social loss SL of this pandemic.3 The coefficient λ is a parameter for converting the social loss (specifically, the loss of human life) to economic loss. The value represents each society’s value judgment of human life. From equation (1), it is clear that the combined pandemic loss CPL is minimized at the point that the government determine the intensity of its intervention I so as to make the increment of economic loss (ΔEL/ΔI) equal to incremental value of social gain (λΔSG/ΔI), i.e., the value of the resulting reduction in the number of deaths. At first glance, it may seem unethical to replace human life with an economic measure. However, our society has been constantly making certain implicit value judgments about the economic value of human life. The evidence of this is that economic activity is never halted, even though many lives are lost each year to asthma, which is due to the air pollution resulting from economic activity. Nevertheless, the economic value of human life is extremely high in many modern democratic societies; the degree to which this is the case is roughly proportional to the material wealth from economic growth. This is illustrated by the fact that many developed countries have suppressed economic activity to a minimum, albeit temporarily, to contain the pandemic. If there is a dictatorship that does not recognize any value in the lives of its citizens, what choice will that government make in the event of a pandemic? In that case, the conversion rate λ for replacing human life with economic loss would be zero, and the total loss of the pandemic would be the economic loss itself. The best option for such a government would be to do nothing to deter the spread of the infection that would curtail economic activity, but to continue to let the infection continue as shown in Figure 2. In ordinary democracies where the economic value of human life is high, choices about the nature and strength of public intervention are made after considering the economic losses resulting from the intervention in comparison to the social losses. However, as noted above, the choice depends on the phase on the epidemic curve where the public intervention takes place. Public interventions at an early stage on the epidemic curve, such as those in Taiwan and Hong Kong, can minimize pandemic losses without strong public interventions. The intervention path would be, for example, (I1=1, I2=1, I3=0, I4=0). It means that weak public interventions continue during the spread of the infection, but zero public interventions are used after it starts to shrink. The total combined loss resulting from the infection is the sum of the economic losses and the 3 This function is formally similar to the “central bank loss function” that is often used in macroeconomics to formulate the principles of central bank behavior. It is based on the idea that central banks typically conduct monetary policy in such a way so as to minimize social losses, defined by the weighted sum of inflation and the output gap. The trade-off between the inflation rate and the output gap, known as the Phillips curve, plays an important role. 12 value of human losses borne at phase 1 and 2 only. In contrast, the optimal path when public intervention is used from the stage where the infection has already spread (P=2) would be, for example, (I1=0, I2=2, I3=1, I4=0). Here, the initial phase of the infection was left unattended, which forced strong public intervention in the spread phase. Even then, however, the degree of public intervention can be progressively relaxed as the infection lessens. This is because easing the behavioral restrictions did not increase the cumulative death toll much; the value of the additional human lives that would have been saved by strong restrictions fell short of the additional economic loss that would have been caused by them. Thus, when public intervention is applied at a stage when the infection has already spread, the cumulative economic and human losses are large, and consequently, the total losses of a pandemic are very high. Even so, this is certainly the optimal choice, in the sense that it minimizes the total combined loss of the pandemic since the time of public intervention. 3 
Economic losses from counter-pandemic measures and their sharing As discussed in the previous section, governments minimize the combined value of the social and economic losses, by determining the path of public interventions for each phase of the epidemic curve, always taking into account the effect of their interventions on curbing the spread of infection and the extent to which they will result in economic losses. In this case, the social loss is the cumulative death toll from the pandemic; the economic loss is the difference between the average income in peacetime (before the pandemic occurred) and the lower income brought about by economic regulation due to the public intervention. Figure 6 shows the magnitude of the economic costs incurred when the government locks down in the event of a pandemic. In a lockdown, any human activity that involves external contact is regulated, leaving only the minimum economic activity necessary to maintain people’s livelihoods. Therefore, the income that is being brought in at that time can be considered the minimum required income in that economy. In other words, the economic cost of lockdown is the difference between the average income in peacetime and the minimum required income in the economy.4 Realistically, if an economy growing at an average rate of 2% per year were to shift to zero growth due to the lockdown, the lost income (2%) would be considered the economic cost of the lockdown. 4 However, this required minimum income may rise gradually over time. This is because people’s livelihoods, which could be sustained in a short period of time by withdrawing their stocks of daily necessities, will become unsustainable if the lockdown is prolonged. 13 The path by which the decline in income and its growth rate come about will be dealt case-by-case. If the government halts business operations or firms' productive activities through legal regulation, the economic impact would appear to be a reduction in the supply of goods and services due to government regulation. In contrast, if the government asks people to avoid eating out, and people abstain from doing so in response, there should be a decrease in supply through the decrease in demand for food service. Thus, if the government merely calls for voluntary restraint, most of the income decline will be the result of demand shocks. However, the stronger the government regulation, the stronger will be the supply shock. Importantly, all these consequences, whether they are demand shocks or supply shocks, unfold as a decline in people’s current income, which constitutes the economic cost of pandemic preparedness, shown in Figure 6. In this sense, the question of whether the onset of decline is a demand or a supply shock is not so essential. The more pertinent question is about whose income it will reduce. The government’s counter-pandemic measures reduced the income of individuals and firms basically through reducing the supply of goods and services, whether directly or indirectly. In short, those who reduced the supply of goods and services faced income losses. In contrast, the individuals and firms that did not reduce the supply of goods and services did not have to contend with reduced incomes. This means that the economic costs of securing the health capital needed to prevent the spread of infection are being passed on in an extremely unequal manner among the members of society. Thus, the government’s counter-pandemic measures usually place a significant burden on some members of the society, in the form of loss of income, as the supply of goods and services shrinks. The distribution of that burden is extremely uneven with respect to industrial and Health capital Income (production of goods and services) Figure 6  Economic cost of a lockdown peacetime point
peacetime income lockdown  point
the minimum 
required income economic cost of 
a lockdown 14 employment conditions, because it depends on how government regulation affects firm and household incomes through demand and supply shocks. Hence, many governments that have dealt with pandemics have simultaneously offered some form of financial assistance, such as compensation for lost work or fixed benefits for companies and individuals who bear a particularly heavy burden. While the policy objectives of such public support are not always clear, the following benefits are generally expected. The first is livelihood compensation for economically deprived households that have lost their income. This is primarily a measure aimed at low-income people, such as those with little to no savings. The second is the provision of economic incentives for leaves or absence from work. This becomes especially important when government measures remain a moral imperative rather than a legal regulation. In the absence of absenteeism or fixed benefits, there will always be people who are willing to go to work (instead of staying home) for financial reasons. The trend is particularly strong among low-income people who have no savings. In this sense, the government’s support measures are not just focused on livelihood compensation, but also serve as an economic incentive to keep people at home, instead of permitting them to go out to work. Another important role of the government’s economic support is to enable the social sharing of the economic costs incurred due to the counter-pandemic measures. As noted above, the economic costs of preventing the spread of infection (or securing the necessary health capital for epidemic prevention) fall very unequally on certain members of the society. It is transferred to those who have reduced the supply of goods and services as a result of demand and supply shocks. They thereby lost the claims on goods and services from the present to the future. However, those who have not reduced the supply of goods and services have also suffered economic welfare losses. This is because they have been forced to cut back on consumption due to the government’s counter-pandemic measures, even though their incomes may not have reduced. Since they were inhibited to consume goods and services to the optimum level, their economic welfare has clearly declined. Since their incomes were not reduced, however, they necessarily increased their savings, which would result in their future claims on goods and services. In this regard, the economic welfare losses they have suffered might be almost negligible in comparison to the losses of those who have reduced their incomes. In view of the above, the government’s economic support should, in principle, be paid according to the share of the economic burden of fighting the pandemic (i.e., how much one’s income is reduced by it). The government would be using its own funds to compensate for the damages based on an insurance-like principle, as argued by Hayashi [2020]. A major problem with this compensation policy is that it takes a long time to calculate the value of damages. If the government intervenes to combat the pandemic, many people will face an immediate reduction in their income. However, the manner in which the burden is 15 ultimately passed on to the members of society can only be determined after the entire infection passes. For this reason, the government may be reluctant to provide financial support, which would result in low-income individuals with little to no savings pushed to further financial trouble. It is not that difficult to solve these problems. Hayashi [2020] suggests that the government should provide unsecured bridge loans to households until the final compensation amount is finalized by the government. Alternatively, as many countries have already done, the funds can be distributed uniformly for the time being and adjusted afterwards through taxation. In that case, the government would make a distinction between those whose incomes have been significantly reduced by the government’s counter-pandemic measures and those whose incomes have not been reduced, and subsequently give preferential tax treatment to the former. 4 
Real and fictitious future burdens 4-1 
Future burden from pandemic measures The financial size of the public compensation could pile up to an amount not seen since wartime spending in World War II. Most of the public spending, at least for the time being, will be funded by public debt, and not by tax increases. This is because not many governments have the political capital to dare to raise taxes in the midst of the growing economic hardship. Therefore, few mass media outlets and pundits have already begun to express concern over the future expansion of the budget deficit. Since the expansion of the budget deficit due to public support from the government is inevitable, such concerns will probably become more prevalent among the general public. This is because many people tend to assume that a larger government budget deficit means a larger future burden. Government policies that deter the spread of infection may reduce not just people’s current incomes, but also their future incomes. If the supply of capital goods is cut off due to the cessation of corporate production activities, capital investment by corporations and governments would become impossible, and capital formation in a country will stagnate. Moreover, if the accumulation and renewal of the capital stock is thus stalled, it will inevitably lead to stagnation of production and income in the future. In addition, the expansion of leave of absence and furloughs that was implemented to deter the spread of infection will, if prolonged, lead to a degradation of human capital embodied in people’s labor. Its probability becomes even stronger if dismissal of workers by companies increases. Furthermore, corporate bankruptcy due to a contraction of economic activity would imply a loss of management capital. Altogether, these shall result in a contraction of production and income in the future, rather than in the present. However, these are all future costs that the society has no choice but to accept to deter 16 the spread of the infection. They are never future burdens created by public support through government debt. On the contrary, if sufficient government support for businesses and households can effectively prevent the expansion of corporate bankruptcies and unemployment, the future burden created by the deterrence of the spread of the infection will certainly be reduced, at least to that extent. 4-2 
The future burden of deficit financing More generally, there are cases in which government support measures using deficit financing will lead to future burdens. It is a situation where deficit financing tightens capital markets, which in turn raises interest rates and crowds out private investment. There, to be sure, deficit financing, irrespective of how it is produced, usually stagnates capital formation and shrinks production and income in the future. However, in a situation where such a crowding-out mechanism does not arise, deficit financing, by itself, will not lead to a burden on future generations. The economist who articulated this logic most clearly was Paul Samuelson, one of the leading economists of the 20th century, who discussed in detail the problem of future generations' burden of government deficit financing.5 Samuelson first presents the conclusion of this issue as follows: The main ways that one generation can put a burden on a later generation is by using up currently the nation’s stock of capital goods, or by failing to add the usual investment increment to the stock of capital. (Samuelson [1967] p.346) The meaning of this brief statement is explained in more detail in a summary at the end of the chapter. The public debt does not burden the shoulders of a nation as if each citizen were made to carry rocks on his back. To the degree that we now follow policies of reduced capital formation which will pass on to posterity less capital goods, we can directly affect the production possibilities open to them. To the degree that we borrow from abroad for some transitory consumption purpose and pledge posterity to pay back the interest and principal on such external debt, we do place upon that posterity a net burden, which will be a subtraction from what they can later produce. To the degree that we bequeath to 5 In the seventh edition of Samuelson’s Economics published in 1967, the problem is mainly dealt in the second section of Chapter 19 entitled “The Public Debt and Modern Fiscal Policy,” and in the appendix to Chapter 19 entitled “False and Genuine Burdens of the Public Debt.” 17 posterity an internal debt but no change in capital stock beyond what would anyway have been given them, there may be various internal transfer effects as one group in the community receives a larger share of the goods then produced at the expense of another group. (Samuelson [1967] p.353) This explanation by Samuelson can be generalized as the proposition that government debt is not a burden on future generations unless it reduces their future consumption possibilities. Conversely, government debt is a future generational burden only if it reduces the future consumption possibilities of people. This may be called the “Lerner-Samuelson proposition on the future generational burden of government deficit financing,” since it was Abba Lerner who laid the foundation for this reasoning. Lerner pointed out that whether public debt will be a burden for future generations depends on whether the national debt issued to finance the budget deficit will be absorbed domestically or abroad.6 In the case of internal debt, even if future generations are taxed in order to redeem government bonds, they will still receive the payments, and their consumption possibilities as a whole will not necessarily decline.7 In contrast, in the case of external debt, while the current generation does not have to cut spending, the future generations will have to truncate theirs (through government tax collection) to repay the debt to other citizens, and the consumption possibilities of future generations will necessarily be reduced by that amount.8 Thus, external public debt will necessarily be a burden for future generations. While Lerner’s argument implies that future generational burden will necessarily arise as long as the public debt is external, it does not imply that it will not arise in principle in the case of internal debt. As Samuelson points out, even if all of the public debt is internal, the future possibilities of production and consumption will decrease if the renewal and accumulation of the capital stock is inhibited by the debt. Generally, deficit-financing policies undertaken in a full employment economy are more likely to result in tighter capital markets, higher interest rates, and a crowding out of private investment. Therefore, future generational 6 It should be noted that “generation” in the discussion of Lerner and Samuelson means “all people living at a certain point in time”, which is different from the usual generational concept of “all people born at a certain point in time.” 7 Lerner says of this, “Very few economists need to be reminded that if our children or grandchildren repay some of the national debt these payments will be made to our children or grandchildren and to nobody else. Taking them altogether they will no more be impoverished by making the repayments than they will be enriched by receiving them.” (Lerner [1948] p.256) 8 The distinction regarding whether the debt is internal or not should be made not by whether the bond is denominated in its own currency, but by whether it was purchased by a resident of the country who is subject to the government’s power of tax collection. 18 burdens will certainly arise, even if the debts are internal. The above consideration leads to a more precise version of the Lerner-Samuelson proposition that public debt will not be a burden on future generations if it is absorbed domestically and does not reduce the domestic capital stock, subsequently implying that there is no reduction in future consumption possibilities. This proposition holds strictly true in a Ricardian economy, wherein people behave as if they equate current tax increases with future tax increases due to bond issues. In this case, people’s saving and spending behavior will not change whether government spending is financed by higher taxes or by bond issues. Therefore, more fiscal deficits mean neither higher interest rates nor greater external debts. Even in non-Ricardian economies, the Lerner-Samuelson proposition holds, at least approximately, in an underemployment economy where unemployment due to lack of demand exists. As traditional Keynesian models (such as the IS-LM model) show, if the government implements a deficit fiscal policy, there will be some increase in interest rates and crowding out of private investment. However, the degree of crowding out will be contained because income and savings will increase due to the demand-led growth induced by fiscal spending. In short, public debt does not necessarily imply a future generational burden. Whether or not the burden arises depends entirely on the economic situation. As Samuelson points out, the impact of the buildup of public debt as internal debt appears not so much as an expansion of the future burden itself, but rather as an income transfer between different strata of society. Government bonds as public debt are nothing more than assets for bondholders and a right to acquire goods and services in the future. Thus, as long as government-issued bonds continue to be absorbed in the country, private sector assets will continue to grow. The economic conditions in the country will decide the macroeconomic effect of the expansion of private assets. In a Ricardian economy, this private sector asset signifies future tax increases, therefore would be completely offset by public debt. Thus, deficit financing would be macroeconomically neutral. This means that, as Robert Barro once argued, government debt cannot be the net wealth of the private sector (Barro [1974]). However, in the non-Ricardian economy on which traditional Keynesian economics has been premised, the economic expansionary effect of deficit-financing government spending is usually greater than tax-financing government spending. The most obvious difference between the two arises in the domestic distribution of purchasing power. The holders of government bonds in the private sector should have relatively greater access to goods and services than those without them. This purchasing power effect does not depend on whether the economy is Ricardian or non- Ricardian, but on the decision of each economic agent about whether to cut their spending and buy the government-issued bonds that created the debt. It is merely the result of the intertemporal consumption choices of each economic agent—whether to cut current spending to obtain future goods and services or simply to desire current goods and services. 19 4-3 
Inherent currentness in the burden of counter-pandemic measures The important message in Samuelson’s argument on the future burden of deficit financing is the economic truth that most of the burden created by government spending, whether it is financed by bond issues or tax increases, will ultimately be borne by the current generation, and not by the future generations. Samuelson uses the example of wartime costs to illustrate this. It is worth considering why this is so by following the context of the supplementary argument in Chapter 19 of Economics that “all debt came from World War II” (Samuelson [1967] p.356). War usually requires artillery and ammunition, and the government spending for it is funded by either higher taxes or public debt. Suppose that all the costs were covered by bonds rather than taxes. Even then, the burden on future generations will not arise if all the public debt is absorbed domestically and does not result in the crowding out of private investment. To confirm this, consider an economy in which there is no capital stock at all, and therefore no investment; people devote all their income from time to time to consumption. It is, so to speak, an economy that is based solely on hunting and gathering. In this consumption- only, investment-free economy, it is self-evident that no matter how much manpower is used in the production of artillery and ammunition, it will not result in a burden for future generations. While future generations could produce artillery and ammunition, they cannot bring it to the present unless they possess a time machine. Thus, in this case, the burden of producing artillery and ammunition would eventually be borne entirely by the current generation in the form of a reduction in its consumption. This is because there are always constraints on production resources, such as labor. As more human resources are allocated in the production of artillery and ammunition, the production of consumer goods must be reduced. In conclusion, whether wartime costs are financed by higher taxes or public debt is merely a question of how that burden will be shared within the current generation. If it is financed by a tax increase, then the taxpayer who has reduced their disposable income will bear the cost through reduced consumption. In contrast, if it is financed by public bonds, the cost is borne by those who voluntarily reduce consumption and purchase the bonds. However, this strong conclusion that all fiscal burdens are borne not by future generations but by the present generation is derived from the assumption that there is no foreign sector or capital stock; thus, it is by no means universally true. There will always be future burdens when public bonds are absorbed overseas. Even if they are absorbed internally, there will be future burdens if they reduce domestic investment and capital stock. Regardless of the burden on future generations, if there are individuals who must reduce their consumption due to the government’s policy, it is clearly a burden for that generation of people. In this sense, the burden of the government’s policy is considered to be borne by the current generation rather than future generations. 20 The same conclusion holds for the government’s counter-pandemic measures. As shown in the previous section, imposing public regulations to prevent the spread of infection necessarily reduces the very level of economic activity, through a reduction in demand or supply. In a lockdown, the difference between the income in peacetime and in lockdown is precisely that economic loss (Figure 5). Thus, public regulations to prevent the spread of the infection affects the economy in the form of demand and supply shocks, shrinking the overall production of goods and services. However, the extent of the impact will vary depending on the circumstances in which each firm and household is located. While some areas will be devastated (such as the entertainment industry or the food and beverage industry), others will rather expand supply (such as the mask-making industry). The economic burden thus created will be entirely borne by the present generation, whether or not it is financially supported by public debt (exactly as it is in the case of the burden of war expenditures). This is because it is the current generation of people who are reducing the production and consumption of goods and services through the imposed counter-pandemic measures. The amount of public debt that piles up at that time will not deter private investment as unemployment expands due to falling demand and interest rates continue to fall. Thus, future production and consumption possibilities are not depressed by the bond itself. Rather, these possibilities would be better than they would be without it if the government’s economic support measures successfully prevented the expansion of corporate bankruptcies and unemployment. Apparently, these measures should be deficit-financed, and not involve tax, since they must be implemented without reducing people’s disposable income. In a wartime economy, people are sometimes forced to severely cut back on their consumption. In Japan, during World War II, moral slogans such as “luxury is the enemy” and “we don't want it, not until we win” were imposed on the people by the government. It comes from the economic principle that there is no free lunch; everything involves trade-offs. Therefore, to obtain the artillery and ammunition needed to carry out the war, the current generation has no choice but to reduce consumption and divert the productive resources (that were used to produce consumer goods) to the production of artillery and ammunition. The resulting decline in consumption is a burden on the current generation. Meanwhile, in an economy under pandemic preparedness (such as lockdowns), the greatest burden for the current generation will come in the form of a decline in income. The present generation is subsequently involuntarily forced to produce non-income-generating leisure instead of producing artillery and ammunition. People would prefer to earn income by engaging in economic activities and enjoy consumption of goods and services from that income, rather than such excessive leisure. It is the current generation’s income and consumption that is lost in this situation resulting in the economic burden of the counter-pandemic measures. 21 5 
Relevance and irrelevance of various considerations regarding pandemic preparedness The above considerations about the economic costs of pandemic deterrence, unfortunately, do not help us to derive some necessary policy measures. However, it does teach us what we should consider and what we should not be trapped in, when formulating a specific policy. This is important because, on this issue, there are things to consider and avoid at the same time. When a society is faced with a pandemic, the preliminary decisions involve the measures to be used and the extent to which they will be imposed to deter the spread of the disease. Our goal is to minimize the losses that the society suffers. While the goal, in itself, is quite simple, the decision is far from simple. This is because the stronger the policy to prevent the spread of the disease, the greater the economic losses are likely to be. This is the trade-off between economic activity and epidemic prevention. It is precisely because of this trade-off that we continue to struggle with the decision to impose lockdowns once an infection has spread, and the duration for which it should continue. In fact, if the lockdown does not result in any economic loss, the society will happily continue to comply. This trade-off between economic activity and epidemic prevention signifies that our decisions regarding pandemic deterrence are nothing less than judgments about our priorities. The extent to which a given quarantine policy is effective in deterring infection is a purely empirical epidemiological question that has nothing to do with value judgments. However, no matter how epidemiologically effective the policy may be, there is no immediate justification for its implementation if it involves an economic loss that results in income reduction. In order to implement the policy, a social consensus based on the exact value judgment is necessary; the benefits of reducing the social losses are sure to exceed the economic losses caused by it. Another issue we must consider regarding the economic costs of pandemic deterrence is how it will ultimately be shared by the society. We see that the economic burden of policies to prevent the spread of infection falls on each class of the society in a very unequal way. It varies across industries, and it depends on an individual’s employment category. Some people have lost their jobs and income entirely because of the policies; others have been able to maintain their existing income and enjoy their extra leisure time. Preventing the spread of infection is an obvious public good that benefits all members of the society, and yet, the cost is unequally shared. Therefore, governments should provide some form of financial support, such as absenteeism and fixed benefits, along with its policies to deter the spread of infection. The biggest obstacle to such public support measures is a very universal concern that they will lead to a deterioration of government finances. The existence of such apprehensions may, in some cases, distort judgments about what and how far to go to deter the spread of the infection itself. It is plausible that some nations may lift the lockdown to avoid the expansion of public assistance, even though there remains a significant risk of the spread of the infection. 22 However, as Samuelson have argued, since many of the government’s policies can only be carried out using current rather than future labor resources, much of the burden will eventually have to be borne by the current generation. Even if public bonds are issued to implement these measures, it does not mean that the burden will be passed on to the future generations. The extent of public debt merely changes the distribution of goods and services in the future according to the current saving and spending behavior of each economic agent. This applies to the public assistance provided as part of the counter-pandemic measures. The current generation is already bearing the heavy burden of shrinking incomes and consumption. Even if all public support was provided by issuing public bonds, thereby expanding government debt, the burden of the current generation could not be passed on to future generations. Therefore, there is no basis for the notion that government economic support should be withheld as much as possible in order to avoid leaving a burden for future generations. However, the above considerations about the future burden of deficit financing do not at all deny that the accumulation of public debt has any effect on the economy as a whole. Generally, if the economy were non-Ricardian, then an increase in public debt would act as a greater expansion of people’s spending because it would mean an increase in private assets as a future claim to goods and services. Therefore, at some stage, governments and central banks may have to carry out macroeconomic tightening (using fiscal and monetary policies) to contain price increases due to the expansion of private spending. This suggests that the failure of governments and central banks to make such macroeconomic adjustments could lead to economic disruptions, such as fiscal collapse and soaring inflation. Economists of the Keynesian position have been consistently emphasizing that government fiscal deficits are not generally evil, but rather that deficit financing during recessions is necessary for the stabilization of the macroeconomy. However, they did not deny the need for fiscal discipline in the sense of long-run fiscal equilibrium through the business cycle. This is because it is difficult even for Keynesians to deny, in principle, the possibility of macroeconomic turmoil that would occur if there were no fiscal discipline.9 Nevertheless, the claims made by some media outlets that the expansion of public support will lead to immediate fiscal collapse and hyperinflation can be considered a product of 9 However, there is a position on the part of some Keynesians that denies even the need for fiscal equilibrium in the long-run sense. For example, modern money theory (MMT), proposed in mid-1990s by Warren Mosler, Randall Wray, and others, maintains that there is no inherent government fiscal constraint in an economy with a sovereign currency. They termed the rigid fiscal equilibrium doctrine as “deficit hawks,” the cyclical deficit doctrine (on which the traditional Keynesians have relied) as “deficit doves,” and their own position (that government finances do not even need to be balanced through the business cycle) as “deficit owls.” (Mitchell, Wray, and Watts [2019] p.333-4) 23 mere sensationalism. There is almost no chance that the excessive demand supported by deficit financing will make it difficult for governments and central banks to control inflation in a situation where many companies are facing a business downturn due to reduced consumer demand as a result of counter-pandemic measures. Rather, as Christopher Sims has suggested as a policy implication of the fiscal theory of price level (FTPL), the evils of being too captive to fiscal discipline are far greater in a demand-deficient, low interest rate economy than they would be otherwise (Sims [2016]). There have been countless examples in history of countries falling into financial ruin due to excessive public debt (Reinhart and Rogoff [2009]). However, fiscal collapse has only occurred in very exceptional circumstances in developed countries with adequate tax collection capacity. Conversely, there are many examples that show noticeable “debt tolerance” in these countries. The United Kingdom had a public debt of 250% of GDP after the end of the World War II with no signs of fiscal collapse. Japan had experienced a sustained deterioration in public finances caused by the prolonged economic stagnation since the 1990s, which caused government bond rates to continue to fall, rather than rise. In short, it is highly unlikely, at least in the developed world, that a transient increase in budget deficits due to an increase in public support will lead to immediate fiscal collapse or hyperinflation. Thus, in terms of pandemic preparedness, it is clearly not about what we should consider, but about what we should not. 24",0
"In this paper, it is argued that theoretical physics is more akin 
to an organism than to a rigid structure. It is in this sense that the 
epithet, “sick”, applies to it. It is argued that classical physics is a 
model of a healthy science, and the degree of sickness of modern physics is measured accordingly. The malady is located in the 
relationship between mathematics and physical meaning in 
physical theory. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Introduction 
 
 
Increasingly, and as a concerned physicist delves deeper into 
modern physical theory, and exerts a genuine effort to understand 
what is going on in theoretical physics, he or she cannot help 
thinking that something is deeply amiss in modern physics; that it 
is afflicted with some sort of a malady or sickness that could 
threaten its very credibility. It is a feeling that could not be 1 dispelled easily. One smells corruption in the air of modern 
theoretical physics. But, of course, this feeling implies that 
theoretical physics is some sort of a socio--epistemological 
organism. Are we justified-- logically, conceptually and 
historically-- in talking about physics in this strain, as though it 
were an organism? If so, where is the malady located? In what 
sense is modern physics sick? And, what is the cure? 
 
 
In this paper, we argue that physical theory is indeed an 
organism -- a socio-epistemological organism -- that is born, and 
that lives, flourishes, and declines under various internal and 
external conditions. We, then, focus our attention on the 
relationship between mathematics and physical meaning in 
physical theory, and argue that classical physics is a paragon of 
health in this regard. In contrast, modern physical theory, 
particularly general relativity and quantum mechanics, suffers from 
a serious disjuncture between mathematics and physical meaning, 
which opens it to a barrage of absurdities and pre-scientific 
mystifications. This malady is not showing any sign of abating. On 
the contrary, it is growing at an alarming rate, which threatens the 
efficacy and the very existence of theoretical physics as a rational 
enterprise. The paper ends with a note of warning to the theoretical 
physics community to ponder this malady and effect fundamental 
changes in their methodology before the situation gets out of hand (Lindley, 1994). 
 
Physics as Organism The fundamental question that pertains to the nature of 
physical theory is the question concerning the interconnections 
between physical theories in the modern era. Are physical theories 
a heap of independent thought systems, each with a unique 
structure reminiscent of a work of art? Are they free creations of 
the mind; leaps of creativity that emanate from a semi-rational 2 subterranean realm of consciousness? Are they closed conceptual 
universes, each with a unique logic of its own? 
 
 
    The prevalent trend in modern physics and 20th century 
philosophy of science tends to answer the above questions in the 
affirmative. Physicists and philosophers of science, as varied and 
wide apart as the positivists (Reichenbach, 1970; Reichenbach, 
1958), Karl Popper (Popper, 1986), Thomas Kuhn (Kuhn, 1970), 
Paul 
Feyerabend 
(Feyerabend, 
1995), 
Gaston 
Bachelard (Bachelard, 1984), Werner Heisenberg (Heisenberg, 1966; 
Heisenberg, 1958)), and Stephen Hawking (Hawking, 1988), seem 
to share many views pertaining to these questions. To them, 
modern physical theories are not organically related to each other, 
but are at best contingently related. The similarities between 
modern physical theories are dismissed as being superficial, verbal 
- merely verbal – similarities. Each theory stands on its own, and 
comes about (I have deliberately avoided the word, “emerges”) as 
a result of a creative irrational leap, be it individual (Popper) (Bhaskar, 1978) or social (Kuhn) (Ghassib, 1994a). It is an eternal 
present, unrelated either to its past or its future. The fundamental 
concepts that are common to the various theories, such as: mass, 
particle, field, Lagrangian, and Hamiltonian, are, in fact, nominally 
common; only the names, not the conceptual contents, are 
common. Thus, modern physical theories are merely a collection of 
essentially contingently related thought systems and ideas. In opposition to the current wisdom regarding the nature of 
physical theory, we propose a different approach, which views 
modern theoretical physics ever since its emergence during the 17th 
century from the heart of medieval metaphysics, as an organism, as 
a contradictory open totality, which evolves and develops, 
ultimately via leaps and revolutions. According to this dialectical 
approach (Ghassib, 1994b), modern physical theories are 
dialectically related to each other, which means that they are 
related via necessary transformational objective relations. The 3 In a previous publication (Ghassib, 1988), I located the 
fundamental objective contradiction, that has animated physics 
since its inception in the 17th century, in the contradiction between 
the classical concepts of particle and field. Initially, this 
contradiction manifested itself in embryonic form in the two self-
contradictory concepts of ether and action-at-a-distance (Palter, 
1960). These were embryonic substitutes for the fully developed 
concept of field. However, the moment the latter emerged in its 
fullness in the second half of the 19th century, it had to rip classical 
physics apart and expose it to a deep revolutionary crisis. This 
fundamental contradiction was partially resolved by special 
relativity (Miller, 1986; Miller, 1977), Einstein’s photon theory of 
light (Einstein, 1905; Jammer, 1966), general relativity (D'Abro, 
1950), de Broglie’s theory of matter waves (Ghassib, 1983), and 
Schrodinger’s wave mechanics (Ludwig, 1968). However, these 
partial resolutions have helped to reproduce this basic 
contradiction in various new forms, which means that it is still 
animating physical theory and its development. This implies that 
modern physical theories are not various creative, basically 
unrelated, conceptual systems created to cope with data and unify 
them. They are closer to being ways to cope with contradictions 4 lying at the heart of prevalent theories; ways to transform existing 
theories into more comprehensive theories by resolving the 
contradictions of the former. It is in this sense that we have come 
to consider physical theory an organism, rather than a collection of 
ideas. Since physical theories emerge from each other and are 
transformed into each other, they cannot be severed from each 
other, and, thus, constitute an organic whole, a developing 
organism, an open  totality  (Ghassib, 1992). 
 
      Hitherto, two general methods have been invariably followed 
to resolve the basic contradictions of physical theory: (i) the 
reductionist method, (ii) the dialectical method. The reductionist 
method has invariably failed and reached an impasse, whilst the 
dialectical method has proven to be very fruitful, even though it 
has not yet totally resolved the basic contradiction. The 
reductionist method consists in reducing particles to fields, or vice’ 
versa. At the turn of the 20th century, this was followed by a 
number of physicists who attempted to reduce classical particles to 
electromagnetic fields. Amongst them were J.J. Thomson (Ghassib, 
1988), Oliver Heaviside (Ghassib, 1988), Wilhelm Wien (Ghassib, 
1988), Max Abraham (Ghassib, 1988), and Lorentz. Their failure 
led Einstein along the dialectical route on to special relativity. 
However, Einstein himself was later to fall into the reductionist 
trap in his thirty-year relentless effort to create a comprehensive 
unified field theory that would resolve the contradictions of general 
relativity (Weinberg, 1992). It was a wasted quest. Meanwhile, 
physical theory has evolved to forms that bear scant resemblance 
to Eintein’s dream .  
 
        If we subscribe to this viewpoint and consider physical theory 
a socio - epistemological organism, we are bound to consider the 
question of the state of health of physical theory a legitimate 
question. We shall approach this question by focusing on the role 
of mathematics in physical theory, and, particularly, on the 5 relationship between mathematics and physical meaning in the 
major physical theories of the modern era . 
 
Classical Physics   
 
 
           In classical physics, the relationship between mathematics 
and physical meaning is transparent and straight-forward. The 
essential point to notice in this regard is that classical physics uses 
mathematics to express physical and related philosophical concepts 
and meanings, which means that the physical meaning precedes the 
mathematical expression. The mathematics is part of the physical 
meaning, and not the other way round. For example, force is given 
a specific meaning based on a broad conception of the material 
world. It is viewed as a measure of material interaction between 
fundamental particles (atoms), and as the primary cause of change 
in the motion of atoms, and, thus, of all phenomena in the universe. 
Its properties are empirically determined, and mathematics is then 
used to unify and give expression to these properties (Cohen, 
1983). The physical meaning is given mathematical expression 
because the former is fundamentally quantitative. The mathematics 
is not inseparable from the physical meaning; it is a necessary 
component of, and tool for constructing, the meaning. Newtonian 
mechanics is, therefore, a healthy coherent theory; a clear and 
distinct embodiment of a specific rationality and reason. It is based 
on a clear-cut family of philosophies of nature and a special broad 
conception, and it consists of a network of meanings and meaning 
systems, which underlies theoretical and empirical practices. The 
mathematics is part and parcel of this network and its scientific 
specificity. 
 
       The other disciplines of classical physics, which were modeled 
and based on classical mechanics, are no less clear, distinct and 
meaningful. The electric and magnetic fields had been discovered 
and physically defined before Maxwell formulated his differential 6 equations for the electromagnetic field (Jammer1980). The latter 
are mathematical expressions of physical relations between 
physically well-defined entities. Likewise with thermodynamics 
and the theory of matter. In classical physics, the equations come 
as the climax of a process of meaning and knowledge construction, 
the basic tools of which are mathematics and measurement 
(experiment). Thus, classical equations are the highest stage of 
physical meaning construction. Where they are used to investigate 
new phenomena, they are indeed used as a tool for knowing, 
understanding and explaining these phenomena – i.e., for 
determining their essential structures, causes and mechanisms of 
generation. Their use is essentially a search for their concrete 
manifestation in the phenomena. This process deepens our 
knowledge of the equations and sheds more light on their efficacy, 
extent and significance. Thus, theory and phenomenon are 
dialectically related. However, there is an asymmetry in this 
relationship. It is the theory that is used to understand the 
phenomenon, and not the other way round. The theory is enriched 
and its meaning deepened and concretized by this process, but the 
process is not essential for conferring meaning on the theory, or, 
for gaining a basic understanding of it. That explains the perfect 
balance between theory and experiment in classical physics. 
Theory grows in parallel with experiment. The element of 
collaboration, coordination and cooperation between them is more 
pronounced than the element of antagonistic imbalance. Classical 
physics is indeed a sane, rationalist enterprise. 
 
 
Special and General Relativity Modern physics (relativity and quantum mechanics) is an 
altogether different story. The balance, the relative harmony, 
hitherto encountered in classical physics, is violently severed in 
modern, 20th century, physics. Now, the equation precedes the 
physical meaning. It is discovered prior to physical meaning, using 7 very abstract mathematical arguments and relations. The search for 
meaning starts after the discovery of the equation and continues 
seemingly indefinitely. Experiment, philosophy and hypothesis are 
all utilized to discover the missing physical meaning. The only 
objective reality practically recognized is the uninterpreted 
mathematical equation; a clear return to Pythagorean-Platonic 
thinking (Heisenberg, 1971). The equation is the only objective 
fact. All else is subjectivity.  
 
          Needless to say, it all starts with the maestro of modern 
physics, Albert Einstein. This Pythagorean trend is evident right 
from the start, even in special relativity, that indispensable tool of 
modern investigation. The special retativistic transformation 
(Lorentz) equations are first derived, and then given a number of 
physical interpretations. The controversy, of course, is not yet over. 
The energy-mass equation is first mathematically derived, using 
the solid methods of classical mechanics, and later on interpreted 
physically. A mathematical, four-dimensional, space is constructed 
(Minkowski’s spacetime), and its mathematical properties and 
symmetries are used to reconstruct classical mechanics special 
relativistically. The physical interpretation comes later, of course 
not without the help of the “discarded’’ classical mechanics. Thus, 
modern physics is mathematics in search of physical meaning. It 
has indeed been reduced to a branch of mathematics.  
 
            What is a barely noticeable trend in special relativity 
assumes enormous proportions and the status of a fundamental 
principle in Einstein’s general relativity -- namely, the principle of 
covariance. The coordinates lose all specific physical meaning, and 
the laws of physics are constructed out of tensors on a four-
dimensional manifold in which real space and real time are 
dissolved beyond recognition. The abstract engulfs the concrete 
and consumes it beyond retrieval. Being covariant, the laws of 
physics are constructed in such a way that they are impervious and 
indifferent to the choice of coordinate system. In constructing his 8 field equations, Einstein was guided as much by mathematical 
considerations as by physical grounds. His search focused on 
finding a tensor which satisfied certain mathematical (geometric) 
criteria, which he then equated with an ambiguous generalization 
of classical mass and energy (energy - momentum tensor) (Schilpp, 
1951). Admittedly, the Einstein field equations did (and do) have a 
broad geometrico-physical meaning embedded in them-- namely, 
spacetime curvature is related to matter and motion. However, this 
apriori assignation of meaning is rather deceptive. When it comes 
to solving the Einstein field equations, one is immediately 
confronted with the choice of a suitable coordinate system and the 
physical interpretation of the system chosen (Fok). Needless to say, 
recourse in invariably made to classical mechanics to interpret 
certain fundamental constants in the theory (e.g., Newton’s 
universal gravitational constant, G, and the mass density) (Eddington, 1965). Also, finding a suitable coordinate system and 
interpreting it reasonably are no straight-forward matter. It is a 
very laborious process which relies heavily on the clear and 
distinct ideas of classical physics. This is most clearly evident in 
the history of the Schwarzschild solution to Einstein’s field 
equations (Israel, 1987). It took the best minds of the scientific 
community more than thirty years after Schwarzschild’s discovery 
to arrive at a “natural” coordinate system (the Kruskal-Szekeres 
system) (Misner, 1971), and a tolerably reasonable interpretation of 
it. Meanwhile, the whole matter led to such real and apparent 
absurdities that no one working in the field at the time, least of all 
Einstein and Eddington, made heads or tails (apart from the 
mathematics, of course) of what one was actually doing. However, 
even after the major discoveries of Kruskal, Szekeres, Kerr, 
Wheeler, Penrose, Hawking, Zeldovich, Novikov, Ellis, Israel and 
others (Misner, 1971),  the majority of theoretical physicists still 
feel rather lost when it comes to physically understand black hole 
and wormhole physics. The mathematics is pretty clear, though 
heavy, but the physics is lost in the mathematical entanglement. In 
spite of the apparent experimental successes of general relativity, 9 one is left with an uneasy feeling of intangibility and contrivance (Dunbar, 1996).  
 
             Contrary to the myth propagated by a number of 20th 
century philosophers of science, general relativity is not a 
“beautiful” self-contained and self-sufficient theory -- a finished 
work of art (Bachelard, 1984). Rather, and like all other physical 
theories, it is a contradictory open totality, which is heavily 
dependent on classical theory in both form and content. In a sense, 
it is a field (as opposed to particle) generalization of classical 
theory (Einstein, 1961; Einstein, 1950). Being afflicted with the 
malady of building its mathematical equations before establishing 
physical meanings, it is bound to be heavily dependent on classical 
physics in its search for physical meaning.  
 
Quantum Mechanics  
 
      This malady reaches absurd proportions in quantum mechanics. 
Once again, a myth has been propagated, by both theoretical 
physicists and philosophers of science, to the effect that quantum 
mechanics is a complete theory, which has solved the 
contradictions, and overcome the inadequacies, of the old quantum 
theory. But, has it really done that? Is it less inadequate and 
contradictory than the old quantum theory? Is it not more prudent 
to recognize its incompleteness and inadequacies, and look for 
ways to “develop” it back towards, rather than away from, the 
great insights of classical physics? 
 
         Instead, we have seen leading theoretical physicists trying to 
enthrone quantum mechanics and elevate it to the status of “the” 
complete theory of physics (Cushing, 1994). Of course, to conceal 
its glaring inadequacies, they have had to invite all sorts of absurd 
ideas and interpretations. In the process, they have opened large 
holes in the edifice of physics for all sorts of pre-scientific and 
irrationalist, almost solipsistic, nonsense. How else would a 10 rationalist describe the Copenhagen assertion that the conscious 
observer (consciousness) creates external material reality (Mermin, 
1985; Popper, 1982), that the humanly willful and purposeful act 
of measurement creates microscopic reality (Landau, 1965), that 
each event splits the universe into many universes (Everett III, 
1957), or, that the present fixes the past (Wheeler, 1957). These 
absurdities satirized by Schrodinger with his famous cat (Schrodinger, 1935; Beller, 1977), are, in fact, rooted in the 
aforementioned malady of modern physics -- namely, that the 
mathematics precedes the physical meaning and conditions it; that 
the latter is a mere aftereffect or epiphenomenon. When Schrodinger discovered his equation, he thought that 
he was discovering a new type of classical field, and that its nature 
would be specified later in a manner similar to the way the optical 
wave–field, discovered by Huygens, Young and Fresnel, was later 
shown to be Maxwell’s electromagnetic field (Harman, 1995). Of 
course, things have not developed in this way, and, already, almost 
ninety years have passed since the construction of quantum 
mechanics, and no convincing interpretation seems to be in sight (Squires, 1996).  
 
 
This state of affairs applies equally to the other formulations 
of quantum mechanics. Heisenberg started with an insistence on 
observables (D'Abro, 1952), and ended up endorsing a form of 
solipsism, whereby the laws of quantum physics do not reflect 
physical reality, but a mental state (our possible knowledge) (Heisenberg, 1960; Heisenberg, 1961). The same dilemma recurs 
in Wigner’s formulation of quantum mechanics (Wigner, 1932). In 
classical transport theory, the distribution function is first defined 
and assigned a specific meaning, and the Liouville equation is then 
derived by translating the physical meaning into mathematical 
operations (Jancel, 1963). Thus, the Liouville equation is a 
mathematical embodiment, or, rather, a development, of the 
physical meaning. In quantum transport theory, the opposite 11 occurs. The Wigner distribution function is defined only 
mathematically, in terms of the ill-defined Schrodinger wave 
function, and the Schrodinger equation is used to derive the 
quantum Wigner equation (Ghassib, 1996). The never-ending 
search for physical meaning starts after the equation is specified. 
Needless to say, this search is principally guided by the 
“discarded” classical Liouville equation. Once again, we have an 
equation in search of physical meaning (Planck, 1933; Lewontin, 
1993). We have a fictional phase space unrelated to real events, 
and we have a physically indefinite distribution function 
mathematically defined on it. The latter seems to be a mere 
calculational device devoid of physical meaning. 
 
 Conclusion The foregoing analysis raises a number of urgent questions 
which should be confronted by modern theoretical physics. Can it 
be that the very methods of modern theoretical physics, as opposed 
to classical theoretical physics, are at fault? Is it rationally 
permissible to construct equations for physically undefined, ill-
defined or incomprehensible mathematical constructs? Was the 
path connecting classical physics to modern physics inevitable       
-- i.e., written in the heart of the theoretical physics organism? Or, 
could this sickness have been avoided? Was it the result of an 
ideological intervention that managed to deflect physics from its 
true, healthy path? Should modern theoretical physicists not slow 
down their feverish, frenzied competition to ponder, and rethink, 
the methods they have been employing for the last ninety years? 
Should they not revise these methods and their results in 
relationship to classical theoretical physics? Should they not carry 
out a rigorous ideological analysis of their enterprise, and learn the 
critical methods of the social sciences? Should they not relearn 
how to think?(53) 12 We tend to answer these questions in the affirmative, and we 
leave them standing as a warning sign to the theoretical physics 
community. In future publications, we intend to develop the principal 
ideas propounded in this paper in relation to recent developments 
in quantum gravity, superstring theory and quantum cosmology.",0
"We propose an alternative deﬁnition for pseudo-bosons. This simpliﬁes the mathematical
structure, minimizing the required assumptions. Some physical examples are discussed,
as well as some mathematical results related to the biorthogonal sets arising out of our
framework.
We also brieﬂy extend the results to the so-called non linear pseudo-bosons. I
Introduction In a series of papers, [1]-[8], we have considered two operators a and b, with b ̸= a†, acting on a
Hilbert space H, and satisfying, in some suitable sense, the commutation rule [a, b] = 1
1. A nice
functional structure has been deduced under suitable assumptions, and some connections with
physics, and in particular with quasi-hermitian quantum mechanics and with the technique of
intertwining operators, have been established. Following Trifonov, [9], we have called pseudo-
bosons (PB) the particle-like excitations associated to this structure. The assumptions used
in our construction have been checked for a series of (quantum mechanical) models. Among
other things, we have been forced to introduce a diﬀerence between regular and ordinary PB.
The ﬁrst ones are those for which, see Section II, the biorthogonal sets of eigenvectors of the
operators N = ba and N†, Fϕ and FΨ, are Riesz bases. On the other hand, when these sets
are not Riesz bases, then our PB are not regular.
This paper is motivated by the following, very natural, questions: in the deﬁnition of PB
we have often required both Fϕ and FΨ to be bases for H. But, is this really necessary? It
is enough, maybe, to require that just one of these two sets is a basis? Or: can we replace
this requirement with that of Fϕ and/or FΨ being complete? We should recall, in fact, that
completeness of a set F is equivalent to F being a basis if F is an orthonormal (o.n.) set, but
not in general, at least if H is inﬁnite dimensional, which is the only situation we are interested
here in this paper1. Actually, there exist intriguingly simple examples of non o.n. sets, which
are complete in H but which are not bases, [14, 15]: let E = {en, n ≥1} be an o.n. basis for
H, and let us introduce a new set ˜
E := {˜
en := en + e1, n = 2, 3, 4, . . .}. It is clear that ˜
E is no
longer o.n., and it is easy to check that is complete but it is not a basis. Also, its biorthogonal
set is easily identiﬁed: ˆ
G = {ˆ
gn := en, n ≥2}, which is not even complete.
Other natural questions are the following: is it, for some reason, automatic that the two
sets Fϕ and FΨ are complete? Or that they are even bases in H?
This is the kind of problems we originally wished to address here. To begin with, it is easy
to deduce that the answer to the last two questions is, in general, negative. Indeed, without
further assumptions, it is easy to understand that already for ordinary bosonic operators c
and c†, with [c, c†] = 1
1, the set χ = {χn :=
1
√ n! c†nχ0}, where cχ0 = 0, is not even necessarily
complete in H. In fact, if c = x+ip
√ 2 , with [x, p] = i1
1, the set χ is an o.n. basis for H = L2(R) but
it is not, for instance, if H = L2(R2). In this case, completeness is lost: it is easy, in fact, to ﬁnd 1A simple reminder: a set Ff = {fn ∈H, n ≥0} is a basis if any h ∈H admits an unique decomposition in
terms of the fn‘s. It is complete if zero is the only vector which is orthogonal to all its vectors. 2 a nonzero function of L2(R2) which is orthogonal to all of χn. As it is well known, completeness
is recovered if we double the family of ladder operators, that is we consider two operators c1
and c2 satisfying [cj, c†
k] = δj,k1
1. This is because L2(R2) is isomorphic to L2(R) ⊗L2(R). For
this reason, and to avoid these kind of problems, we will ﬁx H = L2(R) in the rest of the paper,
where not stated diﬀerently, and we will concentrate on this particular situation.
This article is organized as follows: in the next section we propose a diﬀerent deﬁnition for
what we call D-PB, that is for those PB which are, somehow, associated to a certain subspace
D, dense in the Hilbert space H on which our operators a and b act. This slightly diﬀerent
deﬁnition simpliﬁes the treatment of PB quite a bit. In Section III we show how an interesting
intertwining relation can be deduced assuming that a and b are related by a third operator,
Θ, and we also deduce that the two sets of eigenvectors of the operators N and N† are related
by Θ. In Section IV, after some useful results on biorthogonal sets, we give some physically-
motivated examples, while some comments on non linear PB, [10]-[12], and our conclusions are
discussed in Section V. II
A new deﬁnition We begin this section recalling the deﬁnition of linear pseudo-bosons, as originally given in [1]:
let H be a given Hilbert space with scalar product ⟨., .⟩and related norm ∥.∥. We introduce
a pair of operators, a and b, acting on H and satisfying the commutation rule [a, b] = 1
1,
(2.1) where 1
1 is the identity on H. Of course, this collapses to the canonical commutation rule
(CCR) if b = a†. Let us call D∞(X) := ∩p≥0D(Xp) the common domain of all the powers of
the operator X. In [1] we have considered the following working assumptions: Assumption 1.– there exists a non-zero ϕ0 ∈H such that aϕ0 = 0, and ϕ0 ∈D∞(b).
Assumption 2.– there exists a non-zero Ψ0 ∈H such that b†Ψ0 = 0, and Ψ0 ∈D∞(a†).
Assumption 3.– Fϕ = {ϕn =
1
√ n! bn ϕ0} and FΨ = {Ψn =
1
√ n! a†n Ψ0} span the whole H. We have also considered the following extra assumption, useful but, apparently, not quite
physical: Assumption 4.– FΨ and Fϕ are Riesz bases for H. For reasons which will appear clear soon, we prefer to consider here a slightly diﬀerent point
of view, which allows us to simplify signiﬁcantly the procedure. In the present approach the 3 relevant ingredients of our structure will be the two pseudo-bosonic operators a and b, and
a certain dense subset D ⊂H, which is stable under the action of a, b and of their adjoints.
More explicitly, let a and b be two operators on H, a† and b† their adjoint, and let D be such
that a♯D ⊆D and b♯D ⊆D, where x♯is x or x†. Notice that we are not requiring here that D
coincides with, e.g. D(a) or D(b). Of course, D ⊆D(a♯) and D ⊆D(b♯). Deﬁnition 1 The operators (a, b) are D-pseudo bosonic (D-pb) if, for all f ∈D, we have a b f −b a f = f.
(2.2) Due to the stability of D, the above equality is well deﬁned: for instance, since b f ∈D, it
follows that a can safely act on it. Sometimes, to simplify the notation, instead of (2.2) we will
simply write [a, b] = 1
1, having in mind that both sides of this equation have to act on f ∈D.
It might be interesting to notice that two operators (a, b) which are not D1-pb, could still
be D2-pb, if a, b, D1 and D2 are chosen properly. Example:– Let H = L2(R), a =
d dx, b = x. Let us take D1 = {f(x) ∈L2(R) : f ′(x) ∈
L2(R)}. This set is dense in H, since it contains the set of the test functions S(R), but it is not
stable under the action of both a♯and b♯. For instance, if f(x) ∈D1, (bf)(x) = xf(x) does not
need to belong to D1 as well. On the other hand, if we take D2 = S(R), this set is stable under
a♯and b♯. Furthermore, [a, b]f(x) = f(x), for all f(x) ∈D2. Hence (a, b) are D2-pb, while they
are not D1-pb. For these operators the ﬁrst two assumptions above can be simpliﬁed. We now assume that Assumption D-pb 1.– there exists a non-zero ϕ0 ∈D such that aϕ0 = 0. Assumption D-pb 2.– there exists a non-zero Ψ0 ∈D such that b†Ψ0 = 0. In fact, if (a, b) satisfy Deﬁnition 1, it is obvious that ϕ0 ∈D∞(b) and that Ψ0 ∈D∞(a†),
so that the vectors
ϕn :=
1
√ n!
bnϕ0,
Ψn :=
1
√ n!
a†nΨ0,
(2.3) n ≥0, can be deﬁned and they all belong to D. We introduce, as before, FΨ = {Ψn, n ≥0}
and Fϕ = {ϕn, n ≥0}. Once again, since D is stable under the action of a♯and b♯, we deduce
that both ϕn and Ψn belong to D, so that they belong to the domains of a♯, b♯and N♯. Now
we prove the following Lemma 2 The operators (a, b) are D-pb if and only if (b†, a†) are D-pb. 4 Proof – Suppose that (a, b) are D-pb. Then, because of the deﬁnition of the adjoint, we can
check that

[b†, a†]f, g

= ⟨f, [a, b]g⟩= ⟨f, g⟩, for all f, g ∈D. Let now take Φ ∈H. Then, since D is dense in H, there exists a sequence
{Φn} ⊂D converging to Φ. Therefore, recalling that the scalar product is norm continuous,
we get

[b†, a†]f, Φ

= lim
n

[b†, a†]f, Φn

= lim
n ⟨f, [a, b]Φn⟩= lim
n ⟨f, Φn⟩= ⟨f, Φ⟩. Therefore [b†, a†]f = f for all f ∈D: (b†, a†) are D-pb.
The opposite implication can be deduced in a similar way.
□ It is now simple to deduce the following lowering and raising relations:




 


 b ϕn = √n + 1ϕn+1,
n ≥0,
a ϕ0 = 0,
aϕn = √n ϕn−1,
n ≥1,
a†Ψn = √n + 1Ψn+1,
n ≥0,
b†Ψ0 = 0,
b†Ψn = √n Ψn−1,
n ≥1, (2.4) as well as the following eigenvalue equations: Nϕn = nϕn and N†Ψn = nΨn, n ≥0, where
we recall that N = ba and N† = a†b†. In particular, we don’t have to bother about the fact
that the left-hand sides of these equations are well deﬁned or not, because of what we have
already deduced. As a consequence of the eigenvalue equations for N and N†, choosing the
normalization of ϕ0 and Ψ0 in such a way ⟨ϕ0, Ψ0⟩= 1, we deduce that ⟨ϕn, Ψm⟩= δn,m,
(2.5) for all n, m ≥0. In fact, since ⟨Nϕn, Ψm⟩=

ϕn, N†Ψm

, we have (n−m) ⟨ϕn, Ψm⟩= 0, which
implies that ⟨ϕn, Ψm⟩= 0 if n ̸= m. Moreover, the equality ⟨ϕn, Ψn⟩= 1 can be proved by
induction on n, using the fact that ⟨ϕ0, Ψ0⟩= 1.
So far, no deep diﬀerence appears between PB and D-PB. However, it is clear that the
stability of D makes the treatment of these latter much simpler. The main diﬀerences arise when
considering Assumption 3. The reason is that, in the original deﬁnition, we have sometimes
implicitly identiﬁed completeness of the sets Fϕ and FΨ in H with the requirement of they
being bases of H, at least at the level of the examples2. This is not a problem when the sets 2In fact, in [6] and [7], for instance, we have checked that the sets Fϕ and FΨ are complete. This is not
enough, see Section IV. 5 are Riesz bases, i.e. when also Assumption 4 above is veriﬁed. But, for non regular PB, this is
not true in general. We introduce now the following requirement Assumption D-pb 3.– Fϕ is a basis for H. This assumption introduces, apparently, an asymmetry between Fϕ and FΨ, since this last
is not required to be a basis as well. Notice also that, if we replace Assumption D-pb 3 with the
requirement that Fϕ is complete in H, the example given in Section I shows that, in general,
there is no a priori reason for FΨ to be complete, too. On the other hand, we can prove the
following result: Lemma 3 Fϕ is a basis for H if and only if FΨ is a basis for H. The proof of this statement follows from the uniqueness of the basis biorthogonal to a given
basis, [14, 15, 16]. It might be interesting to notice that (i) this lemma reintroduce a complete
symmetry between Fϕ and FΨ, and that (ii) a similar result is false if we simply ask the sets to
be complete in H, at least for those PB which are not regular. It might be worth also noticing
that, while the completeness of Fϕ does not imply that Fϕ is a basis, the converse is ensured:
any basis is complete. Remarks:– (1) It is interesting to check whether these results can be somehow enriched
for our very speciﬁc sets Fϕ and FΨ, which are constructed in a particular way. In fact, this is
exactly what happens. We will come back on this aspect later.
(2) If Fϕ is a Riesz basis for H, we could call our D-PB regular, as we have done in our
previous papers. However, this aspect will not be considered here. In view of the examples we will discuss later on, it is also convenient to introduce a weaker
form of Assumption D-pb 3: for that we ﬁrst introduce the notion of G-quasi bases, where
G is a suitable dense subspace of H.
Two biorthogonal sets Fη = {ηn ∈G, g ≥0} and
FΦ = {Φn ∈G, g ≥0} are G-quasi bases if, for all f, g ∈G, the following holds: ⟨f, g⟩=
X n≥0
⟨f, ηn⟩⟨Φn, g⟩=
X n≥0
⟨f, Φn⟩⟨ηn, g⟩.
(2.6) Is is clear that, while Assumption D-pb 3 implies (2.6), the reverse is false. However, if Fη
and FΦ satisfy (2.6), we still have at hand some (weak) form of resolution of the identity. In
fact, formally, we could rewrite (2.6) as P n≥0 |ηn ⟩⟨Φn, | = P n≥0 |Φn ⟩⟨ηn, | = 1
1G. Then our
assumption is the following: Assumption D-pbw 3.– Fϕ and FΨ are G-quasi bases for H. 6 III
D-conjugate operators In this section we slightly reﬁne the structure. Notice that, in what follows, we will always
assume that Assumptions D-pb 1, 2 and 3 hold.
We start considering a self-adjoint, invertible, operator Θ, which leaves, together with Θ−1,
D invariant: ΘD ⊆D, Θ−1D ⊆D. Then we introduce the following deﬁnition: Deﬁnition 4 We will say that (a, b†) are Θ−conjugate if af = Θ−1b† Θ f, for all f ∈D. Brieﬂy, we will write a = Θ−1b† Θ, meaning with that the both sides must be applied to vectors
of D. Of course, the fact that D is stable under the action of both Θ and Θ−1, makes the above
deﬁnition well posed, since D is also stable under the action of a and b†.
Then we have: Lemma 5 The following statements are all equivalent: 1. (a, b†) are Θ−conjugate; 2. (b, a†)
are Θ−conjugate; 3. (a†, b) are Θ−1−conjugate; 4. (b†, a) are Θ−1−conjugate. Proof – We just prove here that 1. implies 2. The other statements can be proven in similar
way. Let us assume that (a, b†) are Θ−conjugate, and let f, g ∈D. Then f, a†g

= ⟨af, g⟩=

",0
"We extend Langdon Winner’s idea that artifacts have politics into the realm of
mathematics. To do so, we first provide a list of examples showing the existence
of mathematical artifacts that have politics.
In the second step, we provide an
argument that shows that all mathematical artifacts have politics. We conclude
by showing the implications for embedding ethics into mathematical curricula. We
show how acknowledging that mathematical artifacts have politics can help math-
ematicians design better exercises for their mathematics students. Keywords: artifacts have politics, embedded ethics, mathematics in society, ethics in
mathematics, mathematics education Contents 1
Introduction
2 2
Some Uses and Effects of Mathematics in Society
2 3
Mathematical Artifacts Have Politics
9 4
Embedded Ethics in Mathematics
11 5
Conclusion
15 ∗RWTH Aachen University, Germany. dennis.mueller3@rwth-aachen.de.
†Centre
for
the
Study
of
Existential
Risk,
University
of
Cambridge,
United
Kingdom.
mcc56@cam.ac.uk.
2020 AMS Classification: 01A80. 00A30, 97A40, 97D20. 1 arXiv:2308.04871v1  [math.HO]  9 Aug 2023 1
Introduction In this essay, we present a three-step argument that both supports the idea of embedded
ethics in mathematics, and refutes the concept of political neutrality of mathematical
knowledge.
Our first step, providing a list of examples demonstrating how a modern
society can use and be affected by mathematics, aims to prove the existence of mathe-
matical artifacts that have politics. These examples predicate our second step, in which
we provide an argument for why every mathematical artifact has politics. This then pro-
vides the validation and insights for the final step of embedding ethics into mathematical
teaching and practice. In doing so, we extend Winner’s famous essay Do artifacts have
politics? focusing on engineering artifacts[147], and the many works analysing the politics
of artificial intelligence (e.g. [68, 78, 146]), into the world of mathematics. 2
Some Uses and Effects of Mathematics in Society In this section we list and describe some of the uses and effects of mathematics in society.
Of course, no such list can ever be complete because new applications of mathematics
are found daily. Nonetheless, it is necessary to see a selection of uses and effects in some
detail to get a better feeling for what it means for an artifact to have politics, before we
explore a more general argument in section 3 and to better understand the elements of
embedded ethics in section 4. Such a list, even if incomplete, can serve as a useful starting point and tool for creating
interesting and challenging mathematical exercises for students because it identifies some
of the underlying dynamics involved in the general use of mathematics.
Any area of
mathematics can be connected to some use or effect, and as such, these exemplary uses
can help us to design mathematical exercises that explore mathematics and its politics
simultaneously. The uses and effects we will cover are: prediction, discovery, (re-)organisation, optimi-
sation, protection, fun, extraction, analysis, synthesis, damage, harm, education, pun-
ishment, justice, control, self-image, economics, history, and manipulation; see Figure
1. 2.1
Prediction Predictions typically include statistical results to infer from a sample to an overall pop-
ulation, or a quantitative scientific theory about nature or society to predict the future
or an otherwise unknown socio-technical, technical, biological or physical feature. Any
prediction alters the state of the world, even when it is not acted on, as it increases our
knowledge and potential for actions. When acted on, predictions • about systems involving human interaction can become a self-fulfilling prophecy and
self-destructive as they can amplify or reduce certain behaviours (cf. [16]), • help us to imagine and navigate an unknown future, and by contrast, assist us in
understanding the past (cf. [7]), • can change the minds of those who perform or use them on a biological and psy-
chological level (cf. [67]). 2 Figure 1: All uses and effects are connected to each other. Examples: Feedback loops in recommender machine learning systems can lead to echo
chambers[69]; Covid-19 models have driven policy decisions and put mathematical models
and their makers into the spotlight of political attention[28, 86], and so have models about
climate change[79]; search engines try to predict the most relevant result for their users,
but when they are systematically wrong, they can induce forms of oppression, leading to
lasting effects on minority groups[104]; Einstein’s general relativity predicts the existence
of gravitational waves[24]; chaos theory gives ways to understand and potentially overcome
some of the limits of reliability and predictability[11]; mathematical predictions play an
essential role in creating imaginations and expectations of the future, and thus drive much
of our individual and collective economic activity[7]. 2.2
Discovery As part of the sciences, politics or everyday life, we use mathematics in an attempt to
discover truths about the physical and socio-technical world surrounding us. In the mode
of discovery, we fundamentally build on • the trustworthiness and deductive power of mathematical results to establish trust-
worthy theories of the world (cf. [94, 115]), • the power of mathematics to construct search spaces (cf. [132]), 3 • and abstraction to simplify the object of discovery for it to be captured and for-
malised by mathematical knowledge (cf. [50, 119, 134]). Examples: Predictive policing and crime forecasts can support the everyday job of po-
lice services[113], but they also lead to deep political questions about fairness, bias and
privacy[4]; mathematics underlies the search engines that organise the world’s information
and thus has become fundamental to the discovery of knowledge in everyone’s lives[18],
thereby deeply affecting how people see the world and understand themselves[104]; math-
ematical physics helped to search for, and build the machines detecting gravitational
waves[24]; models for climate change cannot capture everything, and the abstraction
required often leads to deep philosophical and methodological debates about their con-
struction and use[52]. 2.3
(Re-)Organisation Mathematics can be used to (re-)organise technical or socio-technical systems. Such usage • often builds on potentially conflicting metrics, indicators and measures on which
the (re-)organisation is judged (cf. [98]), • regularly requires trade-offs and choices, for example, between importance and sen-
sitivity (cf. [33]), • and often is within the framework of, or breaks with, a widely accepted scientific
theory[73], or may break with their practitioner’s expectations (cf.
[49, 56]) or
other’s expectations. Examples: Different fairness measures can be contradictory[51, 66]; machine learning
algorithms may require a trade-off between fairness and interpretability to maintain accu-
rate predictions[1]; many of the decision problems used in operations research (e.g. mixed
integer programming) are NP-complete, while others (such as linear programming[64])
can be solved in polynomial time, leading to trade-offs in model selection; there exists
an ever-growing list of counterintuitive results in basic probability[102]; the experimental
observations along with new mathematical models broke with the Aristotelian worldview
and led to the scientific revolution in the early modern world[129]. 2.4
Optimisation Any form of optimisation builds on deep ideas about how the world is organised, and how
it should progress and develop. Some of those are forms of colonial or scientific knowledge
that have become standard in the industrialised modern world[87, p. 97]. Through this,
optimisation can manifest itself in many ways, including • as mathematical artifacts building on quantification, abstraction, formalism, gener-
alisation, idealisation and other forms of de-situationing from the specificities of the
problem (cf. [87]), • as a human practice building on people’s aspirations, emotions, desires, imaginations
and needs (cf. [7, 87, 150]), • and as a tool of power and legitimisation it can provide us with answers to the
question of how a physical, technical or socio-technical system, organisation or order
can, or should, look like (cf. [6, 87]). 4 Examples: Proxy variables in the measurement of our economies, such as those used
to model living standards[92], can become distanced from people’s lives and thus may
struggle to capture reality; under the hood of modern search engines lie difficult optimi-
sation problems, and as systems, they build on their creator’s aspirations, imaginations
and technical needs (cf. the initial construction of the Google search engine[18]); as tools
of power, the optimisation techniques employed within a management or operations re-
search context can overstep and change social barriers in various ways[120]; the variational
principles underlying much of physics are an example of how optimisation appears in the
physical world[6]. Optimisation often crosses the lines between the factual and normative, e.g. the statistical
thinking and optimisation ideas behind Darwin’s work on natural evolution[37, 121] have
led to forms of Social Darwinism[61], which then manifested itself in normative ideas
about an optimal world. 2.5
Protection We can use mathematics to protect socio-technical or biological systems from each other.
In doing so, we • often aim to create a mathematical system that is aligned with our norms, values,
utilities and laws (cf. [5, 65, 148]), • may build on the epistemological supremacy, techniques and methodology of pure
mathematics to build (hopefully) provably secure layers of technical protection for
people, institutions, processes or data (cf. [72, 118]), • use mathematical methods to understand the evolution of biological systems and
their struggle for existence, in order to advise on their preservation or destruction
(cf. [55]). Examples: AI alignment research tries to understand how to put ethical constraints
into socio-technical systems[69] and, more generally, how to align AI systems with our
human values[148]; many hope that predictive policing[113] will make our police services
more efficient, and thus, save lives; selecting interpretable machine learning techniques[91]
over uninterpretable, potentially more accurate models, can be a means to protect society
from the systems it builds; as a means of protection, cryptography has become essential
in all our lives and modern infrastructure[118]; mathematical surveillance systems built
to protect us can lead to challenging questions when their existence and scale comes to
light[137]. 2.6
Fun We can use mathematics to derive enjoyment. This may involve • seeing mathematics practice as a puzzle, or solving and constructing mathematical
puzzles (cf. [112, 145]), • appreciating its beauty and understanding it as a form of art (cf. [23, 85, 149]), • the social activity of doing mathematics in a group of like-minded people or by
simply being good at it (cf. [111]). Examples: Many popular books on mathematical puzzles (e.g. [145]), beautiful proofs
(e.g. [2]) and beautiful mathematical identities (e.g. [42]) exist on the market; the beauty 5 and puzzling nature of mathematics can become part of one’s definition of doing mathe-
matics, thereby setting the foundations for discussions around ethics in mathematics[99]. 2.7
Extraction Mathematics can be used to empower the extraction of natural, social or economic re-
sources, or its use may require the extraction of said resources. In doing so, it • may have a morally, legally, ecologically, socially or economically (un-)acceptable
footprint (cf. [29, 32]), • can directly or indirectly reduce or increase the consumption of resources by other
institutions, groups or systems (cf. [29, 32]) • or lead to other planetary and social costs and imbalances. The extraction of natural
resources may lead to advantages for some to the detriment of others (cf. [29, 32]). Examples: The planetary and social footprint of modern AI systems is growing massively
and is no longer negligible[32]; Bitcoin has a famously large energy consumption, and in
order to be profitable, mining often requires specialist hardware[106, 138]. 2.8
Analysis and Synthesis Mathematics gives us the logical and quantitative tools to • break down mathematics itself and much of the world surrounding us (cf. [108, 143]), • create new abstract, physical or digital items, systems or structures (cf. [108, 143]), • and to deploy new creations efficiently by amplifying their reach (cf. [29, 97]). Examples:
The effectiveness and amplification power of mathematics in Big Data has
led to a new form of capitalism, nowadays often called “Surveillance Capitalism”[151];
modern science would not be possible without modern mathematics and the effectiveness
of mathematical forms of reasoning, quantification and argumentation (cf. [143]). 2.9
Damage or Harm Mathematics can be accidentally, willfully or unknowingly used to do damage or harm.
Such actions may include • abusing the power and standing of mathematical arguments (cf. [27]), • the act of learning mathematics (cf. [43]), • and preventing it may require a holistic look at the target of the mathematics, its
social context and a mathematician’s training (cf. [97]). Examples: Due to being hard to challenge, statistical reasoning has misled judges to
order false convictions[105], potentially requiring professional bodies to stand up and
take a position in public (the Royal Statistical Society issued a public statement in the
case of R v Sally Clark [131]); the misuses and abuses of mathematics are almost never-
ending, and thus can be called “weapons of maths destruction” endangering our modern
democratic societies[107]; AI systems can learn to be racist or antisemitic from biased
data or harmful user interactions[60]; the potential for misuse and harm has led to many
calls for Hippocratic oaths (for an overview, see [97]). 6 2.10
Education Mathematics education is deeply connected to the organisation and structure of modern
societies. At all levels, mathematics education • is not blind to existing societal problems, such as issues of racial discrimination,
equity and fairness (cf. [15, 30, 47, 81, 83, 84]), • can promote or reduce a student’s self-worth, anxiety and feelings of accomplishment
(cf. [22, 77, 110]), • can be hindered or fostered by one’s attitude towards mathematics (cf. [103]). Examples: The discourse surrounding mathematics education may include discussions
about equity, social responsibility and specialised ethics frameworks (e.g. [122]); some
have called good mathematics education a civil right because of its necessity for people
to function in and successfully navigate modern society[20, 96]. 2.11
Punishment and Justice Mathematics can be used to falsely or correctly support, execute or promote punishment • in the legal system through correct or incorrect mathematical arguments (cf. [80]), • in education through the use of problematic educational philosophies (cf. [43, 122]), • or in other institutional settings through the implementation of problematic perfor-
mance metrics or other quantitative indicators (cf. [98]). Examples: Misleading statistics have led to unjustified convictions[105]; the use of pre-
dictive algorithms for A-level grades created a public outcry due to its potential for
injustices[62]; the overuse of metric-based decision-making can be punishing to those
who are not adequately captured by them[98]. 2.12
Control We can use mathematics to exert control over biological, technical, digital or physical
systems, social groups or institutions. In doing so, we • have created the matured mathematical area of control theory that developed many
of its modern foundations in the political circumstances of war (cf. [114]), • may use mathematics to help organise, classify or protect certain aspects or groups
of our modern societies (cf. [46, 54, 90]), • have used, misused and abused it in various situations (cf. [144]). Examples: The transport problem can be used to create and destroy rail networks[125];
the form, act and results of grading students may be impacted by a student’s socio-
economic background and social capital(cf. [63]); the Gaussian copula, used in financial
models that estimated and attempted to control risks, played a fundamental role in the
global financial crisis of 2008[82, 124]. 2.13
Self-Image The existence and use of modern mathematics, and its deep roots in the Greco-Roman
tradition of thought, impact the self-image of 7 • all people through its dominance in modern education and everyday life (cf. [43]), • all our institutions through the promotion of quantifiable decision-making and other
rational forms of reasoning (cf. [98, 133, 141], • and other societies worldwide throughout history and in the present (cf. [34, 35]). Examples: There is a heated debate about whether one must, should or can decolonise
mathematics (e.g.[14, 53, 116]). These debates, as well as similarly situated discourses on
ethics in mathematics and mathematics for social justice, regularly challenge or defend the
potential universality of mathematics and other self-understandings of the discipline[100,
p. 8]. 2.14
Economics Building on its solid foundations and modes of reasoning, mathematics has impacted
modern capitalist societies at various levels, e.g. mathematics • has revolutionised economics helping it become a quantitative rather than purely
social science (cf. [126]) • is the cornerstone of many of the organisational principles of modern economies and
capitalist dynamics (cf. [7]), • can be named a production factor of the modern digital economies, right next to
land, labour, capital and entrepreneur (cf. [59]). Examples: The introduction of utility functions has led to a “marginal revolution”
in economics[95], clashing with approaches focusing on the social psychology of human
behaviour (cf. [3, 48]), which can see economics as a predominantly social and not math-
ematical science which is (thus) unable to always adequately quantify human decision
making; standard measures of economics (e.g. the GPD[41]) are used to drive policy,
even though they regularly fail to capture social or environmental well-being; the effi-
ciency and amplification potential of mathematical digital products and services have
lead to a form of “surveillance capitalism”[151]. 2.15
History The history of mathematics is in some aspects very similar to the history of other natural
sciences, i.e. • it is a history of many different forms and cultures engaging in mathematics (cf.
[17, 34, 35]), • it cannot be written as a history of linear, constant progress (cf. [40, 57]), • is full of different reasons for doing mathematics (cf. [21]). Examples: The study of ethnomathematics has led to new challenges in the writing of
histories of mathematics (cf. [35]); geometry was done for different reasons, with different
methods, and different intentions throughout the ages[21]; the 19th century may have
been a century of anxiety for many mathematicians[57]. 8 2.16
Manipulation Rational quantification through mathematics regularly sets the standard for proper rea-
soning about the world (cf. [39, 115]). In this context, mathematics can be used • to efficiently and effectively manipulate the physical world (cf. [134, 143]), • to (in)effectively manipulate the social world using potentially unfalsifiable quanti-
tative theories and models (cf. [87, 134, 136]), • and to manipulate the world of socio-technical systems by embedding one’s politics
into them (cf. [93, 134]). Examples: Modern physics and engineering would not be possible without modern math-
ematics, and it is “unreasonably effective”[143]; the machine learning models behind social
media’s ranking algorithms can promote certain beauty standards and thus inadvertently
manipulate viewers and content producers(e.g. [10]); micro-targeting as part of political
advertising, potentially crossing the line between persuasion and manipulation[8, p. 86]. 3
Mathematical Artifacts Have Politics In the previous section, we provided examples of how mathematical artifacts can have a
political dimension, thereby establishing the existence of some mathematical artifacts that
have politics. This section will argue that all mathematical artifacts have politics, just
like Langdon Winner[147] argued that all technical artifacts have politics. This has also
been asserted in the recent Manifesto for the Responsible Development for Mathematical
Works, albeit without a complete argument[29]. Of course, any such argument must lie
on certain assumptions. Ours builds on two. We take as given the lessons from critical
theory (cf. [12]), i.e. • Politics is any action involving or affecting you or someone else directly or indirectly,
i.e. them as a person or their values, norms, wishes or desires. • Every activity which is exercised by a human or by a machine is political. Any
activity always affects or is affected by you or others, as it is supported or opposed
by you or others or it is related to your or someone else’s values, norms, wishes or
desires. We understand this as morally necessary, since otherwise there exist moral vacuums in
which people can commit harm without the moral judgement of themselves or others.
Implicit in this moral necessity is a form of Kant’s categorical imperative, i.e.
every
individual must not solely be treated as a means to an end but always be treated as an
end in itself[71]. Why is this relevant for mathematics? A unique freedom governs mathematical practice.
The structure of a mathematical argument might look inevitable, and every line logically
follows from its predecessor, yet it is full of human decisions and choices[101]. These
decisions reflect the mathematician’s or someone else’s1 desires, needs, wishes or norms.
As such, every mathematical decision or choice is political and contains deep normative
and political questions (this is especially true about foundational mathematical decisions, 1These can include your students, colleagues, your funder, manager, your users, and many other
affected parties, see also [29]. 9 see [139]). Consequently, every mathematical artifact, i.e. the product of a human or
machine performing a mathematical task, has politics. Yes, this means that “2+3=5” is a mathematical artifact that has politics. Without fur-
ther context, it does not imply that it is good or bad, but it is not politically neutral. The
politics of such a statement include, but are not limited to, the use of widely accepted
notations, the use of a widely accepted numbering and counting system, as well as the
normative component of writing down a true equation within that system instead of writ-
ing it down in alternative systems found throughout history or in indigenous communities
around the world[109]. Even simple acts like doing arithmetic or counting are social and
political[9]. Just like the (natural) sciences’ success and progress are intimately connected
to its norms and standards[130, p. 8], so is that of mathematics. The meaning and context
of what even simple equations like “2+3=5” stand for are not politically neutral. Thus, all mathematical artifacts are material actors in our world (cf. [76]) that have
politics beyond the politics of their maker. They can promote or restrict certain actions,
styles of thought or modes of reasoning, and what feels natural to us (e.g., the construction
of natural numbers using Peano’s axioms[127]), might not be all too natural for someone
who did not grow up with our heavily axiomatised form of arithmetic using Hindu-Arabic
numerals. Consider, as another example, the Neutron Diffusion Equation governing the diffusion
of neutrons in different materials: ∂Φ ∂t = σ −1 τ
Φ + λ2 3τ ∇2Φ where Φ(x, t) is the (free) neutron density, σ is the average number of neutrons re-
leased in a fission event, τ is the average time between fission events, and λ is the
average distance a neutron travels before being absorbed by a nucleus. This equa-
tion, and its mathematical solution showing that the mass of Uranium needed for an
atomic bomb is a feasibly obtainable quantity, is an example where knowledge of the
existence had huge political influence before anything remotely similar to a bomb was
even built with it. In a very real sense, it was enough to shift the war effort during
World War 2[128]. The equation had ambition deeply embedded into it, giving us
both the atomic bomb and nuclear power plants, but its politics go beyond dual-use
issues of research with its varying forms of political and ethical interpretations and
points of view. The definition of politics also means that the decision to create a mathematical output
without obvious applications is deeply political as it follows someone’s values, norms,
wishes or desires; depending on its motivation and the consciousness of the decision. In
particular, decisions not to be political (with or about one’s or another’s mathematics)
are always political. Once again, this does not imply that it is good or bad. It can be
either, depending on the specific circumstances of these decisions. But politically neutral,
it is not. Hence, in some sense, the statement that “mathematical artifacts have politics” is, at the
same time, extremely benign and radical. Unlike much of the modern political discourse that we are used to, the statement that
“mathematical artifacts have politics” neither claims the moral ground of good or bad, 10 nor tells you how to believe or act. The statement is thus fundamentally different from
many of the notions of politics that are found in the news, social media or other forms
of discourse. In essence, “mathematical artifacts have politics” does not mean they have
“one kind of politics”. Each artifact can come with a spectrum of politics that depends
on its wider social and historical circumstances. It tells us very little and a lot at the
same time. On the other hand, it is deeply radical since it pulls the rug out from under any argu-
ment that aims to foster a belief in the potential neutrality of mathematical artifacts. It
means that mathematical artifacts have power. “It is [not just] in laboratories that most
new sources of power are generated”[75, p. 163], but ever increasingly in mathematics
departments and in the minds of those using and doing mathematics. Its foundational
research doesn’t just regularly give us the foundations for entirely new fields, such as
computer science[38], that shape much of our current era, but its language and tools have
become the de-facto standard of most of what we understand of good (natural) science
and (institutional) organisation today. Mathematical artifacts are so deeply ingrained
into our everyday lives that one cannot escape them anymore, at least not if one wants to
participate in society. Viewed from this perspective, they possess the same mechanisms to
induce moral change that have already been explored for other technologies: they affect
the making of moral decisions, our potentiality for relationships with the world and our
modes of perception[36]. All of this now begs the question: When mathematical artifacts
have politics, what are the implications for the teaching and practice of mathematics? 4
Embedded Ethics in Mathematics We understand the aims of a good mathematical education in accordance with classical
humanistic values, i.e. the instruction of students in mathematical skills, strengthening
their abilities in rational and logical reasoning, widening their curiosity, providing a sense
of responsibility towards their own and other societies, people and nature, teaching them
the politics, and hence ethics, of artifacts and the necessary practices to wield the power
of modern mathematics for good. In essence, we want to teach them to be good people
doing good mathematics in the truest sense of the word. This means that students need to learn to navigate the world of politics and ethics within
mathematics in a gentle but intellectually stimulating way in order to foster these educa-
tional aims, to later avoid the common problem of moral overloading[135], and to be able
to function at various different levels of ethical engagement[25]. Such an approach includes
expanding the common definition of what good or bad mathematics actually means[100,
p. 3], showcasing the variety of what it can mean to be a mathematician[19], and deeply
embedding ethical training into mathematics by incorporating it into the teaching at all
stages of the curriculum, including appropriate mathematical exercises with a realistic
and relevant ethical dimension (by going beyond “prove this”- or “calculate that”-type of
questions and including the social, political or ethical context into them, e.g. [123]), in
order to normalise the experience of encountering such problems[26]. It is important to remember that “mathematicians [can be] more or less aware of philo-
sophical tensions. But, not being philosophers (or at least not very often), they do not
need to resolve these tensions — it’s enough for them to manage them by mediations
and analogies”[140, p. 19], and that “ethics eludes computation because ethics is social.
The concepts at the heart of ethics are not fixed or determinate in their precise meaning. 11 To be applied they must be interpreted, and interpretations vary among individuals and
groups, from context to context, and may change over time”[70, p. 33]. For approaches to
embedded ethics, this implies teaching enough to navigate the issues of ethics and politics
in current and future mathematical work, i.e. teaching some familiarity with philosophy
is important, but it probably should not overwrite the focus on normalising discussions
surrounding ethics, politics and usage, and providing the theoretical and practical knowl-
edge necessary to navigate their often murky waters. Such foundational approaches may
be particularly relevant in light of the large potential for denying social responsibility
within mathematics (e.g. [25, 44, 45]) and adjacent fields (e.g. [142]). These insights serve as the foundation for embedding ethics into mathematical curricula.
In sections 2 and 3 we saw the embedded politics of mathematics.
But to properly
transfer this into mathematical education, it needs to be combined with a knowledge of
the different levels of ethical awareness that professional mathematicians and students
may have[25], as well as a description or definition of the process of doing mathematics
(one such description can be found in the pillars of the Manifesto for the Responsible
Development of Mathematical Works[29]). The combination of all three provides a usable
way to design new mathematical exercises. We summarise this in Figure 2. Figure 2: Elements of Embedded Ethics in Mathematics (based on section 2 and [25, 29]) At this point, we must note that a more hands-off approach, such as merely introducing a
code of conduct or, indeed, a “Hippocratic oath for mathematicians” would be insufficient
to accomplish these educational goals as there is no system, infrastructure and training to
support (or enforce) any student or professional mathematician in their adherence to such
a code [97, 117]. It is important that any attempt to embed ethics into the mathematical
curricula at universities must not just attempt to cover many of the political dimensions,
but also should cover the entire process of doing mathematics in a social context (e.g.
the 10 pillars outlined in the Manifesto for the Responsible Development of Mathematical
Works[29] and the different potential levels of ethical engagement[25]). Only then can
students see the politics of their mathematics in action. 12 While this task might appear daunting at first, it essentially reduces to finding the ethical
or political dimensions of one’s own areas of mathematical expertise if everyone is on
board. Similar approaches are actively tested with good results by many universities in the
subjects of computer science and artificial intelligence[13, 31, 58, 88, 89]. It is important
to remember that no single lecturer needs to design questions that cover everything, but
throughout their studies, students should see as many different aspects as possible. This
will automatically be assured if each lecturer covers their own courses with the help and
input of others (e.g. philosophers, ethicists, practitioners, etc). The Embedded EthiCS
programme at Harvard has set up a special seminar for lecturers to learn and discuss
the setting of problems.
Their website also includes a link to questions for many of
the standard CS modules2. The Cambridge University Ethics in Mathematics Project
also provides useful resources for those who consider implementing embedded ethics into
their programme. Their resources include exercises and additional material specifically
designed for mathematicians3. 4.1
From Theory to Practice: Example Questions We now briefly show what exercises exploring the politics of mathematical artifacts could
look like. These will be exemplary in that they show that one can construct interesting and
challenging mathematical exercises that balance the mathematical difficulty and political
aspects without neglecting either side. The fact that mathematical artifacts have politics
does not harm mathematics, and indeed, acknowledging it helps us to create and do better
mathematics. To construct these exercises, we used the insights from Figure 2 to cover at
least one political dimension, one or more levels of ethical awareness, and specific pillars
from the lifecycle of mathematical work. Differential Equations (Applied): A detective arrives at the scene of a crime
at 5:00pm. They find a warm cup of tea and measure its temperature at 40°C. By
5:30pm the tea’s temperature has reduced to 30°C. 1. The police approach you with this data and ask you when the tea was likely
made. Briefly discuss any questions that you still need to ask the police officers
and their potential ethical relevance. What are potential barriers of communi-
cation? 2. The police are unable to provide you with more information but they ask you to
give an estimate based on idealised conditions and a constant room temperature
of 20°C. Giving all mathematical details and assumptions, use Newton’s law of
cooling to estimate when the tea was likely made. This question finds its origins in the first-year differential equations course of the Math-
ematical Tripos at the University of Cambridge. We adjusted it to include some of the
elements of embedded ethics, including • communication and deciding whether to begin (i.e. understanding mathematical
assumptions and limitations of the available information), • level 1 (realising there is ethics in mathematics), • as well as the political dimension of judgement, prediction and punishment. 2Embedded EthiCS: Module Repository https://embeddedethics.seas.harvard.edu/module.
3Cambridge University Ethics in Mathematics Project: https://www.ethics.maths.cam.ac.uk/. 13 Differential Equations (Foundations): This question will ask you to explore the
concept of dual-use within the study of differential equations. “Dual Use Research is defined as research conducted for legitimate purposes that gen-
erates knowledge, information, technologies, and/or products that could be utilised
for both benevolent and harmful purposes.” See: http://www.bu.edu/research/ethics-
compliance/safety/biological-safety/ibc/dual-use-research-of-concern. 1. How does dual use come up in the study of differential equations? 2. Do you know a differential equation that can be applied in a benevolent and
harmful way? (Hint: Consider differential equations and their applications from
your lectures. Can you apply some of them somewhere else?) This question teaches students to explore the use cases of their theoretical mathematics.
Students should learn the power of models and abstractions, and to experience that from
this power can also come harmful use. It gently explores • level 1 (realising there is ethics in mathematics), • level 4 (calling out harmful mathematics), • the politics of discovery, harm, analysis and synthesis, • and leads them to think about the ethics of communication, problem formulation
and abstraction. Analysis (Foundations): This question asks you to consider what is natural about
the natural numbers N. Briefly recall the Peano axioms which we have used in our
lectures to construct N. Let N be a set satisfying 1. N contains a special element which we call 1. 2. There exists a bijective map σ : N →N \ {1}. 3. For every subset S ⊂N such that 1 ∈S and if n ∈S, then σ(n) ∈S, it follows
that S = N. Which (if any) of (a), (b), and (c) feel natural to you, and why? Now consider that
there are indigenous tribes who count differently. Some only have the conceptual
language for the first few numbers and then use “many” for every larger set, e.g.,
they’d count 1, 2, 3, 4, many. Which of (a), (b) and (c) would feel natural to them?
Do you see why we call it “Peano’s axioms”, and not “Peano’s Theorem”? You can find a brief discussion of this phenomenon here: Butterworth, B. (Oct 21,
2004).
What happens when you can’t count past four?.
The Guardian.
https:
//www.theguardian.com/education/2004/oct/21/research.highereducation1. This question is designed to make students aware that what seems natural to them might
not be natural to someone else. It uses recent research from ethnomathematics to show
students that even the basics of what we perceive as pure mathematics and counting were
constructed in a social context. This exercise also teaches students that Peano’s axioms
don’t work for a finite set of numbers. It gently explores • level 1 (realising there is ethics in mathematics), • the politics of discovery, history and the self-image of mathematics, 14 • the role and impact of foundational axioms on how we see the world, • and respect for and communication with other cultures. 5
Conclusion By providing a multi-step approach, starting with specific examples showcasing that math-
ematical artifacts can have politics and abstracting these examples into an argument that
all mathematical artifacts have politics, we constructed an applicable theory for embed-
ded ethics in mathematics. By arguing that all mathematical artifacts have politics, we
deduced the necessity to embed ethical training into mathematical curricula and showed
how these insights naturally lead to new mathematical exercises. We ended the paper by
briefly outlining the coverage needed for an embedded ethics curriculum in mathematics,
and why this seemingly impossible task is not impossible after all. In a sense, follow Imre
Lakatos, and let us all bring some well-reasoned heuristics, in this case of an ethical and
political nature, into the practice of mathematics. Indeed, let us try[74, p. 144], even
when it may appear impossible.",0
"We study lightlike submanifolds of indeﬁnite statistical manifolds. Contrary to
the classical theory of submanifolds of statistical manifolds, lightlike submanifolds of
indeﬁnite statistical manifolds need not to be statistical submanifold. Therefore we
obtain some conditions for a lightlike submanifold of indeﬁnite statistical manifolds to
be a lightlike statistical submanifold. We derive the expression of statistical sectional
curvature and ﬁnally obtain some conditions for the induced statistical Ricci tensor
on a lightlike submanifold of indeﬁnite statistical manifolds to be symmetric. 2010 Mathematics Subject Classiﬁcation: 53B05, 53B30, 53C40.
Keywords: Indeﬁnite statistical manifolds, lightlike submanifolds, statistical curvature
tensor, induced statistical Ricci tensor. 1
Introduction Information geometry uses tools of diﬀerential geometry to study statistical inference,
information loss, and estimation. In fact the set of normal distributions p(x; µ, σ) =
1
√ 2πσ
e−(x−µ)2 2σ2 ,
x ∈R, with (µ, σ) ∈R×(0, +∞), can be considered as a two-dimensional surface and the amount
of information between the distributions is measured by the endowed Riemannian metric,
the Fisher information metric. Then the family of normal distributions p(x; µ, σ) becomes
a space of constant negative curvature and any normal distribution can be visualized as
a point in the Poincare upper-half plane. Furthermore, the notion of statistical manifolds
was studied in terms of information geometry. Statistical manifolds [1] are inspired from
statistical model, where the density function, the Fisher information matrix, the skewness
tensor (which measures the cummulants of third order), the dual connections ∇(−1) and
∇(1) are replaced by an arbitrary Riemannian manifold ˜
M, the Riemannian metric ˜
g of
˜
M, a 3-covariant skewness tensor, the dual connections ˜
∇and ˜
∇∗, respectively. Since the
geometry of statistical manifolds includes dual connections which are similar to the con-
jugate connections of the aﬃne geometry, therefore the geometry of statistical manifolds
is related to aﬃne diﬀerential geometry. The geometry of statistical manifolds has signiﬁ-
cant applications in various ﬁelds of science and engineering but very limited information 1 available.
To ﬁll up important missing parts in the general theory of submanifolds, Duggal and
Bejancu [4] introduced the notion of lightlike submanifolds of semi-Riemannian manifolds
and further developed by many others, see [6] and many references therein. The geome-
try of lightlike submanifolds has extensive uses in mathematical physics, particularly, in
general theory of relativity. Moreover, the theory of lightlike submanifolds has interaction
with some results on Killing horizon, electromagnetic and radiation ﬁelds and asymptot-
ically ﬂat spacetimes. Therefore the study of lightlike submanifolds is an active area of
study in the geometry of submanifolds.
Although the notion of lightlike submanifolds of semi-Riemannian manifolds is well
known, the one for indeﬁnite statistical manifolds is not yet established. In this paper, our
aim is to establish the theory of lightlike submanifolds of indeﬁnite statistical manifolds.
We obtain some conditions for a lightlike submanifold of indeﬁnite statistical manifolds
to be a lightlike statistical submanifold. We derive the expression of statistical sectional
curvature and ﬁnally obtain some conditions for the induced statistical Ricci tensor on a
lightlike submanifold of indeﬁnite statistical manifolds to be symmetric. 2
Lightlike submanifolds In this paper, we consider smooth manifolds and Γ(T ¯
M), Γ(T ¯
M(p,q)) means the set of all
vector ﬁelds and the set of all tensor ﬁelds of the type (p, q) on the smooth manifold ¯
M,
respectively.
Let ( ¯
M, ¯
g) be a real (m + n)-dimensional semi-Riemannian manifold of constant index
q such that m, n ≥1, 1 ≤q ≤m + n −1 and (M, g) be an m-dimensional submanifold of
¯
M and g be the induced metric of ¯
g on M. If ¯
g is degenerate on the tangent bundle TM
of M then M is called a lightlike submanifold of ¯
M. For a degenerate metric g on M,
TxM⊥is a degenerate n-dimensional subspace of Tx ¯
M. Thus, both TxM and TxM⊥are
degenerate orthogonal subspaces but no longer complementary. In this case, there exists
a subspace Rad(TxM) = TxM ∩TxM⊥, known as radical (null) subspace. If the mapping
Rad(TM) : x ∈M −
→Rad(TxM), deﬁnes a smooth distribution on M of rank r > 0 then
submanifold M of ¯
M is called an r-lightlike submanifold and Rad(TM) is called the radical
distribution on M.
Screen distribution S(TM) is a semi-Riemannian complementary
distribution of Rad(TM) in TM, that is, TM = Rad(TM)⊥S(TM). Let S(TM⊥) be
a complementary vector subbundle to Rad(TM) in TM⊥which is also non-degenerate
with respect to ¯
g. Let tr(TM) be complementary (but not orthogonal) vector bundle to
TM in T ¯
M |M then tr(TM) = ltr(TM)⊥S(TM⊥), where ltr(TM) is complementary
to Rad(TM) in S(TM⊥)⊥and is an arbitrary lightlike transversal vector bundle of M.
Thus we have T ¯
M |M= TM ⊕tr(TM) = (Rad(TM) ⊕ltr(TM))⊥S(TM)⊥S(TM⊥), (for
detail see [4]). Let U be a local coordinate neighborhood of M then local quasi-orthonormal
ﬁeld of frames on ¯
M along M is {ξ1, ..., ξr, Xr+1, ..., Xm, N1, ..., Nr, Wr+1, ..., Wn}, where
{ξi}r
i=1 and {Ni}r
i=1 are lightlike basis of Γ(Rad(TM)|U) and Γ(ltr(TM)|U), respectively
and {Xα}m
α=r+1 and {Wa}n
a=r+1 are orthonormal basis of Γ(S(TM)|U) and Γ(S(TM⊥)|U),
respectively. These local quasi-orthonormal ﬁeld of frames on ¯
M satisfy ¯
g(Ni, ξj) = δi
j,
¯
g(Ni, Nj) = ¯
g(Ni, Xα) = ¯
g(Ni, Wa) = 0. 2 Let ˜
∇be the Levi-Civita connection on ¯
M, then Gauss and Weingarten formulae are (1)
˜
∇XY = ∇XY + h(X, Y ),
˜
∇XU = −AUX + ∇t
XU, for X, Y ∈Γ(TM) and U ∈Γ(tr(TM)), where {∇XY, AUX} and {h(X, Y ), ∇t
XU} belongs
to Γ(TM) and Γ(tr(TM)), respectively. Here ∇is a torsion-free linear connection on M,
h is a symmetric bilinear form on TM which is called the second fundamental form, AU
is a linear operator on M and known as the shape operator. Considering the projection
morphisms L and S of tr(TM) on ltr(TM) and S(TM⊥), respectively, then (1) becomes (2)
˜
∇XY = ∇XY + hl(X, Y ) + hs(X, Y ),
˜
∇XU = −AUX + Dl
XU + Ds
XU, where hl(X, Y ) = L(h(X, Y )), hs(X, Y ) = S(h(X, Y )), Dl
XU = L(∇⊥
XU), Ds
XU =
S(∇⊥
XU).
As hl and hs are Γ(ltr(TM))-valued and Γ(S(TM⊥))-valued, respectively,
therefore they are called as the lightlike second fundamental form and the screen second
fundamental form on M. In particular, we have (3)
˜
∇XN = −ANX + ∇l
XN + Ds(X, N),
˜
∇XW = −AWX + ∇s
XW + Dl(X, W), where X ∈Γ(TM), N ∈Γ(ltr(TM)) and W ∈Γ(S(TM⊥)). Then using (2) and (3), we
have (4)
¯
g(hs(X, Y ), W) + ¯
g(Y, Dl(X, W)) = g(AW X, Y ). Let P be the projection morphism of TM on S(TM) then (5)
∇XPY = ∇
′
XPY + h
′(X, PY ),
∇Xξ = −A
′
ξX + ∇
′t
Xξ, where {∇
′
XPY, A
′
ξX} and {h
′(X, Y ), ∇
′t
Xξ} belongs to Γ(S(TM)) and Γ(Rad(TM)), re-
spectively. ∇
′ and ∇
′t are linear connections on S(TM) and Rad(TM), respectively. h
′ and A
′ are Γ(Rad(TM))-valued and Γ(S(TM))-valued bilinear forms and they are called
as the second fundamental forms of distributions S(TM) and Rad(TM), respectively.
Using (2) and (5), we obtain (6)
¯
g(hl(X, PY ), ξ) = g(A
′
ξX, PY ),
¯
g(h
′(X, PY ), N) = g(ANX, PY ), for any X, Y ∈Γ(TM), ξ ∈Γ(Rad(TM)) and N ∈Γ(ltr(TM)).
From the geometry of non-degenerate submanifolds, it is known that the induced con-
nection ∇on a non-degenerate submanifold is always a metric connection. Unfortunately,
this is not true for lightlike submanifolds and particularly satisﬁes (7)
(∇Xg)(Y, Z) = ¯
g(hl(X, Y ), Z) + ¯
g(hl(X, Z), Y ), for any X, Y, Z ∈Γ(TM). 3 3
Lightlike Submanifolds of Indeﬁnite Statistical Manifolds Let ( ¯
M, ¯
g) be a semi-Riemannian manifold equipped with a semi-Riemannian metric ¯
g of
constant index q and ¯
∇be an aﬃne torsion free connection on ¯
M. A pair ( ¯
∇, ¯
g) is called
a statistical structure on ¯
M if ¯
∇is torsion free and the Codazzi equation (8)
( ¯
∇X¯
g)(Y, Z) = ( ¯
∇Y ¯
g)(X, Z), holds for any vector ﬁelds X, Y and Z of
¯
M.
If ( ¯
∇, ¯
g) is a statistical structure on a
semi-Riemannian manifold ¯
M then the triplet ( ¯
M, ¯
g, ¯
∇) is called an indeﬁnite statistical
manifold. The aﬃne connection ¯
∇∗which is also assumed to be torsion free on ( ¯
M, ¯
g, ¯
∇)
is called the dual connection of ¯
∇with respect to ¯
g if it satisﬁes (9)
X¯
g(Y, Z) = ¯
g( ¯
∇XY, Z) + ¯
g(Y, ¯
∇∗
XZ), for any vector ﬁelds X, Y and Z of ¯
M. If ( ¯
∇, ¯
g) is a statistical structure on ¯
M then so
is ( ¯
∇∗, ¯
g) and furthermore ( ¯
∇∗)∗= ¯
∇. Therefore now onwards we denote an indeﬁnite
statistical manifold by ( ¯
M, ¯
g, ¯
∇, ¯
∇∗). Let ¯
∇¯
g be the Levi-Civita connection of ¯
g then we
have ¯
∇¯
g = 1 2( ¯
∇+ ¯
∇∗). Moreover from (8), it is clear that a semi-Riemannian manifold is
always a statistical manifold and ( ¯
∇¯
g)∗= ¯
∇¯
g.
In fact the notion of statistical structure comes from information geometry, for details
see [2]. Let p(·, θ) : (χ, dx) →(0, ∞) be the probability density parameterized by θ =
(θ1, . . . , θn) ∈Θ ⊂Rn. Then for any α ∈R, we have gθ =
X n Z χ ∂log p ∂θi
(x, θ)∂log p ∂θj
(x, θ)p(x, θ)dx
o
dθidθj, and Γ(α)
ijk(θ) =
Z χ n∂2 log p ∂θi∂θi (x, θ) + 1 −α 2
∂log p ∂θi
(x, θ)∂log p ∂θj
(x, θ)
o∂log p ∂θk (x, θ)p(x, θ)dx. It is easy to see that gθ is a positive semi-deﬁnite quadratic form on TθΘ. If g is a Rie-
mannian metric on Θ then (Θ, ∇(α), g) is a statistical manifold, where ∇(α) is an aﬃne
connection, known as the Amari’s α-connection with respect to {p(·, θ)|θ ∈Θ} and deﬁned
by Γ(α)
ijk = g(∇(α)
∂ ∂θi ∂ ∂θj ,
∂ ∂θk ) Let ( ¯
∇, ¯
g) be a statistical structure on ¯
M then the diﬀerence tensor ﬁeld K ∈Γ(T ¯
M(1,2))
is given by (see [7]) (10)
K(X, Y ) = ¯
∇XY −¯
∇¯
g
XY, for any X, Y ∈Γ(T ¯
M) and the diﬀerence tensor ﬁeld satisﬁes (11)
K(X, Y ) = K(Y, X),
¯
g(K(X, Y ), Z) = ¯
g(Y, K(X, Z)). Conversely, for a Riemannian metric ¯
g if the given K ∈Γ(T ¯
M(1,2)) satisﬁes (11) then a
pair ( ¯
∇= ¯
∇¯
g + K, ¯
g) becomes a statistical structure on ¯
M and also (12)
K = ¯
∇−¯
∇¯
g = 1 2( ¯
∇−¯
∇∗). 4 Theorem 3.1. Let ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) be an indeﬁnite statistical manifold and ¯
∇∗is a dual
connection of ¯
∇with respect to the metric ¯
g. Then (13)
( ¯
∇X¯
g)(Y, Z) + ( ¯
∇∗
X¯
g)(Y, Z) = 0, for any X, Y ∈Γ(T ¯
M). Proof. Let X, Y ∈Γ(T ¯
M) then using (10), we obtain ( ¯
∇X¯
g)(Y, Z) = ( ¯
∇¯
g
X¯
g)(Y, Z) −¯
g(K(X, Y ), Z) −¯
g(Y, K(X, Z), then using the fact that ¯
∇¯
g is a metric connection with (11), we get (14)
( ¯
∇X¯
g)(Y, Z) = −2¯
g(K(X, Y ), Z). Analogously using (11) and (12), we also obtain (15)
( ¯
∇X¯
g)(Y, Z) = ( ¯
∇∗
X¯
g)(Y, Z) −4¯
g(K(X, Y ), Z), then using (14) in (15), we have (16)
( ¯
∇∗
X¯
g)(Y, Z) = 2¯
g(K(X, Y ), Z). Hence from (14) and (16), the assertion follows. Let (M, g) be a lightlike submanifold of an indeﬁnite statistical manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗)
and let ∇, ∇∗be the induced linear connections on M from the connections ¯
∇, ¯
∇∗,
respectively. Then using geometry of lightlike submanifolds of semi-Riemannian manifolds,
the Gauss and Weingarten formulas for a lightlike submanifold of an indeﬁnite statistical
manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) are given by (17)
¯
∇XY = ∇XY + hl(X, Y ) + hs(X, Y ),
¯
∇∗
XY = ∇∗
XY + h∗l(X, Y ) + h∗s(X, Y ), (18)
¯
∇XN = −ANX + ∇l
XN + Ds(X, N),
¯
∇∗
XN = −A∗
NX + ∇∗l
XN + D∗s(X, N), (19)
¯
∇XW = −AWX + ∇s
XW + Dl(X, W),
¯
∇∗
XW = −A∗
W X + ∇∗s
X W + D∗l(X, W), for any X, Y ∈Γ(TM), N ∈Γ(ltr(TM)) and W ∈Γ(S(TM⊥)). Let P be the projection
morphism of TM on S(TM) then following (5), we have (20)
∇XPY = ∇
′
XPY + h
′(X, PY ),
∇∗
XPY = ∇∗′
XPY + h∗′(X, PY ), (21)
∇Xξ = −A
′
ξX + ∇
′t
Xξ,
∇∗
Xξ = −A∗′
ξ X + ∇∗′t
X ξ. Since ¯
∇is a torsion free aﬃne connection therefore using (17) in ¯
∇XY −¯
∇Y X −[X, Y ] = 0
and then on equating the tangential components, it follows that the induced connection
∇is a torsion free linear connection and analogously ∇∗is also a torsion free linear
connection. 5 Lemma 3.2. Let (M, g) be a lightlike submanifold of an indeﬁnite statistical manifold
( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then we have (22)
(∇Xg)(Y, Z) −(∇Y g)(X, Z) = ¯
g(Y, hl(X, Z)) −¯
g(X, hl(Y, Z)), (23)
g(AW X, Y ) −g(X, AW Y ) = ¯
g(Dl(X, W), Y ) −¯
g(X, Dl(Y, W)), (24)
¯
g(hl(X, ξ), Y ) −¯
g(hl(Y, ξ), X) = g(X, ∇Y ξ) −g(∇Xξ, Y ), (25)
(∇l
X¯
g)(Y, N) −(∇l
Y ¯
g)(X, N) = g(ANY, X) −g(ANX, Y ), for any X, Y, Z ∈Γ(TM), ξ ∈Γ(Rad(TM)), N ∈Γ(ltr(TM)) and W ∈Γ(S(TM⊥)),
where (∇l
X¯
g)(Y, N) = X¯
g(Y, N) −¯
g(∇XY, N) −¯
g(Y, ∇l
XN). Proof. By straightforward calculations using (17) to (19) in (8), the Lemma follows. Analogously using the fact that ( ¯
∇∗, ¯
g) is also a statistical structure, we have the following
lemma immediately. Lemma 3.3. Let (M, g) be a lightlike submanifold of an indeﬁnite statistical manifold
( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then we have (22*)
(∇∗
Xg)(Y, Z) −(∇∗
Y g)(X, Z) = ¯
g(Y, h∗l(X, Z)) −¯
g(X, h∗l(Y, Z)), (23*)
g(A∗
W X, Y ) −g(X, A∗
W Y ) = ¯
g(D∗l(X, W), Y ) −¯
g(X, D∗l(Y, W)), (24*)
¯
g(h∗l(X, ξ), Y ) −¯
g(h∗l(Y, ξ), X) = g(X, ∇∗
Y ξ) −g(∇∗
Xξ, Y ), (25*)
(∇∗l
X¯
g)(Y, N) −(∇∗l
Y ¯
g)(X, N) = g(A∗
NY, X) −g(A∗
NX, Y ), for any X, Y, Z ∈Γ(TM), ξ ∈Γ(Rad(TM)), N ∈Γ(ltr(TM)) and W ∈Γ(S(TM⊥)),
where (∇∗l
X¯
g)(Y, N) = X¯
g(Y, N) −¯
g(∇∗
XY, N) −¯
g(Y, ∇∗l
XN). Remark 1. The equation (22*) is consider as the dual of (22) and similar assumptions
for other equations. Now onwards we omit the dual equation and put ∗to the number of
original equation, if needed. Lemma 3.4. Let (M, g) be a lightlike submanifold of an indeﬁnite statistical manifold
( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then we have (26)
¯
g(hl(X, Y ), ξ) + ¯
g(Y, h∗l(X, ξ)) + g(Y, ∇∗
Xξ) = 0, (27)
¯
g(hs(X, Y ), W) + ¯
g(D∗l(X, W), Y ) = g(A∗
W X, Y ), (28)
¯
g(∇XY, N) + ¯
g(Y, ∇∗l
XN) = X¯
g(Y, N) + g(Y, A∗
NX), (29)
Xg(Y, Z) = g(∇XY, Z) + g(Y, ∇∗
XZ) + ¯
g(hl(X, Y ), Z) + ¯
g(Y, h∗l(X, Z)), for any X, Y, Z ∈Γ(TM), ξ ∈Γ(Rad(TM)), N ∈Γ(ltr(TM)) and W ∈Γ(S(TM⊥)). 6 Proof. Lemma follows by using (17) to (19) in (9). Remark 2. Particularly let X, Y ∈Γ(S(TM)) then from (23), we have g(AW X, Y ) =
g(X, AW Y ), implies that AW is a self-adjoint operator on S(TM). From (22), (22*) and (29), it is obvious that the lightlike submanifolds (M, g, ∇) and
(M, g, ∇∗) of an indeﬁnite statistical manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) are not lightlike statistical
submanifolds. Hence we have the following observation from (22), (22*) and (29) imme-
diately. Theorem 3.5. Let ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) be an indeﬁnite statistical manifold. If hl and h∗l vanish
identically then lightlike submanifolds (M, g, ∇) and (M, g, ∇∗) become lightlike statistical
submanifolds of indeﬁnite statistical manifold ¯
M and ∇∗becomes a dual connection of ∇
with respect to the induced metric g. Theorem 3.6. Let ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) be an indeﬁnite statistical manifold such that ∇∗is a
dual connection of ∇with respect to the induced metric g. Then (M, g, ∇) is a lightlike
statistical submanifold of ¯
M if and only if (M, g, ∇∗) is a lightlike statistical submanifold
of ¯
M. Proof. Suppose (M, g, ∇) is a lightlike statistical submanifold of an indeﬁnite statistical
manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then from (22), it follows that ¯
g(Y, hl(X, Z)) −¯
g(X, hl(Y, Z)) = 0,
for any X, Y, Z ∈Γ(TM). Further using (9) and (17), we obtain 0
=
¯
g(Y, h∗l(X, Z)) −¯
g(X, h∗l(Y, Z)) + {Zg(X, Y ) −g(∇ZX, Y ) −g(X, ∇∗
ZY )} −{Zg(Y, X) −g(∇ZY, X) −g(Y, ∇∗
ZX)}. On using the hypothesis that ∇∗is a dual connection of ∇with respect to the induced
metric g in above expression, it gives ¯
g(Y, h∗l(X, Z))−¯
g(X, h∗l(Y, Z)) = 0 and hence from
(22*) it follows that (M, g, ∇∗) becomes a lightlike statistical submanifold of ¯
M. Similarly
we can prove the converse part. Recall that a vector ﬁeld X on a semi-Riemannian manifold ( ¯
M, ¯
g) is said to be a
Killing vector ﬁeld (inﬁnitesimal isometry) if (LX¯
g)(Y, Z) = 0, where (30)
(LX¯
g)(Y, Z) = X(¯
g(Y, Z)) −¯
g([X, Y ], Z) −¯
g(Y, [X, Z]), for any vector ﬁelds Y and Z on ¯
M. Further a distribution D on ¯
M is said to be a Killing
distribution if each vector ﬁeld belonging to D is a Killing vector ﬁeld. Let X, Y, Z ∈
Γ(TM) then using (17) in (30), we obtain (LX¯
g)(Y, Z) = (∇Xg)(Y, Z) + g(∇Y X, Z) + g(Y, ∇ZX). Interchange the role of X and Y in above equation and take Z = ξ, then on subtracting
the resulting equation from it, we derive (LX¯
g)(Y, ξ) −(LY ¯
g)(X, ξ) = g(∇ξX, Y ) −g(∇ξY, X). 7 Theorem 3.7. Let (M, g, ∇) be a lightlike statistical submanifold of an indeﬁnite statistical
manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗). The the following statements are equivalent: (a) The radical distribution Rad(TM) is a Killing distribution. (b) The radical distribution Rad(TM) is a parallel distribution with respect to the in-
duced connection ∇, that is, ∇Xξ ∈Γ(Rad(TM)), for any X ∈Γ(TM) and ξ ∈
Γ(Rad(TM)). (c) A
′
ξ vanishes of Γ(TM) for any ξ ∈Γ(Rad(TM)). Proof. Let (M, g, ∇) be a lightlike statistical submanifold of an indeﬁnite statistical man-
ifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then (31)
(∇Xg)(Y, Z) = (∇Y g)(X, Z), for any X, Y, Z ∈Γ(TM). Then from (22), we have ¯
g(Y, hl(X, Z)) = ¯
g(X, hl(Y, Z)) and
on taking Z = ξ it follows that ¯
g(hl(X, ξ), Y ) = ¯
g(hl(Y, ξ), X). Using this result in (24),
we further obtain (32)
g(∇Xξ, Y ) = g(∇Y ξ, X), for any X, Y ∈Γ(TM) and ξ ∈Γ(Rad(TM)).
Next for any X, Y ∈Γ(TM) and ξ ∈Γ(Rad(TM)), using (17) in (30), we derive
(Lξg)(X, Y ) = {ξg(X, Y ) −g(∇ξX, Y ) −g(X, ∇ξY )} + {g(∇Xξ, Y ) + g(∇Y ξ, X)} and
further using (31) and (32), it implies that (Lξg)(X, Y )
= (∇ξg)(X, Y ) + 2g(∇Xξ, Y ) = (∇Xg)(ξ, Y ) + 2g(∇Xξ, Y ) = g(∇Xξ, Y ).
(33) On using (21) in (33), it gives that (34)
(Lξg)(X, Y ) = g(∇Xξ, Y ) = −g(A
′
ξX, Y ). Thus the assertion follows from (34) Theorem 3.8. Let (M, g, ∇) be a lightlike statistical submanifold of an indeﬁnite statistical
manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗). The the following statements are equivalent: (a) The screen distribution S(TM) is integrable. (b) AN is self adjoint operator on S(TM) with respect to g. (c) h
′ is symmetric on S(TM). Proof. Let X, Y ∈Γ(S(TM)) and N ∈Γ(ltr(TM)) then from (8), we have ( ¯
∇X¯
g)(Y, N) =
( ¯
∇Y ¯
g)(X, N), using (17) and the fact that ∇is torsion free, it follows that (35)
¯
g([X, Y ], N) = g(ANX, Y ) −g(X, ANY ). 8 Further, since ∇is torsion free therefore for any X, Y ∈Γ(S(TM)) and N ∈Γ(ltr(TM)),
using (20), we have ¯
g([X, Y ], N) = ¯
g(∇XY, N) −¯
g(∇Y X, N) = ¯
g(h
′(X, Y ) −h
′(Y, X), N).
(36) Hence assertions follows from (35) and (36). Theorem 3.9. Let (M, g, ∇) be a lightlike statistical submanifold of an indeﬁnite statistical
manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗). Then the radical distribution Rad(TM) is always integrable. Proof. Let (M, g, ∇) be a lightlike statistical submanifold of an indeﬁnite statistical man-
ifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then for any ξ1, ξ2 ∈Γ(Rad(TM)) and X ∈Γ(S(TM)) from (31), we
obtain (∇ξ1g)(ξ2, X) = (∇ξ2g)(ξ1, X). This implies that g(∇ξ1ξ2, X) = g(∇ξ2ξ1, X), that
is, g([ξ1, ξ2], X) = 0 and hence the proof is complete. Let M be a lightlike submanifold of an indeﬁnite statistical manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗).
Let ¯
R, R, Rl and Rs be the curvature tensor ﬁelds of ¯
∇, ∇, ∇l and ∇s respectively. Then
using (17) to (19) for any X, Y, Z ∈Γ(TM), N ∈Γ(ltr(TM)) and W ∈Γ(S(TM⊥)), we
have the following observation (see also [4]): Theorem 3.10. Let M be a lightlike submanifold of an indeﬁnite statistical manifold
( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then following equations of Gauss, Codazzi and Ricci hold ( ¯
R(X, Y )Z)tangential
=
R(X, Y )Z + Ahl(X,Z)Y −Ahl(Y,Z)X + Ahs(X,Z)Y −Ahs(Y,Z)X,
(37) ( ¯
R(X, Y )Z)transversal
=
(∇Xhl)(Y, Z) −(∇Y hl)(X, Z) + (∇Xhs)(Y, Z) −(∇Y hs)(X, Z) + Dl(X, hs(Y, Z)) −Dl(Y, hs(X, Z)) +Ds(X, hl(Y, Z)) −Ds(Y, hl(X, Z)),
(38) ( ¯
R(X, Y )N)tangential
=
(∇Y A)(N, X) −(∇XA)(N, Y ) + ADs(X,N)Y −ADs(Y,N)X,
(39) ( ¯
R(X, Y )N)transversal
=
Rl(X, Y )N + hl(Y, ANX) −hl(X, ANY ) + hs(Y, ANX) −hs(X, ANY ) + (∇XDs)(Y, N) −(∇Y Ds)(X, N) +Dl(X, Ds(Y, N)) −Dl(Y, Ds(X, N)),
(40) ( ¯
R(X, Y )W)tangential
=
(∇Y A)(W, X) −(∇XA)(W, Y ) + ADl(X,W )Y −ADl(Y,W )X,
(41) ( ¯
R(X, Y )W)transversal
=
Rs(X, Y )W + hl(Y, AW X) −hl(X, AW Y ) + hs(Y, AW X) −hs(X, AW Y ) + (∇XDl)(Y, W) −(∇Y Dl)(X, W) +Ds(X, Dl(Y, W)) −Ds(Y, Dl(X, W)),
(42) 9 where (∇Xhl)(Y, Z) = ∇l
X(hl(Y, Z)) −hl(∇XY, Z) −hl(Y, ∇XZ), (∇Xhs)(Y, Z) = ∇s
X(hs(Y, Z)) −hs(∇XY, Z) −hs(Y, ∇XZ), (∇XA)(N, Y ) = ∇X(A(N, Y )) −A(∇l
XN, Y ) −A(N, ∇XY ), (∇XDl)(Y, W) = ∇l
X(Dl(Y, W)) −Dl(∇XY, W) −Dl(Y, ∇s
XW), (∇XA)(W, Y ) = ∇X(A(W, Y )) −A(∇s
XW, Y ) −A(W, ∇XY ), (∇XDs)(Y, N) = ∇s
X(Ds(Y, N)) −Ds(∇XY, N) −Ds(Y, ∇l
XN). Recall that a submanifold (M, g) of a Riemannian manifold ( ¯
M, ¯
g) is called totally
geodesic if any geodesic on the submanifold M with its induced Riemannian metric g is
also a geodesic on the Riemannian manifold ( ¯
M, ¯
g) and moreover M is totally geodesic
in ¯
M if and only if the second fundamental form on M vanishes identically. Therefore
Duggal and Jin [5] deﬁned that a lightlike submanifold M of a semi-Riemannian manifold
( ¯
M, ¯
g) with the Levi-civita connection ¯
∇is a totally geodesic lightlike submanifold if
hl(X, Y ) = hs(X, Y ) = 0, for all X, Y ∈Γ(TM). Now, let ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) be an indeﬁnite
statistical manifold with aﬃne connections ¯
∇and ¯
∇∗then a lightlike submanifold M of
¯
M is said to be ¯
∇-autoparallel (respectively, ¯
∇∗-autoparallel) if ( ¯
∇XY )p = (( ¯
∇|M)XY )p
(respectively, ( ¯
∇∗
XY )p = (( ¯
∇∗|M)XY )p, for any p ∈M and X, Y ∈Γ(TM), that is, if
hl(X, Y ) = hs(X, Y ) = 0 (respectively, h∗l(X, Y ) = h∗s(X, Y ) = 0), for all X, Y ∈Γ(TM).
The submanifold M is called dual-autoparallel if M is both ¯
∇- and ¯
∇∗-autoparallel, that
is, if hl(X, Y ) = h∗l(X, Y ) = 0 and hs(X, Y ) = h∗s(X, Y ) = 0, for all X, Y ∈Γ(TM), see
[3]. Remark 3. Let M be ¯
∇-autoparallel (respectively, ¯
∇∗-autoparallel) lightlike submanifold
of an indeﬁnite statistical ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then using (22) (respectively, (22*)), (M, ∇, g)
(respectively, (M, ∇∗, g)) becomes a lightlike statistical submanifold of ¯
M. Further, if M
is dual-autoparallel then using (22), (22*) and (29), both (M, ∇, g) and (M, ∇∗, g) become
lightlike statistical submanifolds of ¯
M and ∇∗becomes a dual connection of ∇with respect
to the induced metric g. Theorem 3.11. Let M be ¯
∇-autoparallel (respectively, ¯
∇∗-autoparallel) lightlike subman-
ifold of an indeﬁnite statistical ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) then ¯
R(X, Y )Z = R(X, Y )Z, (respectively,
¯
R∗(X, Y )Z = R∗(X, Y )Z), for any X, Y ∈Γ(TM). Using (9) for X, Y, Z, U ∈Γ(TM), we get ¯
g( ¯
∇Xhs(Y, Z), U) = −¯
g(hs(Y, Z), h∗s(X, U))
and ¯
g( ¯
∇∗
Y h∗s(X, U), Z) = −¯
g(hs(Y, Z), h∗s(X, U)) therefore it implies that (43)
¯
g( ¯
∇Xhs(Y, Z), U) = ¯
g( ¯
∇∗
Y h∗s(X, U), Z). Then from (43), we have the following observation. Lemma 3.12. Let (M, g) be a lightlike submanifold of an indeﬁnite statistical manifold
( ¯
M, ¯
g, ¯
∇, ¯
∇∗). If M is ¯
∇-autoparallel then h∗s is parallel with respect to ¯
∇∗and if M is
¯
∇∗-autoparallel then hs is parallel with respect to ¯
∇. Deﬁnition 3.13. A statistical structure ( ¯
∇, ¯
g) is said to be of constant curvature k ∈R if
¯
R(X, Y )Z = k{¯
g(Y, Z)X −¯
g(X, Z)Y } holds for any vector ﬁelds X, Y and Z on ¯
M. A
statistical structure ( ¯
∇, ¯
g) of constant curvature 0 is called a Hessian structure. 10 Let K ∈Γ(T ¯
M(1,2)) be the diﬀerence tensor ﬁeld for a statistical structure ( ¯
∇, ¯
g) then
Q = −¯
∇K ∈Γ(T ¯
M(1,3)) is called the Hessian curvature tensor [10].
If for c ∈R,
( ¯
∇XK)(Y, Z) = −c 2{¯
g(X, Y )Z + ¯
g(X, Z)Y }, for any X, Y, Z ∈Γ(T ¯
M) then the Hessian
structure is said to be of constant Hessian curvature c. Example 1. [7] Let (H, ˜
g) be the upper half space of constant curvature −1 then H = {y = (y1, . . . , yn+1) ∈Rn+1|yn+1 > 0}, ˜
g = (yn+1)−2
n+1
X A=1
dyAdyA. If an aﬃne connection ˜
∇on H given by ˜
∇
∂ ∂yn+1
∂ ∂yn+1 = (yn+1)−1
∂ ∂yn+1 ,
˜
∇
∂ ∂yi
∂ ∂yj = 2δij(yn+1)−1
∂ ∂yn+1 , ˜
∇
∂ ∂yi
∂ ∂yn+1 = ˜
∇
∂ ∂yn+1
∂ ∂yj = 0, where i, j = 1, . . . , n. Then (H, ˜
∇, ˜
g) becomes a Hessian manifold of constant Hessian
curvature 4.
Moreover (H, ˜
∇, ˜
g) expresses the statistical model of normal distribution
when dimH = 2 and the normal distribution with mean µ and variance σ2 is given by N(x, µ, σ2) =
1
√ 2πσ2 exp
n
−
1 2σ2 (x −µ)2o
, x, µ ∈R, σ > 0. Deﬁnition 3.14. ([8]) For a statistical manifold ( ¯
M, ¯
∇, ¯
g), the statistical curvature tensor
ﬁeld ¯
S ∈Γ(T ¯
M(1,3)) of ( ¯
M, ¯
∇, ¯
g) is given by (44)
¯
S(X, Y )Z = 1 2{ ¯
R(X, Y )Z + ¯
R∗(X, Y )Z}, for any X, Y, Z ∈Γ(T ¯
M). Further a statistical manifold ( ¯
M, ¯
∇, ¯
g) is said to be of constant
sectional curvature c ∈R if (45)
¯
S(X, Y )Z = c{¯
g(Y, Z)X −¯
g(X, Z)Y }, holds for any X, Y, Z ∈Γ(T ¯
M). The statistical curvature tensor ﬁeld S satisﬁes the following identities (46)
¯
g( ¯
S(U, Z)Y, X) = −¯
g( ¯
S(Z, U)Y, X),
¯
g( ¯
S(U, Z)Y, X) = −¯
g( ¯
S(U, Z)X, Y ), (47)
¯
g( ¯
S(X, Y )Z, U) = ¯
g( ¯
S(U, Z)Y, X),
¯
S(X, Y )Z + ¯
S(Y, Z)X + ¯
S(Z, X)Y = 0, for any X, Y, Z, U ∈Γ(T ¯
M).
Let S be the induced statistical curvature tensor ﬁeld induced on lightlike subman-
ifold M of an indeﬁnite statistical manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) and given by S(X, Y )Z =
1
2{R(X, Y )Z + R∗(X, Y )Z}, for any X, Y Z ∈Γ(TM).
Then using the expressions of 11 the Theorem 3.10 and their dual, we have the following expressions for the statistical
curvature tensor ﬁeld on ¯
M: 2( ¯
S(X, Y )Z)tangential
=
2S(X, Y )Z + Ahl(X,Z)Y −Ahl(Y,Z)X + Ahs(X,Z)Y −Ahs(Y,Z)X + A∗
h∗l(X,Z)Y −A∗
h∗l(Y,Z)X + A∗
h∗s(X,Z)Y −A∗
h∗s(Y,Z)X,
(48) 2( ¯
S(X, Y )Z)transversal
=
(∇Xhl)(Y, Z) −(∇Y hl)(X, Z) + (∇Xhs)(Y, Z) −(∇Y hs)(X, Z) + Dl(X, hs(Y, Z)) −Dl(Y, hs(X, Z)) +Ds(X, hl(Y, Z)) −Ds(Y, hl(X, Z)) + (∇∗
Xh∗l)(Y, Z) −(∇∗
Y h∗l)(X, Z) + (∇∗
Xh∗s)(Y, Z) −(∇∗
Y h∗s)(X, Z) +D∗l(X, h∗s(Y, Z)) −D∗l(Y, h∗s(X, Z)) +D∗s(X, h∗l(Y, Z)) −D∗s(Y, h∗l(X, Z)),
(49) for X, Y, Z ∈Γ(TM) and analogously we can write expressions for ( ¯
S(X, Y )N)tangential,
( ¯
S(X, Y )N)transversal, ( ¯
S(X, Y )W)tangential and ( ¯
S(X, Y )W)transversal.
Next, consider the frames ﬁeld {ξ1, ..., ξr, er+1, ..., em, N1, ..., Nr, Wr+1, ..., Wn}, on ¯
M
along M, where {ξi}r
i=1 and {Ni}r
i=1 are lightlike bases of Γ(Rad(TM)|U) and Γ(ltr(TM)|U),
respectively and {eα}m
α=r+1 and {Wβ}n
β=r+1 are orthonormal bases of Γ(S(TM)|U) and
Γ(S(TM⊥)|U), respectively. Then the statistical Ricci tensor ¯
Ric on ¯
M is deﬁned by ¯
Ric(X, Y ) = trace{Z →¯
S(X, Z)Y }, for any X, Y ∈Γ(T ¯
M) and locally ¯
Ric on ¯
M is given by ¯
Ric(X, Y ) = m+n
X i=1
ǫi¯
g( ¯
S(X, ei)Y, ei), where ǫi is signature of ei. Form (46) and (47), it is clear that the Ricci tensor ¯
Ric on ¯
M
is symmetric. For the induced statistical curvature tensor ﬁeld S, the induced statistical
Ricci tensor Ric on M is deﬁned as Ric(X, Y ) = trace{Z →S(X, Z)Y }, for any X, Y ∈Γ(M) and locally Ric on M is given by Ric(X, Y ) = r
X i=1
¯
g(S(X, ξi)Y, Ni) + m
X α=r+1
¯
g(S(X, eα)Y, eα).
(50) It should be noted that the induced Ricci tensor of a lightlike submanifold M of a semi-
Riemannian manifold ¯
M is not symmetric because the induced connection ∇on a lightlike
submanifold M is not a metric connection. Therefore induced Ricci tensor is just a tensor
quantity and has no geometric or physical meaning contrary to the symmetric Ricci tensor
¯
Ric of ¯
M. We have analogous situation for a lightlike submanifold M of an indeﬁnite sta-
tistical manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗), where the lightlike submanifold M need not be statistical
and moreover the induced metric g is not metric but the statistical Ricci tensor ¯
Ric on
¯
M is symmetric. Since the symmetry of induced statistical Ricci tensor Ric is important
and we have following important observation from the Theorem 3.11 immediately. 12 Theorem 3.15. Let M be a dual-autoparallel lightlike submanifold of an indeﬁnite sta-
tistical manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗).
Then the induced statistical Ricci tensor Ric on M is
symmetric. Deﬁnition 3.16. ([9]) A lightlike submanifold M of a semi-Riemannian manifold
¯
M is
said to be irrotational lightlike submanifold if ˜
∇Xξ ∈Γ(TM), for any X ∈Γ(TM) and
ξ ∈Γ(Rad(TM)). Hence it is clear that if M is an irrotational lightlike submanifold then hl(X, ξ) = hs(X, ξ) =
0, for any X ∈Γ(TM) and ξ ∈Γ(Rad(TM)). Theorem 3.17. Let M be an irrotational lightlike submanifold of an indeﬁnite statistical
manifold ( ¯
M, ¯
g, ¯
∇, ¯
∇∗) of constant sectional curvature c such that the screen distribution
S(TM) is parallel with respect to the induced connection ∇and Dl(X, W) = 0, for any
X ∈Γ(TM) and W ∈Γ(S(TM⊥)).
Then the induced statistical Ricci tensor Ric is
symmetric. Proof. Let {ξi}r
i=1 and {Ni}r
i=1 be lightlike bases of Γ(Rad(TM)|U) and Γ(ltr(TM)|U),
respectively then using (45) and (48) for any X, Y ∈Γ(TM), we obtain r
X i=1
¯
g(S(X, ξi)Y, Ni)
=
−rc¯
g(X, Y ) −1 2 r
X i=1
{¯
g(Ahl(X,Y )ξi, Ni) −¯
g(Ahl(ξi,Y )X, Ni) +¯
g(Ahs(X,Y )ξi, Ni) −¯
g(Ahs(ξi,Y )X, Ni) +¯
g(A∗
h∗l(X,Y )ξi, Ni) −¯
g(A∗
h∗l(ξi,Y )X, Ni) +¯
g(A∗
h∗s(X,Y )ξi, Ni) −¯
g(A∗
h∗s(ξi,Y )X, Ni)}.
(51) On putting Y = ξi ∈Γ(Rad(TM)) in (26), it implies that ¯
g(hl(X, ξi) + h∗l(X, ξi), ξi) = 0,
it further implies hl(X, ξi) = −h∗l(X, ξi). Since M is an irrotational lightlike submanifold
therefore hl(X, ξi) = h∗l(X, ξi) = 0.
Also on putting Y = ξi in the dual of (27) and
then using Dl(X, W) = 0, we obtain ¯
g(h∗s(X, ξi), W) = 0 then further non-degeneracy of
S(TM⊥) implies that h∗s(X, ξi) = 0 and moreover hs(X, ξi) = 0, as M is an irrotational
lightlike submanifold. On using these facts in (51), we obtain r
X i=1
¯
g(S(X, ξi)Y, Ni)
=
−rc¯
g(X, Y ) −1 2 r
X i=1
{¯
g(Ahl(X,Y )ξi, Ni) + ¯
g(Ahs(X,Y )ξi, Ni) +¯
g(A∗
h∗l(X,Y )ξi, Ni) + ¯
g(A∗
h∗s(X,Y )ξi, Ni)}.
(52) Let {eα}m
α=r+1 be an orthonormal basis of Γ(S(TM)) then using (45) and (48) for any
X, Y ∈Γ(TM), we obtain m
X α=r+1
¯
g(S(X, eα)Y, eα)
=
c m
X α=r+1
¯
g(eα, Y )¯
g(X, eα) −¯
g(X, Y )(m −r −1) −1 2 m
X α=r+1
{¯
g(Ahl(X,Y )eα, eα) −¯
g(Ahl(eα,Y )X, eα) +¯
g(Ahs(X,Y )eα, eα) −¯
g(Ahs(eα,Y )X, eα) +¯
g(A∗
h∗l(X,Y )eα, eα) −¯
g(A∗
h∗l(eα,Y )X, eα) +¯
g(A∗
h∗s(X,Y )eα, eα) −¯
g(A∗
h∗s(eα,Y )X, eα)}.
(53) 13 On putting Y = eα in (27) and its dual, we obtain (54)
¯
g(A∗
W X, eα) = ¯
g(hs(X, eα), W),
¯
g(AW X, eα) = ¯
g(h∗s(X, eα), W), for any X ∈Γ(TM) and W ∈Γ(S(TM⊥)). Further using (54) in (53), we derive m
X α=r+1
¯
g(S(X, eα)Y, eα)
=
c m
X α=r+1
¯
g(eα, Y )¯
g(X, eα) −¯
g(X, Y )(m −r −1) −1 2 m
X α=r+1
{¯
g(Ahl(X,Y )eα, eα) −¯
g(Ahl(eα,Y )X, eα) +¯
g(Ahs(X,Y )eα, eα) −¯
g(h∗s(X, eα), hs(eα, Y )) +¯
g(A∗
h∗l(X,Y )eα, eα) −¯
g(A∗
h∗l(eα,Y )X, eα) +¯
g(A∗
h∗s(X,Y )eα, eα) −¯
g(hs(X, eα), h∗s(eα, Y ))}.
(55) Now assume that the screen distribution be parallel with respect to the induced connec-
tion ∇therefore ∇Xeα ∈Γ(S(TM)). On taking Y = Z = eα in (29) and then taking
summation from α = r + 1 to m, we get m
X α=r+1
{¯
g(∇Xeα, eα) + ¯
g(∇∗
Xeα, eα)} = 0.
(56) Since ∇Xeα ∈Γ(S(TM)) then using the non-degeneracy of the screen distribution, we
have ¯
g(∇Xeα, eα) ̸= 0 and on using this fact in the last expression, we get ¯
g(∇∗
Xeα, eα) ̸= 0
which implies ∇∗
Xeα ∈Γ(S(TM)). Using (9) and (18), we derive ¯
g(Ahl(eα,Y )X, eα)
= −¯
g( ¯
∇Xhl(eα, Y ), eα) = ¯
g(hl(eα, Y ), ¯
∇∗
Xeα) = ¯
g(hl(eα, Y ), ∇∗
Xeα).
(57) Since ∇∗
Xeα ∈Γ(S(TM)) then the last expression implies ¯
g(Ahl(eα,Y )X, eα) = 0. Analo-
gously, on using the duality, we have ¯
g(A∗
h∗l(eα,Y )X, eα) = 0. Using these facts in (55), we
obtain m
X α=r+1
¯
g(S(X, eα)Y, eα)
=
c m
X α=r+1
¯
g(eα, Y )¯
g(X, eα) −¯
g(X, Y )(m −r −1) −1 2 m
X α=r+1
{¯
g(Ahl(X,Y )eα, eα) + ¯
g(Ahs(X,Y )eα, eα) −¯
g(h∗s(X, eα), hs(eα, Y )) + ¯
g(A∗
h∗l(X,Y )eα, eα) +¯
g(A∗
h∗s(X,Y )eα, eα) −¯
g(hs(X, eα), h∗s(eα, Y ))}.
(58) Thus from (52) and (58), it is immediate that the induced statistical Ricci tensor Ric on
M is symmetric. Theorem 3.18. Let M be a lightlike submanifold of an indeﬁnite statistical manifold
( ¯
M, ¯
g, ¯
∇, ¯
∇∗) of constant sectional curvature c such that Ds(X, N) = 0, for any X ∈ 14 Γ(TM) and N ∈Γ(ltr(TM)).
Suppose there exists a transversal vector bundle of M
which is parallel along TM with respect to the connection ¯
∇on ¯
M, that is, ¯
∇XV ∈Γ(tr(TM)),
∀V ∈Γ(tr(TM)), X ∈Γ(TM).
(59) Then the induced statistical Ricci tensor Ric is symmetric. Proof. Let N ∈Γ(ltr(TM)) ⊂Γ(tr(TM)) and W ∈Γ(S(TM⊥)) ⊂Γ(tr(TM)) then using
the hypothesis (59) in (18) and (19), we obtain respectively (60)
ANX = 0,
AW X = 0. On putting Y = N ∈Γ(ltr(TM)) and Z = W ∈Γ(S(TM⊥)) in (9) and then using (60),
we get ¯
g(A∗
W X, N) = ¯
g(Ds(X, N), W), further using the hypothesis Ds(X, N) = 0, it
follows that ¯
g(A∗
W X, N) = 0. This implies that (61)
A∗
W X ∈Γ(S(TM)). On putting Y = Z = N ∈Γ(ltr(TM)) in (9) and then using (60), we get ¯
g(A∗
NX, N) = 0,
this implies that (62)
A∗
NX ∈Γ(S(TM)). On using (60) to (62) in (51), we obtain (63) r
X i=1
¯
g(S(X, ξi)Y, Ni) = −rc¯
g(X, Y ). Let {eα}m
α=r+1 be an orthonormal basis of Γ(S(TM)) then using (9), (17), (18) and (60),
we have 0 = ¯
g(ANX, eα) = ¯
g(N, ∇∗
Xeα), this implies that ∇∗
Xeα ∈Γ(S(TM)).
Then
following the proof of the Theorem 3.17 with (56), it implies that ∇Xeα ∈Γ(S(TM))
using this fact, after putting Y = eα in (28), we obtain (64)
¯
g(A∗
NX, eα) = 0. Now, let X, Y ∈Γ(TM) then using (9), (17) and (19), we have ¯
g(Ahs(eα,X)Y, eα) =
¯
g(hs(eα, X), h∗s(Y, eα)) therefore using (60), we obtain (65)
¯
g(hs(eα, X), h∗s(Y, eα)) = 0,
∀X, Y ∈Γ(TM). Also using (9), (17) and (19), we have ¯
g(A∗
h∗s(eα,Y )X, eα) = ¯
g(h∗s(eα, Y ), hs(X, eα)) then
using (65), it follows that (66)
¯
g(A∗
h∗s(eα,Y )X, eα) = 0. On using (60), (64) and (66) in (53), we derive (67) m
X α=r+1
¯
g(S(X, eα)Y, eα) = c m
X α=r+1
¯
g(eα, Y )¯
g(X, eα) −¯
g(X, Y )(m −r −1). Thus from (63) and (67), our assertion follows. 15",0
"A short introduction to the mathematical methods and technics of
diﬀerential algebras and modules adapted to the problems of mathe-
matical and theoretical physics is presented. Keywords: algebra, diﬀerential algebra, module, diﬀerential module,
multiplicator, diﬀerentiation, de Rham complex, spectral sequence, variation
bicomplex. 1
Introduction. Diﬀerential algebras (see, for example, [3],[4],[5]) are widely known and used
in algebra and topology, while their applications in mathematical physics
are far less acknowleged (but used implicitly, especially in partial diﬀeren-
tial equations). Here we propose a short introduction to the mathematical
methods and technics of diﬀerential algebras and modules adapted to the
problems of mathematical and theoretical physics. Exposition is based on
the personal experience of the author, the books [3],[4],[5],[6],[7],[10],[17] and
the works [8],[9],[11],[2],[12],[13].
We freely use notation, conventions and results of the paper [1]. In par-
ticular: ∗Steklov Mathematical Institute
†E-mail: zharinov@mi.ras.ru
‡This research was performed at the Steklov Mathematical Institute of Russian
Academy of Sciences and supported by a grant from the Russian Science Foundation
(Project No. 14-50-00005). 1 • F = R, C; • N = {1, 2, 3, . . .} ⊂Z+ = {0, 1, 2, . . .} ⊂Z = {0, ±1, ±2, . . . }. We, also, use the following notation of homology theory (see, e.g., [6]): • Hom(S; S′) is the set of all mappings from a set S to a set S′; • HomF(L; L′) is the linear space of all linear mappings from a linear
space L to a linear space L′; • HomA(M; M′) is the A-module of all A-linear mappings from an A-
module M to an A-module M′, where A is an associative commutative
algebra; • Homalg(A; A′) is the set of all algebra morphisms from A to A′, where
A, A′ are associative commutative algebras; • HomLie(A; A′) is the set of all Lie algebra morphisms from a Lie algebra
A to a Lie algebra A′. Remind, a set A is called a Lie A-algebra if it has two structures: • the structure of a Lie algebra with a Lie bracket [·, ·]; • the structure of an A-module, where A is an associative commutative
algebra; and these structures are related by the matching condition, • [X, f · Y ] = Xf · Y + f · [X, Y ] for all X, Y ∈A and f ∈A. We denote by HomLie∩A(A; A′) = HomLie(A; A′) ∩HomA(A; A′) the set of all
Lie A-morphisms from a Lie A-algebra A to a Lie A-algebra A′.
All linear operations are done over the number ﬁeld F. The summation
over repeated upper and lower indices is as a rule assumed. If the corre-
sponding index set is inﬁnite, we assume that the summation is correctly
deﬁned. If objects under the study have natural topologies, we assume that
the corresponding mappings are continuous. For example, if S and S′ are
topological spaces, then Hom(S; S′) is the set of all continuous mappings
from S to S′.
We use the terminology accepted in the algebra-geometrical approach
to partial diﬀerential equations, because they are the main example of the
technics developed below. 2 2
Diﬀerential algebras. In this section (see [1],[2] for more detail): • A is an associative commutative algebra; • M = M(A) = EndA(A) is the unital associative algebra of all multi-
plicators of the algebra A; • D = D(A) is the Lie M-algebra of all diﬀerentiations of the algebra A. Deﬁnition 1. A diﬀerential algebra is a pair (A, D), where D = D(A) is a
ﬁxed subalgebra (Cartan subalgebra) of the Lie M-algebra D = D(A). Deﬁnition 2. A pair (F, ϕ), where the mapping F ∈Homalg(A; B), and the
mapping ϕ ∈HomLie∩M(D(A); D(B)), is called a morphism of a diﬀerential
algebra (A, D(A)) into a diﬀerential algebra (B, D(B)), if the action F(Xf) =
(ϕX)(Ff) for all f ∈A, X ∈D(A). In this case we shall write (F, ϕ) :
(A, D(A)) →(B, D(B)). Let (A, D) be a diﬀerential algebra. Deﬁnition 3. A subalgebra B (an ideal I) of the algebra A is called diﬀer-
ential if DB ⊂B, i.e., Xf ∈B for all X ∈D and f ∈B, (DI ⊂I). In this
case the pair (B, D) (the pair (I, D)) is a diﬀerential algebra. In particular, if (I, D) is a diﬀerential ideal of the diﬀerential algebra
(A, D), then the quotient diﬀerential algebra (A
A
A,D
D
D) is deﬁned by the rule:
A
A
A = A

I, D
D
D = {X = [X] ∈D(A
A
A) | X ∈D}, [X][f] = [Xf] for all f ∈A,
where [f] = f + I, [Xf] = Xf + I ∈A
A
A. Proposition 1. For every morphism (F, ϕ) : (A, D(A)) →(B, D(B)) the
kernel Ker F is a diﬀerential ideal of (A, D(A)), while the image Im F will
be a diﬀerential subalgebra of (B, D(B)) if, in addition, the mapping ϕ :
D(A) →D(B) is a surjection. Deﬁnition 4. An element f ∈A is called D-constant if Df = 0, i.e., Xf = 0
for all X ∈D. Let AD be the set of all D-constant elements of the algebra A. Clear, AD
is a subalgebra of A. 3 Proposition 2. Let (F, ϕ) : (A, D(A)) →(B, D(B)), where ϕ is a surjection.
Then F


AD : AD →BD. Deﬁnition 5. A multiplicator R ∈M is called D-constant if DR = [D, R] =
0, i.e., [X, R] = 0 for all X ∈D. Let MD be the set of all D-constant multiplicators of the algebra M.
Clear, • MD is an unital subalgebra of the algebra M; • AD is a submodule of the MD-module A. Deﬁnition 6. A diﬀerentiation X ∈D is called a Lie-B¨
aclund diﬀerentiation
if [D, X] ⊂D, i.e., [Y, X] ∈D for all Y ∈D. Let DD = DD(A) be the set of all Lie-B¨
aclund diﬀerentiations of the
diﬀerential algebra (A, D). Clear, • DD is a subalgebra of the Lie MD-algebra D. Proposition 3. The ascending ﬁltration D = D(−1)
D
⊂DD = D(0)
D ⊂· · · ⊂D(q)
D ⊂D(q+1)
D
⊂· · · of Lie MD-algebras is deﬁned, where D(q)
D =

X ∈D

 [D, X] ⊂D(q−1)
D
	
,
q ∈Z+. Moreover, • D is an ideal of the Lie MD-algebra DD; • [D(p)
D , D(q)
D ] ⊂D(p+q)
D
, p, q ∈Z+. The general deﬁnition of ﬁltration one can ﬁnd in [7],[6].
Note, that
here and in similar situations below we don’t claim that ∪q∈Z+D(q)
D = D or
limq→∞D(q)
D = D in some sense. Deﬁnition 7. A diﬀerential algebra (A, D) is called regular if: • the Lie M-algebra D is splitted into vertical and horizontal subalgebras,
D = DV ⊕M DH; • the vertical subalgebra DV = DV (A) has a M-basis ∂= {∂a ∈DV |
a ∈a}, a is an index set, [∂a, ∂b] = 0, a, b ∈a; 4 • the horizontal (Cartan) subalgebra DH = DH(A) = D has a M-basis
D = {Dµ ∈DH | µ ∈m}, m = {1, . . . , m}, m ∈N, [Dµ, Dν] = 0,
µ, ν ∈m; • the commutators [Dµ, ∂a] = Γb
µa∂b ∈DV , µ ∈m, a, b ∈a, the coeﬃ-
cients Γa
µb ∈M, in particular, [Dµ, X] = ∇µX = (∇µX)a∂a ∈DV for
any X = Xa∂a ∈DV , Xa ∈M, where (∇µX)a = DµXa + Γa
µbXb. Let (A, DH) be a regular diﬀerential algebra. Proposition 4. The commutator [∇µ, ∇ν]a
bXb =
",0
"The present paper aims to demonstrate the usage of 
Convolutional Neural Networks as a generative 
model for stochastic processes, enabling research-
ers from a wide range of fields – such as quantita-
tive finance and physics – to develop a general tool 
for forecasts and simulations without the need to 
identify/assume a specific system structure or esti-
mate its parameters. 1 Introduction It is widely known that procedures of identification, estima-
tion and simulation of stochastic processes are somewhat 
already well established in mathematics, computing and 
related fields and sub-fields such as statistics, physics and 
econometrics. On the other hand, most of them rely on the 
fact that, to work properly, the observer must impose a sys-
tem structure to estimate its respective parameters – see 
Hamilton [1994] and Hayashi [2000], and, very often, as-
sume a probability distribution function. On top of this specified and estimated system, the re- searcher can accomplish tasks such as forecasting and simu-
lating the system’s states. In parallel to these strategies, it is possible to verify the revival of artificial neural networks, which have mostly 
been forgotten during the 1990s and mid-2000s because 
they were considered black-boxes without any intelligibility 
of its inner computations, as can be seen in Benitez et. al 
[1997] and in Kolman and Margaliot [2007]. One of the most important reasons for this revival is due to the success of Deep Neural Networks (neural networks 
with several layers), which have been successfully em-
ployed on tasks such as pattern recognition, classification 
and prediction, performing better than humans do. These 
tasks encompass a wide range of different problems such as 
computer vision problems – character recognition, object 
recognition and others; audio processing; and defeating 
world-class human players in complex computer games, Go 
and Chess. [see Silver et. al, 2017; Oshri and Khandwala, 
2015]. A great part of these successes has been attributed to a new hybrid architecture called Convolutional Neural Net-
works, where convolutional filters are placed and stacked composing deep networks – enabling the filtering of desired 
multiscale/multidimensional features that enhance classifi-
cation/forecasting capabilities – that can be interpreted and 
understood – plus a Softmax classifier/regressor, which ba-
sically works as a normalized multinomial logistic regressor 
– see Bishop [2006]. Given this huge success, the idea of the present paper is to adapt this specific kind of deep neural network, which has 
been successfully applied on the generation of raw audio 
waveforms as in Van den Oord et. al [2016-a], images as in 
Van den Oord et. al [2016-b], text [see Józefowicz et. al, 
2016] and multivariate systems [see Borovykh et. al, 2017]; 
and show that this kind of neural network can also be used 
to work as a generative model on top of data retrieved from 
a wide set of known deterministic/stochastic data generation 
processes – from the simplest to the most complex process-
es, from damped oscillators to autoregressive conditional 
heteroskedastic (ARCH) and jump-diffusion models. Avoiding the traditional identification and estimation procedures, a new approach is proposed here: estimate only 
the hyperparameters of a convolutional neural network, i.e. 
number of convolutional layers, discretization scheme (en-
coding) and dilations. Hence, we hope to demonstrate that 
data generation processes can be understood and simulated 
using a new statistical approach, without the need of assum-
ing any hard-structural form or imposing any kind of re-
strictions. Moreover, as the data is encoded/decoded outside 
the neural network, by means of transforming a regression 
task into a classification task, no assumption about the dis-
tribution of the data generating process must be made. In 
addition to that, we demonstrate that the original data distri-
bution can be recovered. Potential applications are huge. Being able to simulate and predict stochastic processes properly is desired in a 
wide range of sub-fields within the scope of finance and 
economics, such as asset pricing, time series analysis and 
risk analysis. To accomplish this goal, we have modified an existing Python/Tensorflow implementation of WaveNet as in Van 
den Oord et. al [2016-a], in order to read synthetic time se-
ries instead of raw audio files, avoiding any discussion 
about implementation strategies, focusing solely on the 
mathematical/statistical aspects of its usage. After that, we 
simulate each data generating process using a properly Generative Models for Stochastic Processes Using Convolutional Neural Networks Fernando Fernandes Neto University of São Paulo - Brazil fernando_fernandes_neto@usp.br Rodrigo de Losso da Silveira Bueno University of São Paulo - Brazil delosso@usp.br trained WaveNet, which their respective true parameters are 
known. Finally, using the R statistical package, the parame-
ters of the simulated processes are estimated and analyzed. That said, the paper is divided into these topics as fol- lows: • 
a brief description of the WaveNet architec- ture; • 
description of the synthetic time series gener- ated according to their respective generation process-
es; • 
discussion of the hyperparameters chosen for each time series and a brief methodology of how to 
set up them; • 
discussion of research findings; • 
conclusions and propositions for future works. 2 A Brief Description of the WaveNet Archi- tecture The main idea of the original WaveNet paper [Van den 
Oord et. al, 2016-a] is to model the joint probability of a 
stochastic process  
 as a product of conditional probabilities In other words, the probability of 
 is conditioned to all previous observations. To model the time series following this approach in terms of a Convolutional Neural Network, the WaveNet architec-
ture consists of stacking what is called dilated causal convo-
lutional layers, which consists of stacking structures as in 
Figure 1 and, as pointed before, a Softmax layer, which con-
sists of a multinomial logistic classifier given by: where 
 denotes the number of different classes;  denotes the features (independent variables) extracted in the previ-
ous layers;  
 denotes the weights of the features used to classify the output, which are filtered by means of convo-
lution operations specified in Figure 1; and 
 denotes the hypothesis of the output pertaining to a specific class – here, 
it is worth mentioning that, given this classification struc-
ture, the observed variables in the stochastic process must be encoded into a discrete variable with 
 different classes, where: Figure 1: Dilation Causal Convolutional Layer. Source: Van den Oord et. al, 2016-a In terms of the dilation causal convolutional layers, it is worth pointing out some important observations about the 
main features of these structures: • 
stacked layers structures act as a generaliza- tion of Discrete Wavelet filters [see Borovykh et. al, 
2017], given the fact that, basically, Discrete Wavelet 
transforms can be thought as a cascade of linear oper-
ations; • 
stacking such features with non-linear opera- tors can provide a general approximator due to the 
shift-invariance, as discussed in Bruna and Mallat 
[2013], Cheng et. al [2016] and Fernandes [2017], en-
abling the researcher to capture important non-
linearities; • 
this structure helps obtaining a very wide re- ceptive field, which facilitates extracting long-range 
dependencies in conjunction with short memory (as 
can be seen in Figure 2). Figure 2: Multiscale features in Dilation Causal Convolutional Layers. Source: Van den Oord et. al, 2016-a Keeping that in mind, these dilation convolutional layers are stacked and organized in a way that each feature is add-
ed to the previous layer features following a residual 
scheme, as follows in Figure 3, enabling deeper models and 
faster convergence, according to He et. al [2015]. Figure 3: Overview of the Architecture. Source: Van den Oord et.al, 2016- a It is also important to notice that there is a Gated Activa- tion Function instead of Rectified Linear Activation Func-
tions (ReLU), which are the most popular activation funci-
ton in this kind of neural network, due to the fact that, as 
shown in Van den Oord et. al [2016-b], it outperforms the 
traditional approach. In this case, this Gated Activation 
Function is described by: where 
 denotes the element-wise multiplication operator; denotes the convolution operator;  denotes the input; denotes a learnable convolutional filter at the -th layer; and 
 denotes a learnable convolutional gate. Aiming to focus on the main subject of this paper, for fur- ther architecture details, one should read the original 
WaveNet implementation paper. 3 Description of the Synthetic Time Series In the present paper, four deterministic data generation pro-
cesses and five stochastic process were tested, in order to 
verify the generative capabilities of the WaveNet architec-
ture. The chosen deterministic processes were: • Harmonic Oscillator • Damped Harmonic Oscillator (without any external forces) • Logistic Map with a chaotic choice of the parameter [see May, 1976] • Lorenz System, as simulated in Borovykh et. al [2017] In these four deterministic cases, the Convolutional Neu- ral Network should behave similar to a standard ordinary 
differential equation solver / difference equation solver. 
In the case of the stochastic processes, we have chosen the 
following processes: • Standard diffusion process with mean-reversion • Jump-Diffusion Process (see Matsuda, 2004) • Autoregressive Process of Order 1 (AR) • Autoregressive-Moving Average Process of Order 1 (ARMA) • Autoregressive Conditional Heteroskedastic Process of Order 1 (ARCH) In these cases, the Convolutional Neural Network should simulate a stochastic process compatible with the original 
one, as in a standard stochastic differential/difference equa-
tion simulator. 4 Description of the Methodology for the Choice of Hyperparameters In order to setup the hyperparameters of the Convolutional 
Neural Network according to each synthetic time series, it 
was adopted a forward method, where we start with two 
layers and dilations up to the second order (i.e. dilations 1,2) and increased the dilations up to 256th order, which were the 
dilations in the original paper chosen to deal with eventual 
long-range dependencies found in text-to-speech applica-
tions. 
 In addition to that, it was generated a single time series 
(for each data generating process) with 12000 samples – 
10000 samples were used to train the neural network and 
2000 samples were used for back-testing purposes. 
 Whenever poor results were obtained, an additional con-
volutional layer was added, starting again with only dila-
tions up to the second order. 
 Moreover, in this specific application, an 8-bit encoding 
was used to discretize the data into 256 classes; 256 skip-
channels were used; with a filter width set equal to 2 – cov-
ering all the hyperparameters established in Van den Oord 
et. al [2016-a]. 5 Simulations and Discussion of Research Findings To test the capabilities of this architecture, we show first the 
results of the deterministic processes simulations and, af-
terwards, the results of the stochastic processes simulations. 
For visibility purposes, in the case of the Logistic Map 
(Figure 7), we show only the first 90 observations out-of-
sample.  It is also worth mentioning that, as we are model-
ling deterministic processes, in this case we use the 
numpy.argsort() method  (present in the NumPy package for 
Python), to generate deterministic choices of the Softmax 
layer output, obeying the magnitude of the probabilities. Figure 4: Simulation of a Deterministic Harmonic Oscillator (WaveNet 
with two layers with dilations up to 8) Figure 5: Simulation of a Deterministic Harmonic Oscillator (WaveNet 
with three layers with dilations up to 256) Figure 6: Simulation of a Deterministic Damped Harmonic Oscillator 
(WaveNet with nine layers with dilations up to 4) Figure 7: Simulation of a Deterministic Chaotic Logistic Map (WaveNet 
with five layers with dilations up to 2) In the Figures 4 to 7, in all four graphics, dashed green lines represent observations used for back-testing purposes; 
the red lines represent predicted outputs by the model; and 
dashed black lines represent the original signal used to train 
the Neural Network. Given that, it is possible to see that the WaveNet architec- ture is able to capture and fairly reproduce the main dynam-
ics of the processes in Figures 4 and 5. It is also interesting to notice that, in Figure 7, until the 10th observation, the model is able to obtain precise out-of-
sample forecasts of the data, while in the Figure 6, given the 
non-linear multivariable nature of the system, the best out-
of-sample guess is an average of the past occurrences. That said, we repeat the same experiment with the five proposed stochastic processes. However, instead of only 
plotting the time series (except for the jump diffusion pro-
cess – plotted in Figure 8, which would require a lot of ef-
fort to be estimated, extrapolating the original scope of this 
present work), we also plot the distribution of the structural 
parameters of the simulated series. Our hypothesis is supported by the fact that, if the struc- tural parameters are compatible with the original ones – 
given the fact that we know the true data generating process 
parameters – the simulated process is compatible with the 
original one. Hence, in Figure 8, we show first a Jump-Diffusion pro- cess with a negative drift, with a low probability of occur-
ring a high intensity positive jump. It is also worth noticing that in Figures 9 to 12, the medi- an values are plotted in red and the true parameter values are 
plotted in blue.  So, a direct comparison between the true 
known structural parameter values and the estimated one 
from the simulated series can be done. To generate different realizations of the processes (repre- sented by different colors), 100 simulations of each process 
were carried out, as in a Monte-Carlo approach, using the 
numpy.random.choice() method  (present in the NumPy 
package for Python), to generate random choices that obey a 
given distribution, which is returned by the Softmax layer of 
WaveNet. Figure 8: Simulation of a Jump-Diffusion Process (WaveNet with five 
layers with dilations up to 4) Figure 9 – Simulation and Inference of a Mean-Reverting Diffusion Pro-
cess (True Mean Reversion Speed Parameter = 0.1 – blue line not shown) Figure 10: Simulation and Inference of an AR(1) Process (WaveNet with 
five layers with dilations up to 4) Figure 11: Simulation and Inference of an ARMA(1,1) Process (WaveNet 
with five layers with dilations up to 4) In these three first linear models (Figures 9 to 11), it is possible to verify that the results are very reasonable, given 
the fact that the Networks have learnt from only one realiza-
tion of each stochastic process (despite a large sample), and 
the parameters deviation from the true values are not large. Also, it is important to state that, as all experiments used training sets where the first samples were equal to zero, it 
explains some of the valleys found in the simulations. Figure 12: Simulation and Inference of an ARCH(1) Process (WaveNet 
with five layers with dilations up to 4) In Figure 12, we can also verify that the structural charac- teristics of the data generation process are compatible with 
the true parameters. 6 Conclusions and Propositions for Future Works In the present work, we aimed to establish a new simulation 
of data generation processes that avoid the traditional identi-
fication and estimation procedures, proposing here a new 
technique based on a Convolutional Neural Network – 
namely WaveNet architecture – where we estimate only its 
hyperparameters, in this case: number of convolutional lay-
ers, discretization scheme (encoding) and dilations. To accomplish that, we have simulated different deter- ministic and stochastic processes, using an existing imple-
mentation of the WaveNet architecture code, adapted for 
this specific purpose, in conjunction with the R Statistical 
Package. On top of these different simulations, we show that 
the generated data is compatible with original data genera-
tion process, in a fair wide extent, being a potential attrac-
tive tool that can be employed in several different research 
areas. As perspective for future works and research, we suggest following these experiments with more complex stochastic 
processes and, also, given the high computational cost of the 
training procedures (all of them were done using only one 
GPU in TensorFlow), it is important to develop an infor- mation criterion for such kind of models, in order to guide 
and speed-up the hyperparameters choice. Moreover, extending this architecture for multivariate processes is a desirable path towards new interesting results.",0
"The authors of the article have reviewed the scientific literature on the development of the Russian-Chinese cooperation in the field of combining economic and logistics projects of the Eurasian Economic Union and the Silk Road Economic Belt. The opinions of not only Russian, but also Chinese experts on these projects are indicated, which provides the expansion of the vision of the concept of the New Silk Road in both countries. Key words: logistics, partnership, Eurasian Economic Union, Silk Road Economic Belt. JEL codes:  F-01; F-02; F-15. 1. 
Introduction The topic of the research is extremely important due to the fact that the relations between China and Russia at the present stage have a significant impact on the development of the entire system of international relations. Both China and 2 Russia are permanent members of the UN Security Council and have great economic potential, and are also included in the list of the ten leading countries in the world in terms of GDP, have solid nuclear missile weapons and their means of delivery, have great political weight in the international arena in the system of modern international relations. Russia and China have established ‘strategic partnership relations turned into the twenty-first century’, concluded an agreement on ‘Good-neighborliness, Friendship and Cooperation’, and thereby created a strong legal basis for the development of stable relations between these states. 2. 
Main part China is becoming an increasingly powerful geopolitical player in the political arena and is taking a more active part in international politics, including in such organizations as APEC, SCO, the UN, BRICS, etc. Such politics allows China to realize its national economic, political, and cultural interests. The Russian Federation, just like China, plays a significant role in the world. In fact, still a young player in the geopolitical arena of the world, Russia has been able to achieve considerable success in strengthening and defending its influence in the world, including through the establishment of mutually beneficial Russian- Chinese relations. The expansion of cooperation between Russia and China is explained not only by economic benefits and neighboring position. The two major player countries face common tasks, common challenges, and new opportunities are opening up. In our age of globalization, transparent economies and democratic values, they can be realized only by uniting. Cooperation between the development of the Eurasian Economic Union and the construction of the Silk Road Economic Belt (see Fig. 1 and Fig.2) is of great importance for both countries, and makes a significant contribution to the process of economic globalization and political multipolarity. 3 Fig.1. Eurasian Economic Union Source: Eurasian Economic Commission. The problem of studying the processes of conjugation of the Chinese Silk Road Economic Belt (SREB) and the Eurasian Economic Union( EAEU), as well as the opportunities for their interaction with the Shanghai Cooperation Organization (SCO), is rapidly becoming more active in the Russian, Chinese and Central Asian expert space. Fig.2. Silk Road Economic Belt Source: ‘One Belt, One Road’. URL: https://eurasiainteraction.com/index.php/blog/175-one- belt-one-road-initiative-economic-strategic-prospects-for-southeast-asia Every year, dozens of different expert and analytical materials are published, written by both our sinologists and Chinese experts. The development of the 4 research issues was carried out in the format of interpreting and commenting on scientific works that represent popular views on integration processes on the Eurasian continent in Russia, China and the West, and which can be a demonstration of the geopolitical picture in the Eurasian space. The first group of works is devoted to the study of the Chinese concept of ‘One Belt – One Road’. With the promotion of the concept of ‘One Belt – One Road’ in 2013, the dedicated topic is gradually attracting great attention from Russian scientific circles. The following Russian scientists worked on the analysis of the foreign economic concept adopted by China: Uyanaev S. V. [1; 2], Larin A. G. [3; 4], Lukin A.V. [5], Luzyanin S. G. [6], Remyga V. N. [7], Kuziev N. A. [8], whose works consider the main goals, contents, principles and methods of implementing the ‘One Belt – One Road’, explore the prospects and challenges for both the development of China and for Sino-Russian cooperation in the implementation of the strategy. These works helped to make a deep analysis of the characteristics of the Chinese project. Chinese researchers are engaged in the most intensive development of this issue, among which Wu Jianming [9], Huang Yipin [10], Jin Lin [11], Su Ge [12], Shi Yan [13], Yuan Xintao [14] can be singled out. In their works, the researchers provide a detailed analysis of the political and economic aspects of the new Chinese strategy, demonstrate its essence and specifics. In particular, it is necessary to note Wu Jianmin's monograph ‘The Belt and the Road’ and Great- power Diplomacy with Chinese specifics’, which evidently analyzes the motivation and specifics of the Chinese initiative under the changing external and internal environment. The second group includes works devoted to the study of the Eurasian Economic Union. Since the Eurasian Economic Union is a necessary subject that directly shows Russia's interests in the regional arena. The analysis of this group allows the author to reveal the interests of Russia and the possibilities of combining the two projects. Great attention is paid by Russian scientists such as Mansurov T. A. [16], Volodin V. M., Kaftulina Yu. A., 5 Rusakova Yu. I. [17], Bekyashev K. A. and Moiseev E. G. [18]. Their works analyze the main provisions, the evolution of the Eurasian Economic Union, identify the prospects and opportunities of the EAEU on the path of the process of deep economic integration. It is necessary to note the works of  N. A. Vasilyeva and M. L. Lagutina [19; 20], whose  articles consider the issue of the integration paradigm on the example of the Eurasian Economic Union, comprehensively analyze the positions regarding the project of the Eurasian Economic Union. The analysis of the issue of the Eurasian Economic Union is still contained in the works of Chinese scientists, such as Li Ziguo [21], Zhou Mi [22], Li Xin [23], Liu Qingcai and Zhi Zichao [24]. In their works, the authors investigated the situation of the EAEU member states, analyzed significant problems, risks of the EAEU impact, and identified possible solutions. Among them, it is worth noting the monograph by Fu Jingjun [25], where the scientist examines in detail the process of creating and developing the EAEU, highlights the approaches of the EAEU member states to the Chinese ‘Silk Road Economic Belt’, also analyzes how to implement the interface of the two projects. In addition, this topic includes the work of Chinese scientists Li Jianming and Li Yongquan [26], where the prerequisites, state, difficulties and prospects of the Eurasian Economic Union are revealed, as well as numerous factors affecting the possibility of combining this and the ‘One Belt, One Road’ project are investigated. Russian scientists such as Vasiliev L. E. [27], Matveev V. A. [28], Glinkina S. P., Turaeva M. O. and Yakovlev A. A. are engaged in the study of the interrelationships between China and Central Asia [29]. The works of these scientists assess the current state of cooperation between China and Central Asia and characterize China's policy in Central Asia. Among Chinese researchers, Zhao Changqing [30], Yuan Shenyu and Wang Weimin [31] dealt in detail with issues related to China's relations with the countries of Central Asia. The next group includes works devoted to the study of Russia's position on the Chinese initiative and Sino-Russian cooperation in the process of its implementation. Russian researchers such as Denisov I. E. [32], Vladimir 6 Berezhnoy [33], Morozov Yu. V. [34], Ostrovsky A.V. [35]. we worked on forming a position and analyzing Russia's place on the new Silk Road. Nevertheless, the problem of the Russian assessment of the concept of ‘One Belt – One Road’ occupies a significant place in the works of Chinese scientists-Wang Kei [36], Li Xiujiao [37], where they are devoted to considering the approaches of the Russian scientific community, identifying the reasons for changing the assessment with the international and domestic changing situation. When analyzing the current Sino-Russian cooperation within the framework of ‘One Belt – One Road’, it is impossible to ignore Wang Qi's monograph [38], where the author comprehensively explains strategic cooperation in the fields of trade and economy, science and technology, military security, culture and education, considering the strategy for the development of bilateral relations, at the same time presents a proposal. It should be noted that Chinese researchers are engaged in the most intensive development of this issue, among which Zhang Jianping and Li Jing [39], Li Jianmin [40], Jiang Zhenjun [41] can be particularly distinguished. Their works touch upon the practical steps of interaction between China and Russia, problems and challenges in the process of cooperation in the joint creation of the ‘One Belt, One Road’. In addition, the work of expert Zhao Huizhong is devoted to the issues of Sino-Russian cooperation within the framework of the ‘One Belt, One Road’ project, where the author interprets the importance of Russia in the Chinese project, as well as the importance of Sino – Russian joint cooperation in the implementation of the initiative, finds out the difficulties facing Beijing and Moscow, and presents objective proposals for their resolution [42]. 3. 
Conclusion The 
Sino-Russian 
comprehensive 
strategic 
partnership 
meets 
the fundamental interests of the two countries and the two peoples. The common interests of the two countries in a wide range of areas are an endogenous driving force for the sustainable development of their relations. The strategic pressure of the West is also an external factor in the development of Sino-Russian cooperation. 7 The framework of cooperation of the ‘Silk Road Economic Belt’, which China stands for, does not have a fundamental conflict of interests with the construction of the EAEU. On the contrary, they can be useful to each other. In the long term, successful coordination of the two projects can effectively eliminate the serious discrepancy between political development and economic cooperation in Sino- Russian relations. This is a strategic step to create a Sino-Russian community of interests and strengthen the Sino-Russian comprehensive strategic partnership.",0
"I show that, if a term is SN for β, it remains SN when some permutation
rules are added. 1
Introduction Strong normalization (abbreviated as SN) is a property of rewriting systems that is
often desired. Since about 10 years many researchers have considered the following
question : If a λ-term is SN for the β-reduction, does it remain SN if some other
reduction rules are added ? They are mainly interested with permutation rules
they introduce to be able to delay some β-reductions in, for example, let x = ...
in ... constructions or in calculi with explicit substitutions. Here are some papers
considering such permutations rules: L. Regnier [7], F. Kamareddine [3], E. Moggi
[5], R. Dyckhoﬀand S. Lengrand [2], A. J. Kfoury and J. B. Wells [4], Y. Ohta and
M. Hasegawa [6], J. Esp´
ırito Santo [8], [9], and [10].
Some of these papers show that SN is preserved by the addition of the permu-
tation rules they introduce but, most often, authors do not consider the whole set
of rules or add restrictions to some rules. For example the rule (M (λx.N P)) ⊲
(λx.(M N) P) is often restricted to the case when M is an abstraction (in this case
it is usually called assoc).
I give here a simple and short proof that the permutations rules preserve SN
when they are added all together and with no restriction. It is done as follows. I
show that every term which is typable in the system (often called system D) of types
built with →and ∧is strongly normalizing for all the rules (β and the permutation
rules). Since it is well known that a term is SN for the β-rule iﬀit is typable in
this system, the result follows. The proof is an extension of my proof of SN for
the simply typed λ-calculus where the main result is a substitution theorem (here
Theorem 3.3): if t and a are in SN, then so is t[x := a].
To my knowledge, only one other paper ([9] and its recent version [10]) considers
all the rules with no restriction. The technic used there is completely diﬀerent from
the one used in this paper. 2
Deﬁnitions and notations Deﬁnition 2.1
• The set of λ-terms is deﬁned by the following grammar M := x | λx.M | (M M) 1 • The set T of types is deﬁned (simultaneously with the set S of simple types)
by the following grammars where A is a set of atomic constants S ::= A | T →S T ::= S | S ∧T • The typing rules are the following where Γ is a set of declarations as x : A
where x is a variable and the mentioned types (A, B) are in T : Γ, x : A ⊢x : A Γ ⊢M : A →B
Γ ⊢N : A Γ ⊢(M N) : B
Γ, x : A ⊢M : B Γ ⊢λx.M : A →B Γ ⊢M : A ∧B Γ ⊢M : A
Γ ⊢M : A ∧B Γ ⊢M : B Γ ⊢M : A
Γ ⊢M : B Γ ⊢M : A ∧B Remarks and Notation 1. To avoid too many brackets in the lambda terms I will adopt the following
conventions. An application (or a sequence of applications) is always sur-
rounded by brackets (i.e. the application of M to N is written (M N) with a
blank between M and N) and, as usual, application associates to the left i.e.
(M N P) means ((M N) P). An abstraction is always written as λx.M (i.e.
there is a dot after the variable but no blank between the dot and M) where
either M is a letter or an application (and thus between brackets) or another
abstraction. For example λy.(MN) represents an abstraction and (λy.MN) a redex. 2. Note that in the usual deﬁnition of the types with intersection →and ∧can
be used with no restriction.
Here we forbid to have an ∧at the right of
an →.
For example A →(B ∧C) is forbidden and must be replaced by
(A →B) ∧(A →C). It is well known that both systems are equivalent since
it is easily proved that any type derivation in the unrestricted system can
be transformed into a type derivation in the restricted one. Actually note
that, in fact, the type derivation given by Theorem 3.2 already satisﬁes this
restriction. We have used this restricted version to make simpler the analysis of type
derivations in the proof of Theorem 3.3 3. Also note (this is well known and easy to prove) that any type derivation
can be transformed into a normal derivation i.e. a derivation in which the
introduction of an ∧is never immediately followed by its elimination. 4. The lemmas and theorems using types will be indicated by the mention
“typed”. If a type derivation is given to M, type(M) will denote the size
(i.e the number of symbols) of the type of M. Deﬁnition 2.2
The reduction rules are the following. 2 • β : (λx.M N) ⊲M[x := N] • δ : (λy.λx.M N) ⊲λx.(λy.M N) • γ : (λx.M N P) ⊲(λx.(M P) N) • assoc : (M (λx.N P)) ⊲(λx.(M N) P) Using Barendregt’s convention for the names of variables, we assume that, in γ
(resp. δ, assoc), x is not free in P (resp. in N, in M).
The rules δ and γ have been introduced by Regnier in [7] and are called there
the σ-reduction. It seems that the ﬁrst formulation of assoc appears in Moggi [5] in
the restricted case where M is an abstraction and in a “let ... in ...” formulation.
Note that γ (resp. δ, assoc) are called θ1 (resp. γ, θ3) in [4] and π1 or σ1 (resp.
σ2, π2) in [10]. Notation 2.1
• If t is a term, size(t) denotes its size. • If t ∈SN (i.e. every sequence of reductions starting from t is ﬁnite), η(t)
denotes the length of the longest reduction of t.
Since various notions of
reductions are considered in this paper, by default these concepts are relative
to the union of all four reduction rules. When this is not the case (e.g. SN
wrt to β), then the reduction rule intended is indicated explicitly. • Let σ be a substitution. We say that σ is fair if the σ(x) for x ∈dom(σ) all
have the same type (that will be denoted as type(σ)). We say that σ ∈SN if,
for each x ∈dom(σ), σ(x) ∈SN. • Let σ ∈SN be a substitution and t be a term. We denote by size(σ, t) (resp.
η(σ, t)) the sum, over x ∈dom(σ), of nb(t, x).size(σ(x)) (resp. nb(t, x).η(σ(x)))
where nb(t, x) is the number of free occurrences of x in t. • If −
→
M is a sequence of terms, lg(−
→
M) denotes its length, M(i) denotes the i-th
element of the sequence and tail(−
→
M) denotes −
→
M from which the ﬁrst element
has been deleted. • Assume t = (H −
→
M) where H is an abstraction or a variable and lg(−
→
M) ≥1. – If H is an abstraction (in this case we say that t is β-head reducible),
then M(1) will be denoted as Arg[t] and (R′ tail(−
→
M)) will be denoted by
B[t] where R′ is the reduct of the β-redex (H Arg[t]). – If H = λx.N and lg(−
→
M) ≥2 (in this case we say that t is γ-head
reducible), then (λx.(N M(2)) M(1) M(3) ... M(lg(−
→
M))) will be denoted
by C[t].
– If H = λx.λy.N (in this case we say that t is δ-head reducible), then
(λy.(λx.N M(1)) M(2) ... M(lg(−
→
M))) will be denoted by D[t].
– If M(i) = (λx.N P), then the term (λx.(H M(1) ... M(i−1) N) P M(i+
1) ... M(lg(−
→
M))) will be denoted by A[t, i] and we say that M(i) is the
β-redex put in head position. • Finally, in a proof by induction, IH will denote the induction hypothesis. 3
The theorem Theorem 3.1
Let t be a term. Assume t is strongly normalizing for β. Then t is
strongly normalizing for β, δ, γ and assoc.
Proof
This follows immediately from Theorem 3.2 and corollary 3.1 below.
□ 3 Theorem 3.2
A term is SN for the β-rule iﬀit is typable in system D.
Proof
This is a classical result. For the sake of completeness I recall here the
proof of the only if direction given in [1]. Note that it is the only direction that is
used in this paper and that corollary 3.1 below actually gives the other direction.
The proof is by induction on ⟨η(t), size(t)⟩.
- If t = λx u. This follows immediately from the IH.
- If t = (x v1 ... vn). By the IH, for every j, let x : Aj, Γj ⊢vj : Bj. Then
x : V Aj ∧(B1, ..., Bn →C), V Γj ⊢t : C where C is any type, for example any
atomic type.
- If t = (λx.a b −
→
c ). By the IH, (a[x := b] −
→
c ) is typable. If x occurs in a, let
A1 ... An be the types of the occurrences of b in the typing of (a[x := b] −
→
c ). Then t
is typable by giving to x and b the type A1 ∧... ∧An. Otherwise, by the induction
hypothesis b is typable of type B and then t is typable by giving to x the type B. □
From now on, ⊲denotes the reduction by one of the rules β, δ, γ and assoc. Lemma 3.1
1. The system satisﬁes subject reduction i.e. if Γ ⊢t : A and t ⊲t′ then Γ ⊢t′ : A. 2. If t ⊲t′ then t[x := u] ⊲t′[x := u]. 3. If t′ = t[x := u] ∈SN then t ∈SN and η(t) ≤η(t′).
Proof
Immediate.
□ Lemma 3.2
Let t = (H −
→
M) be such that H is an abstraction or a variable and
lg(−
→
M) ≥1. Assume H, −
→
M ∈SN and that 1. If t is δ-head reducible (resp. γ-head reducible, β-head reducible), then D[t] ∈
SN (resp. C[t] ∈SN, Arg[t], B[t] ∈SN). 2. For each i such that M(i) is a β-redex, A[t, i] ∈SN, Then t ∈SN.
Proof
By induction on η(H) + P η(M(i)). Show that each reduct of t is in SN.
Note that the assumption H, −
→
M ∈SN is implied by the others if at least one of
them is not “empty” i.e. if t is head reducible for at least one rule.
□ Lemma 3.3 (typed)
If (t −
→
u ) ∈SN then (λx.t x −
→
u ) ∈SN.
Proof
Note that, if (λx.t x −
→
u ) has a head redex for the δ-rule, its reduct has
not the desirable shape and an induction hypothesis will not be applicable. We
thus generalize a bit the statement with the notion of left context, i.e. a context
with exactly one hole on the left branch. More precisely the set L of left contexts
is deﬁned by the following grammar: L := [] | λx.L | (L M). The result is thus a
special case of the following claim.
Claim : Let L be a left context and t be a term.
If L[t] is in SN then so is
w = L[(λx.t x)].
Proof : By induction on ⟨type(t), η(L[t])⟩. We show that every reduct of w is in
SN. There are 4 possibilities for the reduced redex. If it is in L or in t, the result
follows immediately from the IH. If it is the (λx.t x) substituted in the hole of L the
result is clear. The last situation is when the redex is created by the substitution
in the hole of L. These cases are given below. Note that the assoc and β rules can
only be used either in t or in L.
- t = λy.t1 and w ⊲δ L[λy.(λx.t1 x)] = L′[(λx.t1 x)] where L′ = L[λy.[]]. The
result follows from the IH applied to L′ and t1 (since t1 can be given a type less
than the one of t).
- L = L′[([] v)] and w ⊲γ L′[(λx.(t v) x)]. The result follows from the IH applied
to L′ and t1 = (t v) (since t1 can be given a type less than the one of t).
□ 4 Theorem 3.3 (typed)
Let t ∈SN and σ ∈SN be a fair substitution. Then
σ(t) ∈SN.
Proof
Formally, what we prove is the following. Let U = {(t, σ, A) | t ∈SN,
σ ∈SN and A is assignable to each σ(x)}. Then, for all (t, σ, A) ∈U, σ(t) ∈SN.
Theorem follows since, if σ is fair, (t, σ, A) ∈U for some A .
We assume all the derivations are normal (see the remark after deﬁnition 2.1).
The proof is by induction on ⟨size(A), η(t), size(t), η(σ, t), size(σ, t)⟩. We will have
to use the induction hypothesis to some (t′, σ′, A′) for which we have to give type
derivations and to show that the 5-uplet has decreased. For the types (since the
veriﬁcation is fastidious but easy) we give some details only for one example (the
ﬁrst time in case 1.c below) and, for the others, we simply say “type(t1) < type(t2)”
(resp. “type(t1) = type(t2)”) instead of saying something as “t1 can be given a type
less than (resp. equal to) type(t2)”.
Note that this theorem will be only used with unary substitutions but its proof
needs the general case because, starting with a unary substitution, it may happen
that we have to use the induction hypothesis with a non unary substitution. It will
be the case, for example, in 1.c below.
Let (t, σ, A) ∈U. If t is an abstraction or a variable the result is trivial. Thus
assume t = (H −
→
M) where H is an abstraction or a variable and n = lg(−
→
M) ≥1.
Let −
→
N = σ(−
→
M).
Claim : Let −
→
P be a (strict) initial or a ﬁnal sub-sequence of −
→
N. Then (z −
→
P ) ∈SN.
Proof : Let −
→
Q be the sub-sequence of −
→
M corresponding to −
→
P . Then (z −
→
P ) = τ(t′)
where t′ = (z −
→
Q) and τ is the same as σ for the variables in −
→
Q and z ̸∈dom(τ).
The result follows from the IH since size(t′) < size(t).
□ We use Lemma 3.2 to show that σ(t) ∈SN. 1. Assume σ(t) is δ-head reducible. We have to show that D[σ(t)] ∈SN. There
are 3 cases to consider. (a) If t was already δ-head reducible, then D[σ(t)] = σ(D[t]) and the result
follows from the IH. (b) If H is a variable and σ(H) = λx.λy.a, then D[σ(t)] = t′[z := λy.(λx.a N(1))]
where t′ = (z tail(−
→
N)).
By the claim, t′ ∈SN and since type(z) <
size(A) it is enough, by the IH, to check that λy.(λx.a N(1)) ∈SN.
But this is λy.(z′ N(1))[z′ := λx.a]. But, by the claim, (z′ N(1)) ∈SN
and we conclude by the IH since type(z′) < size(A). (c) If H = λx.z and σ(z) = λy.a, then D[σ(t)] = (λy.(λx.a N(1)) tail(−
→
N)) =
τ(t′) where t′ = (z′ tail(−
→
M)) and τ is the same as σ on the variables of
tail(−
→
M) and τ(z′) = λy.(λx.a N(1)). Note that, by Lemma 3.1, t′ is in
SN and η(t′) ≤η(t). Since size(t′) < size(t) to get the result by the IH
we have to show that (1) (t′, τ, A) ∈U and (2) that (λx.a N(1)) ∈SN.
To prove (1) it is enough to show that we can give to Q = λy.(λx.a M(1))
the same type as P = (λx.λy.a M(1)). In the typing of P, λx.λy.a has
type (A1 →B1 →C1) ∧... ∧(Ak →Bk →Ck) and M(1) has type
A1 ∧...∧Ak and thus P has type (B1 →C1)∧...∧(Bk →Ck). It follows
that we can type Q by typing (λx.a M(1)) with type C1 ∧... ∧Ck and
thus Q with type (B1 →C1) ∧... ∧(Bk →Ck).
To prove (2) we remark that (λx.a N(1)) = (λx.z′′ N(1))[z′′ := a]
and, since type(a) < size(A) it is enough, by the IH, to show that
u = (λx.z′′ N(1)) ∈SN. This is done as follows: u = σ′(t′′) where
t′′ = (λx.z′′ M(1)) (which is, up to the renaming of z into z′′ a sub-term 5 of t) and σ′ is as σ but where z′′ is not in the domain of σ′ whereas the oc-
currence of z in H was in the domain of σ. Thus, size(σ′, t′′) < size(σ, t)
and the result follows from the IH. 2. Assume σ(t) is γ-head reducible. We have to show that L[σ(t)] ∈SN. There
are 4 cases to consider. (a) If H is an abstraction, then C[σ(t)] = σ(C[t]) and the result follows
immediately from the IH. (b) H is a variable and σ(H) = λy.a, then C[σ(t)] = (λy.(a N(2)) N(1) N(3)
... N(n)) = (λy.(a N(2)) y N(3) ... N(n))[y := N(1)]. Since type(N(1)) <
size(A), it is enough, by the IH, to show (λy.(a N(2)) y N(3) ... N(n)) ∈
SN and so, by Lemma 3.3, that u = (a N(2) N(3) ... N(n)) ∈SN. By
the claim, (z tail(−
→
N)) ∈SN and the result follows from the IH since
u = (z tail(−
→
N))[z := a] and type(a) < size(A). (c) H is a variable and σ(H) = (λy.a b), then C[σ(t)] = (λy.(a N(1)) b
N(2) ... N(n)) = (z tail(−
→
N))[z := (λy.(a N(1)) b)]. Since type(z) <
size(A), by the IH it is enough to show that u = (λy.(a N(1)) b) ∈SN.
We use Lemma 3.2.
- We ﬁrst have to show that B[u] ∈SN. But this is (a[y := b] N(1))
which is in SN since u1 = (a[y := b] −
→
N) ∈SN since u1 = τ(t1) where t1
is the same as t but where we have given to the variable H the fresh name
z, τ is the same as σ for the variables in dom(σ) and τ(z) = a[y := b]
and thus we may conclude by the IH since η(τ, t) < η(σ, t).
- We then have to show that, if b is a β-redex say (λz.b1 b2), then A[u, 1] =
(λz.(λy.a N(1) b1) b2) ∈SN. Let u2 = τ(t2) where t2 is the same as
t but where we have given to the variable H the fresh name z, τ is the
same as σ for the variables in dom(σ) and τ(z) = A[σ(H), 1]. By the
IH, u2 ∈SN. Note that that t2 ∈SN, η(t2) ≤η(t) by Lemma 3.1
and that η(τ, t2) < η(σ, t).
But u2 = (λz.(λy.a b1) b2
−
→
N) and thus
u3 = (λz.(λy.a b1) b2 N(1)) ∈SN. Since u3 reduces to A[u, 1] by using
twice by the γ rule, it follows that A[u, 1] ∈SN. (d) If H is a variable and σ(H) is γ-head reducible, then C[σ(t)] = τ(t′)
where t′ is the same as t but where we have given to the variable H the
fresh name z and τ is the same as σ for the variables in dom(σ) and
τ(z) = C[σ(H)]. The result follows then from the IH since η(τ, t′) <
η(σ, t). 3. Assume that σ(t) is β-head reducible. We have to show that Arg[σ(t)] ∈SN
and that B[σ(t)] ∈SN. There are 3 cases to consider. (a) If H is an abstraction, the result follows immediately from the IH since
then Arg[σ(t)] = σ(Arg[t]) and B[σ(t)] = σ(B[t]). (b) If H is a variable and σ(H) = λy.v for some v. Then Arg[σ(t)] = N(1) ∈
SN by the IH and B[σ(t)] = (v[y := N(1)] tail(−
→
N)) = (z tail(−
→
N))[z :=
v[y := N(1)]].
By the claim, (z tail(−
→
N)) ∈SN.
By the IH, v[y :=
N(1)] ∈SN since type(N(1)) < size(A). Finally the IH implies that
B[σ(t)] ∈SN since type(v) < size(A). (c) H is a variable and σ(H) = (R −
→
M ′) where R is a β-redex.
Then
Arg[σ(t)] = Arg[σ(H)] ∈SN and B[σ(t)] = (R′ −
→
M ′ −
→
N) where R′ is
the reduct of R. But then B[σ(t)] = τ(t′) and t′ is the same as t but 6 where we have given to the variable H the fresh name z and τ is the
same as σ for the variables in dom(σ) and τ(z) = (R′ −
→
M ′). Note that
that t′ ∈SN and η(t′) ≤η(t), by Lemma 3.1. We conclude by the IH
since η(τ, t′) < η(σ, t). 4. We, ﬁnally, have to show that, for each i, A[σ(t), i] ∈SN. There are again 3
cases to consider. (a) If the β-redex put in head position is some N(j) and M(j) was already
a redex. Then A[σ(t), j] = σ(A[t, j]) and the result follows from the IH. (b) If the β-redex put in head position is some N(j) and M(j) = (x a) and
σ(x) = λy.b then A[σ(t), i] = λy.(σ(H) N(1) ... N(j −1) b) σ(a) N(j +
1) ... N(n)).
Since type(σ(a)) < size(A) it is enough, by the IH, to
show that λy.(σ(H) N(1) ... N(j −1) b) y N(j + 1) ... N(n)) and so,
by Lemma 3.3, that (σ(H) N(1) ... N(j −1) b N(j + 1) ... N(n)) ∈
SN.
Since type(b) < size(A) it is enough, by the IH, to show u =
(σ(H) N(1) ... N(j −1) z N(j + 1) ... N(n)) ∈SN. Let t′ = (H −
→
M ′)
where −
→
M ′ is deﬁned by M ′(k) = M(k), for k ̸= j, M ′(j) = z. Since
t = t′[z := (x a)] and u = σ(t′) the result follows from Lemma 3.1 and
the IH. (c) If, ﬁnally, H is a variable, σ(H) = (H′ −
→
M ′) and the β-redex put in head
position is some M ′(j). Then, A[σ(t), j] = τ(A[t′, j]) where t′ is the same
as t but where we have given to the variable H the fresh variable z and
τ is the same as σ for the variables in dom(σ) and τ(z) = A[σ(H), j].
Note that that t′ ∈SN and η(t′) ≤η(t), by Lemma 3.1. We conclude
by the IH since η(τ, t′) < η(σ, t).
□ Corollary 3.1
Let t be a typable term. Then t is strongly normalizing.
Proof
By induction on size(t). If t is an abstraction or a variable the result is
trivial. Otherwise t = (u v) and, by the IH, u, v ∈SN. Thus, by Theorem 3.3,
(u y) = (x y)[x := u] ∈SN and, by applying again Theorem 3.3, (u v) = (u y)[y :=
v] ∈SN.
□",0
"The Cloud has become a principal paradigm of computing in the
last ten years, and Computer Science curricula must be updated to re-
ﬂect that reality. This paper examines simple ways to accomplish cur-
riculum cloudiﬁcation using Amazon Web Services (AWS), for Com-
puter Science and other disciplines such as Business, Communication
and Mathematics. 1
Introduction Whether aware of it or not, most computer users have moved to the Cloud
in the last ﬁfteen years. They have done so by using Webmail, Google Docs,
photo-sharing services or on-line gaming. Business has followed the trend
a few years later, by replacing or supplementing in-house data centers with
cloud services.
At Universities, Computer Science (CS) and Information Technology (IT)
faculty have been using the Cloud just like everyone else, but many have
realized over the last decade that they can access compute power in the cloud
without making large capital investments on campus, and can start using
services, such as AWS virtual computers, with ease and speed. Since faculty
work on research projects with students, senior students had to acquire cloud
skills as well.
At the same time, job sites such as LinkedIn listed knowledge of the
Cloud as the top job skill over the last ﬁve years [1], and currently 14% ∗California State University Channel Islands, Professor in the Department of Computer
Science, URL: www.msoltys.com, Email: michael.soltys@csuci.edu 1 arXiv:2002.04020v1  [cs.CY]  10 Feb 2020 of all job listings require some understanding of the fundamentals of the
cloud. As students and their families invest in costly education, they are
keenly aware of this job market reality [2] (see blog entry [3]), and especially
of AWS’ growing dominance in this ﬁeld [4]. Therefore, a demand for cloud
instruction arose on campuses, and some faculty started adding cloud content
to the curriculum. One of the earliest adopters of a cloud curriculum was
the Santa Monica College [5].
This paper examines methods to cloudify the curriculum with AWS [6]. It
starts from the assumption that the CS and IT curricula are well formed and
mature, and that radical changes are not desirable. Instead, non-invasive and
easy to implement ideas are proposed. Furthermore, the paper examines how
cloud content can be presented to students in other disciplines, in particular
students in Business, Communication and Mathematics. We concentrate on
four-year Universities, while we are aware the two-year Community Colleges
often led the charge in cloudiﬁcation.
A few years ago we started Engineering on our campus at California
State University Channel Islands, with Mechatronics Engineering. We found
it very helpful to discuss with other Universities their experience in starting
Mechatronics, and we found it especially helpful to consult the experience of
the University of Utah which was recorded in [7]. The goal of this paper is
to provide a similar template but for cloud adoption. The paper is meant to
oﬀer suggestions, rather than deﬁnitive solutions, and represents the opinions
of the author.
Each institution should adopt the cloud according to its
particular circumstances.
Feedback to the author from those who have experience in bringing the
Cloud to the curriculum would be most welcome. 2
Cloudifying the Curriculum At the beginning of the discussion on curriculum cloudiﬁcation we decided to
examine the students we intended to serve, and we grouped them as follows: 1. Computer Science and Information Technology undergraduate majors. 2. Masters in Computer Science. 3. Business, Communication and Mathematics majors. 4. Working professionals. 2 As these students are served by diﬀerent curricular pathways, this allowed
us a divide-and-conquer approach to cloudiﬁcation. And by “serve” we mean
that we expose them to the Cloud to the degree they want, so that they can
take advantage of the cloud-favorable job market. 2.1
CS and IT The ﬁrst group, CS and IT students, form our largest contingent of students,
about ﬁve hundred currently, but growing quickly, as we have doubled our
number of such majors over the last three years. About 80% of those students
are in CS. 2.1.1
CS Our CS curriculum is based on the ACM curriculum 2013 ([8]), where the
four main pillars of AWS, Compute, Storage, Databases and Networking,
are covered in depth. For example, Compute and Storage are covered in a
sequence of three “systems” classes, COMP 162, 262 and 362, that cover
everything from computer architecture and assembler to advanced topics in
operating and ﬁle systems.
Databases are covered in our senior course COMP 420, and Networking
in our senior course COMP 429. CS students also receive a solid grounding
in programming capped with a junior Software Engineering class COMP 350.
Thus, our senior CS students are conceptually more than ready to study
for the AWS Solutions Architect certiﬁcation, and in fact any of the AWS
certiﬁcations, except the professional one that requires some years of practical
experience.
A light-footed approach to deliver cloud content to CS majors is to “sprin-
kle” it throughout our undergraduate classes as use cases and illustrations
of concepts being taught. This approach relies on the faculty discre-
tion to decide on examples and platforms1, and does not require time-
consuming curricular changes and paying attention to accreditation issues 1See [9] for a discussion of the meaning of “Academic Freedom.” Keep in mind that
an instructor is responsible for covering the content in the syllabus of a course, as the
syllabus is a contract between the university and the student. However, point 11 of [9]
says that “Academic freedom gives faculty members substantial latitude in deciding how
to teach the courses for which they are responsible.” In our case, instructors can illustrate
the concepts with examples, use cases and technologies of their choosing, especially since
syllabi tend to be technology agnostic. 3 with ABET2 and WASC3. Core concepts are presented in class as promised
in the class syllabus, but the illustration of those concepts with AWS tools
and use cases falls under the discretion of the instructor.
Some instructors may choose to use AWS, some may not. However, for
all students interested in absorbing more cloud material we oﬀer two ways
to do so: one, to use AWS in their capstone (a full year class in senior year),
two, to take a special topics COMP 490 class that follows the AWS Solutions
Architect certiﬁcation. Both ways may be chosen.
Of course, as cloud content becomes embedded in the curriculum in the
upcoming years, reﬂecting the new paradigm in the industry, some instruc-
tors may choose to formalize it in the syllabi, possibly requiring curriculum
committee approvals; however, this does not preclude teaching the material
already now under our “faculty discretion” model explained above. 2.1.2
IT In our school the IT program was brought about in order to accommodate
transfer students from local colleges. The program was built using a grant,
and at the time of its founding it was designed to emphasize web development.
The principal diﬀerence between CS and IT is that IT students take less
mathematics classes, less programming, and have more electives (partly to
assist in the transfer that would allow their community college classes to
count toward graduation).
In recent years we have been looking for a new, more current, emphasis
for our IT program, and we considered business or cybersecurity. But we
started discussing the possibility to make it Cloud centric by adding AWS
to the curriculum more deliberately. This would require better coordination
with Community Colleges. However, at this moment, the approach to oﬀering
AWS to our IT students is similar to CS: the choice to take an AWS special
topics class in the senior year. 2.2
Masters in CS In the Masters program in Computer Science, we introduced the cloud into
three core classes: Networking, Security and Software Engineering. 2https://www.abet.org/
3https://www.acswasc.org/ 4 2.2.1
Networking For the last decade we oﬀered a graduate course in Networking with the title
“Cloud Computing” (COMP 529) [10]. This course is a graduate version of
our undergraduate Networking class (COMP 429). Recently we introduced
the material from the AWS Solutions Architect (SA) certiﬁcation into its
syllabus. The symbiosis of Networking and AWS is a very natural way to
deliver this class, and students get an oﬀering that not only covers the entire
SA certiﬁcation, but goes far beyond it. For example, students learn about
routing at the IP level, and ﬂow and congestion algorithms at the TCP level,
as well as most of the content in [11]. 2.2.2
Security Cybersecurity is one of the areas of emphasis in our department, and we
have been teaching a graduate version of Cybersecurity (COMP 524) for
several years [12]. (The undergraduate version of that course is COMP 424.)
Since the author received an AWS Specialty Security (SS) certiﬁcation in
December 2019, we have been updating the class material to include the
entire certiﬁcation curriculum as use cases and examples. The author wrote
a manuscript containing the notes for the class, with advanced material such
as Cryptography, Malware, Distributed Denial of Service attacks, as well as
an overview of most of the tools in Kali Linux.
Adding the material in the AWS SS certiﬁcation allows us to update the
course to a more current oﬀering as it now includes security in the Cloud.
This includes details of Identity Access Management (IAM), S3, Security
Policies, Logging and Monitoring, Key Management System (KMS) and use
cases of CloudTrail, CloudWatch, Inspector, Cloud Formation, Cloud Conﬁg,
and other AWS tools that allow a more hands-on approach.
The class has really improved as the result of including the AWS SS
material. For example, it is one thing to talk about the importance of data
encryption at rest for PCI compliance; it is quite another to demonstrate to
the students the automated enforcement of such compliance with Amazon
Macie machine learning tool. The upgraded class will be taught for the ﬁrst
time in the fall 2020. 5 2.2.3
Software Engineering We invested quite heavily in our Software Engineering oﬀering, both at the
undergraduate and graduate levels. A signiﬁcant portion of our students are
employed as programmers, and we want them to be team leaders and to
think as engineers in their future roles in the industry. Programming and
Software Engineering skills are in great demand by employers.
Hence, we invest eﬀort in teaching Software Engineering, using the Agile
Methodology, and standard engineering techniques such as design, require-
ments, speciﬁcations, etc. Students learn to work on a team, using collabo-
rative tools such as GitHub.
We have taught the graduate version of Software Engineering, COMP
550, using AWS as a platform. There were two large teams of students: the
ﬁrst team built a collaborative code editor similar in spirit to Google Docs,
but with text highlighting for a few languages. The second team used Unity
to develop a network based game. Both use AWS to host their application
and data storage of user information.
The plan for the near future is for some faculty to obtain the AWS De-
veloper certiﬁcation, and combine the curriculum in that certiﬁcation with
our own very well developed content. 2.3
Business, Communication and Mathematics It is easy to include Mathematics majors in our cloudiﬁcation initiative, as
CS and Math are closely integrated as departments, and our majors are not
far from being double-majors. Hence what we wrote about CS students in
Section 2.1.1 applies directly to Math students. Mathematics also delivers
emphases in Data Analytics and Imaging, both of which can take advantage
of AWS tools such as Amazon Athena, Redshift, Rekognition, etc. We plan
to introduce these tools into Mathematics.
As was mentioned in the Introduction, LinkedIn has listed the Cloud
as a top job skill over the last ﬁve years (in 2020 the top spot may be
replaced by Blockchain [13], but even so the Cloud will be a close second).
However, the top soft skills include persuasion, collaboration, adaptability
and emotional intelligence, associated with Business and Communication
students. As mentioned in the Introduction, 14% of jobs will require some
cloud knowledge, and so Business and Communication students can expect to
work in environments where the Cloud will be central to a business mission. 6 We have designed a new course, Online Communication and Society,
COMP 347, cross-listed with Communication, where we have embedded the
Cloud with Business and Communication students in mind [14]. In this class,
the students learn the basics of AWS, launch an EC2 instance, and install a
LAMP stack on it in order to install Wordpress. Once Wordpress is running,
they integrate it with social media (e.g., LinkedIn and Twitter), and publish
content as posts and pages.
In order to make the exercise more practical, we propose to the students
as framework the following use case: you have been hired by a non-proﬁt with
few resources but big ambitions, and you need to promote your mission with a
minimal budget. The content was developed by the author, and the technical
material was based on AWS excellent whitepapers, in particular [15, 16].
It is important to remember that for most companies, the decision to move
to the cloud is principally a business decision, not an IT decision. Both the
basic AWS Cloud Foundations certiﬁcation, and the advanced AWS Solutions
Architect certiﬁcation emphasize the business aspect of cloud solutions. This
is a natural arena for business students, and as the CEO of AWS mentioned
at 2019 re:Invent in Las Vegas, only 3% of world IT is currently in the cloud
(while it comprises over 60% of all IT spending!), and so as more IT moves
to the cloud, this will be an area of activity for business oriented students.
One more thing ought to be mentioned: we have joined the AWS Activate
for Startups program, where we can nominate startups for low cost AWS
credits. This works well with our campus entrepreneurship initiative. 2.4
Working professionals We are oﬀering two certiﬁcation classes open to non-matriculated students
(i.e., anyone who wants to take them): AWS Cloud Foundations and AWS
Solutions Architect, both of them through AWS Academy [17]. These classes
are aimed at working professionals, they are eight weeks long, and delivered
mostly online. We will seek approval for a Cloud Certiﬁcate through our
campus curriculum committee. The mechanism for delivering this certiﬁcate
program is our Extended University4, which does not receive State funding,
and is therefore more ﬂexible in how it can mount programs. 4Extended University is often called the “School of Continued Education,” but diplomas
from EU are simply diplomas from our school. It is an internal administrative entity, but
part of the university as such. 7 The Cloud certiﬁcate is a service to our community, especially the Navy
(there are two large Navy bases in Ventura County), as well as the industry
comprised of the Department of Defense contractors as well as companies in
the “101 Technology Corridor.” Furthermore, as [18] writes in his paper The
great enrollment crash, most universities are expecting up to a 15% drop
in enrollment due to demographics over the next decade, and serving the
working population will become only more important to our viability as an
institution. 3
Ancillary Eﬀorts Instructor training and certiﬁcation: there has to be a critical mass
of instructors, both tenure-track faculty and lecturers, who are willing to
learn and deliver the AWS content. We currently have two AWS Academy
accredited instructors, and three more in the pipeline. The author is the
principal point of contact for AWS Academy on campus, and can nominate
interested educators.
Communicate to the faculty the beneﬁts of cloudifying: tenure-
track faculty have invested many years into their careers: a PhD, then post-
docs, then working toward tenure, this can be 15 years or more.
At the
same time, in an era of great specialization, they work to become respected
members of a research community. Their time to learn the foundations of
other areas is very limited.
One way to motivate them is to present the
advantages of AWS in their research; this is one of the aims of the AWS
Ambassador program.
Curriculum Changes vs Faculty Discretion: As already discussed
in the paper in Section 2.1.1, it may be more eﬀective at ﬁrst to invoke fac-
ulty discretion in introducing cloud content rather than an eﬀort to codify
a new curriculum. Curricular decisions are always lengthy, take up several
teaching cycles, and the department may end up missing the boat of cloud-
iﬁcation. We argue that it is best to cloudify now with faculty discretion,
while pursuing curricular changes as they become necessary. This provides
a mechanism to synchronize the diﬀerent speeds of innovation in industry
versus the academia.
Advisory Board: a departmental advisory board can be a great ally in
bringing about the cloudiﬁcation of the curriculum. For one, the members
of the board are probably contemplating a possible move to the cloud, and 8 understand the need to train the workforce. In our case we are lucky to have a
large and supportive advisory board, comprised of about 20 local companies,
and we kept them abreast of the AWS initiative from the beginning. [19]
Support of the administration: faculty have ten great ideas every day,
but they all require precious and limited campus resources. It is imperative to
have the support of the administration while pursuing this initiative, which,
as in the case of faculty, requires communicating the beneﬁts and low costs
of the eﬀort. In our case we were very fortunate to have such support. It
helped to receive a $35K credit grant from AWS. 4
Conclusion Cloud Computing is a new paradigm and also an inversion of an old one.
When the telegraph was invented in the early 1800s, it was “smart at the
edges” (the unit and its operator), while the connecting network was very
simple (just a cable and repeaters [20]). Then came the telephone, with dumb
terminals (rotary phones) at the edges, and a complex network inside with
switchboards for circuit-switching.
Then the paradigm was inverted once
again with the Internet, where now we had smart terminals at the edges,
and a relatively simple packet-switching network inside. The simplicity of the
network allowed for the quick innovation that followed, and the result of that
innovation was to invert the paradigm once again with Cloud Computing,
where now computers at the edge are simply entry portals into a complex
network, i.e, “the Cloud” [21].
Computer Science is a ﬁeld with a fast paced proposal of new paradigms,
and curricula must, on the one hand, oﬀer the fundamentals of a now well
establish ﬁeld, but on the other hand be nimble enough to accommodate
the fast rate of innovation. One way to accomplish this is to oﬀer a core of
fundamentals in each area, illustrated with use cases from the latest trends.
We propose that this is a practical and eﬀective approach to introduce Cloud
Computing into the curriculum. 5
Acknowledgments We are grateful to the AWS Academy and AWS Educate teams for guiding
us through the process, and for all high quality educational materials they 9 oﬀer for free to participating institutions. We are also grateful to Michael
Berman for comments on an early draft.",0
"Cancer, as the uncontrollable cell growth, is related to many branches of biology. In
this review, we will discuss three mathematical approaches for studying cancer biology:
population dynamics, gene regulation, and developmental biology. If we understand
all biochemical mechanisms of cancer cells, we can directly calculate how the cancer
cell population behaves. Inversely, just from the cell count data, we can use population
dynamics to infer the mechanisms. Cancer cells emerge from certain genetic mutations,
which aﬀect the expression of other genes through gene regulation. Therefore, knowl-
edge of gene regulation can help with cancer prevention and treatment. Developmental
biology studies acquisition and maintenance of normal cellular function, which is in-
spiring to cancer biology in the opposite direction. Besides, cancer cells implanted into
an embryo can diﬀerentiate into normal tissues, which provides a possible approach
of curing cancer. This review illustrates the role of mathematics in these three ﬁelds:
what mathematical models are used, what data analysis tools are applied, and what
mathematical theorems need to be proved.
We hope that applied mathematicians
and even pure mathematicians can ﬁnd meaningful mathematical problems related to
cancer biology. 1
Introduction Cancer, as a group of more than 100 types of diseases, is one of the main causes of human
death. It is the dream of countless scientists to further understand cancer and ﬁnally cure
cancer. However, many experiments might cause harm [36], and cannot be performed on
human beings. Therefore, given limited types of data, we have to develop indirect methods
to study cancer biology, especially with mathematical tools.
In this review, we will go through some mathematical biology studies in these ﬁelds,
and discuss how research in these ﬁelds improves our understanding of cancer biology.
This review is targeted to researchers who have backgrounds in mathematics and want to
contribute to cancer biology research. We will discuss what mathematical models are used,
what data analysis tools are applied, and what mathematical theorems need to be proved. 1 arXiv:2301.11126v1  [q-bio.OT]  24 Jan 2023 Population dynamics studies how the population level changes along time under dif-
ferent circumstances. The growth of tumor can be described by population dynamics. In
Section 2, we introduce some mathematical studies in cancer cell population dynamics in
normal environment and under treatment.
These studies illustrate what mathematical
problems arise in population dynamics, and what insights can be obtained from mathe-
matical analyses of population data. Speciﬁcally, there are still open problems arising from
related stochastic models.
Gene regulation describes how the expression of one gene aﬀects the expression of an-
other gene. Cancer-related genetic mutations can deviate the normal cell fate through gene
regulation. Nevertheless, gene regulation is diﬃcult to determine directly by experiments.
In Section 3, we introduce some mathematical studies in the inference of gene regulation
(both mutual regulation and autoregulation) from certain types of data. These studies
represent a broad ﬁeld of developing mathematical inference methods in gene regulation
and other subjects. Various problems in probability and discrete mathematics need to be
solved.
Developmental biology studies how organized normal tissues arise from a single zygote.
It is the opposite of cancer biology, which studies the emergence of abnormal tissues. A
developing embryo can invert the fate of cancer cells. In Section 4, we introduce some
mathematical studies in developmental biology, which could provide insights of studying
cancer biology from the opposite direction. Speciﬁcally, we introduce an inference method
for certain experiments, which leads to a coloring problem in lattice.
There have been many papers and books discussing the role of mathematics in cancer
biology [37, 68, 1, 19, 2]. We hope that this review can provide a little more insights and
encourage applied mathematicians and even pure mathematicians to work on interesting
problems related to cancer biology and contribute to the cure of cancer. 2
Population dynamics The most signiﬁcant feature of cancer cells is the uncontrollable growth. Therefore, mea-
suring and studying the growth of cancer cell population should be a central topic in cancer
research. Many studies build models for cancer cells living in a free environment [28]. Some
other studies consider the eﬀect of nutrients [25] and drugs [3].
Traditional population dynamics models are deterministic, based on ordinary diﬀeren-
tial equations (ODEs) [78] or partial diﬀerential equations (PDEs) [56]. Since cell growth
is highly stochastic, various stochastic processes are used to model cellular clonal growth
and evolution: birth-death processes [52], Poisson processes [7], Markov chains [21], ran-
dom sums of birth-death processes [16], and branching processes [28]. Other stochastic
approaches, such as random dynamical systems [74], lifted Markov chains [61], continuous
Markov processes [45, 73], or more complicated models based on the above methods [79],
also have the potential of modeling cell population growth. 2 2.1
Phenotypic equilibrium phenomena Mathematical models based on diﬀerent biological assumptions can produce diﬀerent pre-
dictions, which can be veriﬁed by experiments. For instance, in the traditional hierarchical
model [47, 29, 14], the transition from cancer stem cells to non-stem cancer cells is ir-
reversible. In recent studies, there is evidence indicating that non-stem cancer cells can
convert back to cancer stem cells in certain cancer types [39, 11, 46, 72, 17]. In math-
ematical models that consider such reversible transitions, there are two phenomena that
do not exist in the hierarchical model: overshoot and phenotypic equilibrium [40, 13].
Overshoot means that if the initial population value is lower than the steady state, the
population might ﬁrst increase above the steady state and then return to it. Phenotypic
equilibrium means that starting from any proportions of diﬀerent phenotypes, the ﬁnal
proportions always converge to the same constants. For example, breast cancer has three
cell phenotypes: stem-like, basal, and luminal. Starting from any one phenotype, after cul-
tivating for some time, the ﬁnal proportions are always stem-like 2%, basal 97%, luminal
1% [21]. Therefore, one can perform corresponding experiments to detect the existence
of such phenomena, and determine whether the cancer cell type in the experiment has
conversions from the non-stem state to the stem state.
In the following, we will review diﬀerent models for cancer cell population dynamics, and
corresponding explanations for the phenotypic equilibrium phenomena. We consider n phe-
notypes in the cancer cell population: Y1, Y2, · · · , Yn. Deﬁne ⃗
X(t) = [X1(t), X2(t), · · · , Xn(t)]
to be the population at time t, where Xi(t) is the population of phenotype Yi. Pi(t) =
Xi(t)/[Pn
i=1 Xi(t)] is the proportion of phenotype Yi, which is not deﬁned if Pn
i=1 Xi(t) =
0. Deﬁne ⃗
P(t) = [P1(t), P2(t), · · · , Pn(t)]. 2.1.1
Markov chain model The ﬁrst explanation for the phenotypic equilibrium phenomena is based on a Markov
chain model [21]. In this model, ⃗
P(t) satisﬁes d⃗
P(t)/dt = ⃗
P(t)Q, where Q is the transition rate matrix, satisfying ⃗
1Q′ = ⃗
0. This is the evolution equation
for a Markov chain. From classical Markov chain theory, we have the following [41]: If
Q is irreducible, then starting from any initial probability distribution ⃗
P(0), ⃗
P(t) always
converges to the same vector ⃗
v, which satisﬁes ⃗
vQ = ⃗
v. This explains the phenotypic
equilibrium phenomena in the Markov chain model. However, this model has been criticized
as unrealistic [28], since cells can grow, and diﬀerent phenotypes might have diﬀerent
growth rates. 3 2.1.2
ODE model A classic model for population dynamics is the linear ODE model. We assume that the
population (not the proportion) satisﬁes d ⃗
X(t)/dt = ⃗
X(t)A, where A = {ai,j} for i, j = 1, . . . , n, and ai,j ≥0 for i ̸= j [78]. To study the limiting
behavior of ⃗
X(t), we ﬁrst need a result in matrix theory [32]: Theorem 1. A has a real eigenvalue λ1, such that for any eigenvalue µ ̸= λ1, ℜ(µ) < λ1.
λ1 has a left eigenvector ⃗
u = (u1, . . . , un) that satisﬁes ui ≥0 and Pn
i=1 ui = 1. Now we can present the convergence result in the ODE model [78]: Theorem 2. Assume that λ1 is a simple eigenvalue of A. Starting from any initial value
⃗
X(0) except for a zero-measure set, we have limt→∞⃗
P(t) →⃗
u. Therefore, the proportion ⃗
P(t) will converge to the unique attracting ﬁxed point ⃗
u.
This explains the phenotypic equilibrium phenomena in the ODE model. Nevertheless, on
single cell level, the cell growth is essentially stochastic. The ODE model is just a mass
action approximation. 2.1.3
Branching process model A more realistic model for cell population dynamics is the multi-type branching process
model. In this model, diﬀerent cells at the same time point are independent. For each cell
of the Yi type, the behavior is described by the following: Yi
αi
→di1Y1 + di2Y2 + · · · + dinYn. Therefore, a Yi cell lives an exponential time with expectation 1/αi and turns into di1 Y1
cells, di2 Y2 cells, · · · , din Yn cells, where di1, di2, · · · , din are random variables in N. The
random coeﬃcients di1, di2, · · · , din may not be independent, but they are independent
with the exponential waiting time. For example, a symmetric division Y1 →2Y1 means
(d11, d12, · · · , d1n) = (2, 0, 0, · · · , 0).
A transition Y1 →Y2 means (d11, d12, · · · , d1n) =
(0, 1, 0, · · · , 0). A death Y1 →∅means (d11, d12, · · · , d1n) = (0, 0, 0, · · · , 0). Deﬁne ai,i = αi(Edii −1) ≥−αi,
ai,j = αjEdij for i ̸= j. Then the matrix A = {ai,j} corresponds to the coeﬃcient matrix in the ODE model. We
have the same λ1 and ⃗
u as in Theorem 1.
In the branching process model, due to randomness, we need to assume that no pheno-
type Yi has Xi(t) = 0 for any t > T. This condition is called non-extinction. We can prove
the convergence of ⃗
P(t), namely a law of large numbers, conditioned on non-extinction. In
a monograph on branching process in 1972, a law of large numbers is proven for certain A
[4]: 4 Theorem 3. Assume that λ1 is positive, and it is a simple eigenvalue of A. Assume A
is irreducible. Conditioned on non-extinction, ⃗
P(t) →⃗
u almost surely. This law of large numbers can be used to explain the phenotypic equilibrium phenom-
ena.
Nevertheless, the irreducibility of A means that any two phenotypes can convert
between each other (might through some other phenotypes). This might not hold for some
cancer types. The irreducibility assumption is partially lifted in a 2004 paper [26] (only as-
sume that the phenotype with the highest growth rate can convert to any other phenotype),
and fully lifted in a 2017 paper [28]: Theorem 4. Assume that λ1 is positive, and it is a simple eigenvalue of A. Conditioned
on non-extinction, ⃗
P(t) →⃗
u almost surely. This explains the phenotypic equilibrium phenomena in the branching process model.
In the branching process model, the waiting time for cell division is exponential, which
is not realistic. To overcome this ﬂaw, one can generalize the law of large numbers to
non-Markovian branching processes. The trick is to add auxiliary phenotypes, so that the
overall waiting time is close to the real distribution. Nevertheless, this trick is not rigorous,
and general cases are still open. This illustrates that the need in cancer research might
inspire new mathematical results. 2.2
Inferring the existence of multiple phenotypes The experiments for phenotypic equilibrium need to measure the cell states.
In fact,
even without knowing the cell state, but just counting cell numbers, we can infer the
existence of multiple phenotypes.
In some experiments, leukemia cells were cultivated.
Some populations started from 10 cells, while other populations started from 1 cell. The
cell number of each population was measured everyday.
By comparing cell population
levels, we can ﬁnd that after reaching the same population level, the number of initial
cells still aﬀects cell behavior. Speciﬁcally, when reaching cell area 5 (approximately 2500
cells), the growth rates of populations growing from 10 initial cells are higher than those
of populations from 1 initial cell. See Fig. 1 for translated growth curves. This diﬀerence
indicates the existence of multiple phenotypes with diﬀerent growth rates: starting from 10
initial cells, it is very likely that at least one cell is of the fast type, which will dominate the
population; starting from 1 initial cell, the population might only have the slow type, which
corresponds to a lower growth rate [67]. See Fig. 2 for a schematic illustration. Certainly,
this explanation needs to be veriﬁed by further experiments that measure the phenotypes,
especially with gene sequencing. Without such data, one needs to run a parameter scan to
conﬁrm that the model is robust. 2.3
Cancer cell under treatment Various drugs can be used to kill cancer cells. Besides traditional chemical drugs, aptamers
[60] and T cells [15] can also be used. Therefore, the population dynamics of cancer cells 5 0
5
10
15
time (day) after
reaching cell area 5 5 10 20 40 80 cell area 0
5
10
15
time (day) after
reaching cell area 5 5 10 20 40 80 cell area Figure 1: Growth curves of cell populations starting from the time of reaching 5 area units,
with a logarithm scale for the y-axis. The x-axis is the time from reaching 5 area units.
Red (left) and blue (right) curves correspond to 10 or 1 initial cell(s). Although starting
from the same population level, patterns are diﬀerent for distinct initial cell numbers. The
N0 = 1-cell group has higher diversity and lower average population level. This ﬁgure
corresponds to the contents of Subsection 2.2. under treatment is also worth studying.
The behavior of cancer under treatment can be complicated. The tumor cell regrowth
after treatment is generally regarded as the result of a Darwinian somatic evolution: Given
suﬃcient genetic variability in a suﬃciently large initial cell population, it is very likely
that the population contains cells with genomic mutations that lead to drug-resistance.
A single cell with such a mutation will survive the treatment and clonally expand, thus
driving the tumor regrowth under treatment. If this simple Darwinian model holds, then
the recurrence time of cancer should be positively related to the drug dose. The reason is
that at a higher drug dose, the number of survived cells is smaller, thus taking a longer time
to recover. However, in some experiments, it is observed that the treatment eﬀect does
not always become better when the drug dose increases. The treatment eﬀect gradually
saturates. If the drug dose is suﬃciently high, the cancer cell population recovers even
faster. Due to such phenomena, we have to introduce more complicated mechanisms. One
possible approach is to assume that at higher drug doses, the killing rate does not further
increase, but the drug-induced transition rate from the sensitive state to the resistant
state still increases [3]. Therefore, when we increase the drug dose, the treatment eﬀect
ﬁrst increases, since the killing rate increases. Then the killing rate is almost saturated,
and the treatment eﬀect does not further increase. Finally, the killing rate stops at its
maximum, and the transition rate still increases, which decreases the treatment eﬀect.
Besides the phenomena that higher drug dose leads to worse treatment eﬀects, de-
creasing the drug dose might lead to overcompensation, which means that the population 6 Figure 2: Schematic illustration of the qualitative argument: Three cell types and growth
patterns (three colors) with diﬀerent seeding numbers. One N0 = 10-cell well will have
at least one fast type cell with high probability, which will dominate the population. One
N0 = 1-cell well can only have one cell type, thus in the microculture ensemble of replicate
wells, three possible growth patterns for wells can be observed. This ﬁgure corresponds to
the contents of Subsection 2.2. recovers to a higher level than the initial population [70].
Therefore, the treatment of
cancer is a highly complicated decision process. It is necessary to introduce reinforcement
learning (especially with stochasticity) into this ﬁeld [50, 65, 76, 66, 77]. 3
Gene regulation Coding genes are transcribed to mRNAs and translated to proteins. Genes (through their
corresponding proteins) can regulate the expressions of other genes. The origin of cancer is
certain genetic mutations (e.g., p53) [22]. Such mutations change the gene expression pat-
terns through gene regulation [49], and the cell landscape also transforms [35]. Therefore,
the study of gene regulation is important in understanding and curing cancer [5]. 3.1
Framework for gene regulation inference Genes and their regulatory relations form gene regulatory networks (GRNs). Since the gene
expression and regulation are mostly conﬁned within living cells, it is almost impossible to 7 determine gene regulation with direct biochemical methods. Nevertheless, there have been
many methods to infer the GRN structure, namely whether one gene regulates another,
from experimental data [6, 9, 23]. Such inference methods use models based on diﬀerent
assumptions, and need diﬀerent types of data.
To unify diﬀerent approaches for this
problem, there is a framework for data classiﬁcation, which has four major dimensions:
(1) whether measuring gene expression levels, or the levels of certain phenotypes that are
determined by such genes; (2) whether the measurement is for a single cell (stochastic) or
for a large population of cells (deterministic); (3) whether the measurement is for one time
point or multiple time points; (4) whether one can add interventions on certain genes. If the
measurement is on single-cell level and at multiple time points, depending on whether the
same cell can be measured multiple times, there is an extra dimension of whether we have
the joint probability distribution or just marginal probability distributions. In total, there
are 20 diﬀerent data types [63]. Diﬀerent data types need diﬀerent inference methods, and
some data types (especially non-interventional) are not useful in inferring gene regulation.
Due to the large amount of related data, machine learning is a promising future direction
for inferring gene regulatory relations [24]. See Table 1 for the classiﬁcation of data types
and corresponding methods that can be applied to infer gene regulations. 3.2
Inference methods for mutual regulation We brieﬂy introduce some inference methods for mutual regulation. Each method may
apply in multiple scenarios. Diﬀerent methods need diﬀerent assumptions. DAG assump-
tion means that the GRN is a directed graph that has no directed cycles. MF assumption
means that the joint probability distribution of gene expressions provide exactly all and
only the conditional independence relations implied by the GRN. LS assumption means
that the gene expression satisﬁes a linear ODE system. PB assumption means that if all
paths between two genes in a GRN have been blocked by intervened genes, then adding
intervention on one gene cannot further aﬀect the other gene. 3.2.1
Causal DAG method This method needs the MF and DAG assumptions, and applies to Scenarios 1 and 3b
[20]. In this case, the GRN is a causal DAG, and we can use the conditional independence
relations to determine the corresponding causal DAG. However, some diﬀerent causal DAGs
(all sharing the same edges, but with diﬀerent directions) produce the same conditional
independence relations. Therefore, we can only partially determine the GRN structure. 3.2.2
Path blocking relation method in gene expression This method needs the PB assumption, and applies to Scenarios 2, 4b, 6, 8 [63]. From such
data types, we know whether some genes fully block all paths between two genes. There is 8 One-Time
Time Series Non-
Intervention
Intervention
Non-
Intervention
Intervention Gene
Expression Single-
Cell Scenario 1 MF+DAG:
partial Scenario 2 PB: full
DAG: partial
MF+DAG:
full Scenario 3 a/b 3a Joint:
UC: full
3b Marginal:
MF+DAG:
partial Scenario 4 a/b 4a Joint:
UC: full
4b Marginal:
LS: full
PB: full
DAG: partial
MF+DAG: full Bulk Scenario 5 No Scenario 6 PB: full
DAG: partial Scenario 7 No Scenario 8 LS: full
PB: full
DAG: partial Phenotype Single-
Cell Scenario 9 No Scenario 10 PB: partial Scenario 11 a/b No Scenario 12 a/b PB: partial
LS+DAG:
partial*
PB+LS+DAG:
partial* Bulk Scenario 13 No Scenario 14 PB: partial Scenario 15 No Scenario 16 PB: partial
LS+DAG:
partial*
PB+LS+DAG:
partial* Table 1: GRN structure inference with diﬀerent data types: under what assumptions, what
structures can be inferred. There are 16 scenarios classiﬁed by the following dimensions
of data types: Gene Expression vs. Phenotype; Single-Cell vs. Bulk; One-Time vs. Time
Series; Non-Interventional vs. Interventional. In Scenarios 3/4/11/12, there is an extra
dimension of Joint vs. Marginal. There are diﬀerent assumptions: PB: path blocking;
DAG: directed acyclic graph; MF: Markov and faithful; LS: linear system; UC: uncondi-
tional. Full/partial/no means all/some/no GRN structures can be inferred. For example,
“MF+DAG: partial” means under MF assumption and DAG assumption, GRN structure
can be partially inferred. The asterisk * in Scenarios 12/16 means for some identiﬁed reg-
ulatory relations, we cannot determine whether they are activation or inhibition [63]. This
table corresponds to the contents of Subsection 3.1.
9 an edge G1 →G2 if and only if all other genes cannot block the path from G1 to G2. All
edges can be inferred in this case. 3.2.3
Ancestor-descendant relation method This method needs the DAG assumption, and applies to Scenarios 2, 4b, 6, 8 [63]. From
such data types, we can use interventions to determine whether one gene G1 has a directed
path to another gene G2 in the GRN (ancestor-descendant relations). We can determine
an equivalent class that share the same ancestor-descendant relations by the following
theorem: Theorem 5. The following procedure describes how to determine certain edges with ancestor-
descendant relations. (1) If Gj is not a descendant of Gi, then we can determine that the
edge Gi →Gj does not exist. (2) If Gj is a descendant of Gi, and Gi has another descen-
dant Gk, which is an ancestor of Gj, then we cannot determine the existence of the edge
Gi →Gj, since we can ﬁnd two GRNs with the same ancestor-descendant relations, one
with Gi →Gj, and one without Gi →Gj. (3) If Gj is a descendant of Gi, and Gi does
not have another descendant Gk, which is an ancestor of Gj, then we can determine that
the edge Gi →Gj exists. Therefore, we can partially determine the GRN structure. About the number of inferred
edges by this method, we have a lower bound [63]: Theorem 6. If the GRN is a connected DAG with n genes, then we can use ancestor-
descendant relations to identify at least n −1 edges. 3.2.4
Ancestor-descendant relation and causal DAG method This method needs the MF and DAG assumptions, and applies to Scenarios 2 and 4b [63].
By the causal DAG method, we can determine all edges in the GRN, but the directions of
some edges are unknown. Using the ancestor-descendant relation, we can determine such
directions. Therefore, the GRN can be fully determined. 3.2.5
Causal time series method This method does not need any assumption, and applies to Scenarios 3a and 4a [20]. This is
basically the same as the causal DAG method, but the time series data naturally determines
the direction of a causal relation. Therefore, we can use conditional independence relations
to fully determine the GRN structure. 10 3.2.6
Linear ODE method This method needs the LS assumption, and applies to Scenarios 4b and 8 [43]. We have a
linear system for the gene expression levels, each with the form dXi/dt = n
X j=1
ajiXj. We can measure Xi(t) at diﬀerent time points under diﬀerent interventions, and calculate
dXi/dt. Then we obtain a linear algebraic equation system for aji. We can solve this and
determine the GRN structure. 3.2.7
Path blocking relation method in phenotype This method needs the PB assumption, and applies to Scenarios 10, 12a/b, 14, 16 [63].
From such data types, we know whether some genes fully block all paths from a gene Gi to
a phenotype P. If a subset S of {G1, . . . , Gn}\{Gi} blocks Gi to P, but any proper subset
of S cannot block Gi to P, then S is called a minimal blocking set. If a blocking set S
is not minimal, then S contains a blocking subset that is minimal. Deﬁne β(Gi) to be all
minimal subsets that block Gi to P. The following theorem describes what edges can or
cannot be inferred from the path blocking relations [63]. Theorem 7. The following procedure describes how to determine certain edges with path
blocking relations for the phenotype. (1) There is an edge Gi →P if and only if β(Gi) = ∅.
(2) If there exists S ∈β(Gi), so that Gj /
∈S, and S cannot block Gj to P, then there is
no edge Gi →Gj. (3) If (2) is not satisﬁed, but there exists S ∈β(Gi), so that Gj ∈S,
then there is an edge Gi →Gj. (4) If β(Gi) = ∅, or for any S ∈β(Gi), we have Gj /
∈S,
and S blocks Gj to P, then we cannot determine whether Gi →Gj exists, since we can
ﬁnd two GRNs with the same path blocking relations, one with Gi →Gj, and one without
Gi →Gj. Using the above procedure, we can partially determine the GRN structure. About the
number of inferred edges by this method, we have a lower bound [63]: Theorem 8. If the GRN with n genes has a directed path from each gene Gi to the
phenotype P, then we can use path blocking relations for the phenotype to identify at least
n edges. 3.3
Inference methods for autoregulation Some genes can regulate (activate or inhibit) their own expressions, which is called autoreg-
ulation. Certain types of data can be used to infer the existence of gene autoregulation,
such as time series data [48, 71, 51] or interventional data for single-cell gene expression 11 [27]. There also exists an inference method that only requires one-time non-interventional
single-cell gene expression data [57]. Although diﬀerent inference methods are based on
diﬀerent types of models, and require diﬀerent data types, the basic idea is the same: build
two models, one with autoregulation, and one without autoregulation; then try to ﬁnd dif-
ferent behaviors between these two models. This idea can be generalized to other problems
of detecting the existence of certain mechanisms. If these two models are indistinguish-
able, it is not a negative result either: one proves that this mechanism is not detectable in
this setting [10]! Here we introduce a simple Markov chain model for gene expression and
present an inference method for autoregulation.
Consider m genes G1, . . . , Gm for a single cell. Denote their expression levels by random
variables X1, . . . , Xm. Each state of this Markov chain, (X1 = n1, . . . , Xi = ni, . . . , Xm =
nm), can be abbreviated as n = (n1, . . . , ni, . . . , nm). For gene Gi, the transition rate of
ni −1 →ni is fi(n), and the transition rate of ni →ni −1 is gi(n)ni. Transitions with
more than one step are not allowed. The master equation of this process is dP(n) dt
=
X i
P(n1, . . . , ni + 1, . . . , nm)gi(n1, . . . , ni + 1, . . . , nm)(ni + 1) +
X i
P(n1, . . . , ni −1, . . . , nm)fi(n) −P(n)
X i
[fi(n1, . . . , ni + 1, . . . , nm) + gi(n)ni]. Deﬁne n¯
i = (n1, . . . , ni−1, ni+1, . . . , nm). Deﬁne hi(n) = fi(n)/gi(n) to be the relative
growth rate of gene Vi. Autoregulation means for some ﬁxed n¯
i, hi(n) is (locally) increas-
ing/decreasing with ni, thus fi(n) increases/decreases and/or gi(n) decreases/increases
with ni. In this model, we have the following result [57]: Theorem 9. In the above model, assume the GRN has no directed cycle that contains gene
Gk. Assume gk(·) is a constant for all n. If Gk has no autoregulation, meaning that hk(·)
and fk(·) do not depend on nk, then the expression level of Gk satisﬁes var(Xk)/E(Xk) ≥1.
Therefore, Gk has var(Xk)/E(Xk) < 1 means Gk has autoregulation. Nevertheless, this method fails to determine whether autoregulation exists when we
have var(Xk)/E(Xk) ≥1 for one gene. It is hypothesized that the requirement for gk(·)
can be dropped: Conjecture 1. In the above model, assume the GRN has no directed cycle that contains
gene Gk. If hk(·) and fk(·) do not depend on nk, then the expression level of Gk satisﬁes
var(Xk)/E(Xk) ≥1. 3.4
Other topics in gene regulation If we cannot add intervention, and cannot measure the same cell(s) multiple times, then
data from diﬀerent time points are essentially the same, and we cannot obtain information 12 of the dynamics. In this case, sometimes it is impossible to build the causal relationship
in gene regulation (including autoregulation) [62].
With the inferred knowledge of gene regulations, one can explain some phenomena in
cancer biology. For example, myeloproliferative neoplasm needs two mutations, JAK2 and
TET2. The order of the appearance of these two mutations can aﬀect clinical features and
responses to therapy [42, 34, 33]. Possible explanations include threshold phenomena and
hidden variables.
Gene regulation has many complicated mechanisms.
Certain genes can translocate
in gene sequences, which are called transposons. Genes at diﬀerent locations can have
diﬀerent eﬀects in gene regulation [38]. Therefore, it is also important to design methods
to determine transposons in gene sequences [55, 31, 18, 12, 75]. The general idea is to ﬁnd
the longest common subsequence, whose complement is all the transposons. Determining
the longest common subsequence is a classic problem in computer science. 4
Developmental biology Developmental biology studies how a zygote diﬀerentiates into normal tissues, while cancer
biology studies how normal cells become malfunctional. Seemingly in opposite directions,
these two subjects are interconnected. The microenvironment of embryo can suppress the
emergence of cancer [8]. In fact, certain cancer cells transplanted to an embryo will diﬀer-
entiate into normal tissues [44]. The development of an embryo is extremely complicated,
so that a cumbersome model is still an oversimpliﬁcation [59]. 4.1
Inference on transplantation experiments The transplantation experiments of cancer cells and embryos indicate a possible direction of
cancer treatment. However, such transplantation experiments are expensive and diﬃcult to
perform. Therefore, inference methods that determine unknown experimental results from
known results are necessary [64]. The idea is inspired by Ising model in statistical physics.
Each transplantation experiment is determined by the donor tissue and the host tissue. We
can construct the similarities between tissues. Then transplantation experiments can be
compared by the similarities between their donor tissues and host tissues. We can assume
that similar experiments are more likely to have similar results. Given some transplantation
experiments, where some results are known, and some results are unknown, we can take a
guess for those unknown results. Then we feed all results to a penalty function. Assume
the transplantation experiments are X1, . . . , Xn; their results are x1, . . . , xn; the similarity
function between two transplantation experiments is F(Xi, Xj); the similarity function
between results is f(xi, xj). Then the penalty function should be P(x1, . . . , xn) = − n
X i=1 n
X j=1
F(Xi, Xj)f(xi, xj). 13 Therefore, for each result conﬁguration (x1, . . . , xn), we can calculate its penalty.
The
penalty can be mapped to its probability: P(x1, . . . , xn) =
e−βP(x1,...,xn) P
x′
1,...,x′
n e−βP(x′
1,...,x′
n) , where β is a parameter that should be determined. We can output the result conﬁguration
with the largest probability, or take expectation for all conﬁgurations to output the prob-
ability distribution for each experiment. In some situations, known results are not deter-
ministic, but stochastic. For example, assume each experiment has two possible results: N
(normal) and A (abnormal). Assume one experiment has result P(N) = 1/3, and the other
experiment has result P(N) = 2/3. We can assume those two experiments are independent,
and sample deterministic results. Then P(N, N) = 2/9, P(N, A) = 1/9, P(A, N) = 4/9,
P(A, A) = 2/9. For each possible conﬁguration, apply the inference method, and then take
expectations for all conﬁgurations.
The above inference method can be applied in the opposite direction.
Assume we
want to know the results of some transplantation experiments. We only need to perform
some experiments, and use the inference method to determine other results.
Now the
question is to design experiments, so that all the results can be determined, while the cost
is minimized. After transforming the similarities between experiments into the similarity
chart, the experiment design problem becomes a coloring problem. See Table 2 for the
optimal coloring conﬁgurations in two-dimensional lattice, where each not colored unit
(not conducted experiment) is neighboring to 4/2/1 colored units (conducted experiment).
Since the actual transplantation experiment should be described by at least four factors
(donor type, donor stage, host type, host stage), the coloring problem should be generalized
to any n-dimensional lattice.
In this general setting, we need to color each node of the n-dimensional square lattice
Zn black or white, so that each white node is neighboring to at least k black node, and
the number of black nodes is minimized. Here each node has a coordinate (x1, x2, . . . , xn),
and two nodes are neighboring if one component of their coordinates diﬀers by 1, and
other components are equal. Each white node is neighboring to at least k black nodes,
and each black node is neighboring to at most 2n white nodes. Therefore, the theoretical
upper bound of white-black ratio is 2n : k, and the minimal proportion of black nodes is
k/(2n + k). The following results solve the coloring problem for certain k [57]: Theorem 10. If k is a divisor of n, color a node black if and only if its coordinate satisﬁes
a1x1 + a2x2 + · · · + anxn ≡0 (mod (2n/k) + 1), where the coeﬃcients a1, a2, . . . , an are k
groups of 1, 2, . . . , n/k: 1, 2, . . . , n/k, 1, 2, . . . , n/k, . . . , 1, 2, . . . , n/k. Then black nodes are
not neighboring, each white node is neighboring to k black nodes, and the proportion of
black nodes is k/(2n + k). Theorem 11. If k is a divisor of 2n, but not a divisor of n, color a node black if and only if
its coordinate satisﬁes a1x1 +a2x2 +· · ·+anxn ≡0 (mod (2n/k)+1), where the coeﬃcients 14 Donor
T1
T2
T3
T4
T5
T6
T7 T1
T2
T3 Host
T4
T5
T6
T7 T1
T2
T3
T4
T5
T6
T7 T1
T2
T3 Host
T4
T5
T6
T7 T1
T2
T3
T4
T5
T6
T7 T1
T2
T3 Host
T4
T5
T6
T7 Table 2: Experimental designs in diﬀerent cases, corresponding to two-dimensional exper-
iment similarity chart Z2. Each cell is an experiment, and neighboring cells are similar
experiments. Black cells are conducted experiments, and white cells are non-conducted
experiments. Black cells are not neighboring. Each non-boundary white cell is neighboring
to k black cells, where k = 4 (upper), k = 2 (middle), k = 1 (lower) [64]. This table
corresponds to the contents of Subsection 4.1. 15 a1, a2, . . . , an are k/2 groups of 1, 2, . . . , 2n/k: 1, 2, . . . , 2n/k, 1, 2, . . . , 2n/k, . . . , 1, 2, . . . , 2n/k.
Then black nodes are not neighboring, each white node is neighboring to k black nodes, and
the proportion of black nodes is k/(2n + k). This is another example that cancer research can lead to mathematical progresses. 4.2
Other topics in developmental biology When transplanting cancer cells into an embryo, the position of transplantation determines
the fates of these cancer cells [30].
This leads to a central problem in developmental
biology: why cells at diﬀerent positions of an embryo know where they are, and diﬀerentiate
accordingly into normal tissues? The notion of positional information is invented to explain
this problem [69]. Nevertheless, the usages of this term cause confusions, since it might
mean diﬀerent objects in diﬀerent studies. In a recent work, the deﬁnition of positional
information is clariﬁed with some criteria: it is position-related; it is extracellular; it is the
direct cause of cell fate [58]. This method of clarifying a notion with criteria can be applied
to other ﬁelds [53].
To study the development of an embryo, one convenient way is to represent the devel-
opment process as a rooted tree, where each node is a cell along with its cell event (label),
while an edge links the parent cell and the child cell. Besides, we cannot distinguish two
children cells after division. Therefore, each development process is a rooted unordered
tree with possibly repeated labels.
To compare such trees, there are diﬀerent metrics,
where some can be calculated within polynomial time [54]. One straightforward idea is to
calculate the minimal number of operations to transform one tree into another. Another
idea is to transform both trees into some regular forms and compare them. 5
Discussion In this review, we go through three facets of mathematical cancer biology: population
dynamics, gene regulation, and developmental biology. These ﬁelds have many meaningful
mathematical problems, and solving them can help with the understanding of cancer.
Besides, skills in mathematical modeling and data analysis are also essential. There are
other ﬁelds related to cancer biology that need applied mathematicians. Although most
mathematical results in this review are related to probability and discrete mathematics,
other mathematical tools might be useful in such ﬁelds, such as diﬀerential equations and
dynamical system, and even geometry. We hope that this review can inspire more applied
mathematicians to work on mathematical problems related to cancer biology. 16",0
"The authors of the article analyze the content of the Eurasian integration, from the initial initiative to the modern Eurasian Economic Union, paying attention to the factors that led to the transition from the Customs Union and the Single Economic Space to a stronger integration association. The main method of research is historical and legal analysis. Key words: integration, Eurasian Economic Union, economic integration.  
 
JEL codes:  F-02; F-15. 1. 
Introduction The experience of Eurasian integration in the post-Soviet space, obtained through numerous initiatives, mistakes and unresolved issues, has become a necessary basis for the formation of a functional functioning association. In addition, it contributed to the identification of countries among the entire Eurasian space whose interests coincide in modern geopolitical and economic realities. The current stage of Eurasian integration is the formation of the Eurasian Economic Union on the basis of the Customs Union and a Single Economic Space. 2. 
Main part The idea of creating such an association belongs to the former President of the Republic of Kazakhstan Nursultan Nazarbayev, who in 1994 had put forward the initiative of rapprochement of states in the format of the Eurasian Union. At that time, Kazakhstan, like other countries of the former USSR, had many representatives of various ethnic groups in its national composition. To strengthen the newly formed state, it was necessary to increase the welfare of the people and ensure equal rights of all nationalities in order to prevent the outflow of the 2 population. At the same time, it was extremely important to establish and maintain friendly relations with the former Soviet republics, to establish cooperation in the field of security. The peculiarity of the project proposed by Nazarbayev was to combine the national interests of Kazakhstan with the priority tasks of the development of other states of the Eurasian space. As a result, the Eurasian idea became one of the foundations of foreign policy and determined the vector of development of Kazakhstan. The President of Kazakhstan considered it necessary to achieve a high level of development of the Eurasian states by combining their economic (see Fig.1), transport and resource potentials. In order to further develop integration processes in the post-Soviet space, the experience of international associations, primarily the European Union, was studied. The new format of the association, similar to the EU and unlike the Economic Union of 1993, implied the formation of supranational bodies, whose decisions are binding on the member states. However, it was emphasized that domestic and foreign policy issues should be within the competence of a sovereign state without delegation to the supranational level. Fig.1. External and Internal Trade: the EU and EAEU in comparison (in 2017) Source: Eurostat 2017, Eurasian Economic Commission 2017 [6; 7] At the first stage, Nazarbayev’s initiative had assumed the formation of the core of integration, that is, the unification of the states, which are most ready for mutual rapprochement - Kazakhstan and Russia. This format of the union was supposed to become the center of attraction of the post-Soviet countries. Later, the main aspects of the construction of the Eurasian Union have been included into the book ‘The Eurasian Union: ideas, Practice, prospects for 1994-1997’. In contrast to the previous concepts of eurasianism, which mainly consider it from a geographical, geopolitical or civilizational perspectives, Nazarbayev's ideas were of an applied, practical nature. At the same time, they have not just taken into account the current situation in 3 the post-Soviet space at that time, but were also formulated for the first time taking into account the further development of globalization processes. The principal feature of the project proposed by Nazarbayev was an appeal to the Eurasian idea, which implied the presence of an ideological component in his initiative. This meant that the economic integration of the states was justified not only by the need for economic cooperation, but also by the historical trends in the development of the entire Eurasian space, objectively aimed at finding optimal forms and mechanisms for preserving the ties that have existed between the Eurasian countries for centuries. Thus, the main provisions of the strategy for the creation of the Eurasian Union, much later set out by Nazarbayev in the newspaper ‘Izvestia’, they were as follows: 1) The Eurasian Union should initially be created as a competitive global economic association. 2) The Eurasian Union should be formed as a strong link linking the Euro-Atlantic and Asian areas of development. 3) The Eurasian Union should be formed as a self-sufficient regional association that will be part of the new global monetary and financial system. 4) The geo-economic, and in the future, the geopolitical maturation of the Eurasian integration should be exclusively evolutionary and voluntary. 5) The creation of the Eurasian Union is possible only on the basis of broad public support. However, at that time, Nazarbayev’s proposal was not practically implemented. In the early 1990s, the post-Soviet states, as already noted, faced serious economic problems, the solution of which depended on the future of these countries. In such conditions, integration processes did not seem to them to be an urgent issue, since first of all it was necessary to stabilize the economic situation. In addition, the independence of the States and the memory of strict centralized management did not allow the proposal to create supranational bodies to be perceived with interest. That is, the development of economic cooperation at that time was opposed to the strengthening of national independence and sovereignty. Despite the fact that Nazarbayev’s initiative on the creation of the Eurasian Union in the 1990s had not found wide support, its role in the subsequent processes in the post-Soviet space was significant. By the beginning of the 2000s, numerous attempts at rapprochement had identified a circle of countries ready to unite and carry out coordinated actions in various spheres of the economy. By this time, such states as Belarus, Kazakhstan and Russia have largely coped with the crises of the 1990s and have become the most economically developed, open and interested in restoring trade relations. 4 Nazarbayev’s initiative was supported by Russia, namely by Vladimir Putin, who took office as President of the Russian Federation. Probably, the integration processes would not have developed with such intensity or remained theoretical inventions of traditional and modern Eurasians, if the new leader had not had a serious impact on the practical implementation of the idea of the Eurasian dialogue in the new historical period. In the process of economic reforms aimed at improving the economy after the unfavorable 1990-s, developing the business climate and modernization, the need for close economic cooperation with neighboring countries was realized, on which not only the dynamics of foreign trade depended, which, of course, affected the welfare of the population, but also national security. It is obvious that in the XXI century, effective economic cooperation can only be carried out on a mutual basis, so Russia, like other countries participating in the rapprochement process, began difficult negotiations to coordinate interests and conditions of interaction and made concessions on some issues. Thus, the factor of political will played an important role in the formation of the modern concept and practice of Eurasian integration. Assessing the political course of the Russian President Vladimir Putin as a whole, as well as actions aimed at updating the Eurasian issue, we can distinguish three bases on which the Eurasian practice is based:  
Civilizational and historical basis. In its historical retrospect, Russia is perceived as a single civilizational space that unites numerous ethnic groups, whose self-worth has been preserved for centuries thanks to the ‘state-civilization’ model, regardless of the historical period and its characteristic values and political structure. At the same time, the dominant principle-the continuity of history-is the optimal assessment of the entire historical path of Russia, since it allows, without giving preference to any political theory underlying a particular model of the state structure, to identify and apply the best practices of managing the Eurasian state. Thus, in an interview with American journalist Charlie Rose for the CBS and PBS TV channels, Vladimir Putin expressed his desire to preserve the common humanitarian space in the region ‘to make sure that state borders do not arise, so that people can freely communicate with each other, so that a joint economy develops, using the advantages that we inherited from the former Soviet Union’ [3]. Such advantages include the presence of ‘a common infrastructure, a single railway transport, a single road network, a single energy system’ and ‘the great Russian language, which unites all the former republics of the Soviet Union and gives us obvious competitive advantages in promoting various integration projects in the territory of the post- Soviet space’ [3].  
Economic basis. Of course, economic pragmatism can be traced in the Eurasian practice, which today primarily explains the project of forming the EAEU. Over the past 15 5 years, the economic climate in Russia had been improved significantly (we will not consider the pandemic crisis separately). At the same time, it is obvious that entering the global market with domestic products requires high quality standards and competitive prices. To achieve these goals, technological and resource potentials are being developed within the framework of the Eurasian Economic Union, production costs are being reduced, and the position of producers of the member states in the single internal market is being strengthened. In the future, joint access to foreign markets is carried out, among other things. Thus, over time, the presence of domestic goods, services and capital in the global economy increases significantly.  
The geopolitical basis. Despite a significant share of economic pragmatism, the economy is not an end in itself. Through established cooperation relations with partners in the EAEU, improving the business climate, Russia's position in the CIS space is strengthened, security is ensured at the borders with neighboring states, and the influence and authority of the state in the world is increased. The priority of the development of Eurasian integration is fixed in the Concept of the Foreign Policy of the Russian Federation. It should be noted that, unlike classical Eurasians, Vladimir Putin does not oppose Russia and, in general, the EAEU to the West. On the contrary, he sees the future of the Eurasian space in close connection with Europe. This is how he actualizes the thesis of Charles de Gaulle about the creation of ‘Europe from the Atlantic to the Urals’, that is in Putin’s interpretation ‘a common economic space from Lisbon to Vladivostok’. However, the elimination of trade and other restrictions between the two major economic centers can only be implemented on an equal and mutually beneficial basis. Throughout the entire process of the Eurasian economic integration, the interest and consistent practical support of the Eurasian project by Belarus in the person of President Alexander Lukashenko can be traced. Thus, in his address to the heads of the member states of the Eurasian Economic Union, sent in connection with Belarus' chairmanship in the EAEU bodies in 2015, the desire to promote the four fundamental economic freedoms of the EAEU had been once again confirmed: the freedom of movement of goods, services, capital and labor, maximum liberalization of the conditions of economic activity within the EAEU, including through the complete abolition of exemptions and restrictions in the movement of goods. In addition, it is Russia and Belarus that have formed the most advanced integration association in the post-Soviet space today, where not only economic but also social freedoms are ensured - the Union State of Belarus and Russia. It is assumed that in the situation of a political conflict between the West and Russia, the support of Belarus, its consistent and clear position on the promotion of the Eurasian project will give a new impetus to economic integration, including taking into account the policy of import 6 substitution. So, having considered the main aspects of the development of the Eurasian idea, set forth by the founders of eurasianism, its transformation into a modern Eurasian concept, implemented already at the official level during the formation of the Eurasian Economic Union, taking into account the accompanying global geopolitical and geo-economic conditions, it is advisable to draw the following conclusions. In the conditions of the collapse of the Russian Empire at the beginning of the XX century and the USSR at the end of the XX century, the rethinking of the role of the Eurasian states in various historical periods is quite a natural process aimed at finding the essence of the co-evolution of various peoples and cultures within the unique, geographically isolated Eurasian space and trying to predict or suggest a further vector of its development. Classical Eurasians come to the following conclusion: in the Eurasian space, taking into account the peculiarities of its geographical location and resource potential, the existence of a single powerful political entity is historically conditioned. In different periods, such formations were the Empire of the Huns, the Turkic Khaganate, the Empire of Genghis Khan, the Russian Empire and the USSR. Despite the trend that has emerged in recent centuries of fragmentation of the common territory of the Eurasian space into various smaller state entities, geographically, historically and economically (bearing in mind the resource potential) this territory remains united, capable of self-sufficiency. A significant role in the integration potential is played by a common history, including the history of joint protection of the Eurasian space during various wars, a similar way of life, mentality and culture, as well as a single nation formed over the centuries, consisting of many ethnic groups and peoples, but united by the widely spread Russian language. Therefore, the creation of various kinds of unions and associations of states is currently not only natural, but also vital for the population of these countries experiencing the pressure of globalization processes. The formation of the EAEU in 2015 also corresponds to this concept, since it is aimed at improving the well-being of citizens of the member states by combining domestic markets, resources, establishing cooperative ties and conducting a coordinated macroeconomic policy. The analysis of classical and modern eurasianism demonstrates the conditionality of the differences associated with modern geopolitical and geo-economic realities. Thus, if the founders of the Eurasian idea primarily opposed it to the Western vector of development and, in general, to the West, at present, not only the possibility, but also the need for the coexistence of integration processes in Europe and Eurasia, including through the creation of a common economic space, is being asserted. This thesis has been repeatedly voiced in the political discourse of official representatives of the EU and the EAEU member states. 7 However, in the current difficult situation of the exchange of sanctions between Russia and the West, the mutual lifting of restrictions in the implementation of economic and other activities is very unlikely. At the same time, the need to liberalize the trade regime between them becomes more obvious. After all, in the current historical period, some countries of the post-Soviet space are forced to choose between European and Eurasian integration. And if, for example, the Baltic countries unambiguously integrated into the EU, and Armenia joined the Eurasian Economic Union in 2015, and relatively painlessly, then the lack of an unambiguous position in Ukraine on this issue led to a civil war. Thus, at present, European and Eurasian integration are rather opposed to each other, therefore, this distinction between classical and modern eurasianism is currently conditional. In addition, a distinctive feature of the modern Eurasian concept is its strict focus on economic integration. If the founders of the Eurasian idea talked more about cultural pluralism, the historical predestination of the cohabitation of different ethnic groups on the same territory, then at the end of the XX – beginning of the XXI century, the self-worth of each people is already fixed at the legislative level, and due to the recent ambiguous imperial and Soviet past, discussions of political or social integration are very sensitive and therefore are practically not brought to the official level. However, it seems that the Eurasian integration is a more complex and complex process. The creation of the Eurasian Economic Union is only the basis for building a broader Eurasian strategy, on the basis of which, if successfully implemented, there will be a deepening of integration in the military, political, social, migration and humanitarian spheres. In this case, in the long term, the consequences of such a rapprochement are likely to include: - the formation of the Eurasian identity as the self-identification of the peoples of the Eurasian space, united by a common economic system and culture, value orientations and a certain system of protection of these values, that is, an integrated security system; - the development of the Eurasian mentality as a stable set of attitudes and predispositions to perceive the world in a similar way, possibly different from the worldview of residents of other global regions. 3. 
 Conclusion It should be concluded that the modern processes of Eurasian integration are taking place in line with the classical idea of eurasianism, which proves its relevance. Today, as a hundred years ago, the regularity of the joint development of the peoples of the Eurasian states in the common space is confirmed. Therefore, in the conditions of the formation of a polycentric world 8 order, the struggle for influence in the centers of power between the existing global leaders, these countries need to become an independent and equal subject of international relations in order to preserve their sovereignty, political independence, the heritage of history, science and culture, modernization and development of innovations in the economy, which is possible only by combining their potentials and peaceful coexistence, complementarity and interpenetration of their peoples.",0
"This paper aims to study the impact of public and private investments on the economic growth of 
developing countries. The study uses the panel data of 39 developing countries covering the 
periods 1990-2019. The study was based on the neoclassical growth models or exogenous 
growth models state in which land, labor, capital accumulation, etc., and technology proved 
substantial for economic growth. The paper finds that public investment has a strong positive 
impact on economic growth than private investment. Gross capital formation, labor growth, and 
government final consumption expenditure were found significant in explaining the economic 
growth. Overall, both public and private investments are substantial for the economic growth and 
development of developing countries. 
Keywords: Public investment, private investment, economic growth, developing countries Introduction In the last few decades, economists are trying to understand the global and country-specific 
factors contributing to economic growth. The variations in market fluctuations, financial crises, 
economic turmoil, and recessions made it hard to predict the economic uncertainty. An array of 
dynamic macroeconomic factors has been analyzed to determine a robust framework and 
effective model to explain the preceding performances and forecast future behavior of the 
economies. Both developed and developing countries face the change in macroeconomic 
dynamics and identified a complex relationship between public & private investment with 
economic growth. Public and private investments play a substantial role in the production 
functions by providing the required capital for development. Public investment may create scope 
for private investment through resource creation and socio-economic infrastructure support. 
However, Public investment can have a crowding-out effect on private investment. The IS-LM 
theory clearly states the impact of public investment on private investment. An increase in 
government spending, taxes, and domestic interest rate can lead to a parallel shift in the IS curve 
and adversely affect private investment. (Buiter, 1977; Ram, 1986). Public investment may also 
deprive private investment by competing in investment goods and utilizing physical and financial 
resources. Empirical studies by Aschauer (1989), Seitz (1994), and Pereira (2001) revealed that 
public investment is crowding in private investment, whereas Zou (2003) finds that public 
investment has a crowding-out effect.  
Researchers are yet to find a conclusive idea about the public and private investment 
collaboration and economic growth in the above circumstances. Public investments are working 
as business stimulus policy for almost the last three decades. Public capital enhances the 
productivity of the private capital and drives the return on investment upward. 1 Department of Economics, Northern Illinois University. Theoretical and Empirical Review Economic Growth Theory 
Researchers have identified two Economic growth models: The neo-classical growth model and 
the New growth model. Neo-classical growth models or exogenous growth models state that 
factors of production such as land, labor, capital accumulation, etc., and technological variables 
affect long-run productivity and economic growth (Solow, 1956). The new growth model or 
endogenous growth model argues that innovation and public-private investments in human 
capital fuel economic growth. (Romer, 1996; Lucas, 1988; Barro, 1990; Rebelo, 1991). Apart 
from these models, researchers also used Cobb-Douglas production function (1928) to identify 
the effect of labor and capital on output. (Aschauer, 1989). Most of the models confirm that 
public investment influences economic growth affecting aggregate demand and aggregate 
supply.  
Let’s consider, there is an representative agent in the economy wants to maximize the utility with 
the following function: V= In the equation δ is the discount factor where 0< δ <1, c is the consumption, w is the work effort 
and g implies the total government expenditure. Consumption and work effort is associated with 
the utility and disutility. It is assumed that government expenditure connects with private utility 
through private consumption goods or leisure activities. The representative agent can draw the 
production function with constant return to scale as follows: Y=Ft(L, K, G)=ALαKβGγ                                              (1) Where, L is the labor, K is private contribution in capital stock, G is government contribution in 
capital stock and A is technical coefficient.   
Case 1: Lets consider α+β=1 where total output is allocated to its labor and private capital 
factors. In this case, government contribution in capital stock affects the other two factors. When 
the public capital works as supplement for the private capital stock, affecting the marginal 
productivity of private capital stock through inducing private investment and output. The 
marginal productivity of labor also faces positive impact as both public and private capital per 
worker increases.  Taking the derivates of labor and private capital in Equation (1) we get, Y=
L+
K = (αALα-1KβGγ)L+(βALαKβ-1Gγ)K =(α )L+(β )K = MPL*L+MPK*K Until the marginal productivity of labor and marginal productivity of private capital is equal 
there will be adjustments of labor and capital. 
Case 1: Now, Let’s consider α+β+γ=1 where government investments/capital is an independent 
factor like the other two factors. When the public capital stock is independent and complements 
the private capital stock and labor then an increase in public capital directly affects the output considering the Ceteris Paribus holds. Taking the derivates of labor, Private capital and public 
capital stock in Equation (1) we get, Y=
L+
K+
G = (αALα-1KβGγ)L+(βALαKβ-1Gγ)K+ (γALαKβGγ-1)G =(α )L+(β )K+(γ )G = MPL*L+MPK*K+MPG*G Until the marginal productivities of public capital, labor and private capital are equal there will 
be adjustments of labor and capital(s) to reach an equilibrium.  
Placing the value α=1-β-γ, we get In  =ln A + β ln  + Setting a=ln A, y=Y/L, k= K/L and g=G/L we can write  
 
 
 
 
 
ln y = a +β ln k + γ ln g By adjusting two periods, t and t-1, we can obtain, 
 
 
 
 
 
ln (yt/yt-1) = a + β ln (kt/kt-1) + γ ln (gt/gt-1) Considering constant population growth rate, ln (Yt/Yt-1) = a + β ln (Kt/Kt-1) + γ ln (Gt/Gt-1) The last two equations suggests that there exists direct link between publication and private 
investment with economic growth.  
 
Empirical Studies 
Researchers are working to investigate the relation of public and private investment with 
economic growth for many years. The studies, however, revealed different results depending on 
the sample and method used.  
Le and Suruga (2005) investigate the impact of public investment and foreign direct investment 
(FDI) on economic growth using the panel data of 105 developed and developing countries for 
1970-2009. The results revealed that both public investment and FDI have a positive impact on 
GDP. The results also showed that when the public investment exceeds the threshold of 9%, the 
effect of FDI on economic growth becomes weaker, and the research concludes that excessive 
public investment erases the benefit from FDI. Blejer and Khan (1984) explore the crowding out 
or crowding in public investment on private investment for 24 developing countries from 1971-
1979. They identified that public investment in infrastructure is beneficial for private investment, 
whereas another type dry fund of private investment. Ashauer (1989) examines whether public 
investment crow out private investment using the United States data for 1925-1985. The results 
show that public investment increases the marginal productivity of the private capital, and higher public spending reduces private investment. The study found both the crowd-in and crowd-out 
effect of public investment. Everhart and Sumlinski (2001) tried to find the partial correlation 
between public and private investment using a data panel of 63 developing countries over the 
period 1970-2000. The studies conclude that there exists a negative correlation between public 
and private investment. However, the correlation seems to be positive for the countries with a 
better institutional framework.  
Deverajan et al. (1996) explore the relationship between public expenditure and economic 
growth using a sample of 43 developed and developing countries over 1970-1990. The study 
finds that public capital expenditure has a positive effect on the economic growth of developing 
countries, whereas the effect is negative for developing countries. The excessive allocation of 
resources by public investment can become unproductive and inefficient for the economy. Ghosh 
and Gregoriou (2007) also conclude similar results in developing countries' optimal fiscal policy 
framework. 
Barro (1990) examines the relationship between the growth rate of per real capita GDP and share 
of government expenditure using the extended endogenous growth model for 76 countries from 
1960 to 1985. He concluded that government consumption is inversely related to economic 
growth, but the relationship between public investment and economic growth is found to be 
insignificant. Hsieh and Lai (1994) used a similar model to analyze the relationship among the 
growth rate of per capita GDP, the ratio of private investment to GDP, and government 
expenditure for G-7 countries. The study found that government spending has a significant 
impact on the growth rate of per capita GDP for Canada, the UK, and Japan but an insignificant 
impact for France, Germany, Italy, and the USA. However, the private investment to GDP ratio 
has a significant effect on the USA, Japan, Canada, Germany, and the UK. Zou (2006) performs 
a study on the interaction between public and private investment and economic growth for the 
USA and Japan. He suggests that both public and private investment have a significant 
contribution to Japanese economic growth. For the USA, the private investment seems to play a 
much more significant role than public investment.   
Ghali (1998) conducts a study in Tunisia, in which IMF implemented a debt-stabilization 
program and identified that public investment has a long-run inverse effect on economic growth. 
Ramirez and Nazmi (2003) made an empirical study for nine Latin American countries and 
concluded that both public and private investment positively impact GDP growth. Phetsavong 
and Ichihashi (2012) investigate the impact of FDI, public investment, and private domestic 
investment using a sample of 15 Asian developing countries from 1984 to 2009. They suggest 
that private domestic investment and FDI are the two most crucial contributing factors for 
economic growth, whereas public consumption is inversely related to economic growth. The 
studies also found that public investment reduces the positive impact of private domestic 
investment and FDI on economic growth. Nguyen and Trinh (2018) examined both short and 
long-term influences of public investment on private investment and economic growth. The 
authors use an autoregressive distributed lag model using Vietnam's macro data from 1990-2016. 
The results indicate that public investment has a positive effect on short-term and negative 
effects in constraining long-term growth. However, private investment and FDI have positive 
effects on short-term economic growth. State-owned capital stock has positive impacts on 
economic growth in both the short and long run. Ahamed (2021) suggests that unexpected events like Covid-19 can also impact the private and public investments, thereby hurting the economic 
growth. Data and Methodology Data 
The study uses panel data of 39 developing countries throughout 1990-2019 which comprises 
7020 observations. The data has been collected from mainly two sources: World Development 
Indicator and the International Monetary Fund database. Sample data from developing countries 
across all the continents are chosen to balance the research data. Domestic private credit and 
foreign direct investment are used as a proxy of private investments, whereas the gross capital 
formation, labor growth, and government consumption expenditures are considered public 
investments. Table 1: Summary statistics of the variables Variable Description 
Mean (Std. Dev.) 
Min 
Max Dependent Variable GDP Growth Rate 
3.9367 (4.4138) -50.25 
35.22 Variables of interest Domestic Private Credit (% of GDP) 
30.6404 (31.6157) 1.6155 
160.1248 Foreign Direct Investment (% of GDP) 
2.4002 (3.7685) 8.7031 
46.2752 Gross Capital Formation (% of GDP) 
22.1185 (8.6896) 1.5252 
77.8900 Labor Growth Rate 
0.0261 (0.0168) -0.0449 
0.1208 Government Consumption Expenditure 
(% of GDP) 13.7099 (4.9718) 0.9112 
31.5544 No. of observations 
7020 Empirical Model & Tests 
As the dataset for this study is a panel data set, to examine the research question of whether the 
public and private investments affect the economic growth, a fixed-effect or a random-effect 
model can be used to control for time-invariant un-observables that affect both dependent and 
key independent variables. In particular, the fixed and random-effect models incorporate an 
individual-specific time-invariant factor, αi. If it is certain that, is not correlated with all 
independent variables and is normally distributed, then the random effects model would be 
appropriate. However, in this case, it not certain that αi is not correlated with an independent 
variable.  The fixed-effect model is used in this study to avoid the stronger assumption required 
in this case of the random-effect model. The choice of model is also consistent with the Hausman 
(1978) specification test. A two-way (country and year) fixed-effect model comprised of both endogenous and exogenous 
variables is used as a first step. In the second step, Pooled Ordinary Least Squared (POLS) has 
been used to check the robustness of the model. The model is described below: GDPij = αi + γt+ δ1DCij + δ2FDIij + δ3CAPij + δ4LABij +δ5CONij +€ij where, 
GDPij = Economic Growth Rate 
αi 
= Country specific time-invariant factor γt 
= Year-specific fixed effects DCij 
= Total domestic credit (% of GDP) FDIij 
= Foreign Direct Investment, Net Inflows (% of GDP) CAPij 
= Gross Capital Formation (% of GDP) LABij = Labor Growth Rate 
CONij = Government Final Consumption Expenditure (% of GDP)  
 
In the estimation, i and t denote for the number of cross section country (1,2,3…N) and time 
(1990,1991….2019) respectively. Empirical Results and Analysis Results 
As the data set is a panel, the Hausman test was conducted to identify the fixed-effects model or 
a random-effects model. The tests examine the correlation between the unique errors and the 
regressors in the model. The null hypothesis states that there is no correlation and preferers a 
random-effects model. The estimated p-value is 00001388, so the null hypothesis has been 
rejected. It is concluded that the fixed-effect model is the preferred model for this study. 
Both fixed-effect and ordinary least square methods exhibit consistency in the results. Domestic 
private credit has a positive but insignificant effect on economic growth. Foreign direct 
investment is inversely related to economic growth, yet the effect is insignificant. The private 
investment variables are showing mixed results in affecting the GDP growth. The gross capital 
formation is found to be positive and significantly related to economic growth. Another factor 
that has a statistically significant and positive relationship with economic growth is the labor 
growth rate. In both the fixed effect and OLS model, one unit increase in labor growth increases 
the economic growth of the developing countries 21.2778 and 20.1560 percentage points. The 
government consumption expenditure has a negative yet significant relationship with GDP. Table 2: Empirical results Dependent Variable GDP Growth Rate (1) Fixed-effect model (2) Ordinary Least Square Variables of interest Domestic Private Credit (% of GDP) 
0.00453 (0.00410) 0.004711 (0.004153) Foreign Direct Investment, New Inflows 
(% of GDP) -0.04079 (0.03445) -0.014589 (0.034461) Gross Capital Formation (% of GDP) 
   0.12488*** (0.01523) 0.119190*** (0.015313) Labor Growth Rate 
    21.2778** (7.35090) 20.1560** (7.451923) Government Consumption Expenditure 
(% of GDP) -0.10605*** (0.02529) -0.120566*** (0.025651) Number of observations 
7020 
7020 R-sq 
0.084 
0.082 Adjusted R-sq 
0.057 
0.078 **5% Significance level 
***1% Significance level Table 2: Correlation Matrix GDP 
DC 
FDI 
CAP 
LAB 
CON GDP 
1.000000 DC 
0.049859 1.000000 FDI 
0.058018 0.098937 1.000000 CAP 
0.245537 0.222324 0.291234 1.000000 LAB 
0.066328 
-0.1158 0.040662 0.015698 1.000000 CON 
-0.13415 0.190755 0.031906 
-0.04275 0.073264 1.000000 Analysis 
A general conception is public and private investment induces economic growth and creates 
development opportunities. However, previous researchers suggest the situation varies 
depending on various circumstances of investments. Developing countries lack infrastructural 
capital stock and face weak budgetary positions but possess a strong labor force, so the 
magnitude of public investment affecting economic growth is higher than the private investment. 
In developed countries, however, private investment seems to have a higher effect due to strong 
infrastructure and higher private capital formation. Although government investment in 
infrastructure, etc., sectors are crucial, excessive public investment in developing countries can 
have a crowding-out effect on the private investment. The disparity of public-private investment 
rates of return creates disparity and discourages private domestic investors.  
Public investment may influence private investment and boost the economy for the short term, 
but it may have significant adverse effects in the long term. As the government budgetary 
decisions in developing countries are inefficient and unfocused, overspending may require borrowing from private sources. State-owned enterprises lack transparency and accountability, 
which hinders the plan of creating clean and sustainable development. Developing countries 
have a greater population growth rate and an increased workforce. Still, due to poor quality 
education and inadequate investment in education, the majority of the workforce stays in 
unskilled territory. However, excessive public investment may cause the vitality of the economy 
to be lost and make private investment less productive in the long run. Transition to developed 
countries from developing countries requires policymakers to prioritize private investments and 
design public policies to stimulate private investment. Public investment works as a 
complementary service for domestic private investment and foreign direct investments. 
Developing countries follow public investment-driven economies in terms of sustainable 
economic growth and exert a strong influence over the private sector. Realizing the importance 
of private investment, China has empowered private sectors through synchronization with the 
public policies and investment to create a robust bidirectional relationship. This private-public 
investment correlation triggers economic growth and fosters economic development.   
The Neoclassical growth theory implies that a steady economic growth rate results from three 
driving forces: labor, capital, and technology. Capital accumulation and technological investment 
are the critical challenges faced by developing countries. Limited resource availability can only 
be offset by technological innovation considering capital and labor productivity. The main 
instrument of economic growth varies at various stages of a country’s development. Physical 
capital accumulation and human capital development are substantial for early development 
stages but maintaining steady growth investments in technological research and development is 
vital.  These are lacking in developing countries that fail private investments and discourages 
foreign investments. Lack of policy, skilled human resources, and infrastructure support the 
determinants of economic growth variables that fail to generate interactive relationships. Conclusion The study uses panel data from 39 developing countries during the period 1990-2019. The effect 
of public and private investments in determining economic growth rate has been explored in the 
study.  The results of this paper suggest some implications in constructing a theoretical model 
which structures the impact of public-private investments on economic growth. The empirical 
results show that private investment has mixed effects (mostly positive) on GDP growth. 
Domestic private credit has a positive, whereas foreign direct investment has a slightly negative 
impact on economic growth. The labor force growth rate drives economic growth significantly. 
Gross capital formation is associated with higher economic growth, while the government 
consumption expenditure is inversely related to the growth. Most of the countries in the study 
focus on public investment-driven growth and try to exert control over private investments. 
Focus on human capital development is substantial for economic growth, but the countries have 
less concentration on that sector. The evidence suggests the countries need to improve the public 
sector productivity and develop an analytical framework to stimulate private investment. Policy 
support to facilitate the private investment and skilled labor force can ensure a stable 
macroeconomic environment and sustainable economic growth.",0
"This paper constructs a global economic policy uncertainty index through the principal component analysis of the
economic policy uncertainty indices for twenty primary economies around the world. We ﬁnd that the PCA-based
global economic policy uncertainty index is a good proxy for the economic policy uncertainty on a global scale, which
is quite consistent with the GDP-weighted global economic policy uncertainty index. The PCA-based economic policy
uncertainty index is found to be positively related with the volatility and correlation of the global ﬁnancial market,
which indicates that the stocks are more volatile and correlated when the global economic policy uncertainty is higher.
The PCA-based global economic policy uncertainty index performs slightly better because the relationship between
the PCA-based uncertainty and market volatility and correlation is more signiﬁcant. Keywords: Economic policy uncertainty; Principal component analysis; Volatility; Correlation
JEL Classiﬁcation: D80, G18, E66 1. Introduction The study on uncertainty has attracted much atten-
tion (Bloom, 2009).
P´
astor and Veronesi (2012) and P´
astor and Veronesi (2013) develop a general equilib-
rium model to study how changes in government policy
choice affect stock prices and explore the relationship
between political uncertainty and stock risk premium.
Baker et al. (2016) construct an index as the proxy for
economic policy uncertainty (EPU) in the United States
and 11 other major economies, which was initially put
forward by Baker et al. (2013). Many scholars, such
as Moore (2017) and Arbatli et al. (2017), construct
other indices for different economies successively us-
ing the same method. Bontempi et al. (2016) introduce
a new uncertainty indicator based on Internet searches.
Castelnuovo and Tran (2017) develop uncertainty in-
dices for the United States and Australia, which are
based on Google Trends data.
Many papers have studied the inﬂuence of eco-
nomic policy uncertainty on the international ﬁnan- ∗Corresponding author. Address: 130 Meilong Road, P.O. Box
114, School of Business, East China University of Science and Tech-
nology, Shanghai 200237, China, Phone: +86 21 64250053, Fax: +86
21 64253152.
Email address: wxzhou@ecust.edu.cn (Wei-Xing Zhou ) cial markets.
Li et al. (2015) investigate the im-
pacts of economic policy uncertainty shocks on stock-
bond correlations for the ﬁnancial market in United
States. Kl¨
oßner and Sekkel (2014) discuss international
spillovers of policy uncertainty using the EPU indices
from six developed economies.
Brogaard and Detzel (2015) use a search-based measure to capture economic
policy uncertainty for 21 economies, and found eco-
nomic policy uncertainty has a signiﬁcant effect on the
contemporaneous market returns and volatility.
Recently, the aggregate global economic policy un-
certainty (GEPU) has been proposed and investigated.
Davis (2016) constructs an index of global economic
policy uncertainty which is a GDP-weighted average
of national EPU indices for 20 economies. Fang et al.
(2018) examine whether the GDP-based GEPU in-
dex provides predictability for the gold futures market
volatility. Ersan et al. (2019) access the effect of the
GDP-based GEPU index on the stock returns of travel
and leisure companies.
For a ﬁnancial market with multiple assets, the largest
eigenvalue of the correlation matrix of returns, when
normalized by the number of assets, quantiﬁes the sys-
temic risk of the market, while its eigenvector reﬂects
the whole movement of the market (Billio et al., 2012;
Dai et al., 2016; Emmert-Streib et al., 2018; Han et al., Preprint submitted to XXX
August 2, 2019 2017;
Kenett et al.,
2010;
Kritzman et al.,
2011; Meng et al., 2014; Plerou et al., 2002; Sandoval Jr.,
2017; Shapira et al., 2009; Song et al., 2011). Inspired
by these studies, we construct an alternative index for
the aggregate global economic policy uncertainty based
on the principal component analysis. In addition, we
explore the effect of global economic policy uncertainty
on the volatility and correlation of the global stock
market.
The remainder of this paper is organized as follows.
Section 2 presents a brief description of the data. Sec-
tion 3 focuses on the methodology. Section 4 documents
our ﬁndings. Section 5 concludes. 2. Data In
order
to
calculate
the
PCA-based
GEPU
index,
we
retrieve
the
economic
policy
un-
certainty
indices
for
twenty
economies
from http://www.policyuncertainty.com.
These twenty economies are Australia (AU), Brazil
(BR), Canada (CA), Chile (CL), China (CN), France
(FR), Germany (DE), Greece (GR), India (IN), Ire-
land (IE), Italy (IT), Japan (JP), Mexico (MX), the
Netherlands (NL), Russia (RU), South Korea (KR),
Spain (ES), Sweden (SE), the United Kingdom (UK),
and the United States (US), which are in perfect
accordance with the economies that Davis (2016) apply
to construct the GDP-based GEPU index. Fig. 1 shows
the evolution of these EPU indices from January 2003
to December 2018, where each index includes 192
monthly observations. 0 200 400 600 800 1000 1200 EPU index UK 0 200 400 600 800
CN 0 200 400 600 800
BR 0 200 400 600 FR 0 100 200 300 400 EPU index AU 0 100 200 300 400 500
DE 0 100 200 300 400 500
CA 0 100 200 300 400
CL 0 100 200 300 400 500 EPU index MX 0 100 200 300 400 500
ES 0 100 200 300 IN 0 100 200 300 JP 0 100 200 300 EPU index US 0 100 200 300 400
GR 0 100 200 300 400 500
RU 0 100 200 300 400 KR 2003
2006
2009
2012
2015
2018
Time 0 100 200 EPU index SE 2003
2006
2009
2012
2015
2018
Time 0 100 200 300 NL 2003
2006
2009
2012
2015
2018
Time 0 100 200 300 IE 2003
2006
2009
2012
2015
2018
Time 0 100 200 300 IT 0 200 400 600 800 1000 1200 EPU index UK 0 200 400 600 800
CN 0 200 400 600 800
BR 0 200 400 600 FR 0 100 200 300 400 EPU index AU 0 100 200 300 400 500
DE 0 100 200 300 400 500
CA 0 100 200 300 400
CL 0 100 200 300 400 500 EPU index MX 0 100 200 300 400 500
ES 0 100 200 300 IN 0 100 200 300 JP 0 100 200 300 EPU index US 0 100 200 300 400
GR 0 100 200 300 400 500
RU 0 100 200 300 400 KR 2003
2006
2009
2012
2015
2018
Time 0 100 200 EPU index SE 2003
2006
2009
2012
2015
2018
Time 0 100 200 300 NL 2003
2006
2009
2012
2015
2018
Time 0 100 200 300 IE 2003
2006
2009
2012
2015
2018
Time 0 100 200 300 IT Figure 1: The economic policy uncertainty indices from Jan-
uary 2003 to December 2018 of twenty economies: Australia
(AU), Brazil (BR), Canada (CA), Chile (CL), China (CN),
France (FR), Germany (DE), Greece (GR), India (IN), Ireland
(IE), Italy (IT), Japan (JP), Mexico (MX), the Netherlands
(NL), Russia (RU), South Korea (KR), Spain (ES), Sweden
(SE), the United Kingdom (UK), and the United States (US). In terms of the global ﬁnancial market, we select MSCI’s All Country World Index (ACWI) as its mea-
sure. The MSCI ACWI represents the performance of
the stocks across 23 developed and 24 emerging mar-
kets. Fig. 2 displays the trend of MSCI ACWI, which
covers the daily closing prices from December 2002 to
December 2018. The composite indices for each mar-
ket (from Bloomberg) are utilized to evaluate the cor-
relations between markets also using the daily closing
prices from December 2002 to December 2018. 2003
2006
2009
2012
2015
2018
Time 100 200 300 400 500 600 MSCI ACWI Figure 2: The price trajectory of MSCI’s All Country World
Index from January 2003 to December 2018. 3. Methodology First, we normalize the EPU index i for each econ-
omy over a moving window [t −T + 1, t] of size T: xi(s) ≡(EPUi(s) −⟨EPUi(s)⟩s)/σi,
(1) where EPUi(s) represents the index for the ith economy
in the s-th month and σi is the standard deviation of
EPUis. Second, we obtain the cross-correlation matrix
C by computing the pairwise cross-correlation coefﬁ-
cient between any two EPU indices for the economies: Ci j ≡
D
xi(s)x j(s)
E
.
(2) By deﬁnition, the elements Ci j vary from −1 to 1,
where Ci j = 1 corresponds to a perfect positive cross-
correlation, Ci j
= 1 corresponds to a perfect anti-
correlation, and Ci j = 0 reﬂects no cross-correlations
between the indices for economy i and economy j. The
cross-correlation matrix can also be expressed in the
matrix form:
C = 1 T XX′,
(3) where X is an N × T matrix with elements {xi(s) : i =
1, . . ., N; s = 1, . . ., T} and X′ denotes the transpose of
X. Third, we obtain the eigenvector u1(t) for the largest
eigenvalue λ1(t) of the cross-correlation matrix in the
t-th window:
Cu1(t) = λ1(t)u1(t),
(4) 2 where u1(t) = [u11(t), u12(t), . . ., u1N(t)]. Finally, we
construct the PCA-based GEPU index by the eigenport-
folio of the economic policy uncertainty indices for the
N economies: GEPU(t) = u1(t) · EPU(t) PN
i=1 u1i(t)
,
(5) where EPU(t) = [EPU1(t), EPU2(t), . . ., EPUN(t)]′. 4. Empirical results In order to examine the robustness of the results, we
construct the PCA-based GEPU index for ﬁve window
sizes T = 24, 30, 36, 42, and 48 months. Fig. 3 displays
the comparison between the GEPU-PCA and GEPU-
GDP indices for T = 24 months.
We ﬁnd that the
evolutionary trajectories of the GEPU-GDP and GEPU-
PCA indices are close to each other, which is also indi-
cated by the nice linearity of the data points in the cor-
responding scatter plot. The results for other window
sizes are very similar. Overall, all the GEPU-PCA in-
dices are very close to the GEPU-GDP indices, although
GEPU-PCA is obtained without using any other eco-
nomic data. The discrepancy between the two indices
increases when the uncertainty is high. 2005
2007
2009
2011
2013
2015
2017
2019
Time 0 50 100 150 200 250 300 350 GEPU index (a) 0
50
100
150
200
250
300
350
GEPU-GDP 0 50 100 150 200 250 300 350 GEPU-PCA (b) Figure 3: Comparison between GEPU-PCA and GEPU-GDP.
(a) Evolutionary trajectories of GEPU-GDP (solid line) and
GEPU-PCA (dashed line). (b) Scatter plot of the two GEPU
indices. Table 1 shows the correlation coefﬁcients between
GEPU-PCA and GEPU-GDP for different window sizes T, all of which are greater than 0.94. The window size
seems to have no impact on the correlation. Table 1
Correlation between GEPU-PCA and GEPU-GDP for differ-
ent window size T. T
t0
Obs.
Correlation 24 M
2004.12
169
0.9572
30 M
2005.06
163
0.9521
36 M
2005.12
157
0.9417
42 M
2006.06
151
0.9473
48 M
2007.12
145
0.9525 2005
2007
2009
2011
2013
2015
2017
2019
Time -5 0 5 10 15 20 25 Standard deviation (percent) (a) 2005
2007
2009
2011
2013
2015
2017
2019
Time 0 10 20 30 40 50 60 70 80 Correlation (percent) (a) Figure 4: GEPU-PCA versus global stock market volitility
and correlation. The solid line in each panel plots GEPU-
PCA, which is scaled to have the same mean and variance as
the other variable plotted in the same panel. The other variable
is volatility of MSCI ACWI in the upper panel and the equal-
weighted average of pairwise correlation for all stock compre-
hensive indices corresponding to the 47 economies which are
from MSCI ACWI in the lower panel. Both volatility and cor-
relation are calculated monthly from daily returns within the
month. P´
astor and Veronesi (2013) provide the theoretical
foundation for the positive relations between economic
policy uncertainty and both volatility and correlation
and conduct an empirical analysis in the American mar-
ket with the results supporting the theoretical argu-
ments. To verify that this association does also exist in
the global market, GEPU-PCA and GEPU-GDP are re- 3 spectively used as proxy for global economic policy un-
certainty. Fig. 4 reveals the strong correlation between
GEPU-PCA and the two variables about the global mar-
ket, especially in the ﬁrst half of the samples.
For the sake of a comparative analysis, we adopt the
same empirical models as P´
astor and Veronesi (2013).
For volatility, we employ the following regressions: Vol(t) = β0 + β1GEPU(t) + ε(t)
(6) Vol(t) = β0 + β1GEPU(t) + β2Vol (t −1) + ε(t),
(7) where Vol represents the volatility. For correlation, the
following regressions are considered: Corr(t) = β0 + β1GEPU(t) + ε(t)
(8) Corr(t) = β0 + β1GEPU(t) + β2Corr (t −1) + ε(t), (9) where Corr stands for the correlation.
The lagged
terms, Vol (t −1) and Corr (t −1), eliminate most of the
autocorrelation in the dependent variable series. Table 2
Global economic uncertainty, volatility, and correlation. Panel A: Volatility
Obs.
Eq. (6)
Eq. (7)
Eq. (6)
Eq. (7) GEPU-PCA
GEPU-GDP 169
0.0018
0.0010
0.0017
0.0011
(3.02)
(2.23)
(2.70)
(2.17)
163
0.0017
0.0010
0.0016
0.0011
(2.74)
(2.09)
(2.50)
(2.09)
157
0.0015
0.0009
0.0016
0.0011
(2.36)
(1.88)
(2.31)
(2.01)
151
0.0014
0.0009
0.0015
0.0011
(2.11)
(1.81)
(2.12)
(1.95)
145
0.0013
0.0009
0.0015
0.0011
(1.89)
(1.71)
(1.96)
(1.87) Panel B: Correlation
Obs.
Eq. (8)
Eq. (9)
Eq. (8)
Eq. (9) GEPU-PCA
GEPU-GDP 169
0.0459
0.0194
0.0377
0.0163
(2.88)
(1.36)
(2.30)
(1.14)
163
0.0311
0.0127
0.0262
0.0112
(1.96)
(0.90)
(1.58)
(0.77)
157
0.0152
0.0051
0.0154
0.0063
(0.97)
(0.37)
(0.99)
(0.43)
151
0.0017
0.0003
0.0067
0.0038
(0.10)
(0.02)
(0.39)
(0.25)
145
-0.0063
-0.0054
0.0020
-0.0005
(-0.04)
(-0.04)
(0.11)
(-0.03) Table 2 reports the estimates of β1 and their t-
statistics in all the forty regressions. Panel A shows that
β1 > 0 in all twenty regressions and all the 20 point
estimates are signiﬁcant at the 10% level, which pro-
vides strong evidence for the theoretical foundation that
the global market should be more volatile when there is
higher economic policy uncertainty. Panel B presents
weaker supporting evidence for the associated theoreti-
cal foundation since only three point estimates of β1 are
at the 10% level although β1 is positive in 17 of the 20
regressions.
We also ﬁnd that after removing the autocorrelation
in the volatility and correlation, the coefﬁcient β1 de-
creases. In addition, in most cases, β1 increases with in-
creasing sample length, except for GEPU-GDP versus
volatility using Eq. (9) in which β1 is independent of
the sample length. Indeed, we argue that this trending
phenomenon is caused by the fact that the correlation is
stronger between market volatility (or correlation) and
uncertainty in early years, as shown in Fig. 4. 5. Conclusions This paper constructs a novel index of global eco-
nomic policy uncertainty based on the principal com-
ponent analysis. This index is shown to be quite close
to the GDP-weighted average global economic policy
uncertainty index. We employ both GEPU-PCA and
GEPU-GDP as the proxies for uncertainty to investi-
gate the association between the global ﬁnancial market
and economic policy uncertainty and ﬁnd that the global
market should be more volatile and correlated when
there is higher economic policy uncertainty. Moreover,
GEPU-PCA performs lightly better than GEPU-GDP
when the observations are enough in the sense that the
correlations are more signiﬁcant when GEPU-PCA is
adopted in the analysis. Acknowledgements This work was supported by the National Natural
Science Foundation of China (Grants No. 71532009,
U1811462, 71790594), the Fundamental Research
Funds for the Central Universities, and Tianjin Devel-
opment Program for Innovation and Entrepreneurship.",0
"Research universities in the United States have larger mathematics faculties outside their
mathematics departments than inside. Members of this “extensive” faculty conduct most math-
ematics research, their interests are the most heavily published areas of mathematics, and they
teach this mathematics in upper division courses independent of mathematics departments. The
existence of this de facto faculty challenges the pertinence of institutional and national policies
for higher education in mathematics, and of philosophical and sociological studies of mathe-
matics that are limited to mathematics departments alone. Keywords: bibliometrics, mathematics, research university, sociology of science
2010 Mathematics Subject Classiﬁcation: primary 00A06; secondary 97A40, 97B40 1
Introduction This paper provides a quantitative basis for exploring policy issues about mathematics education,
by painting a comprehensive picture of how, what, and where mathematics is currently studied
in American research universities. The data show that mathematics departments primarily teach
“service” courses for general education while conducting research on subjects unrelated to under-
graduate education. Other departments teach and publish papers on the mathematics subjects that
upper division undergraduates study most heavily.
Mathematics has two qualities that make it unique in universities. First, the eponymous depart-
ment usually has the most student-contact hours of any. This instruction is concentrated in service
courses for liberal education requirements or for prerequisites in other ﬁelds. Both groups of stu-
dents place heavy burdens on the service courses: the ﬁrst by their quantity, the second because the
quality of instruction may determine success in the other ﬁelds. However, the mathematics faculty
who supervise the service courses do not perform quantitative analyses for the most part; indeed,
their research topics need have no physical realization (philosophers discuss the epistemology of
such knowledge as the question of mathematics “foundations”). Second, faculty outside mathemat-
ics departments provide most of the specialized mathematics eduction beyond the service courses.
They also write the majority of mathematics papers, moreover, on subjects relevant to advanced
undergraduate instruction. Since mathematics is not the focus of a single academic department, ∗Please cite the article that will appear in Higher Education: The International Journal of Higher Education and
Educational Planning and which can be accessed online through the DOI link on the arXiv page for this document.
†6059 Castlebrook Drive; Castro Valley, CA 94552-1645 USA
‡jfgrcar@comcast.net, jfgrcar@gmail.com. 1 arXiv:1101.0426v1  [math.HO]  2 Jan 2011 Mathematics Turned Inside Out – Joseph F. Grcar
2 research universities thus are witness to what is best described as the dissolution of mathematics as
a coherent academic discipline.
Education and research in mathematics occur outside mathematics departments because the
social signiﬁcance of “mathematics” remains close to its etymological meaning of all “quantitative
science” as the latter is understood today. Of the 29,506 doctorates awarded annually by the 96
United States universities with very high research activity, 58% were in social science, physical
science, technology, engineering, or mathematics (Carnegie Foundation, 2010), all ﬁelds whose
research is likely to involve mathematics. For example, the natural and social sciences formulate
and test quantitative theories, which in effect are mathematical models, so progress often depends on
mathematical prowess. Additionally, the social and health sciences gather voluminous survey and
clinical data, which they analyze using mathematics developed in ﬁelds such as econometrics and
epidemiology. Finally, the engineering and management sciences build models to mimic complex
systems which they understand and may improve, through the models, using the mathematics of
game theory, operations research, and systems theory. Few subjects are so pervasive as mathematics
in the intellectual life of the research university; consequently, expertise in mathematics is also
ubiquitous. 2
Research Universities in the United States The subject of this paper are United States research universities. By one assessment there are 96 in-
stitutions with very high research activity, and another 103 with high research activity (McCormick
and Zhao, 2005; Carnegie Foundation, 2010). Nevertheless, while many institutions declare they
support research and attempt to gain ﬁnancial resources for it, only about 50 universities succeed
in obtaining signiﬁcant funding and perform the majority of notable research (Calhoun, 2000, p.
58). Their effort is considerable. Surveys indicate that the faculty at such universities devote equal
amounts of time on average to teaching and research, while the proportion of research time at other
doctorate-granting institutions is very much less (Finkelstein, 2001, p. 330).
In the United States and in many other countries the smallest administrative units within univer-
sities are departments (Walvoord et al., 2000). These units formulate curricula, teach the courses,
originate the hiring of staff, and initiate the promotion of ladder staff.1 The departments of large or
research universities specialize in one academic subject. This tradition of specialization originated
at some European universities in the 19th century and was partly responsible for promoting aca-
demic disciplines by creating job markets for faculty with specialized credentials.2 This symbiotic
relationship between departments and disciplines is not entirely benign because the employment
prospects of specialists implicitly depend on maintaining well demarcated disciplines.3
In this
sense, disciplines are “antithetical to a strong local intellectual community” within the university
(Calhoun, 2000, 74–75). These deep, sociological roots of specialization are perhaps overlooked
when departments are criticized for “inattention” (Walvoord et al., 2000, 25) to the interdisciplinary
needs of undergraduate education (Lattuca and Stark, 1994).
The emphasis on interdisciplinary undergraduate education is a curricular feature that especially
distinguishes universities in the United States from universities in many other countries. The need
for interdisciplinary eduction arises because entering students are assumed not to have completed 1Ladder staff are faculty members who have been given tenure or a promise to be considered.
2Clark (2006) amusingly describes the “academic-charisma” that universities seek in faculty members.
3A good example of the importance of the academic job market to disciplines is the Annual Survey of the Mathematical
Sciences in the United States, which in spite of the ambitious title is only an accounting of supply and demand for
mathematics department faculty (Phipps et al., 2009). Mathematics Turned Inside Out – Joseph F. Grcar
3 their general educations (Calhoun, 2000, p. 51), and the university is responsible for providing that
education. Thus, in the nominally 4 year curricula for bachelor’s degrees, the lower division years
1–2 are devoted to introductory courses from many ﬁelds. Only during the upper division years
3–4 do students concentrate on advanced subjects usually from one ﬁeld, or what is often the same
thing, from one department. 3
Method 3.1
Empirical Methodology The methodology of this paper differs from previous sociological studies of mathematics in several
respects. Most signiﬁcant is the examination of mathematics throughout the university rather than
only in eponymous departments. All previous studies implicitly equate mathematics with the intel-
lectual content of papers and syllabi written by the faculty of mathematics departments. It is novel in
the extreme to investigate where else mathematics expertise can be found. Further, only quantitative
data is gathered to reveal the actual roles of participants rather than perceptions of those roles from
opinion survey data. Mathematics activity is observed through databases for university courses,
enrollments, and publications. Comments on the straightforward procedures that were employed to
gather each type of data are included with the results. 3.2
Institutional Sample The data used throughout this paper are from 50 institutions that are proﬁled in Tables 2 and 3
of the appendix. These institutions have the most highly rated mathematics departments in the
following sense. The National Research Council produced the most authoritative recent ranking
of mathematics research-doctorate programs in United States universities (Goldberger et al., 1995,
appendix table H-4). Each department received a composite, numerical rating between 0.00 and
5.00. The American Mathematical Society groups departments with similar ratings to report annual
statistics on employment in mathematics departments (Amer. Math. Soc., 2009; Phipps et al.,
2009). Group I consists of the 48 most highly ranked departments, with ratings from 3.00 to 5.00.
All but two of the 48 are from the 96 universities with very high research activity in the Carnegie
classiﬁcation, and the remaining 2 are from the 103 universities with high activity. The group I cadre
thus provides a reasonably objective and quite broad sample of mathematics research departments.
Two additional institutions are included because they provide some rarely available data that is used
in section 4.1. 3.3
Identifying Mathematics Research This investigation requires a method to recognize, quantify, and classify mathematics research. For-
tunately, that burden of subjectivity is assumed by two comprehensive surveys of research publica-
tions. Unlike the social sciences with which readers may be more familiar, many of the physical
sciences index their research publications. These indexes include all peer-reviewed publications
from any source that skilled editors judge to have contributed to the ﬁeld. Mathematics has two
indexing agencies sponsored respectively by the American and European Mathematical Societies.
Both Mathematical Reviews (2009) and Zentralblatt MATH (2009) maintain databases of papers
indexed by mathematics subject. Currently, Zentralblatt surveys 3,500 journals and 1,100 serials
for mathematics papers and indexes approximately 100,000 new peer-reviewed publications per
year. Since 1971 Zentralblatt has collected 2.39 million publications all of which are indexed by Mathematics Turned Inside Out – Joseph F. Grcar
4 the Mathematical Subject Classiﬁcation (MSC). This subject index consists of alphanumeric codes
beginning with 2-digit numbers that reﬂect the coarsest level of differentiation, that is, the major
branches of mathematics. Sixty-three of these 2-digit numbers are assigned.4 For the purposes of this paper, mathematics research is quantiﬁed by the numbers of peer-
reviewed papers indexed by Zentralblatt. This data can be used to make a census of mathematics
papers in the form of the percent of papers that address each major subject class (Figure 1). For
example, when the data were gathered in spring 2010, the Zentralblatt database had 86,517 papers
with an index code in the “quantum theory” classiﬁcation, 81.5 As noted, 2.39 million papers in
the database had been assigned codes. Thus, approximately 86,517/(2.39×106) or 3.62 percent of
all mathematics papers contribute to the mathematics of quantum theory. This value is recorded in
Figure 1. Since a paper may have multiple subjects, the percentages in the ﬁgure sum to 155.7. The
10 subjects listed atop Figure 1 account for half the 2.39 million peer-reviewed papers.
This census of mathematical research reﬂects current interests because most of the indexed pa-
pers have been written in the recent past. The quantity of mathematics publications has consistently
increased by approximately 2,000 papers per year in recent decades, with the result that 68% of the
papers indexed by Zentralblatt are from 1990–2009. If the data in ﬁgure 1 were restricted to just
the past two decades, then the primary change among the heavily published subjects is to increase
the percent of papers about the mathematics of computer science because computer science is a
comparatively new ﬁeld. 4
Results 4.1
Mathematics Department Coursework Undergraduate instruction is a natural starting point for an investigation of mathematics in uni-
versities. The numeraire for measuring university instruction is the student credit-hour, SCH. The
acronym is sometimes misinterpreted, and the concept is better understood, as a student-contact
hour. An SCH represents one student who is being “credited,” for purposes of earning a degree,
with one hour per week of supervised study per academic term. The concept arose in the United
States as a quantitative measure of educational achievement to replace comprehensive examina-
tions; the latter persist in Europe and in graduate studies. Shedd (2003a,b) describes the history of
the credit-hour and the vagaries of its calculation. From the standpoint of the university, credit-hours
or contact hours measure how much effort university employees devote to education each week.
All universities compile credit-hour data for their own use and for accreditation purposes. Data
are usually given to the public summarized by campus, college, or school, but only occasionally by
department (equivalently, by subject) and in a very few cases in even more detail. The lack of public
access is responsible for considering relatively few universities in this section; however, the results
for them are so overwhelming that there is no evidence opposed to the conclusions.
The University of Texas at Austin (2009a, pp. 85–96) supplies credit-hour data by semester,
department, and the degree level of the student. Undergraduate credit-hours for fall 2008 totaled
499,650 to which the Department of Mathematics contributed an astonishing 38,386 or 7.7%. The
mathematics department is the largest educational steward in Austin (Figure 2A). This rank seems 4The Amer. Math. Soc. (2010) posts the MSC on the world wide web. Fairweather and Wegner (2009) discuss
the latest revision. The MSC changes very infrequently at the 2-digit level, most recently in 2000 when class 04 was
combined into 03.
5In comparison, the American Institute of Physics (2009) database had 216,747 papers on “quantum” subjects, thereby
illustrating the selectivity of Zentralblatt editors in choosing only mathematically relevant papers. Roth (2005) describes
publication databases for the physical sciences. Mathematics Turned Inside Out – Joseph F. Grcar
5 0
 1
 2
 3
 4
 5
 6
 7
 8
 9  10 Computer science  68 Partial differential eq.  35 Numerical analysis  65 Fluid mechanics  76 Statistics  62 Probability theory  60 Operations research  90 Mech. of deform. solids  74
Ordinary differential eq.  34
Systems theory, control  93 Combinatorics  05 Operator theory  47 Functional analysis  46 Number theory  11 Quantum theory  81 Game theory, economics  91 Differential geometry  53 Group theory  20 Logic and foundations  03 Dynamical systems  37 Information and comm.  94 Biology  92 Calculus of variations  49 One complex variable  30 General topology  54 Algebraic geometry  14 General  00 Global analysis  58 Statistical mechanics  82 Mech. of particles and sys.  70 Approximations  41 Associative rings  16 Manifolds, cell complexes  57 Linear, multilinear algebra  15 Fourier analysis  42
Relativity theory  83 History  01 Real functions  26 Several complex variables  32 Nonassociative rings  17 Integral equations  45 Topological groups  22 Classical thermodynamics  80 Geometry  51 Measure and integration  28 Special functions  33 Ordered alg. structures  06 Commutative rings  13 Algebraic topology  55 Optics  78 Convex & discrete geom.  52 Category theory  18 Functional equations  39 Geophysics  86 Field theory, polynomials  12 Potential theory  31 Abstract harmonic anal.  43 General alg. systems  08 Integral transforms  44 Sequences and series  40 Astronomy  85 K-theory  19 Mathematics education  97 All Mathematics Papers Percentage About Each Mathematics Subject Area Figure 1: A census of mathematics in the form of the percent of all mathematics papers addressing a
particular Mathematics Subject Classiﬁcation. The data were gathered from the Zentralblatt MATH
online database in spring 2010. The percentages sum to 155.7 because many papers have more than
one subject classiﬁcation. Subjects are shaded as in Figure 5. to be held generally. Mathematics departments are among the two largest in terms of undergraduate
credit-hours for each of the few universities providing detailed data to the public. Perhaps more
remarkable is that, across all the universities considered, mathematics is the only department that
consistently ranks among those with the most undergraduate credit hours.
More insight into mathematics coursework comes from universities that report data for more Mathematics Turned Inside Out – Joseph F. Grcar
6 Undergraduate Student Credit Hours 0 2 4 6 8 Percent of UG SCH 70 Departments (A) U of TX Austin Mathematics Biological Sciences Chemistry and Biochemistry 0 2 4 6 8 Percent of UG SCH 47 Departments (B) U of OR Eugene Upper Division
Lower Division English Mathematics Romance Languages 0
 1
 2
 3
 4
 5
 6
 7 Percent of UG SCH 49 Departments (C) U of CA Santa Cruz Mathematics Psychology Economics 0 2 4 6 8 Percent of UG SCH 54 Departments (D) Auburn U Mathematics
English
Biological Sciences 0 1 2 3 4 5 Percent of UG SCH 96 Departments (E) U of MN Twin Cities 1xxx
2xxx
3xxx
4xxx
5xxx Chemistry Mathematics English 10 Upper Division
Lower Division Upper D.
Lower D. Figure 2: Undergraduate student credit-hours by department for ﬁve universities. Departments are
ordered by increasing upper division enrollment when the distinction is made. Only departments
with values at least 0.1% of credit-hours are shown. Data are for the fall 2008 semester except
Minnesota data are for fall, spring, and summer 2005-2006. ﬁnely differentiated student levels. The University of Oregon (2009), the University of California at
Santa Cruz (2009), and Auburn University (2010) report credit-hours separately for lower division
undergraduate students and for upper division students. These universities are comparably sized
with respectively 236,302, 225,801, and 267,607 undergraduate student credit-hours in fall 2008.
The mathematics departments supply 6% of the undergraduate student credit-hours at the ﬁrst two Mathematics Turned Inside Out – Joseph F. Grcar
7 institutions and a very high 10% at the third (Figures 2B–D). The more detailed data reveal most
of these credit-hours are for lower division students. The upper to lower division ratio of SCH in
mathematics courses is 0.37 : 1 in Eugene and a remarkably small 0.074 : 1 and 0.037 : 1 at Santa
Cruz and Auburn, respectively.
Still greater insight comes from the University of Minnesota (2006) campus in the “Twin Cities”
of Minneapolis and Saint Paul. This campus is comprehensive in the extreme, with professional
schools of agriculture, business, engineering, law, and medicine. The university had 947,540 un-
dergraduate student credit-hours in total for the fall, spring, and summer terms of the 2005–2006
academic year. Minnesota credit-hour data are reported by course numbers that reﬂect the year after
matriculation in which students are anticipated to take the courses. Freshman courses are “1xxx,”
sophomore “2xxx,” and so on. The University of Minnesota Senate (2006) indicates this course
numbering scheme is informally honored, so the numbers provide a reasonable indicator of course
sophistication. Interpreting lower division courses as those numbered “1xxx” and “2xxx,” the upper
to lower division ratio of credit-hours for the School of Mathematics is 0.19 : 1, consistent with the
other schools. The observation enabled by this data is, mathematics departments provide by far the
majority of their credit-hours in freshman courses (Figure 2E). 4.2
Upper Division Mathematics Education The conceit, that science and technology are impossible without sophisticated mathematics, can be
reconciled with Figures 2B–2E (which show that mathematics departments mostly teach lower di-
vision courses) only if substantial mathematics education occurs outside mathematics departments.
The extent of this teaching is difﬁcult to quantify because course catalogs do not itemize the per-
tinent courses under the rubric “mathematics.” Consequently, class catalogs, course descriptions,
syllabi, and textbooks all have to be examined to identify courses with mathematical content.6 These
documents at the University of Texas at Austin (2009b) are easily perused, and the coursework at
this institution has already been examined in section 4.1, so the very large student body of this re-
search university is taken as representative.7 All upper division courses offered in the fall and spring
semesters of the 2008–2009 academic year are canvassed.
Mathematics courses are identiﬁed by matching them to speciﬁc items in the Mathematics Sub-
ject Classiﬁcation. This approach assures a rigorous selection process. For example, the Department
of Finance offered FIN 357, Business Finance. One professor (Rao, 2008) assigned chapter 4 of
the text (Ross, Westerﬁeld, and Jaffe, 2008) “Discounted Cash Flow Valuation,” and he scheduled
lectures on net present value. This course therefore taught subjects under Mathematics Subject
Classiﬁcation 91B28, “Finance, portfolios, investment,” or most broadly under subject 91, “Game
theory, economics, social and behavioral sciences.” There were 15 sections of FIN 357 in the fall
and 8 in the spring, each meeting 3 hours per week, totaling 69 weekly class-hours devoted to
subject area 91 (game theory, economics).
For further example, 80% of the 203 upper division class-hours in Department of Computer Sci-
ences courses have mathematics content. Some courses are associated with more than one subject
area. CS 320N, Practical Linear Algebra I (van de Geijn, 2009), teaches how “to attain high perfor-
mance on current cache-based architectures,” which is Mathematics Subject Classiﬁcation 68N19,
“Other programming techniques.” The course textbook (Strang, 2006) discusses topics in Math-
ematics Subject Classiﬁcation 15, “Linear and multilinear algebra, matrix theory.” Consequently, 6Clark (2006, chap. 2) refers to the academic catalog as “the single most condensed academic document, the royal
road to the academic subconscious.”
7Only three institutions with very high research activity have over 50,000 students (Carnegie Foundation, 2010). Mathematics Turned Inside Out – Joseph F. Grcar
8 the 6 class-hours for this course are divided between subject classes 15 (linear algebra) and 68
(computer science).
Altogether 252 upper division courses were identiﬁed with 667 sections meeting 2,197 hours per
week during the fall or spring semesters. The distribution of these upper division class-hours with
respect to the branches of mathematics and the departments offering the courses gives a reasonably
accurate and very novel quantiﬁcation of upper division instruction in mathematics subjects (Figure
3). Several areas of mathematics individually contribute 4% or more of class-hours, as follows: • Computer science, mathematics subject code 68, accounts for about 14% of the upper di-
vision class-hours. The Departments of Computer Sciences and of Electrical and Computer
Engineering provide most of the instruction in the mathematics of computer science; their
courses overlap considerably as a result. • Economics, code 91, also accounts for about 14% of mathematics class-hours, but in sev-
eral departments. The Department of Finance teaches how to value monetary instruments in
business ﬁnance courses such as the one discussed above, FIN 357. Many engineering depart-
ments teach similar mathematics in courses about the ﬁnancial management of engineering
projects. The Department of Economics teaches the broadest range of subjects including
game theory, ECO 354K, and econometrics, ECO 341K. The latter is one of those courses
that address two areas of mathematics, 91 and 62. • Statistics, 62, accounts for 8% of the class-hours, the third largest. If the University of Texas
at Austin had a statistics department, it likely would inherit the class-hours of the Department
of Mathematics, which are only a quarter of those offered. The subject is also taught in many
departments of engineering and social science. The campus does not have a medical school,
else courses in biostatistics and epidemiology might well increase the class hours in statistics
to rival computer science and economics. • If mathematical physics were considered a separate subject class to consolidate the math-
ematics of ﬂuid and solid mechanics 76 and 74, statistical and orbital mechanics 70, and
thermodynamics 80, then it would account for over 14% of class-hours. The Department of
Mechanical Engineering teaches half the class-hours for these subjects. • Control theory, 93, is mostly taught in the Department of Mechanical Engineering, although
at other universities it may also be found in aerospace engineering. • The Department of Electrical and Computer Engineering alone teaches the mathematics of
information and communication theory, 94. • The Department of Mathematics teaches half the class hours in numerical analysis, 65, with
the rest taught by a variety of engineering departments. • Operations research, 90, is taught in the Department of Information, Risk and Operations
Management with the rest again taught by various engineering departments. This data has been gathered from a single university but the conclusion is broadly supported, that
much mathematics education occurs outside mathematics departments. Since mathematics is used
throughout the sciences, and since mathematics departments primarily teach lower division courses
(as seen in section 4.1), the necessary instruction in specialized or advanced mathematics must occur
elsewhere. Mathematics Turned Inside Out – Joseph F. Grcar
9 0
 2
 4
 6
 8
 10  12  14 Computer science  68 Partial differential eq.  35 Numerical analysis  65 Fluid mechanics  76 Statistics  62 Probability theory  60 Operations research  90 Mech. of deform. solids  74
Ordinary differential eq.  34
Systems theory, control  93 Combinatorics  05 Operator theory  47 Functional analysis  46 Number theory  11 Quantum theory  81 Game theory, economics  91 Differential geometry  53 Group theory  20 Logic and foundations  03 Dynamical systems  37 Information and comm.  94 Biology  92 Calculus of variations  49 One complex variable  30 General topology  54 Algebraic geometry  14 General  00 Global analysis  58 Statistical mechanics  82 Mech. of particles and sys.  70 Approximations  41 Associative rings  16 Manifolds, cell complexes  57 Linear, multilinear algebra  15 Fourier analysis  42
Relativity theory  83 History  01 Real functions  26 Several complex variables  32 Nonassociative rings  17 Integral equations  45 Topological groups  22 Classical thermodynamics  80 Geometry  51 Measure and integration  28 Special functions  33 Ordered alg. structures  06 Commutative rings  13 Algebraic topology  55 Optics  78 Convex & discrete geom.  52 Category theory  18 Functional equations  39 Geophysics  86 Field theory, polynomials  12 Potential theory  31 Abstract harmonic anal.  43 General alg. systems  08 Integral transforms  44 Sequences and series  40 Astronomy  85 K-theory  19 Mathematics education  97 Upper Div. Class-Hours with Mathematics Content Percentage for Each Mathematics Subject 21% Mathematics 14% Mechanical Eng. 12% Elec. & Comp. Eng. 8% Computer Science 8% Finance 7% other (16) 4% Economics 4% Aerospace Eng. 3% Chemical Eng. 2% Biomedical Eng. 2% Civil Eng. 2% Physics 2% Operations Mgmt. 2% Education 1% Geological Sci Figure 3: A census of mathematics instruction in upper division courses at the University of Texas at
Austin in fall 2008 and spring 2009. Student credit hours for courses with mathematics content are
reported by the percentage that address each mathematics subject. The inset shows the distribution
by academic department. The 2,197 class hours weekly in mathematics give a rough estimate of the student credit-hours.
The university does not provide registration data for individual courses; however, Texas statutes
require universities to “offer only such courses and teach such classes as are economically justiﬁed”
(Texas Legislative Council, 2009, title 3 “education code” §51.403(d)). The University of Texas
at Austin College of Liberal Arts (2009) interprets this law to mean undergraduate courses should Mathematics Turned Inside Out – Joseph F. Grcar
10 have at least 10 students. Upper division courses are not anticipated to be large, so it is reasonable to
suppose an average section size of 25. The 2,197 class-hours thus suggest 54,925 weekly student-
contact hours in upper division mathematics instruction in total for both semesters. Recall, the
university has approximately one-half million undergraduate student credit-hours per semester.
With at most only about 5% of student-contact hours devoted to upper division mathematics, the
methods of instruction must be particularly effective. Even though most courses may be taught in
conventional lectures, syllabi that contain a mixture of mathematics and other subjects are uncon-
ventional in their treatment of mathematics. They introduce the mathematics early, so the remainder
of the course effectively becomes a practicum for the mathematics. For example, PHY 333, Modern
Optics (Keto, 2009), successively treats geometric optics, vectors and matrices in the Jones calcu-
lus, and Fourier transforms, each followed by studies of optics using this mathematics. Mathematics
departments have recognized the value of this type of instruction where it is generally called “ex-
perience in model building.” Among the 2,197 section hours, the mathematics department has 3
for M 474M, Mathematical Modeling in Science and Engineering. Most of the courses outside the
mathematics department teach mathematics in this way. 4.3
Research Emphasis of Mathematics Departments Having obtained data on instruction, the discussion turns to research activity. The 48 leading math-
ematics departments have 2,108 ladder staff with primary appointments in them as of spring 2010.
Their research papers were obtained from the Zentralblatt database by subject classiﬁcation and
by university. This method avoids double-counting jointly-written papers when all authors belong
to the same institution: 63,874 papers were obtained. These peer-reviewed papers are all research
of all ladder faculty at all departments that the American Mathematical Society regards as leading
research in mathematics; therefore, there is no sampling error in the following discussion.
The 63,874 papers have the subject distribution shown in Figure 4A. Comparing Figures 1 and
4A indicates the research interests of leading mathematics departments are unrepresentative of all
mathematics. For example, in Figure 1, about 1.6% of all mathematics papers are about manifolds
and cell complexes 57, while in Figure 4A, mathematics departments write 7.0% of their papers
about this subject. Perhaps more signiﬁcant, 9.9% of all mathematics papers in Figure 1 are about
computer science 68, the most of any subject, while in Figure 4A only 4.3% of the papers written
in mathematics departments are about the mathematics of computer science.
The research emphasis of a group of mathematicians may be deﬁned as the ratio of the fraction
of their papers on a subject to the fraction of all mathematics papers on the subject. This quantity
provides a criterion to identify subjects in which mathematics departments have more or less ex-
pertise. It is convenient to represent the ratio as a base 2 logarithm, so an emphasis in favor has a
positive value and an emphasis against has a negative value. Figure 5 reports the research emphasis
of mathematics department ladder staff. Mathematics departments have a strong negative emphasis
(less than 2−1) against 13 subjects that are darkly shaded in Figures 1, 4, and 5. They have a moder-
ate negative emphasis (between 2−1 and 20) against 19 subjects that are lightly shaded in the ﬁgures.
Remarkably, these ﬁgures show mathematics departments have a negative research emphasis for 8
of the 10 areas of mathematics at the top of Figure 1 about which the most research is conducted. 4.4
Mathematics Research Outside the Mathematics Department Mathematics departments de-emphasize research in the most heavily published mathematics sub-
jects (comparing Figure 5 with Figure 1). Consequently others must write many of the papers.
In order to identify the authors, this section examines departments other than mathematics at the Mathematics Turned Inside Out – Joseph F. Grcar
11 0
 4
 8
 12
 16
 20
 24 68
35
65
76
62
60
90
74
34
93
05
47
46
11
81
91
53
20
03
37
94
92
49
30
54
14
00
58
82
70
41
16
57
15
42
83
01
26
32
17
45
22
80
51
28
33
06
13
55
78
52
18
39
86
12
31
43
08
44
40
85
19
97 (B) by non-Math Depts 0
 4
 8
12 (A) by Math Depts % of Publications Addressing Each Mathematics Subject Figure 4: Of the papers written (A) by mathematics departments, and (B) by non-mathematics
departments, the percent that address each mathematics subject. Percentages sum to 184.0 and
155.7, respectively, because many papers have more than one subject. Subjects are shaded as in
Figure 5. same 48 universities with the leading mathematics departments. Table 4 in the appendix lists the
selected departments. They have been chosen to be among those that Figure 3 shows provide much
of the upper division mathematics instruction at the University of Texas. These non-mathematics Mathematics Turned Inside Out – Joseph F. Grcar
12 1
 2
 4 K-theory  19 Manifolds, cell complexes  57 Algebraic geometry  14 Several complex variables  32 Topological groups  22 Algebraic topology  55 Global analysis  58 Commutative rings  13 General  00 Differential geometry  53 Number theory  11 Abstract harmonic anal.  43 Dynamical systems  37 Nonassociative rings  17 Field theory, polynomials  12 Potential theory  31 Convex & discrete geom.  52 Measure and integration  28 Group theory  20 Partial differential eq.  35 Category theory  18
Fourier analysis  42 Combinatorics  05 Statistical mechanics  82 Probability theory  60 Associative rings  16 One complex variable  30 Logic and foundations  03 Special functions  33 Functional analysis  46 Quantum theory  81 Positive Emphasis of Mathematics Departments 1/4
1/2
 1 08  General alg. systems
85  Astronomy
40  Sequences and series
45  Integral equations
90  Operations research
62  Statistics
93  Systems theory, control
41  Approximations
68  Computer science
74  Mech. of deform. solids
80  Classical thermodynamics
91  Game theory, economics
70  Mech. of particles and sys.
51  Geometry
39  Functional equations
54  General topology
94  Information and comm.
83  Relativity theory
34  Ordinary differential eq.
26  Real functions
06  Ordered alg. structures
01  History
92  Biology
76  Fluid mechanics
65  Numerical analysis
97  Mathematics education
86  Geophysics
47  Operator theory
44  Integral transforms
49  Calculus of variations
78  Optics
15  Linear, multilinear algebra Negative Emphasis of Mathematics Departments Figure 5: Publication emphasis of mathematics departments, the ratio of the fraction of department
publications that are about a given subject to the fraction of publications in all of mathematics about
the subject. Note the logarithmic scale. Shading is repeated in Figures 1 and 4. departments had 13,701 ladder staff with primary appointments in them in spring 2010.8 Like the
investigation of mathematics departments, papers in the Zentralblatt database are gathered by sub-
ject classiﬁcation for the whole sample at each university to avoid double-counting jointly-written 8Only faculty members with primary appointments outside mathematics departments are considered in case of joint
appointments. When a university has two mathematics departments then only faculty outside both are considered. Mathematics Turned Inside Out – Joseph F. Grcar
13 10 100 0
 500
 1000
 1500
 2000
 2500 Quantity of Publications per Author Quantity of Faculty Members in the Respective Group Mathematics Publications by Faculty in the Math Departments and in Selected non-Math Depts of the 48 Group I Universities Math and Applied Math Depts Selected non-Math Depts Figure 6: Quantity of peer-reviewed mathematics publications by each ladder faculty member of
mathematics, or of selected non-mathematics departments, at the 48 Group I universities with the
most highly rated mathematics departments. Only authors with 10 or more mathematics publica-
tions are shown. Table 4 in the appendix lists the selected non-mathematics departments. papers. The subject distribution of the 75,125 papers that were written by members of the chosen
non-mathematics departments indicates that the mathematics research interests of authors outside
mathematics departments are largely complementary to those inside (Figure 4B).
A reasonable measure of expertise in any subject is having written 10 peer-reviewed publica-
tions. The ratio of such authors outside and inside the mathematics departments of the 48 universi-
ties is at least 2249 : 1522 = 1.47 (Figure 6). Thus research universities have more mathematically
expert faculty outside their mathematics departments than inside. The ratio can only increase by
including more departments of natural or social science in the selection of non-mathematics depart-
ments. With the present sample the mathematics departments have more authors of mathematics
papers only for authors of 22 or more papers. Writing so many mathematics papers is a remarkable
achievement for faculty outside mathematics departments because they also may write papers in
their primary ﬁelds of study.
Faculty who conduct research in mathematics are equally productive inside and outside math-
ematics departments. Based on the criterion of having written 10 or more mathematics papers, the
contingent outside mathematics department is larger at 38 of the 48 institutions (Figure 7). This
group is smaller at some institutions such as Brandeis and New York University that lack exten-
sive collections of professional schools. A characteristic of excellence in comprehensive research
universities may be that mathematics research skills are distributed throughout the faculty. 5
Discussion 5.1
Intensive and Extensive Nomenclature The aforementioned mathematics faculties inside and outside mathematics departments are not dis-
cussed in the sociology of science or of higher education, so some new naming convention is needed
to refer to them. Borrowing terminology from the physical sciences, the “intensive” mathematics Mathematics Turned Inside Out – Joseph F. Grcar
14 0
 10
 20
 30
 40
 50
 60
 70
 80
 90 Purdue Univ.-West Lafayette Rutgers Univ.-New Brunswick Univ. of California-Berkeley Univ. of Minnesota-Twin Cities CUNY-Graduate School Univ. of California-Los Angeles Univ. of Illinois-Urbana Univ. of Maryland-College Park Univ. of Michigan-Ann Arbor Univ. of Washington-Seattle Ohio State Univ.-Columbus Univ. of Wisconsin-Madison New York Univ. Georgia Inst. of Tech.-Atlanta Penn. State Univ.-State College Massachusetts Inst. of Tech. Michigan State Univ. Cornell Univ. Univ. of California-San Diego Univ. of Notre Dame Univ. of Texas-Austin Indiana Univ.-Bloomington Univ. of Chicago Carnegie Mellon Univ. Univ. of Illinois-Chicago Princeton Univ. Univ. of Southern California Univ. of California-Santa Barbara Stanford Univ. Northwestern Univ. Univ. of Utah Boston Univ. Univ. of N. Carolina-Chapel Hill Univ. of Pennsylvania SUNY-Stony Brook Duke Univ. Univ. of Oregon Columbia Univ.-New York City Harvard Univ. Brown Univ. Rensselaer Polytechnic Inst. Univ. of Virginia-Charlottesville Yale Univ. California Inst. of Tech. Washington Univ.-St. Louis Johns Hopkins Univ. Rice Univ. Brandeis Univ. Number of Ladder Faculty with 10+ Math Publications Math Dept Applied Math Dept Selected non-Math Depts Math Dept Applied Math Dept Selected non-Math Depts Figure 7: Quantity of ladder faculty with 10 or more peer-reviewed mathematics publications in the
mathematics departments, or in selected non-mathematics departments, of the 48 Group I universi-
ties with the most highly rated mathematics departments. Universities are listed in order of number
of faculty in their mathematics department. A few universities have a second (applied) mathe-
matics department whose faculty are shown separately. Table 4 in the appendix lists the selected
non-mathematics departments. faculty are members of mathematics departments. The “extensive” faculty are tenured and tenure-
track staff who teach mathematics or write mathematics papers outside mathematics departments.
The analogy with physical properties is, the quantity of extensive faculty varies in proportion to the
size of the whole natural and social science faculty. Mathematics Turned Inside Out – Joseph F. Grcar
15 The extensive faculty are not necessarily mathematicians by training or preference. Neverthe-
less, they perform as such, by teaching upper division courses whose syllabi include mathematics
subjects not taught elsewhere, and by writing the majority of papers that bibliographic services
regard as having mathematics content. The observation, that signiﬁcant research activity for any
ﬁeld occurs apart from the academic department nominally associated with the ﬁeld, appears to be
unprecedented.
The extensive faculty do not coincide just with those faculty outside mathematics who conduct
research on mathematics subjects. Many faculty may teach mathematics subjects that they know
but on which they do not publish because teaching duties typically are broader than research inter-
ests. Nevertheless, the results of this study show the extensive faculty contains sizeable numbers of
individuals who do publish signiﬁcantly in mathematics (Figures 6, 7), and moreover these faculty
produce the bulk of research about certain mathematics subjects (Figure 4B). 5.2
Convergence or Divergence of Teaching and Research The results of this study reveal a signiﬁcant dichotomy between the extensive and intensive mathe-
matics faculties. The extensive faculty write papers on the mathematics s ubjects they teach, whereas
mathematics departments write papers on subjects that are largely separate from their undergraduate
teaching.
A meaningful data reduction begins by selecting the mathematics subject areas that are most
important in the teaching or publications of the extensive or intensive faculties. Publication subjects
are chosen by applying a 4% threshold to Figures 4A and 4B; that is, a subject must be addressed
in either 4% of papers written by the extensive faculty, or separately 4% of papers written by the
intensive faculty. Teaching subjects are chosen similarly from a 3% threshold on the class-hours
in Figure 3. This ﬁgure reports consolidated percentages that must be adjusted for purposes of
selection to reﬂect class-hours of the extensive or intensive faculties alone. This selection of subjects
paints an objective picture of what mathematics is taught or published in quantity and by whom
(Table 1). The thresholds for inclusion are quite low so any subject omitted is genuinely secondary
compared to others.
The subjects of interest to the intensive and extensive mathematics faculties can be arranged
in a Venn diagram (Figure 8). Mathematics departments have only 5 subjects common to their
publications and undergraduate teaching, and another 11 subjects on which they publish without
appreciable instruction. In marked contrast, the non-mathematics departments have 8 mathemat-
ics subjects common to both their publications and undergraduate courses. This extensive faculty
publishes on 8 of the 11 most heavily published mathematics subjects, it teaches all 9 most heavily
taught subjects, and it is the primary teacher for 7 of them. 5.3
Mathematics as Meta-discipline Meta-disciplines have been broadly characterized as ﬁelds that encompass the representation of
knowledge in other ﬁelds. They relate to other disciplines in a purely formal way: meta-disciplines
have or contribute process-knowledge as opposed to domain-knowledge.9 This characterization
would appear to be appropriate for mathematics.
If mathematics occurs in another ﬁeld, it should not be presumed that only the application
of a mathematical meta-process has transpired. To the contrary, often the relevant mathematics 9Information science has been called a meta-discipline in this sense (Bates, 1999). Fields that draw expertise from
more than one other ﬁeld have also been called meta-disciplines, for example see Mihelcic et al. (2003). In the latter
sense some ﬁelds may be meta-disciplines only temporarily while they are nascent. Mathematics Turned Inside Out – Joseph F. Grcar
16 Table 1: Mathematics subjects as interest in them is apportioned among the extensive or intensive
faculties. Teaching subjects are determined by applying a 3% threshold in Figure 3 to the courses
taught by the respective group. Research subjects are those that attain 4% in Figures 4 or 1. Overall
interest is determined by applying the 3% and 4% thresholds to Figures 3 and 1, respectively. Math.
non-Math.
Overall Subject
Code tea. res. tea.
res.
tea. res. Combinatorics 05
✓
✓
✓
✓ Number theory 11
✓
✓ Algebraic geometry 14
✓ Linear, multilinear algebra 15
✓ Group theory 20
✓ Real functions 26
✓ Several complex variables 32
✓ Ordinary differential eq. 34
✓
✓ Partial differential eq. 35
✓
✓
✓ Dynamical systems 37
✓ Fourier analysis 42
✓ Functional analysis 46
✓ Differential geometry 53
✓ Manifolds, cell complexes 57
✓ Global analysis 58
✓ Probability theory 60
✓
✓
✓
✓ Statistics 62
✓
✓
✓
✓
✓ Numerical analysis 65
✓
✓
✓
✓
✓
✓ Computer science 68
✓
✓
✓
✓
✓ Mech. of deform. solids 74
✓
✓
✓ Fluid mechanics 76
✓
✓
✓
✓ Classical thermodynamics 80
✓
✓ Quantum theory 81
✓ Operations research 90
✓
✓
✓
✓ Game theory, economics 91
✓
✓
✓
✓ Systems theory, control 93
✓
✓
✓
✓ Information and comm. 94
✓
✓
✓ originated in the other ﬁeld. For example, C. F. Gauss (1809) discovered the method of least squares,
or regression analysis, during his work in astronomy and geodesy.10 Today regression analysis
is taught in mathematics or statistics departments (and others), yet Gauss’s discovery was made
while he was immersed in the physical sciences. Since discovery is the raison d’être for research
universities, it is reckless to ignore how mathematics discoveries have often been made.
United States research universities have no consistent organizational structure for meta-disciplines
such as mathematics whose expertise spreads across many ﬁelds. Considering just academic depart-
ments that involve considerable mathematics knowledge, the 48 group I universities (section 3.2)
have disparate departments (Table 4) often in different colleges of the university. For example,
operations research may be found in one or both of the business and engineering colleges. The
mathematics of computer science is the purview of one or two departments variously called “com-
puter science,” “computer and information science” and “electrical and computer engineering” that
are found in engineering or in science colleges. Separating computer science from mathematics has
been described as detrimental (Kowalik, 2006). One may conjecture that the same applies to other 10Stigler (1986) describes this history, and Hald (2007) derives the theory in the historical context. Gauss, whose career
coincided with the formation of academic departments, is remembered for his mathematical and physical discoveries. He
taught mostly astronomy and geodesy (Dunnington, 2004, pp. 405–410). Mathematics Turned Inside Out – Joseph F. Grcar
17 subjects as well.
Statistics is a speciﬁc organizational challenge that all universities faced simultaneously and for
which there is some historical narrative. Universities were urged to create statistics departments
in the middle half of the 20th century because mathematics departments failed to add statisticians
to their faculties in proportion to the growing demand for statisticians in government and industry.
Hotelling (1940) alleged that mathematics departments viewed teaching statistics as service work
that did not require a commitment to research, thereby allowing new positions to be ﬁlled by spe-
cialists in other mathematics, thus decreasing the acuity of the statistics instruction.11 Whatever the
case, administrations did not respond uniformly to their “statistics problem,” suggesting that they
failed to resolve it. Some universities continue to teach statistics in their mathematics departments,
while many have statistics departments now (Table 4), and all universities allow the subject to be
taught by other departments as well (Raftery, 2000). These solutions may be reasonable except in
so far as addressing what it means to maintain mathematics as a meta-discipline. Exhortations to be
“more interdisciplinary” and “more mathematical” are a common stopgap.12 For several years, mathematics departments in the United States have had signiﬁcant, precipi-
tous declines in both undergraduate degrees and graduate enrollments, while at the same time, there
has been essentially no decline in tenured faculty (Bressoud, 2009; Kirwan, 2001).13 As national
economic conditions strain ﬁnancial support for higher education, many institutions may look for
economies in their mathematics programs. Faced with similar hardships a decade earlier, for ex-
ample, one university proposed to employ only non-ladder staff for lower division mathematics
instruction in the numerous service courses (to be supervised by natural and social science faculty),
and to curtail the graduate mathematics program (Jackson, 1996, 1997). Such proposals may engen-
der vehement objections on the grounds that mathematics is a core academic discipline. However 11If Hotelling is accurate, then the treatment of statistics in mathematics departments is a good example of intra-
disciplinary considerations trumping those of the academic community (Calhoun, 2000, pp. 74–75).
12See recommendations of the National Science Board (2004, II, 2-20) and several studies quoted therein.
13D. M. Bressoud is president of the Mathematical Association of America. W. E. Kirwan is chancellor of the university
system of Maryland and a former mathematics department head. 76 05 60 15 42 26
11 46 53 20 37 14 58 57 32 35 34 80 91 94 93
90 65 74 68 92 62 Mathematics
Department
Teaching Mathematics
Department
Publications non-Mathematics
        Department
        Teaching non-Mathematics
        Department
        Publications Italic subjects – most published
Underscored – most taught 86 81 Figure 8: Venn diagram of mathematics subjects that are prominent in publication or undergraduate
teaching of either the extensive or the intensive mathematics faculties. Table 1 identiﬁes the subjects.
The text explains the selection criteria. Mathematics Turned Inside Out – Joseph F. Grcar
18 compelling that view may be, this paper shows the objections are not apropos the policy question,
because the core of advanced undergraduate mathematics education occurs outside mathematics de-
partments. The proper justiﬁcation for maintaining mathematics does not appear to be that it is a
core discipline, rather that it is a meta-discipline, but then in what form should it be maintained? 5.4
Philosophy and Sociology of Mathematics The failure to recognize the ramiﬁcations of mathematics as a meta-discipline, with a large “exten-
sive” faculty outside mathematics departments, extends beyond university organizational structures.
Studies of the sociology and philosophy of mathematics have focussed on the foundational issue
concerning the epistemology of pure abstractions and its signiﬁcance for education.14 This approach
amounts to restricting the view of mathematics to the intensive faculty and ignoring the extensive
faculty. The pure abstractions in the research of the intensive faculty are doubly removed from
reality because, in view of Figure 8, mathematics departments conduct research predominantly on
subjects they do not teach to the educated public, who consist of university graduates with four-year
degrees. Thus, the “foundational question” is a Gedankenversuch for philosophers and sociologists
that is moot for society.15 In contrast, the extensive mathematics faculty conduct research on tangi-
bles whose meaning is undoubtable. For example, the professor of FIN 357 wrote a paper (Lee and
Rao, 1988) about asset valuation formulas, and the professor of CS 320N wrote a paper (Quintana-
Ort’i and van de Geijn, 2003) about devising algorithms for computers.16 A philosophical question
about mathematics that would appear to be more relevant to society and education is the one asked
by Eugene Wigner (1960): why is mathematics so successful in reasoning about reality? 5.5
K-12 Mathematics Education Twentieth century K-12 educational policy in the United States has been characterized as a continual
disagreement between educators, mathematicians, and parents.17 How to prepare students to enter
universities is a paramount concern, yet in view of the many departments that provide upper divi-
sion mathematics instruction (Figure 3), participants in the K-12 debate may lack a comprehensive
understanding of university mathematics. For example, K-12 educators acquire their own concep-
tion of mathematics from mathematics departments rather than from the majority of mathematics
researchers who make up the extensive faculty. In an era of accountability and amidst brewing controversy concerning mathematics ed-
ucation’s goals and emphasis, mathematicians and mathematics educators continued to
work together to formulate recommendations for how teachers could develop the math-
ematical and pedagogical knowledge and skills they would need to help all students
learn.
— Ferrini-Mundy and Graham (2003, p. 1268) Natural and social science faculty recently insisted that statistics be included in curricular stan-
dards for university-bound students in spite of mathematics faculty who felt statistics was “not
necessarily a prerequisite for success” (Conley, 2003, p. 37). The suggestion that departments other
than mathematics might set standards for K-12 education is considered to be novel (Confrey, 2007). 14For example see the recent collection of articles edited by van Kerkhove and van Bendegem (2007).
15A Gedankenversuch is a thought experiment in physics that is interesting from the standpoint of theory but is impos-
sible to encounter in reality.
16This particular professor of ﬁnance is not heavily published on mathematics subjects while the professor of computer
science is.
17As described by Klein (2002) and Schoenfeld (2004). In the United States, K-12 refers to kindergarten followed by
12 years of pre-university instruction. Mathematics Turned Inside Out – Joseph F. Grcar
19 Since the mathematics educations of many university students are completed by the extensive math-
ematics faculty, their view of the mathematics skills needed by university students would appear to
be relevant to K-12 education. 5.6
National Research Education Policy At the turn of the last century, the United States National Science Foundation (2000) began a grant
program, VIGRE, for “departments in the mathematical sciences to carry out high quality edu-
cational programs, at all levels, that are vertically integrated with the research activities of these
departments.” In short, the purpose was to improve university education in mathematics. Grants
were awarded entirely to mathematics departments (Durrett, 2002; Mackenzie, 2002). These funds
may have been misdirected because, as Figure 3 indicates, 79% of upper division mathematics stu-
dent credit hours are taught outside mathematics departments to students who earn degrees in other
departments. There is no systematic effort to address undergraduate university mathematics in a
comprehensive manner. Referring to the extensive faculty, Steen (2006) ﬁnds an exciting “stealth
curriculum that thrives outside the conﬁning boundaries of college and university mathematics de-
partments.”18 6
Conclusion In summary, this paper shows that United States research universities have an “intensive” mathe-
matics faculty inside a department of the same name, and an “extensive” mathematics faculty spread
across other departments. The two faculties perform different roles in education and research. The
intensive and extensive faculties teach lower or upper division students, respectively, and they con-
duct research in mathematics subjects either unrelated or relevant to upper division students, also
respectively. United States research universities have not been successful in aligning the teaching
and research responsibilities of their mathematics department faculties. In contrast, the mathemat-
ics research conducted by faculty outside the mathematics department is aligned with the vocational
interests of undergraduate students. The existence of the extensive faculty has not been appreci-
ated previously because an examination of the education literature indicates that the contributions
of the faculty outside mathematics departments are not recognized in formulating national policies
for mathematics education.
This study suggests several questions for further investigation. One is whether other ﬁelds be-
sides mathematics, such as other meta-disciplines, have similar “two-tier” faculties consisting of a
formal, intensive faculty in one department and an informal, extensive faculty in other departments.
Further is the question of how these separate faculties developed in the department-based organi-
zation of research universities and what they mean for the discipline-based organization of science.
From the standpoint of educational administration it is relevant to study the attitudes of the intensive
and extensive faculty toward the other group and their perception of their own roles in the two-tier
system. These attitudes evidently affect how university-wide curriculum is established. Coordina-
tion would appear to be through informal arrangements because direct administration would have
to cut across the boundaries of departments and colleges which, ipso facto, are independent in for-
mulating curricula. The utimate question is whether the two-tier approach is effective in promoting
excellence in either research or education. As Calhoun (2000, p. 48) has written, the United States
approach to education involves an interdependent system, but not necessarily one that reﬂects a
rational design or that functions perfectly. 18L. A. Steen is a former president of the Mathematical Association of America. Mathematics Turned Inside Out – Joseph F. Grcar
20 Acknowledgements I wish to thank the editor, Prof. C. Musselin, and the reviewers for comments and suggestions that
greatly improved this paper. Mathematics Turned Inside Out – Joseph F. Grcar
21 Appendix: Institutions Table 2: Overview of institutions discussed in this paper. “R&D” is research and development.
“RU/(V)H” is research university with (very) high research activity. “$M” is one million dollars.
“U’grad” is undergraduate. Data are from the Carnegie Foundation (2010). Science and Engineering R&D Expenditures
Region of the
Enrollment INSTITUTION
Carnegie Classiﬁcation
$M
Control
United States
Total
U’grad. Auburn Univ.
RU/H
117
Public
Southeast
22,928
78%
Boston Univ.
RU/VH
225
Private
New England
29,596
57%
Brandeis Univ.
RU/VH
51
Private
New England
5,072
63%
Brown Univ.
RU/VH
125
Private
New England
8,004
73%
California Inst. of Tech.
RU/VH
241
Private
Far West
2,171
41%
Carnegie Mellon Univ.
RU/VH
186
Private
Mid East
9,803
55%
Columbia Univ.-New York City
RU/VH
438
Private
Mid East
21,648
30%
Cornell Univ.
RU/VH
555
Private
Mid East
19,518
70%
CUNY-Graduate School
RU/H
3
Public
Mid East
4,234
0%
Duke Univ.
RU/VH
520
Private
Southeast
12,770
49% Georgia Inst. of Tech.-Atlanta
RU/VH
364
Public
Southeast
16,841
65%
Harvard Univ.
RU/VH
409
Private
New England
24,648
32%
Indiana Univ.-Bloomington
RU/VH
135
Public
Great Lakes
37,821
75%
Johns Hopkins Univ.
RU/VH
637
Private
Mid East
18,626
28%
Massachusetts Inst. of Tech.
RU/VH
486
Private
New England
10,320
40%
Michigan State Univ.
RU/VH
321
Public
Great Lakes
44,836
73%
New York Univ.
RU/VH
242
Private
Mid East
39,408
48%
Northwestern Univ.
RU/VH
320
Private
Great Lakes
17,747
47%
Ohio State Univ.-Columbus
RU/VH
496
Public
Great Lakes
50,995
68%
Penn. State Univ.-State College
RU/VH
480
Public
Mid East
41,289
82% Princeton Univ.
RU/VH
180
Private
Mid East
6,708
70%
Purdue Univ.-West Lafayette
RU/VH
309
Public
Great Lakes
40,108
76%
Rensselaer Polytechnic Inst.
RU/VH
51
Private
Mid East
6,696
73%
Rice Univ.
RU/VH
52
Private
Southwest
4,855
60%
Rutgers Univ.-New Brunswick
RU/VH
248
Public
Mid East
34,696
73%
Stanford Univ.
RU/VH
603
Private
Far West
18,836
35%
SUNY-Stony Brook
RU/VH
200
Public
Mid East
21,685
60%
Univ. of California-Berkeley
RU/VH
507
Public
Far West
32,803
67%
Univ. of California-Los Angeles
RU/VH
849
Public
Far West
35,966
68%
Univ. of California-San Diego
RU/VH
647
Public
Far West
24,663
80% Univ. of California-Santa Barbara
RU/VH
149
Public
Far West
21,026
84%
Univ. of California-Santa Cruz
RU/VH
78
Public
Far West
15,036
88%
Univ. of Chicago
RU/VH
247
Private
Great Lakes
13,870
33%
Univ. of Illinois-Chicago
RU/VH
292
Public
Great Lakes
24,865
58%
Univ. of Illinois-Urbana
RU/VH
494
Public
Great Lakes
40,687
71%
Univ. of Maryland-College Park
RU/VH
322
Public
Mid East
34,933
68%
Univ. of Michigan-Ann Arbor
RU/VH
780
Public
Great Lakes
39,533
61%
Univ. of Minnesota-Twin Cities
RU/VH
509
Public
Plains
50,954
56%
Univ. of N. Carolina-Chapel Hill
RU/VH
391
Public
Southeast
26,878
59%
Univ. of Notre Dame
RU/VH
60
Private
Great Lakes
11,479
72% Univ. of Oregon
RU/H
45
Public
Far West
20,296
76%
Univ. of Pennsylvania
RU/VH
565
Private
Mid East
23,305
46%
Univ. of Southern California
RU/VH
414
Private
Far West
32,160
50%
Univ. of Texas-Austin
RU/VH
344
Public
Southwest
50,377
70%
Univ. of Utah
RU/VH
224
Public
Rocky Mtns.
28,933
62%
Univ. of Virginia-Charlottesville
RU/VH
206
Public
Southeast
23,341
58%
Univ. of Washington-Seattle
RU/VH
685
Public
Far West
39,199
64%
Univ. of Wisconsin-Madison
RU/VH
717
Public
Great Lakes
40,455
68%
Washington Univ.-St. Louis
RU/VH
474
Private
Plains
13,210
49%
Yale Univ.
RU/VH
388
Private
New England
11,441
46% Aggregate
17,381
1,227,269
63% Mathematics Turned Inside Out – Joseph F. Grcar
22 Table 3: Personnel of the institutions and rating of the mathematics departments. “STEM” is sci-
ence, technology, engineering, and mathematics. “NL” are full-time non-ladder academic staff,
“-R” research only. “GS” are graduate students including part-time assistants. “Rtng” is the quality
rating of mathematics research-doctorate programs (Goldberger et al., 1995, appendix table H-4).
“Grp” is the peer group of mathematics departments (Amer. Math. Soc., 2009) with group I con-
sisting of those rating 3–5.00. Data in columns 2–5 are from the Carnegie Foundation (2010). Data
in columns 6–8 were gathered in Spring 2010. INSTITUTION
Doctorates
All University
Mathematics Department STEM and/or Social Science Doctorates
Personnel
Personnel
Quality All Doctorates Annual Total
%
NL-R
Ladder
NL
GS
Rtng. Grp. Auburn Univ.
161
31%
53
1,088
2.31
II
Boston Univ.
267
57%
88
1,314
34
41
51
3.03
I
Brandeis Univ.
82
52%
107
347
11
6
32
3.64
I
Brown Univ.
147
69%
239
778
20
13
30
3.73
I
California Inst. of Tech.
166
100%
527
368
15
19
32
4.19
I
Carnegie Mellon Univ.
195
79%
248
1,172
38
13
42
3.41
I
Columbia Univ.-New York City
495
52%
444
3,221
24
42
69
4.23
I
Cornell Univ.
412
72%
749
1,734
45
28
68
4.05
I
CUNY-Graduate School
298
51%
77
133
73
0
49
3.65
I
Duke Univ.
259
68%
732
2,902
26
34
43
3.53
I Georgia Inst. of Tech.-Atlanta
311
96%
72
856
55
29
102
3.19
I
Harvard Univ.
572
54%
3,950
3,223
21
72
58
4.90
I
Indiana Univ.-Bloomington
375
37%
231
1,486
43
21
119
3.53
I
Johns Hopkins Univ.
362
57%
1,421
2,963
16
14
32
3.04
I
Massachusetts Inst. of Tech.
467
87%
1,026
1,056
51
92
90
4.92
I
Michigan State Univ.
430
47%
393
2,322
60
40
113
3.05
I
New York Univ.
407
40%
387
3,083
60
109
117
4.49
I
Northwestern Univ.
367
57%
188
1,996
27
23
47
3.71
I
Ohio State Univ.-Columbus
560
51%
414
2,822
64
37
197
3.66
I
Penn. State Univ.-State College
539
60%
313
2,817
56
41
90
3.50
I Princeton Univ.
276
74%
381
797
36
30
69
4.94
I
Purdue Univ.-West Lafayette
446
59%
413
2,060
77
66
159
3.82
I
Rensselaer Polytechnic Inst.
156
88%
83
413
27
13
60
3.02
I
Rice Univ.
126
82%
147
536
15
9
30
3.49
I
Rutgers Univ.-New Brunswick
382
62%
168
2,158
73
77
124
3.96
I
Stanford Univ.
625
72%
1,503
1,685
28
49
74
4.68
I
SUNY-Stony Brook
285
57%
229
1,287
32
28
71
3.94
I
Univ. of California-Berkeley
775
62%
982
1,495
69
34
193
4.94
I
Univ. of California-Los Angeles
666
61%
1,481
2,676
64
56
200
4.14
I
Univ. of California-San Diego
327
83%
1,246
1,287
58
91
131
4.02
I Univ. of California-Santa Barbara
253
69%
237
907
33
25
69
3.04
I
Univ. of California-Santa Cruz
107
77%
113
518
2.92
II
Univ. of Chicago
331
61%
546
2,261
37
58
87
4.69
I
Univ. of Illinois-Chicago
233
44%
292
1,925
84
3
143
3.58
I
Univ. of Illinois-Urbana
574
60%
279
2,316
70
96
181
3.93
I
Univ. of Maryland-College Park
482
61%
244
2,887
59
52
216
3.97
I
Univ. of Michigan-Ann Arbor
660
68%
909
3,376
69
59
143
4.23
I
Univ. of Minnesota-Twin Cities
592
50%
935
2,528
68
30
132
4.08
I
Univ. of N. Carolina-Chapel Hill
439
57%
660
1,659
37
11
66
3.24
I
Univ. of Notre Dame
149
66%
150
783
44
22
67
3.11
I Univ. of Oregon
164
44%
104
936
28
15
55
3.06
I
Univ. of Pennsylvania
413
52%
1,157
1,671
30
29
64
3.97
I
Univ. of Southern California
573
35%
594
2,001
36
17
60
3.23
I
Univ. of Texas-Austin
702
57%
304
2,435
61
48
134
3.85
I
Univ. of Utah
216
61%
288
2,238
44
52
83
3.52
I
Univ. of Virginia-Charlottesville
358
49%
425
1,985
26
13
50
3.18
I
Univ. of Washington-Seattle
503
63%
1,123
3,761
62
20
97
3.39
I
Univ. of Wisconsin-Madison
628
59%
842
4,359
61
71
131
4.10
I
Washington Univ.-St. Louis
241
56%
778
1,849
25
4
33
3.42
I
Yale Univ.
332
57%
1,135
3,147
16
27
35
4.55
I Aggregate
18,886
60%
29,407
93,617 2,108 1,779 4,338 Mathematics Turned Inside Out – Joseph F. Grcar
23 Table 4: University units (chieﬂy, departments) for which data are reported in Figures 6 and 7:
“APPL MATH” is applied mathematics, “BIOS” is biostatistics, “BUS” is school of business, “CS(E)”
is computer science and engineering, “E(C)E” is electrical and computer engineering, “ECO” is
economics, “EPI” is epidemiology, “IE” is industrial engineering, “MATH” is mathematics, “ME” is
mechanical engineering, “OR” is operations research, “PHY” is physics, “STAT” is statistics. The
absence of an entry in this table indicates the university in question does not have the corresponding
department. INSTITUTION APPL MATH MATH BIOS, EPI BUS CS(E) E(C)E ECO ME OR,
IE PHY STAT OTHER UNITS Boston Univ.
✓
2
✓
✓
✓
✓
✓
✓ Brandeis Univ.
✓
✓
✓
✓
✓ Brown Univ.
✓
✓
✓
✓
✓
Sch. of Eng. California Inst. of Tech.
✓
✓
✓
✓
✓
✓
Appl. Phy. Dept. Carnegie Mellon Univ.
✓
✓
✓
✓
✓
✓
✓
✓
Soc. & Dec. Sci. Dept. Columbia Univ.-New York City
✓
2
✓
✓
✓
✓
✓
✓
✓
✓
Appl. Math. & Phy. Dept. Cornell Univ.
✓
✓
✓
✓
✓
✓
✓
✓
✓
Appl. Math. Ctr. CUNY-Graduate School
✓
✓
2
✓
✓
✓
✓ Duke Univ.
✓
✓
✓
✓
✓
✓
✓
✓ Georgia Inst. of Tech.-Atlanta
✓
✓
✓
✓
✓
✓
✓
Col. of Comp. Harvard Univ.
✓
2
✓
✓
✓
✓
Sch. of Eng. Indiana Univ.-Bloomington
✓
✓
✓
✓
✓
✓ Johns Hopkins Univ.
✓
2
✓
✓
✓
✓
✓
✓
✓ Massachusetts Inst. of Tech.
✓
✓
✓
✓
✓
✓ Michigan State Univ.
✓
✓
✓
✓
✓
✓
✓
✓ New York Univ.
✓
✓
✓
✓ Northwestern Univ.
✓
✓
✓
✓
✓
✓
✓
✓
✓ Ohio State Univ.-Columbus
✓
✓
✓
✓
✓
✓
✓
✓
✓ Penn. State Univ.-State College
✓
✓
✓
✓
✓
✓
✓
✓
✓ Princeton Univ.
✓
✓
✓
✓
✓
✓
✓
Appl. & Comp. Math. Pgm. Purdue Univ.-West Lafayette
✓
✓
✓
✓
✓
✓
✓
✓
✓ Rensselaer Polytechnic Inst.
✓
✓
✓
✓
✓
✓
✓ Rice Univ.
✓
✓
✓
✓
✓
✓
✓
✓ Rutgers Univ.-New Brunswick
✓
✓
✓
✓
✓
✓
2
✓
✓ Stanford Univ.
✓
✓
✓
✓
✓
✓
✓
✓
✓
Appl. Phy. Dept.; ICME SUNY-Stony Brook
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of California-Berkeley
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of California-Los Angeles
✓
2
✓
✓
✓
✓
✓
✓
✓ Univ. of California-San Diego
✓
✓
✓
✓
✓
✓
✓ Univ. of California-Santa Barbara
✓
✓
✓
✓
✓
✓ Univ. of Chicago
✓
✓
✓
✓
✓
✓
✓ Univ. of Illinois-Chicago
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Illinois-Urbana
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Maryland-College Park
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Michigan-Ann Arbor
✓
✓
✓
✓
✓
✓
✓ Univ. of Minnesota-Twin Cities
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of N. Carolina-Chapel Hill
✓
2
✓
✓
✓
✓
Stat. & Op. Res. Dept. Univ. of Notre Dame
✓
✓
✓
✓
✓
✓
✓ Univ. of Oregon
✓
✓
✓
✓
✓ Univ. of Pennsylvania
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Southern California
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Texas-Austin
✓
✓
✓
✓
✓
✓
✓
Stat. & Sci. Comp. Div. Univ. of Utah
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Virginia-Charlottesville
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Washington-Seattle
✓
✓
2
✓
✓
✓
✓
✓
✓
✓
✓ Univ. of Wisconsin-Madison
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Eng. Phy. Dept. Washington Univ.-St. Louis
✓
✓
✓
✓
✓
✓
✓
✓
Appl. Stat. Ctr. Yale Univ.
✓
2
✓
✓
✓
✓
✓
✓
✓ Mathematics Turned Inside Out – Joseph F. Grcar
24",0
"The copula representations for conditionally independent random vari-
ables and the distribution properties of order statistics of these random
variables are studied.
Key words: Copula, Conditional independence, order statistics. 1
Introduction Dawid (1979) noted that many of the important concepts of statistical theory
can be regarded as expressions of conditional independence, thus the conditional
independence oﬀers a new language for the expression of statistical concepts and
a framework for their study. Dawid (1979) considered the random variables
X , Y and Z. If the random variables X and Y are independent in their joint dis-
tribution given Z = z, for any value of z, then X and Y are called conditionally
independent given Z and denoted by (X ⊥Y | Z). A general calculus of condi-
tional independence is developed in Dawid (1980), where the general concept of
conditional independence for a statistical operation is introduced. Dawid (1980)
showed how the conditional independence for statistical operations encompasses
the basic properties such as, suﬃciency, (X ⊥Θ | T ), in which X is a random
variable with distributions governed by a parameter Θ, and T is a function of X,
or.pointwise independence, adequacy etc. Pearl et al. (1989) considered condi-
tional independence as a statement and addressed the problem of representing
the sum total of independence statements that logically follow from a given
set of such statements.
Shaked and Spizzichino (1998) considered n nonneg-
ative random variables Ti, i = 1, 2, ..., n which are interpreted as lifetimes of n
units and assuming that T1, T2, ..., Tn are conditionally independent given some
random variable Θ, determined the conditions under which these conditionally
independent random variables are positive dependent. In the Bayesian setting
it is of interest to know which kind of dependence arises when Θ is unknown.
Prakasa Rao (2006) studied the properties of conditionally independent ran-
dom variables and proved the conditional versions of generalized Borel-Cantelli 1 lemma, generalized Kolmogorov’s inequality, H`
ajek-R´
enyi inequality. Prakasa-
Rao (2006) presented also the conditional versions of classical strong law of large
numbers and central limit theorem.
In this paper a diﬀerent approach to conditionally independent random vari-
ables is considered, the necessary and suﬃcient conditions for conditional inde-
pendence in terms of the partial derivatives of distribution functions and copulas
are given. Also, the distributional properties of order statistics of conditionally
independent random variables are studied. The paper is organized as follows: in
section 2 we present a deﬁnition of conditionally independent random variables
X1, X2, ...., Xn given Z and derive a suﬃcient and necessary conditions for con-
ditional independence in terms of copulas. These conditions allow to construct
conditionally independent random variables with given bivariate distributions
of (Xi, Z) and marginal distributions of Xi’s. In Section 3 we study the distri-
butions of order statistics from conditionally independent random variables. It
is shown that these distributions can be expressed in terms of partial derivatives
of copulas of Xi and Z. The permanent expressions for distributions of order
statistics are also presented. 2
Conditionally independent random variables Let (X1, X2, ..., Xn, Z) be n + 1 variate random vector with joint distribution
function (cdf) H(x1, x2, ..., xn, z) = C(FX1(x1), FX2(x2), ..., FXn(xn), FZ(z)),
where FXi(xi) = P{Xi ≤xi}, i = 1, 2, ..., n, FZ(z) = P{Z ≤z} and C is
a connecting copula. Deﬁnition 1 If P{X1
≤
x1, X2 ≤x2, ..., Xn ≤xn | Z = z} =
P{X1
≤
x1 | Z = z}P{X2 ≤x2 | Z = z} · · ·P{Xn ≤xn | Z = z}
(1) for all (x1, x2, ..., xn, z) ∈Rn+1, then the random variables X1, X2, ..., Xn are
said to be conditional independent given Z. It is clear that if the random variables X1, X2, ..., Xn are conditionally inde-
pendent given Z then the conditional random variables Xi,z ≡(Xi | Z = z), i =
1, 2, ..., n are independent for all z ∈R and P{X1 ∈B1, ..., Xn ∈Bn | ̥Z} =
P{X1 ∈B1 | ̥Z} · · · P{Xn ∈Bn | ̥Z} a.s. for any Borel sets B1, B2, ..., Bn,
where ̥Z is a σ−algebra generated by Z. Example 1 Bairamov and Arnold (2008) discussed the residual lifelengths of
the remaining components in an n −k + 1 −out −of −n system with lifetimes
of the components X1, X2, . . . , Xn, respectively. Let Xi’ s are independent and
identically distributed (iid) random variables with common absolutely continuous
distribution F. If we are given Xk:n = x, then the conditional distribution of the
subsequent order statistics Xk+1:n, . . . , Xn:k is the same as the distribution of
order statistics of a sample of size n−k from the distribution F truncated below 2 at x. If we denote by Y (k)
i
, i = 1, 2, . . . , n −k the randomly ordered values of
Xk+1:n, . . . , Xn:n, then given Xk:n = x, these Y (k)
i
’s will be iid with common
survival function ¯
F(x + y)/ ¯
F(x). 2.1
Assumptions and notations Throughout this paper we assume that X1, X2, ..., Xn, Z are absolutely contin-
uous random variables with corresponding probability density functions (pdf)
fXi(x), i = 1, 2, ..., n and fZ(z), respectively. Denote by FX1,X2,...,Xn(x1, x2, ..., xn)
and CX1,X2,...,Xn(u1, ..., un) the joint distribution function and the copula of
(X1, X2, ..., Xn), respectively and denote the pdf by fX1,X2,...,Xn(x1, x2, ..., xn).
We use the following notations for the bivariate marginal distributions and cop-
ulas of random variables Xi and Z : FXi,Z(xi, z) ≡P{Xi ≤x, Z ≤z}, i = 1, 2, ..., n, and
FXi,Z(xi, z) ≡CXi,Z(FXi(xi), FZ(z)), where CXi,Z(u, w), (u, w) ∈[0, 1]2, is the copula of random variables (Xi, Z).
We use also the following notations for partial derivatives: ˙
H(x1, x2, ..., xn, z) = ∂H(x1, x2, ..., xn, z) ∂z
,
˙
FXi,Z(xi, z) = ∂FXi,Z(xi, z) ∂z
, ˙
C(u1, u2, ..., un, w) ≡∂C(u1, u2, ..., un, w) ∂w
,
˙
CXi,Z(u, w) ≡∂ ∂wCXi,Z(u, w). The following theorem gives a necessary and suﬃcient condition for condi-
tional independence of random variables X1, X2, ..., Xn given Z. 2.2
Necessary and suﬃcient conditions for conditional in-
dependence Theorem 1 If fZ(z) > 0, then the random variables X1, X2, ..., Xn are condi-
tionally independent given Z = z if and only if ˙
H(x1, x2, ..., xn, z) =

1 fZ(z) n−1
n
Y i=1
˙
FXi,Z(xi, z).
(2) Proof. It is clear that (1) and (2) both are equivalent to lim
∆z→0 P{X1
≤
x1, X2 ≤x2, ..., Xn ≤xn | z ≤Z < z + ∆z} =
lim
∆z→0 P{X1 ≤x1 | z ≤Z < z + ∆z} · · · P{Xn ≤xn | z ≤Z < z + ∆z}. 3 Corollary 1 The random variables X1, X2, ..., Xn are conditionally indepen-
dent given Z if and only if ˙
C(u1, u2, ..., un, w) = n
Y i=1
˙
CXi,Z(ui, w) for all 0 ≤u1, u2, ..., un, w ≤1.
(3) Proof. From (2) one can write 1 fZ(z)
∂ ∂z [C(FX1(x1), FX2(x2), ..., FXn(xn), FZ(z))] =

1 fZ(z) n
n
Y i=1 ∂ ∂z [CXi,Z(FXi(xi), FZ(z))] .
(4) It follows from (4) that ˙
C(FX1(x1), FX2(x2), ..., FXn(xn), FZ(z)) = n
Y i=1
˙
CXi,Z(FXi(xi), FZ(z)) and the transformation FXi(xi) = ui, (i = 1, 2, ..., n), FZ(z) = w leads to (3). Corollary 2 X1, X2, ..., Xn are conditionally independent given Z, if and only
if the following integral representations for joint distribution function and copula
hold true: FX1,X2,...,Xn(x1, x2, ..., xn)
= ∞
Z −∞ 
1 fZ(z) n−1
n
Y i=1 ∂FXi,Z(xi, z) ∂z
dz −∞
<
x1 < · · · < xn < ∞
(5) and CX1,X2,...,Xn(u1, u2, ..., un)
= 1
Z 0 n
Y i=1
˙
CXi,Z(ui, w)dw, 0
≤
u1, ..., un ≤1.
(6) Proof. The proof can be made easily by integrating (2) and (3). 2.3
Construction of conditionally independent random vari-
ables Using Theorem 1 one can construct the conditionally independent random vari-
ables X1, X2, ..., Xn given Z, with given joint copulas CXi,Z(ui, w), (u, w) ∈
[0, 1]2, of (Xi, Z), i = 1, 2, ..., n. The constructed joint distributions of condi-
tionally independent random variables can be used, for example, in reliability 4 analysis, for modeling lifetimes of the system having n+1 components, such that
knowing the exact lifetime of one of the components allows the independence as-
sumption for other components. Consider for example a system of three depen-
dent components with lifetimes X1, X2, Z having joint cdf FX1,X2,Z(x1, x2, z).
Assume that the lifetime of the system is T = max{min(Z, X1), min(Z, X2)}
and if Z = z is given, then the interaction between two other components be-
comes weaker, hence X1 and X2 can be assumed to be independent. In this and
many similar applications, where the conditional independence is a subject, the
constructed joint distributions of conditionally independent random variables
can be used for modeling of lifetimes or other random variables of interest. In
Shaked and Spizzichino
(1998) some interesting applications of conditionally
independent random variables, such as an imperfect repair with random eﬀec-
tiveness and lifetimes in random environments are discussed.
The following examples demonstrate the construction procedure of condi-
tionally independent random variables by using Theorem 1. Example 2 Let n = 2. Denote by C(u, v, w) the copula of random variables
(X1, X2, Z), by CX1,Z(u, w) the copula of (X1, Z) and by CX2,Z(v, w) the cop-
ula of (X2, Z), respectively. Assume that the joint distributions of (X1, Z) and
(X2, Z) are classic Farlie-Gumbel-Morgenstern (FGM) distribution, i.e. the cop-
ulas are CX1,Z(u, w)
=
uw{1 + α(1 −u)(1 −w)}, (u, w) ∈[0, 1]2, −1 ≤α ≤1
CX2,Z(v, w)
=
vw{1 + α(1 −v)(1 −w)}, (v, w) ∈[0, 1]2, −1 ≤α ≤1, with ˙
CX1,Z(u, w)
=
∂ ∂w {uw{1 + α(1 −u)(1 −v)}} =
u + α(1 −u)(1 −2w), (u, w) ∈[0, 1]2, −1 ≤α ≤1
˙
CX2,Z(v, w)
=
v + α(1 −v)(1 −2w), (v, w) ∈[0, 1]2, −1 ≤α ≤1. Then from equation (3) we have ˙
C(u, v, w)
=
˙
CX1,Z(u, w) ˙
CX2,Z(v, w) =
[u + α(1 −u)(1 −2w)] [v + α(1 −v)(1 −2w)]
=
uv + αuv(2 −u −v)(1 −2w) + α2uv(1 −u)
(7) ×(1 −v)(1 −2w)2. Integrating (7) with respect to w, one obtains C(u, v, w) = uvw + αuvw(2 −u −v)(1 −w)−α2 6 uv(1 −u)(1 −v)(1 −2w)3. (8) Therefore, the random variables X1, X2 with the copula CX1,X2(u, v) = uv + α2 6 uv(1 −u)(1 −v) 5 are conditionally independent if the copula of (X1, X2, Z) is C(u, v, w) given in
(8). Example 3 Using calculations in Example 1 it is not diﬃcult to see that if the
random variables X1, X2, ..., Xn and Z are deﬁned with their copulas as CX1,Z(u1, w)
=
u1w{1 + α(1 −u1)(1 −w)},
CX2,Z(u2, w)
=
u2w{1 + α(1 −u2)(1 −w)}, −1 ≤α ≤1 and
CXi,Z(ui, w) = uiw, i = 3, 4, ..., n, 0 ≤u1, u2, ..., un, w ≤1, then the solution of the equation ˙
C(u1, u2, · · · un, w) = n
Y i=1
˙
CXi,Z(ui, w) is C(u1, u2, ..., un, w)
=
u1u2 · · · unw + αu1u2 · · · unw(2 −u1 −u2)(1 −w) −α2 6 u1u2 · · · un(1 −u1)(1 −u2)(1 −2w)3.
(9) The copula of X1, X2, ..., Xn is CX1,X2,...,Xn(u1, u2, ..., un) = u1u2 · · · un + α2 6 u1u2 · · · un(1 −u1)(1 −u2) (10) and X1, X2, ..., Xn are conditionally independent given Z. Remark 1 The conditional independence of random variables
X1, X2, ..., Xn
makes it possible to evaluate many important probabilities concerning dependent
random variables by replacing them with the independent pairs of random vari-
ables. Assume that we need to calculate the probability of some event connected
with the dependent random variables X1, X2, ..., Xn, for example consider P{(X1, X2, ..., Xn)
∈
B} =
Z B fX1,X2,...,Xn(x1, x2, ..., xn)dx1dx2 · · · dxn, where B ∈ℜn, ℜn is the Borel σ−algebra of subsets of Rn, B = B1 × B2 ×
· · · × Bn and Bi are Borel sets on R and fX1,X2,...,Xn(x1, x2, ..., xn) is the joint
pdf of X1, X2, ..., Xn. If the random variables X1, X2, ..., Xn are conditionally 6 independent given Z, then conditioning on Z we have P{(X1, X2, ..., Xn)
∈
B} = ∞
Z −∞
P{(X1, X2, ..., Xn) ∈B | Z = z}dFZ(z) = ∞
Z −∞ n
Y i=1
P{Xi ∈Bi | Z = z}dFZ(z) = ∞
Z −∞ 
1 fZ(z) n−1

  n
Y i=1 Z Bi ∂˙
FXi,Z(xi, z) ∂xi
dxi 
 dz. (11) Furthermore, if G is some region on Rn, then P{(X1, X2, ..., Xn)
∈
G} = ∞
Z −∞
P{(X1, X2, ..., Xn) ∈G | Z = z}dFZ(z) = ∞
Z −∞
P{(X1,z, X2,z, ..., Xn,z) ∈G}dFZ(z) = ∞
Z −∞  
Z G fX1,z(x1)fX2,z(x2) · · · fXn,z(xn)dx1dx2 · · · dxn  dFZ(z),
(12) where Xi,z = (Xi | Z = z), i = 1, 2, ..., n are independent random variables with
pdf’s fXi,z(xi) =
1 fZ(z)
∂˙
FXi,Z(xi, z) ∂xi
, i = 1, 2, ..., n, respectively.
In the following example we illustrate how to use (12) in calculat-
ing of stress-strength probability P{X1 < X2} for dependent, but conditionally
independent random variables. Example 4 First we construct conditionally independent random variables X1, X2
given Z = z. Let FX1,Z(u, z)
=
u2z{1 + (1 −u2)(1 −z)}, FX1(u) = u2, FZ(z) = z, 0 ≤u, z ≤1 FX2,Z(v, z)
=
vz{1 + (1 −v)(1 −z)}, FX2(v) = v, FZ(z) = z, 0 ≤v, z ≤1, It is easy to calculate ˙
FX1,Z(u, z)
=
u2[1 −(1 −u2)(1 −z)] + u2z(1 −u2)
(13)
˙
FX2,Z(v, z)
=
v[1 + (1 −v)(1 −z)] + vz(v −1).
(14) Then using Theorem 1 we have ˙
FX1,X2,Z(u, v, z) = ˙
FX1,Z(u, z) ˙
FX2,Z(v, z)dz.
(15) 7 Using (13), (14 in 15 ) and then integrating we have FX1,X2,Z(u, v, z) =
4
3u2v2z3 −4 3u2vz3 + 4 3u4vz3 −4 3u4v2z3 −3z2u4v + 2z2u4v2 + 2z2u2v −z2u2v2 + 2u4vz −u4v2z.
(16) The joint cdf of X1 and X2 is FX1,X2(u, v) = 2 3u2v + 1 3u2v2 + 1 3u4v −1 3u4v2
(17) and the pdf is fX1,X2(u, v) = 4 3u + 4 3uv + 4 3u3 −8 3u3v. The corresponding copula of (X1, X2) can be obtained by using transformation
FX1(u) = u2 = t, FX2(v) = v = s and it is CX1,X2(t, s)
=
2
3ts + 1 3ts2 + 1 3t2s −1 3t2s2, 0
≤
t, s ≤1. It follows from Theorem 1 that X1, X2 are conditionally independent given
Z = z.
Now consider the probability P{X1 < X2}. By usual way integrating fX1,X2(u, v)
over the set {(u, v) : u < v} we obtain P{X1 < X2} = 1
Z 0 v
Z 0
fX1,X2(u, v)dudv = 31 90.
(18) On the other hand using conditional independence of X1, X2 given Z, we have P{X1
<
X2} =
Z
P{X1 < X2 | Z = z}dFZ(z) =
Z
P{X1,z < X2,z}dFZ(z) = 1
Z 0   1
Z 0
F1,z(u)dF2,z(u)  dFZ(z),
(19) where X1,z ≡X1 | Z = z and X2,z ≡X2 | Z = z are independent random
variables with cdf’s FX1,z(x)
=
1 fZ(z)
˙
FX1,Z(x, z) and FX2,z(x)
=
1 fZ(z)
˙
FX2,Z(x, z), 8 respectively. Since fZ(z) = 1, then from (19) taking into account (13) and (14)
we have P{X1
<
X2} = 1
Z 0   1
Z 0 
u2[1 −(1 −u2)(1 −z)] + u2z(1 −u2)
	
dF2,z(u)  dz = 1
Z 0   1
Z 0 
u2[1 −(1 −u2)(1 −z)] + u2z(1 −u2)
	
{1 + (1 −2u)(1 −2z)} du  dz = 1
Z 0  1 15 + 7 15z + 2 15z2

dz = 31 90,
(20) which agrees with (18). 3
Order statistics Denote by X1:n, X2:n, ..., Xn:n the order statistics of conditionally independent
random variables X1, X2, ..., Xn given Z. We are interested in distribution of
order statistics Xr:n, 1 ≤r ≤n and the joint distributions of Xr:n and Xs:n, 1 ≤
r < s ≤n. The formulas for distributions of order statistics contains expressions
depending on copulas of pairs (Xi, Z), i = 1, 2, ..., n and permanents. 3.1
Distribution of a single order statistics Conditioning on Z one obtains P{Xn:n
≤
x} = ∞
Z −∞
P{Xn:n ≤x | Z = z}dFZ(z) = ∞
Z −∞ n
Y i=1
P{Xi ≤x | Z = z}dFZ(z) = ∞
Z −∞ 
1 fZ(z) n
n
Y i=1 ∂FXi,Z(xi, z) ∂z
dFZ(z) = ∞
Z −∞ n
Y i=1
˙
CXi,Z(Fi(xi), FZ(z))dFZ(z) = 1
Z 0 n
Y i=1
˙
CXi,Z(Fi(xi), s)ds and similarly P{X1:n ≤x} = 1 − 1
Z 0 n
Y i=1
[1 −˙
CXi,Z(Fi(xi), s)]ds. 9 The distribution of Xr:n ,1 ≤x ≤n can be derived as follows: Fr:n(x)
≡
P{Xr:n ≤x} = n
X i=r ∞
Z −∞
P{exactly i of X’s are ≤x | Z = z}dFZ(z) = n
X i=r 1 i!(n −i)! X (j1,j2,...,jn)∈Sn ∞
Z −∞ ( i
Y l=1
P{Xjl ≤x | Z = z} × n
Y l=i+1
[1 −P{Xjl ≤x | Z = z}] ) dFZ(z) = n
X i=r 1 i!(n −i)! X (j1,j2,...,jn)∈Sn 1
Z 0 n
˙
CXjl ,Z(FXjl (x), s)
(21) × n
Y l=i+1 h
1 −˙
CXjl ,Z(FXjl (x), s)
i) ds, where
Sn = {(j1, j2, ..., jn), 1 ≤j1, j2, ..., jn ≤n},
(22) is the set of all n! permutations of (1, 2, ..., n) and
X (j1,j2,...,jn)∈Sn
extends over all elements of Sn. Remark 2 If X1, X2, ..., Xn are identically distributed, i.e. FXi(x) = FX(x), ∀x ∈
R, i = 1, 2, ..., n and the joint distributions of (X1, Z), (X2, Z), ..., (Xn, Z) are the
same, i.e. CXi,Z(u, w) = CX,Z(u, w), ∀(u, w) ∈[0, 1]2, i = 1, 2, ..., n then Fr:n(x) = n
X i=r n
i  1
Z 0 h
˙
CX,Z(FX(x), s)
ii h
1 −˙
CX,Z(FX(x), s)
in−i
ds.
(23) Remark 3 It follows from (23) that, if CXi,Z(u, w) = uw, i.e. X1, X2, ..., Xn
are independent and identically distributed (iid) random variables, then ˙
CX,Z(FX(x), s) =
FX(x) and Fr:n(x) = n
X i=r n
i 
F i
X(x) [1 −FX(x)]n−i . 3.2
The joint distributions of two order statistics The joint distribution of Xr:n, and Xs:n is Fr,s(x, y) = P{Xr:n ≤x, Xs:n ≤y} 10 = n
X i=r n−i
X j=max(s−i,0) 1 i!(j −i)!(n −i −j)! ∞
Z −∞ 
  X (l1,...,ln)∈Sn i
Y k=1
P{Xlk ≤x | Z = z} × i+j
Y k=i+1
[P{Xlk ≤y | Z = z} −P{Xlk ≤x | Z = z}] × n
Y k=i+j+1
[1 −P{Xlk ≤y | Z = z}] 
 dFZ(z) and in terms of copulas
Fr,s(x, y) = n
X i=r n−i
X j=max(s−i,0) 1 i!(j −i)!(n −i −j)! X (l1,...,ln)∈Sn 1
Z 0 (
i
Y k=1
˙
CXlk ,Z(FXlk (x), s) × i+j
Y k=i+1 h
˙
CXlk ,Z(FXlk (y), s) −˙
CXlk ,Z(Flk(x), s)
i × n
Y k=i+j+1 h
1 −˙
CXlk ,Z(FXlk (y), s)
i

 ds
(24) Remark 4 If X1, X2, ..., Xn are identically distributed with cdf FX(x), and the
joint distributions of (X1, Z), (X2, Z), ..., (Xn, Z) are the same, i.e. CXi,Z(u, w) =
CX,Z(u, w), ∀(u, w) ∈[0, 1]2, i = 1, 2, ..., n, then Fr,s(x, y)
= n
X i=r n−i
X j=max(s−i,0) n! i!(j −i)!(n −i −j)! 1
Z 0 h
˙
CX,Z(FX(x), s)
ii ×
h
˙
CX,Z(FX(y), s) −˙
CXlk ,Z(FX(x), s)
ij−i
(25) ×
h
1 −˙
CX,Z(FX(y), s)
in−i−j
ds Remark 5 It follows from (25) that, if CXi,Z(u, w) = uw, i.e. X1, X2, ..., Xn
are iid random variables, then ˙
CX,Z(FX(x), s) = FX(x) and Fr,s(x, y)
= n
X i=r n−i
X j=max(s−i,0) n! i!(j −i)!(n −i −j)!F i
X(x) ×(FX(y) −FX(x))j−i [1 −FX(y)]n−i−j , which agrees the well known formula for joint cdf of rth and sth order statistics
(see David and Nagaraja (2003)) 11 3.3
Expressions for joint distributions of order statistics
with permanents Suppose A = (aij), i, j = 1, 2, ..., n is the square matrix. The permanent of A is
deﬁned as Per(A) =
X (j1,j2,...,jn)∈Sn n
Y k=1
ak,jk, where Sn is deﬁned in (22) and P
(j1,j2,...,jn)∈Sn denotes the sum over all
n!permutations (j1, j2, ..., jn) of (1, 2, ..., n). Using (21) one can realize that Fr:n(x) = n
X i=r 1 i!(n −i)! 1
Z 0
Per (M1(x, s)) ds, where M1(x, s) =  














 ˙
CX1,Z(FX1(x), s)
˙
CX2,Z(FX2(x), s)
· · ·
˙
CXn,Z(FXn(x), s)
˙
CX1,Z(FX1(x), x)
˙
CX2,Z(FX2(x), s)
· · ·
˙
CXn,Z(FXn(x), s)
.
.
.
.
.
.
.
.
.
˙
CX1,Z(FX1(x), s)
˙
CX2,Z(FX2(x), s)
· · ·
˙
CXn,Z(FXn(x), s) 1 −˙
CX1,Z(FX1(x), s)
1 −˙
CX2,Z(FX2(x), s)
· · ·
1 −˙
CXn,Z(FXn(x), s)
1 −˙
CX1,Z(FX1(x), s)
1 −˙
CX2,Z(FX2(x), s)
· · ·
1 −˙
CXn,Z(FXn(x), s)
.
.
.
.
.
.
.
.
.
1 −˙
CX1,Z(FX1(x), s)
1 −˙
CX2,Z(FX2(x), s)
· · ·
1 −˙
CXn,Z(FXn(x), s)  














 


 

 i
times 


 

 n −i
times . Similar permanent expression for joint distribution of order statistics can be
obtained from (24) Fr,s(x, y) = n
X i=r n−i
X j=max(s−i,0) 1 i!(j −i)!(n −i −j)! 1
Z 0
M2(x, y, s)ds, where
M2(x, y, s) = =  





 ˙
CX1,Z(FX1(x), s)
· · ·
· · ·
˙
CXn,Z(FXn(x), s) ˙
CX1,Z(FX1(y), s)
−˙
CX1,Z(FX1(x), s)
· · ·
· · ·
˙
CXn,Z(FXn(y), s)
−˙
CXn,Z(FXn(x), s) 1 −˙
CX1,Z(FX1(x), s)
· · ·
· · ·
1 −˙
CXn,Z(FXn(x), s)  





 }i }j −i }n −i −j 12 In general the joint distribution function of order statistics Xr1:n, Xr2:n, ..., Xrk:n,
1 ≤r1 < r2 < · · · < rk ≤n is Fr1,r2...,rk(x1, x2, ..., xk) =
X
1 j1!j2! · · · jkjk+1! 1
Z 0
M3(x1, x2, ..., xk, s)ds, −∞
<
x1 < x2 < · · · < xk < ∞, where the sum is over j1, j2, ..., jk+1 with j1 ≥r1, j1 + j2 ≥r2, ..., j1 + j2 + · · · +
jk ≥rk and j1 + j2 + · · · + jk + jk+1 = n and M3(x1, x2, ..., xk, s) =  













 ˙
CX1,Z(FX1(x1), s)
· · ·
· · ·
˙
CXn,Z(FXn(x1), s) ˙
CX1,Z(FX1(x2), s)
−˙
CX1,Z(FX1(x1), s)
· · ·
· · ·
˙
CXn,Z(FXn(x2), s)
−˙
CXn,Z(FXn(x1), s)
.
.
.
.
.
.
˙
CX1,Z(FX1(xk), s)
−˙
CX1,Z(FX1(xk−1), s)
· · ·
· · ·
˙
CXn,Z(FXn(xk), s)
−˙
CXn,Z(FXn(xk−1), s) 1 −˙
CX1,Z(FX1(xk), s)
· · ·
· · ·
1 −˙
CXn,Z(FXn(xk), s)  













 }j1 }j2 }jk }jk+1 . Remark 6 If X1, X2, ..., Xn are identically distributed with cdf FX(x), and the
joint distributions of (X1, Z), (X2, Z), ..., (Xn, Z) are the same, i.e. CXi,Z(u, w) =
CX,Z(u, w), ∀(u, w) ∈[0, 1]2, i = 1, 2, ..., n, then Fr1,r2,...,rk(x1, x2, ..., xn) =
X
1 j1!j2! · · · jkjk+1! 1
Z 0 h
˙
CX,Z(FX(x1), s)
ij1 h
˙
CX,Z(FX(x2), s) −˙
CX,Z(FX(x1), s)
ij2 ×
h
˙
CX,Z(FX(xk), s) −˙
CX,Z(FX(xk−1), s)
ijk h
1 −˙
CX,Z(FX(xk), s)
ijk+1
ds, −∞
<
x1 < x2 < · · · < xk < ∞, where the sum is over j1, j2, ..., jk+1 with j1 ≥r1, j1 + j2 ≥r2, ..., j1 + j2 + · · · +
jk ≥rk and j1 + j2 + · · · + jk + jk+1 = n. The order statistics are widely used in statistical theory of reliability. The
coherent system consisting of n−components with lifetimes X1, X2, ..., Xn has
lifetime which can be expressed with the order statistics (or the linear functions
of order statistics using Samaniego’s signatures).
The n −k + 1-out-of-n co-
herent system, for example, has lifetime T = Xk:n and the mean residual life
function of such a system at the system level is Ψ(t) = E{Xk:n −t | Xr:n > t},
r < k. The function Ψ(t) expresses the mean residual life (MRL) length of a
n −k + 1-out-of-n system given that at least n −r + 1 components are alive 13 at the moment t. It is clear that to compute the reliability or the MRL func-
tions of such systems we need the joint distributions of order statistics. The
results presented in this paper can be used if the system has components with
conditionally independent lifetimes.",0
"This is an informal and sketchy review of six topical, somewhat unrelated subjects in quantitative
ﬁnance: rough volatility models; random covariance matrix theory; copulas; crowded trades; high-
frequency trading & market stability; and “radical complexity” & scenario based (macro)economics. Some
open questions and research directions are brieﬂy discussed. 1
From Random Walks to Rough Volatility Since we will never really know why the prices of ﬁnancial assets move, we should at least make a model
of how they move. This was the motivation of Bachelier in 1900, when he wrote in the introduction of his
thesis that contradictory opinions in regard to [price] ﬂuctuations are so diverse that at the same instant buyers
believe the market is rising and sellers that it is falling. He went on to propose the ﬁrst mathematical model
of prices: the Brownian motion. He then built an option pricing theory that he compared to empirical data
available to him — which already revealed, quite remarkably, what is now called the volatility smile!
After 120 years of improvements and reﬁnements, we are closing in on a remarkably realistic model,
which reproduces almost all known stylized facts of ﬁnancial price series. But are we there yet? As Benoît
Mandelbrot once remarked: In economics, there can never be a “theory of everything”. But I believe each
attempt comes closer to a proper understanding of how markets behave. In order to close the gap, and justify
the modern mathematical apparatus that has slowly matured, we will need to understand the interactions
between the behaviour of zillions of traders — each with his or her own investment style, trading frequency,
risk limits, etc. and the price process itself. Interestingly, recent research strongly suggests that markets
self organise in subtle way, as to be poised at the border between stability and instability. This could be the
missing link — or the holy grail – that researchers have been looking for.
For many years, the only modiﬁcation to Bachelier’s proposal was to consider that log-prices, not prices
themselves, are described by a Brownian motion. Apart from the fact that this modiﬁcation prevents prices
from becoming negative, none of the ﬂaws of the Bachelier model were seriously tackled. Notwithstanding,
the heyday of Brownian ﬁnance came when Black & Scholes published their famous 1973 paper, with the
striking result that perfect delta-hedging is possible. But this is because, in the Black-Scholes world, price
jumps are absent and crashes impossible. This is of course a very problematic assumption, specially because
the fat-tailed distribution of returns had been highlighted as soon as 1963 by Mandelbrot — who noted, in
the same paper, that large changes tend to be followed by large changes, of either sign, and small changes tend
to be followed by small changes, an effect now commonly referred to as “volatility clustering”, and captured
by the extended family of GARCH models.
It took the violent crash of October 1987, exacerbated by the massive impact of Black-Scholes’ delta-
hedging, for new models to emerge. The Heston model, published in 1993, is among the most famous
post-Black-Scholes models, encapsulating volatility clustering within a continuous time, Brownian motion
formalism. However, like GARCH, the Heston model predicts that volatility ﬂuctuations decay over a single
time scale — in other words that periods of high or low volatility have a rather well deﬁned duration. This
is not compatible with market data: volatility ﬂuctuations have no clear characteristic time scale; volatility
bursts can last anything between a few hours and a few years. 1 Mandelbrot had been mulling about this for a long while, and actually proposed in 1974 a model to
describe a very similar phenomenon in turbulent ﬂows, called “multifractality”. He adapted his theory in
1997 to describe currency exchange rates, before Emmanuel Bacry, Jean-Francois Muzy & Jean Delour for-
mulated in 2000 a more convincing version of the model, which they called the Multifractal Random Walk
(MRW) [1]. With a single extra parameter (interpreted as a kind of volatility of volatility), the MRW cap-
tures satisfactorily many important empirical observations: fat-tailed distribution of returns, long-memory
of volatility ﬂuctuations. In 2014, Jim Gatheral, Thibault Jaisson and Mathieu Rosenbaum introduced their
now famous “Rough Volatility” model [2], which can be seen as an extension of the MRW with an extra
parameter allowing one to tune at will the roughness of volatility, while it is ﬁxed in stone in the MRW
model. And indeed, empirical data suggests that volatility is slightly less rough than what the MRW posits.
Technically, the Holder regularity of the volatility is H = 0 in the MRW and found to be H ≈0.1 when
calibrated within the Rough Volatility speciﬁcation.
The next episode of the long saga came in 2009 when Gilles Zumbach noticed a subtle, yet crucial aspect
of empirical ﬁnancial time series: they are not statistically invariant upon time reversal [3]. Past and future
are not equivalent, whereas almost all models to that date, including the MRW
, did not distinguish past
from future. More precisely, past price trends, whether up or down, lead to higher future volatility, but
not the other way round. In 2019, following some work by Pierre Blanc, Jonathan Donier and myself [4],
Aditi Dandapani, Paul Jusselin, Mathieu Rosenbaum proposed to describe ﬁnancial time series with what
they called a “Quadratic Rough Heston Model” [5], which is a synthesis of all the ideas reviewed above.
It is probably the most realistic model of ﬁnancial price series to date. In particular, it provides a natural
solution to a long standing puzzle, namely the joint calibration of the volatility smile of the S&P 500 and
VIX options, which had eluded quants for many years [6]. The missing ingredient was indeed the Zumbach
effect [7].
Is this the end of the saga? From a purely engineering point of view, the latest version of the Rough
Volatility model is probably hard to beat. But the remaining challenge is to justify how this particular
model emerges from the underlying ﬂow of buy and sell trades that interacts with market makers and
high frequency traders. Parts of the story are already clear; in particular, as argued by Jaisson, Jusselin
& Rosenbaum in a remarkable series of papers, the Rough Volatility model is intimately related to the
proximity of an instability [8] (see also [9]), that justiﬁes the rough, multi-timescale nature of volatility.
But what is the self-organising mechanism through which all markets appear to settle close to such a critical
point? Could this scenario allow one to understand why ﬁnancial time series all look so much alike —
stocks, futures, commodities, exchange rates, etc. share very similar statistical features, in particular in the
tails. Beyond being the denouement of a 120 years odyssey, we would be allowed to believe that the ﬁnal
model is not only a ﬁgment of our mathematical imagination, but a robust, trustworthy framework for risk
management and derivative pricing. 2
Random Matrix Theory to the Rescue Harry Markowitz famously quipped that diversiﬁcation is the only free lunch in ﬁnance. This is nevertheless
only true if correlations are known and stable over time. Markowitz’ optimal portfolio offers the best risk-
reward tradeoff, for a given set of predictors, but requires the covariance matrix – of a potentially large pool
of assets – to be known and representative of the future realized correlations. The empirical determination
of large covariance matrices is however fraught with difﬁculties and biases. Interestingly, the vibrant ﬁeld
of “Random Matrix Theory” has provided original solutions to this big data problem, and suggests droves
of possible applications in econometrics, machine learning or other large dimensional models.
But even for the simplest two-asset bond/equity allocation problem, the knowledge of the forward look-
ing correlation has momentous consequences for most asset allocators in the planet. Will this correlation
remain negative in the years to come, as it has been since late 1997, or will it jump back to positive ter-
ritories? But compared to volatility, our understanding of correlation dynamics is remarkably poor and,
surprisingly, the hedging instruments allowing one to mitigate the risk of bond/equity correlation swings
are nowhere as liquid as the VIX itself. 2 So there are two distinct problems in estimating correlation matrices. One is lack of data, the other one
is time non-stationarity. Consider a pool of N assets, with N large. We have at our disposal T observations
(say daily returns) for each of the N time series. The paradoxical situation is this: even though each
individual off-diagonal covariance is accurately determined when T is large, the covariance matrix as a
whole is strongly biased unless T is much larger than N itself. For large portfolios, with N a few thousands,
the number of days in the sample should be in the tens of thousands – say 50 years of data. This is simply
absurd: Amazon and Tesla did not even exist 25 years ago. Maybe use 5 minutes returns then, increasing
the number of data points by a factor 100? Yes, except that 5 minute correlations are not necessarily
representative of the risk of much lower frequency strategies, with other possible biases creeping in the
resulting portfolios.
So in what sense are covariance matrices biased when T is not very large compared to N? The best
way to describe such biases is in terms of eigenvalues. One ﬁnds that the smallest eigenvalues are way too
small and the largest eigenvalues are too large. This results, in the Markowitz optimization program, in
a substantial over-allocation on combination of assets that happened to have a small volatility in the past,
with no guarantee that this will persist looking forward. The Markowitz construction can therefore lead to
a considerable under-estimation of the realized risk in the next period.
Out-of-sample results are of course always worse than expected, but Random Matrix Theory (RMT)
offers a guide to (partially) correct these biases when N is large. In fact, RMT gives an optimal, mathe-
matically rigorous, recipe to tweak the value of the eigenvalues so that the resulting “cleaned” covariance
matrix is as close as possible to the “true” (but unknown) one, in the absence of any prior information [10].
Such a result, ﬁrst derived by Ledoit and Péché in 2011 [11], is already a classic and has been extended in
many directions. The underlying mathematics, initially based on abstract “free probabilities”, are now in
a ready-to-use format, much like Fourier transforms or Ito calculus (see [12] for an introductory account).
One of the exciting, and relatively unexplored direction, is to add some ﬁnancially motivated prior, like
industrial sectors or groups, to improve upon the default “agnostic” recipe.
Now the data problem is solved as best as possible, the stationarity problem pops up. Correlations, like
volatility, are not ﬁxed in stone but evolve with time. Even the sign of correlations can suddenly ﬂip, as was
the case for the S&P500/Treasuries during the 1997 Asian crisis. After 30 years of correlations staunchly
in positive territory (1965 – 1997), bonds and equities have been in a “ﬂight-to-quality” mode (i.e. equities
down and bonds up) ever since. More subtle, but signiﬁcant, changes of correlations can also be observed
between single stocks and/or between sectors in the stock market. For example, a downward move of the
S&P500 leads to an increased average correlation between stocks. Here again, RMT provides powerful
tools to describe the time evolution of the full covariance matrix [13, 14].
As I discussed in the previous section, stochastic volatility models have made signiﬁcant progress re-
cently, and now encode feedback loops that originate at the microstructural level. Unfortunately, we are
very far from having a similar theoretical handle to understand correlation ﬂuctuations, although Matthieu
Wyart and I had proposed in 2007 a self-reﬂexive mechanism to account for correlation jumps like the
one that took place in 1997 [15]. Parallel to the development of descriptive and predictive models, the
introduction of standardized instruments that hedge against such correlation jumps would clearly serve a
purpose. This is especially true in the current environment [16] where inﬂation fears could trigger another
inversion of the equity/bond correlation structure, possibly devastating for many strategies that – implicitly
or explicitly – rely on persistent negative correlations. Markowitz diversiﬁcation free lunch can sometimes
be poisonous! 3
My Kingdom for a Copula As I just discussed, assessing linear correlations between ﬁnancial assets is already hard enough. What
about non-linear correlations then? If ﬁnancial markets were kind enough to abide to Gaussian statistics,
non-linear correlations would be entirely subsumed by linear ones. But this is not the case: genuine non-
linear correlations pervade the ﬁnancial world and are quite relevant, both for the buy side and the sell side.
For example, tail correlations in equity markets (i.e. stocks plummeting simultaneously) are notoriously 3 higher than bulk correlations. Another apposite context is the Gamma-risk of large option portfolios, the
management of which requires an adequate description of quadratic return correlations of the underlying
assets.
In order to deal with non-linear correlations, mathematics has afforded us with a seemingly powerful
tool – “copulas”. Copulas are supposed to encapsulate all possible forms of multivariate dependence. But
in the zoo of all conceivable copulas, which one should one choose to faithfully represent ﬁnancial data?
Following an unfortunate, but typical pattern of mathematical ﬁnance, the introduction of copulas
twenty years ago has been followed by a calibration spree, with academics and ﬁnancial engineers alike
frantically looking for copulas to best represent their pet multivariate problem. But instead of ﬁrst de-
veloping an intuitive understanding of the economic or ﬁnancial mechanisms that suggest some particu-
lar dependence between assets and construct adequate copulas accordingly, the methodology has been to
brute-force calibrate copulas straight out from statistics handbooks. The “best” copula is then decided from
some quality-of-ﬁt criterion, irrespective of whether the copula makes any intuitive sense at all.
This is reminiscent of local volatility models for option markets: although the model makes no intuitive
sense and cannot describe the actual dynamics of the underlying asset, it is versatile enough to allow the
calibration of almost any option smile. Unfortunately, a blind calibration of some unwarranted model
(even when the ﬁt is perfect) is a recipe for disaster. If the underlying reality is not captured by the model,
it will most likely derail in rough times — a particularly bad feature for risk management (recall the use
of Gaussian copulas to price CDOs before the 2008 crisis). Another way to express this point is to use a
Bayesian language: there are families of models for which the ‘prior’ likelihood is clearly extremely small,
because no plausible scenarios for such models to emerge from market mechanisms. Statistical tests are
not enough — the art of modelling is precisely to recognize that not all models are equally likely.
The best way to foster intuition is to look at data before cobbling up a model, and come up with a
few robust “stylized facts” that you deem relevant and that your model should capture. In the case of
copulas, one interesting stylised fact is the way the probability that two assets have returns simultaneously
smaller than their respective medians depends on the linear correlation between the said two assets. Such
a dependence exists clearly and persistently in stocks and, strikingly, it cannot be reproduced by most “out-
of-a-book” copula families.
In particular, the popular class of so-called “elliptical” copulas is ruled out by such an observation. El-
liptical copulas assume, in a nutshell, that there is a common volatility factor for all stocks: when the index
becomes more or less volatile, all stocks follow suit. A moment of reﬂection reveals that this assumption
is absurd, since one expects that volatility patterns are at least industry speciﬁc. But this consideration
also suggests a way to build copulas specially adapted to ﬁnancial markets. In Ref. [17], Rémy Chichepor-
tiche and I showed how to weld the standard factor model for returns with a factor model for volatilities.
Perhaps surprisingly, the common volatility factor is not the market volatility, although it contributes to it.
With a relatively parcimonious parameterisation, most multivariate “stylized facts” of stock returns can be
reproduced, including the non-trivial joint-probability pattern alluded to above.
I have often ranted against the over-mathematisation of quant models, favouring theorems over intu-
ition and convenient models over empirical data. Reliance on rigorous but misguided statistical tests is also
plaguing the ﬁeld. As an illustration related to the topic of copulas, let me consider the following question:
is the univariate distribution of standardized stock returns universal, i.e. independent of the considered
stock? In particular, is the famous “inverse-cubic law” [18] for the tail of the distribution indeed common
to all stocks?
A standard procedure for rejecting such an hypothesis is the Kolmogorov-Smirnov (or Anderson-Darling)
statistics. And lo and behold, the hypothesis is strongly rejected. But, wait – the test is only valid if returns
can be considered as independent, identically distributed random variables. Whereas returns are close to
being uncorrelated, non-linear dependencies along the time axis are strong and long-ranged. Adapting the
Kolmogorov-Smirnov test in the presence of long-ranged “self-copulas” is possible [19] and now leads to
the conclusion that the universality hypothesis cannot be rejected. Here again, thinking about the problem
before blindly applying standard recipes is of paramount importance to get it right.
The ﬁner we want to hone in on the subtleties of ﬁnancial markets, the more we need to rely on making
sense of empirical data, and to remember what the great Richard Feynman used to say: It doesn’t matter 4 how beautiful your theory is, it doesn’t matter how smart you are. If it doesn’t agree with experiment, it’s
wrong. 4
Crowded Trades: Whales or Minnows? It is funny how, sometimes, seemingly obvious concepts do become paradoxical when one starts thinking
harder about what they really mean. One topical example is the idea of “crowded trades” that has recently
become a major talking point in the face of the disheartening performance of many Alternative Beta/Risk
Premia funds. It seems self-evident to many that when investors pile into a given trade, future returns are
necessarily degraded. But on the other hand, for each buyer there must be a seller – so isn’t the opposite
trade crowded too? In what sense, then, is a crowded trade toxic? Can one come up with useful measures
of crowding, that would allow one to construct portfolios as immune as possible to crowding risk?
In the mind of investors, the word “crowding” summons two distinct fears. One is that any mispricing
that motivates a strategy is arbitraged away by the crowd, rendering that strategy unproﬁtable in the future.
The second is crash risk: harmful deleveraging spirals may occur as the crowd suddenly decides to run for
the exit. Here we see how the symmetry between the two sides of the trade can be broken: a trade is
crowded when investors on – say – the buy side are more prone to act in sync than those of the sell side.
In fact, the most crowded trade of all is, and always has been, long the stock market. Crashes indeed
happened many times in the past and will happen again in the future. Be that as it may, is investing in the
stock market a bad decision? Certainly not, in fact all Risk Premia are proﬁtable on the long run precisely
because of such negatively skewed events. The equity risk premium is abnormally high – but in fact it
compensates for the deleveraging risk associated with the madness of crowds. In fact, we have argued in
Ref. [20] that the return of Risk Premia strategies are strongly correlated with their negative skewness, i.e.
their propensity to crash. So in many cases, crowding is simply unavoidable – the only question is whether
the associated downside risk is adequately compensated or not.
This brings us back to our ﬁrst point: that of withering returns. The mechanism usually put forth is
that the spread between the fundamental price and the market price is closed by those who trade based
on that mispricing. If the cake is shared between a larger number of participants, each of them must get
a smaller piece. Although this makes intuitive sense, the story cannot be that simple. When one looks at
different measures of mispricing on which classical factors (momentum, proﬁtability, low volatility, etc.)
are supposed to thrive, there is no sign of a recent narrowing of the valuation spread between the short
and long legs of the corresponding portfolios. The situation is even the exact opposite for price-to-book
spreads, which have become wider since 2016 – tantamount to saying that Value strategies are currently in
the doldrums.
But to argue that the plight of Value is due to crowding is at best misleading. Value as a strategy has
been used extensively by market participants for decades. What we are seeing is more like a Value crash in
slow motion, with investors getting out more aggressively of this strategy in the years 2019-2020, after a
period of disappointing (but certainly not unprecedented!) performance. A rough estimation shows that if
250 B$ are progressively redeemed from value strategies over a year, typical market neutral value portfolios
should suffer a ≈20% loss from price impact alone – all very much in line with recent industry ﬁgures.
While there is no smoking gun that this is what happened recently, the argument that crowding is
detrimental to convergent strategies (i.e. trades that reduce valuation spreads) is not without merit. But
then how do we understand the effect of crowding on divergent strategies, such as momentum or trend
following? Here, price impact arguments would suggest that more trend followers should bolster trends,
not make them weaker. Crowding could even be beneﬁcial for such strategies, at least up to a point.
The problem with this optimistic surmise is that it neglects yet another facet of price impact, namely
transaction costs. The point is that, according to our deﬁnition, crowded strategies are precisely those
leading to correlated trades, i.e. many managers entering or leaving the market simultaneously. As an
extreme outcome, this can lead to crashes, as discussed above. But even in perfectly normal regimes,
trading in the same direction as others can signiﬁcantly increase impact costs, a.k.a. “slippage”. It is not my
own traded quantity that matters, it is the aggregate quantity traded by all managers following the same 5 trading idea. Although the strength of the trading signal is not necessarily impaired, crowded trades may
suffer from so much “co-impact” [21] that the proﬁtability of the strategy quickly shrivels to zero, or even
below zero.
This suggests an interesting metric to detect crowded strategies and estimate such co-impact costs. The
ﬁrst step is to reconstruct the putative trades that a manager following a given strategy – say Fama-French
Momentum – would send to the market on a given day. One then uses order-book tick data to determine
the actual buy/sell order imbalances for each trading day. This allows one to compute the correlation of
the overall market imbalance with the imbalance expected from the strategy under scrutiny. A statistically
signiﬁcant correlation means that the strategy does leave a detectable trace in the markets. One can also
measure the correlation between these reconstructed trades and the price return of each stock; this provides
a direct estimate of the co-impact costs.
This is precisely what we did in Ref. [22]. The conclusion is that the classical Fama-French momentum
has indeed become more and more crowded in the last ten years, and, as of today, the estimated co-impact
costs make the strategy all but unproﬁtable. The good news, on the other hand, is that there are many
different ways to implement a given trading idea – some more, some less correlated with the “crowd”. This
paves the way to portfolio constructions that attempt to minimize the correlation of trades with identiﬁed
trading strategies, with the hope of eschewing the curse of crowded trades – deleveraging spirals and all. 5
High-Frequency Trading & Market Stability In the midst of the ﬁrst COVID lockdown, the 10th anniversary of the infamous May 6th, 2010 “Flash
Crash” went unnoticed. At the time, ﬁngers were pointed at High Frequency Trading (HFT), accused of
both rigging the markets and destabilizing them. Research has since then conﬁrmed that HFT in fact
results in signiﬁcantly lower bid-ask spread costs and, after correcting for technological glitches and bugs,
does not increase the frequency of large price jumps. In fact, recent models explain why market liquidity is
intrinsically unstable: managing the risk associated to market-making, whether by humans or by computers,
unavoidably creates destabilising feedback loops. In order to make markets more resilient, research should
focus on better market design and/or smart regulation that nip nascent instabilities in the bud.
Since orders to buy or to sell arrive at random times, ﬁnancial markets are necessarily most of the
time unbalanced. In such conditions, market-makers play a crucial role in allowing smooth trading and
continuous prices. They act as liquidity buffers, that absorb any temporary surplus of buy orders or sell
orders. Their reward for providing such a service is the bid-ask spread – systematically buying a wee lower
and selling a wee higher, and pocketing the difference.
What is the fair value of the bid-ask spread? Well, it must at least compensate the cost of providing
liquidity, which is adverse selection. Indeed, market-makers must post prices that can be picked up if deemed
advantageous by traders with superior information. The classic Glosten-Milgrom model provides an elegant
conceptual framework to rationalize the trade-off between adverse selection and bid-ask spread, but fails
to give a quantitative, operational answer (see e.g. [23] for a recent discussion). In a 2008 study [24]
we came up with a remarkably simple answer: the fair value of the bid-ask spread is equal to the ratio
of the volatility to the square-root of the trade frequency. This simple rule of thumb has many interesting
consequences.
First, it tells us that for a ﬁxed level of volatility increasing the trade frequency allows market-makers to
reduce the spread, and hence the trading costs for ﬁnal investors. The logic is that trading smaller chunks
more often reduces the risk of adverse selection. This explains in part the rise of HFT as modern market
making, and the corresponding reduction of the spreads. Throughout the period 1900-1980, the spread on
US stocks hovered around a whopping 60 basis points, whereas it is now only a few basis points. In the
meantime, volatility has always wandered around 40% per year – with of course troughs and occasional
spikes, as we discuss below. In other words, investors were paying a much higher price for liquidity before
HFT, in spite of wild claims that nowadays electronic markets are “rigged”. In fact, after a few prosperous
years before 2010, high frequency market-making has become extremely competitive and average spreads
are now compressed to minimum values. 6 From this point of view, the economic rents available to liquidity providers have greatly decreased since
the advent of HFT. But has this made markets more stable, or has the decrease in the proﬁtability of market-
making also made them more fragile? The second consequence of our simple relation between spread and
volatility relates to this important question. The point is that this relation can be understood in a two-way
fashion: clearly, when volatility increases, the impact of adverse selection can be dire for market makers
who mechanically increase their spreads. Periods of high volatility can however be quite proﬁtable for HFT
since competition for liquidity providing is then less ﬁerce.
But in fact higher spreads by themselves lead to higher volatility, since transactions generate a larger
price jump – or even a crash when liquidity is low and the order book is sparse. So we diagnose a funda-
mental destabilising feedback loop, intrinsic to any market-making activity: volatility
−→
higher spreads & lower liquidity
−→
more volatility. Such a feedback loop can actually be included in stochastic order book models (such as the now commonly
used family of “Hawkes processes” [25]). As the strength of the feedback increases, one ﬁnds a phase
transition between a stable market and a market prone to spontaneous liquidity crises, even in the absence
of exogenous shocks or news [26].
This theoretical result suggests that when market-makers (humans or machines) react too strongly to
unexpected events, liquidity can enter a death spiral. But who will blame them? As an old saying goes,
liquidity is a coward, it is never there when it is needed.
Such a paradigm allows one to understand why a large fraction of price jumps in fact occur without
any signiﬁcant news – rather, they result from endogenous, unstable feedback loops [27]. Empirically, the
frequency of 10-sigma daily moves of US stock prices has been fairly constant in the last 30 years, with no
signiﬁcant change between the pre-HFT epoch and more recent years [23]. Even the 6th May 2010 Flash
Crash has a pre-HFT counterpart: on the May 28th 1962, the stock market plunged 9% within a matter of
minutes, for no particular cause, before recovering – much the same weird price trajectory as in 2010. Our
conjecture: markets are intrinsically unstable, and have always been so. As noted in section 1 above, this
chronic instability may lie at the heart of the turbulent, multiscale nature of ﬁnancial ﬂuctuations.
Can one engineer a smart solution that make markets less prone to such dislocations? From our argu-
ments above, we know that the task would be to crush the volatility/liquidity feedback loop, by promoting
liquidity provision when it is on the verge of disappearing. One idea would be to introduce dynamical
make/take fees, which would make cancellations more costly and limit order posting more proﬁtable de-
pending on the current state of the order book. These fees would then funnel into HFT’s optimisation
algorithms, and (hopefully) yank the system away from the regime of recurrent endogenous liquidity cri-
sis. 6
Radical Complexity & Scenario Based Macro-economics Good science is often associated with accurate, testable predictions. Classical economics has tried to con-
form to this standard, and developed an arsenal of methods to come up with precise numbers for next year’s
GDP
, inﬂation and exchange rates, among (many) other things. Few, however, will disagree with the fact
that the economy is a complex system, with a large number of heterogeneous interacting units, of different
categories (ﬁrms, banks, households, public institutions) and very different sizes. In such complex systems,
even qualitative predictions are hard. So maybe we should abandon our pretense of exactitude and turn to
another way to do science, based on scenario identiﬁcation. Aided by qualitative (agent based) simulations,
swans that appear black to the myopic eye may in fact be perfectly white.
The main issue in economics is precisely about the emergent organization, cooperation and coordination
of a motley crowd of micro-units. Treating them as a unique representative ﬁrm or household clearly
throws the baby with the bathwater. Understanding and characterizing such emergent properties is however
difﬁcult: genuine surprises can appear from micro- to macro-. One well-known example is the Schelling
segregation model: even when all agents prefer to live is mixed neighborhoods, myopic dynamics can lead
to completely segregated ghettos [28]. In this case, Adam Smith’s invisible hand badly fails. 7 More generally, slightly different micro-rules/micro parameters can lead to very different macro-states:
this is the idea of “phase transitions”; sudden discontinuities (aka crises) can appear when a parameter
is only slightly changed. Because of feedback loops of different signs, heterogeneities and non-linearities,
these surprises are hard, if not impossible to imagine or anticipate, even aided with the best mathematical
apparatus.
This is what I would like to call “Radical Complexity”. Simple models can lead to unknowable behaviour,
where “Black Swans” or “Unknown Unknowns” can be present, even if all the rules of the model are known
in detail. In these models, even probabilities are hard to pin down, and rationality is de facto limited. For
example, the probability of rare events can be exponentially sensitive to the model parameters, and hence
unknowable in practice [29]. In these circumstances, precise quantitative predictions are unreasonable.
But this does not imply the demise of the scientiﬁc method. For such situations, one should opt for a more
qualitative, scenario based approach, with emphasis on mechanisms, feedback loops, etc. rather than on
precise, but misleading numbers. This is actually the path taken by modern climate change science.
Establishing the list of possible (or plausible) scenarios is itself difﬁcult. We need numerical simulations
of Agent Based Models. While it is still cumbersome to experiment on large scale human systems (although
more and more possible using web-based protocols), experimenting with Agent Based Models is easy and
fun and indeed full of unexpected phenomena. These experiments in silico allow one to elicit scenarios
that would be nearly impossible to imagine, because of said feedback loops and non-linearities. Think for
example of the spontaneous synchronization of ﬁreﬂies (or of neuron activity in our brains). It took nearly
70 years to come up with an explanation. Complex endogenous dynamics is pervasive, but hard to guess
without appropriate tools.
Experimenting with Agent Based Models is interesting on many counts. One hugely important aspect
is, in my opinion, that it allows to teach students in a playful, engaging way how complex social and
economic systems work. Such simulations would foster their intuition and their imagination, much like lab
experiments train the intuition of physicists about the real world, beyond abstract mathematical formalism.
Creating one’s own world and seeing how it unfolds clearly has tremendous pedagogical merits. It is
also an intellectual exercise of genuine value: if we are not able to make sense of an emergent phenomenon
within a world in which we set all the rules, how can we expect to be successful in the real world? We have
to train our minds to grasp these collective phenomena and to understand how and why some scenarios
can materialize and others not. The versatility of ABM allows one to include ingredients that are almost
impossible to accommodate in classical economic models, and explore their impact on the dynamics of the
systems [30, 31].
ABM are often spurned because they are in general hard to calibrate, and therefore the numbers they spit
out cannot and should not be taken at face value. They should rather be regarded as all-purpose scenario
generators, allowing one to shape one’s intuition about phenomena, to uncover different possibilities and
reduce the realm of Black Swans. The latter are often the result of our lack of imagination or of the simplicity
of our models, rather than being inherently impossible to foresee.
Expanding the study of toy-models of economic complexity will create a useful corpus of scenario-
based, qualitative macroeconomics [32, 33]. Instead of aiming for precise numerical predictions based
on unrealistic assumptions, one should make sure that models rely on plausible causal mechanisms and
encompass all plausible scenarios, even when these scenarios cannot be fully characterized mathematically.
A qualitative approach to complexity economics should be high on the research agenda. As Keynes said, it
is better to be roughly right than exactly wrong. Acknowledgments I want to warmly thank all my collaborators on these topics, especially: R. Allez, R. Benichou, M. Benza-
quen, P
. Blanc, J. Bonart, F
. Bucci, J. Bun, R. Chicheportiche, J. Donier, Z. Eisler, A. Fosset, M. Gould, S.
Gualdi, S. Hardiman, A. Karami, Y. Lempérière, F
. Lillo, R. Marcaccioli, I. Mastromatteo, F
. Morelli, M. Pot-
ters, P
. A. Reigneron, P
. Seager, D. Sharma, M. Tarzia, B. Toth, V
. Volpati, M. Wyart, F
. Zamponi. I also want
to pay tribute to various people with whom I had exciting and enlightening discussions on these matters, 8 in particular R. Bookstaber, D. Farmer, X. Gabaix, J. Gatheral, J. Guyon, A. Kirman, C. Lehalle, J. Moran,
M. Rosenbaum, N. Taleb. Finally, I am deeply indebted to Mauro Cesa who offered me the possibility of
putting my thoughts together and publishing them as six monthly columns in Risk.net, from September
2020 to February 2021.",0
"In this paper, we provide conditions which ensure that stochastic Lipschitz BSDEs admit
Malliavin diﬀerentiable solutions. We investigate the problem of existence of densities for
the ﬁrst components of solutions to general path-dependent stochastic Lipschitz BSDEs
and obtain results for the second components in particular cases. We apply these results
to both the study of a gene expression model in biology and to the classical pricing
problems in mathematical ﬁnance. Key words: BSDEs, Malliavin calculus, Nourdin-Viens’ Formula, gene expression, option
pricing. AMS 2010 subject classiﬁcation: Primary: 60H10; Secondary: 60H07, 91G30, 92D20. 1
Introduction The problem of existence of densities for random processes, as e.g. solutions of stochastic
diﬀerential equations (SDEs), has been a very active strand of research in the last two
decades, see among others [26, 35]. A very useful criterion to prove that the law of a
random variable admits a density is the criterion of Bouleau and Hirsch, see e.g. [35,
Theorem 2.1.2]. The analysis of densities has been the subject of several works dealing
with Stochastic Partial Diﬀerential Equations (SPDEs), among which we can mention
the study of the stochastic heat equation, the stochastic wave equation (see for instance
[33], [36], [31]), the Navier-Stokes equation [11] and recently the Landau equation for
Maxwellian molecules (see [12]). Besides, most of these papers investigate tails estimates
of the solutions to SPDEs by using the formula of Nourdin and Viens, introduced in [34],
to have a better understanding of these processes. Although the problem of existence of densities for S(P)DEs, together with estimates on
their tails, has been a prosperous ﬁeld, the corresponding theory for Backward Stochastic
Diﬀerential Equations (BSDEs) has not received the same attention in the literature.
BSDEs were introduced for the ﬁrst time in 1973 by Bismut in [5], in order to study
stochastic control problems and their links to the Pontryagin maximum principle. The
theory of BSDEs was then formalised and developed in the 90’s, with the seminal papers
[38, 39] and [17]. In the last decades, BSDEs have been the object of an ever growing ∗Université Paris-Dauphine, CEREMADE UMR CNRS 7534, Place du Maréchal De Lattre De Tassigny,
75775 Paris Cedex 16, FRANCE, mastrolia@ceremade.dauphine.fr 1 arXiv:1602.06101v1  [math.PR]  19 Feb 2016 interest, since these equations naturally appear in ﬁnancial problems, as for instance
pricing problems (see [17]) and utility maximisation problems (see [45], [22]).
As far as we know, the existence of densities for solutions to BSDEs was studied in three
papers. Conditions ensuring that the ﬁrst component Y of the solution to a Lipschitz
BSDE admits a density were provided for the ﬁrst time in [3]. In this paper, the authors
also investigated both estimates on the existing density and its smoothness. Then, a
result ensuring existence of a density for the second component Z of the solution to a
particular BSDE, in which the generator is linear with respect to its z variable, was
obtained in [1]. Recently, this problem was studied in [29] for both the Y and the Z
components of solutions to BSDEs with a quadratic growth generator. However, [3, 29]
only consider Markovian BSDEs, that is the case where the data ξ and ω 7−
→f(s, ω, y, z)
of such equations are only random through a Markovian process, and [1] only considers
the semi-Markovian case, that is the case where only ω 7−
→f(s, ω, y, z) is Markovian.
Although the previous studies are interesting from a mathematical point of view, these
results seem to be too restrictive for applications. As an example, consider a pricing
problem which could be reduced to solve the following BSDE (see [17] for more details) dYt = (rtYt + θtZt)dt + ZtdWt, YT = ξ, where r denotes the interest rate of the market, θ is the market price of risk and ξ is the
liability. As noticed in [16], assuming that r is bounded, for instance, is not realistic.
This remark led the authors of [16] to deﬁne a new class of BSDEs satisfying a so-called
stochastic Lipschitz condition for their generator. Existence and uniqueness results have
been obtained for this class of BSDEs ﬁrst in [16], and have then been extended in
[4, 48, 8] among others. The problem of existence of densities for the laws of components of solutions to stochastic
Lipschitz BSDEs has not been studied yet, and a fortiori in the non-Markovian frame-
work, i.e. when neither the terminal condition ξ nor ω 7−
→f(s, ω, y, z) depend on the
randomness through a Markovian process. We give in the present paper conditions on
ξ and f to solve these problems. Besides, although it is well-known that under suitable
conditions on the data, a non-Markovian stochastic Lipschitz BSDEs admits a unique
solution (see [16, 48, 4, 8]), the Malliavin diﬀerentiability of the solutions to such BSDEs
has not been studied yet in the general case. In order to apply Bouleau and Hirsch’s Cri-
terion ([35, Theorem 2.1.2]) to solve the problem of existence of densities for the law of Y
and Z, we provide also in this paper conditions which ensure that the components Y and
Z solutions to non-Markovian stochastic Lipschitz BSDEs are Malliavin diﬀerentiable.
The structure of this paper is the following. After some preliminaries and notations in
Section 2, we provide in Section 3 two approaches to study the Malliavin diﬀerentiability
of solutions to stochastic Lipschitz BSDEs. Indeed, in view of the classical literature, we
distinguish two types of assumptions which provide existence and uniqueness of solutions
to stochastic Lipschitz BSDE. On the one hand, we have assumptions as in [16, 4, 48]
dealing with β-spaces (see S2p,β and H2p,β below), on the other hand, we have assump-
tions dealing (mainly) with the BMO-norm of the data, as in [8]. We then reach in this
paper two kind of conditions which ensure that the components of the solution (Y, Z)
to a stochastic Lipschitz BSDE are Malliavin diﬀerentiable. The ﬁrst one, investigated
in Section 3.1, is based on the papers [16, 4, 48]. Using a priori estimates for solutions
to stochastic Lipschitz BSDE, obtained in [48, Proposition 3.6], we have conditions on
the data of such BSDE which provide the Malliavin diﬀerentiability of Y and Z (see
Assumption (DsLp,β)). The second approach, studied in Section 3.2, is based on the
papers [8, 2]. We give assumptions, similar to those obtained in [30], see Assumptions
(sH1,∞) and (sH2,∞) below, which ensure that Y and Z are Malliavin diﬀerentiable.
We then compare these two approaches, and the corresponding conditions, in Section
3.3. By taking advantage of the results obtained in Section 3, we deal in Section 4 with the
problem of existence of densities for the laws of solutions to stochastic Lipschitz BSDE in 2 the non-Markovian case. We give in Section 4.1 conditions which ensure the existence of
densities for the law of the Y component of the solution to stochastic Lipschitz BSDEs,
by using Bouleau and Hirsch’s Criterion. We provide weaker conditions in Section 4.2
for the Y component of the solution to a non-Markovian Lipschitz BSDE. We then turn
to the Z component in Section 5. We ﬁrst provide in Section 5.1 conditions ensuring
that the law of the Zt component has a density for a particular class of BSDE, extending
the results of [1]. We then explain in Section 5.2 why we are not able to adapt the proofs
of [29] to the non-Markovian framework for the Z components of solutions to general
non-Markovian BSDEs and we indicate paths for future researches. We ﬁnally apply our
study in Sections 6 and 7 to biology and ﬁnance respectively. In Section 6, we propose to study mathematically a model of synthesis of proteins intro-
duced in [46], with the Malliavin calculus. Indeed, in order to validate their model, the
authors of [46] need to compare the law of the protein concentration at time t obtained
by solving a BSDE with the data produced by Gillespie Method (see [20]). However,
in [46], the authors assumed implicitly that the law of the ﬁrst component Yt of the
BSDE under consideration admits a density with respect to the Lebesgue measure. The
present paper can be seen as a mathematical strengthening of the model developed in
[46] by using the so-called Nourdin and Viens’ formula to obtain Gaussian estimates of
the density. Besides, we propose to extend their model to the non-Markovian setting,
which could be quite relevant when we study the synthesis of protein in some models
(see for instance [7, 27, 18]). In Section 7, we study classical pricing problems. As showed in [17], this problem can be
reduced to solve a stochastic linear BSDE. In this section we aim at applying the results
obtained in previous sections to Asian and Lookback options in the Vašìček Model to
obtain information on both the regularity of the value function and the regularity of
optimal strategies. 2
Preliminaries and notations 2.1
Notations We denote by λ the Lebesgue measure on R.
Let T > 0 be a time ﬁxed horizon.
Let Ω:= C0([0, T], R) be the canonical Wiener space of continuous function ω from
[0, T] to R such that ω(0) = 0. We denote by W := (Wt)t∈[0,T ] the canonical Wiener
process, that is, for any time t in [0, T], Wt(ω) := ωt for any element ω in Ω.
We
set Fo the natural ﬁltration of W. Under the Wiener measure P, the process W is a
standard Brownian motion and we denote by F := (Ft)t∈[0,T ] the usual right-continuous
and complete augmentation of Fo under P. For the sake of simplicity, we denote all
expectations under P by E and we set for any t ∈[0, T] Et[·] := E[·|Ft]. Besides, all
notions of measurability for elements of Ωwill be with respect to the ﬁltration F or the
σ-ﬁeld FT . We set h := L2([0, T], R), where B([0, T]) is the Borel σ-algebra on [0, T], and consider
the following inner product on h ⟨f, g⟩:=
Z T 0
f(t)g(t)dt,
∀(f, g) ∈h2, with associated norm ∥·∥h. Let now H be the Cameron-Martin space that is the space
of functions in Ωwhich are absolutely continuous with square-integrable derivative and
which start from 0 at 0: H :=

h : [0, T] −
→R, ∃˙
h ∈h, h(t) =
Z t 0
˙
h(x)dx, ∀t ∈[0, T]

. For any h in H, we will always denote by ˙
h a version of its Radon-Nykodym density
with respect to the Lebesgue measure. Notice that H is an Hilbert space equipped with 3 the inner product ⟨h1, h2⟩H := ⟨˙
h1, ˙
h2⟩h, for any (h1, h2) ∈H × H, and with associated
norm ∥h∥2
H := ⟨˙
h, ˙
h⟩h. Let p ≥1. Deﬁne Lp(K) as the set of all FT -measurable random
variables F which are valued in an Hilbert space K, and such that ∥F∥Lp(K) < +∞,
where
∥F∥Lp(K) := E [∥F∥p
K]1/p , where the norm ∥· ∥K is the one canonically induced by the inner product on K. We
deﬁne Lp([t, T]; K) := ( f : [t, T] −
→K, Borel-measurable, s.t.
Z T t
∥f(s)∥p
Kds < +∞ ) . Set BMO(P) as the space of square integrable, continuous, R-valued martingales M such
that
∥M∥BMO := esssup
τ∈T T
0 Eτ
h
(MT −Mτ)2i


∞< +∞, where for any t ∈[0, T], T T
t
is the set of F-stopping times taking their values in [t, T].
Accordingly, H2
BMO is the space of R-valued and F-predictable processes Z such that ∥Z∥2
H2
BMO := Z . 0
ZsdWs BMO
< +∞. Denoting by E(M) the stochastic exponential of a semi-martingale M, we have ﬁnally
the following result Theorem 2.1. [25, Theorem 2.3] If M ∈BMO(P) then E (M) is a uniformly integrable
martingale. For any nonnegative F-adapted process α, we deﬁne the following increasing and contin-
uous process Aα
t :=
Z t 0
α2
sds. Let p > 1 2, β > 0 and let α be a nonnegative F-adapted process, we deﬁne S2p,β,α :=

Y adapted and càdlàg, ∥Y ∥2p
S2p,β,α := E

sup
0≤t≤T
epβAα
t |Yt|2p

< +∞

. H2p,β,α := ( Y progressively measurable, ∥Y ∥2p
H2p,β,α := E "" Z T 0
eβAα
s |Ys|2ds !p# < +∞ ) . Ha
2p,β,α := ( Y progressively measurable, ∥Y ∥2p
Hα
2p,β,α := E ""Z T 0
α2
seβAα
s |Ys|2pds # < +∞ ) . To match with the notations in [8], we deﬁne for any real p > 0 the spaces Sp and Hp by Sp := ( Y adapted and càdlàg processes, ∥Y ∥Sp := E

sup
0≤t≤T
|Yt|p
1∧1/p
< +∞ ) Hp := 

 

Z predictable processes, ∥Z∥Hp := E   Z T 0
|Zs|2ds !p/2  1∧1/p < +∞ 

 

. In particular, for any p > 1 2 we have S2p = S2p,0,α and H2p = H2p,0,α. Notice moreover
that the following inequality holds for any p > 1 2, β > 0 and for any nonnegative F-
adapted process α ∥Y ∥2p
S2p + ∥Z∥2p
H2p ≤∥Y ∥2p
S2p,β,α + ∥Z∥2p
H2p,β,α.
(2.1) 4 2.2
Elements of Malliavin calculus We give in this section some results on the Malliavin calculus that we will use in this
paper. Let now S be the set of cylindrical functionals, that is the set of random variables
F of the form F = f(W(h1), . . . , W(hn)),
(h1, . . . , hn) ∈Hn, f ∈C∞
b (Rn), for some n ≥1,
(2.2) where W(h) :=
R T
0 ˙
hsdWs for any h in H and where C∞
b (Rn) denotes the space of
bounded mappings which are inﬁnitely continuously diﬀerentiable with bounded deriva-
tives. For any F in S of the form (2.2), the Malliavin derivative ∇F of F is deﬁned as
the following H-valued random variable: ∇F := n
X i=1
fxi(W(h1), . . . , W(hn))hi,
(2.3) where fxi :=
d
f dxi .
It is then customary to identify ∇F with the stochastic process
(∇tF)t∈[0,T ]. Denote then by D1,p the closure of S with respect to the Malliavin-Sobolev
semi-norm ∥· ∥1,p, deﬁned as: ∥F∥1,p := (E [|F|p] + E [∥∇F∥p
H])1/p . We set D1,∞:= T p≥2 D1,p. We make use of the notation DF to represent the derivative
of ∇F as: ∇tF =
Z t 0
DsFds,
t ∈[0, T]. To avoid any ambiguity in the non-Markovian case we will consider later on, we need to
introduce immediately some further notations. For any mapping ˜
f from [0, T] × Ω× R
into R, we let D ˜
f(t, y) be the Malliavin derivative, computed at the point (t, y), of
ω 7−
→˜
f(t, ω, y). If D ˜
f is continuously diﬀerentiable with respect to y, we denote by
(D ˜
f)y its derivative with respect to y. Let now Y be an F-progressively measurable real
process, with Yt ∈D1,2 at time t ∈[0, T]. Using the chain rule formula (see for instance
[35]), the Malliavin derivative of D ˜
f at (t, Yt), denoted by D2 ˜
f(t, Yt) is given by D2
v,u ˜
f(t, Yt) := Dv(Du ˜
f)(t, Yt) + (Du ˜
f)y(t, Yt)DvYt, 0 ≤u, v ≤t.
(2.4) Let h be in H and let τ be the following shift operator τh : Ω−
→Ωdeﬁned by τh(ω) := ω + h, ω ∈Ω. Note that the fact that h belongs to H ensures that τh is a measurable shift on the
Wiener space. In the present paper, we will use the characterization of the Malliavin dif-
ferentiability, as a convergence of a diﬀerence quotient in Lp, introduced in [30], recalled
below. Theorem 2.2 (Theorem 4.1 in [30]). Let p > 1 and F ∈Lp(R). Then F belongs to D1,p if and only if there exists DF in Lp(H) and there exists q ∈[1, p) such that for any h in
H lim
ε→0 E




F ◦τεh −F ε
−⟨DF, h⟩H q
= 0. In that case, DF = ∇F. We now recall the criterion that we will use to check the absolute continuity of the law
of a random variable F with respect to the Lebesgue measure. Theorem 2.3 (Bouleau-Hirsch Criterion, see e.g.
Theorem 2.1.2 in [35]). Let F be
in D1,p for some p > 1. Assume that ∥DF∥h > 0, P−a.s. Then F has a probability
distribution which is absolutely continuous with respect to the Lebesgue measure on R. 5 Let F such that ∥DF∥h > 0, P−a.s., then the previous criterion implies that the law of
F admits a density ρF with respect to the Lebesgue measure. Assume that there exists
in addition a measurable mapping ΦF with ΦF : Rh →h, such that DF = ΦF (W), we
then set: gF (x) :=
Z ∞ 0
e−uE
h
E∗[⟨ΦF (W), f
Φu
F (W)⟩h]


F −E(F) = x
i
du, x ∈R,
(2.5) where f
Φu
F (W) := ΦF (e−uW +
√ 1 −e−2uW ∗) with W ∗an independent copy of W deﬁned
on a probability space (Ω∗, F∗, P∗), and E∗denotes the expectation under P∗(ΦF being
extended on Ω× Ω∗). We recall the following result from [34]. Theorem 2.4 (Nourdin-Viens’ Formula). The law of a random variable F has a density
ρF with the respect to the Lebesgue measure if and only if the random variable gF (F −
E[F]) is positive a.s. In this case, the support of ρF , denoted by supp(ρF ), is a closed
interval of R and for all x ∈supp(ρF ): ρF (x) = E(|F −E[F]|) 2gF (x −E[F]) exp −
Z x−E[F ] 0 udu gF (u) ! . 3
Malliavin diﬀerentiability of stochastic Lipschitz BS-
DEs The Malliavin diﬀerentiability of solutions to non-Markovian Lipschitz BSDE has been
studied ﬁrst in [17] and more recently in [30], as well as in [19] for Lévy driven BSDE.
In [19], the authors use the well-known characterization of the Malliavin derivative as
Gâteaux derivative introduced by Sugita in [47] and they obtain similar conditions, for the
Brownian part, to those in [17] (see [19, Section 4, (Af)]), while [30] took the advantage of
a new Lp characterization of the Malliavin diﬀerentiability (see Theorem 2.2) to improve
conditions obtained in [17].
Here, we extend the results of [30] to the stochastic Lipschitz case. We consider the
following non-Markovian BSDE Yt = ξ +
Z T t
f(s, Ys, Zs)ds −
Z T t
ZsdWs, ∀t ∈[0, T], P −a.s.
(3.1) where ξ is an FT -measurable random variable and f : [0, T] × Ω× R2 −
→R is an
F-progressively measurable process where as usual the ω-dependence is omitted. 3.1
Regularity of BSDE (3.1): an approach inspired by [16, 48] We consider the following assumption for p > 1 2 and β > 0, Assumption (sLp,β). (i) There exists two nonnegative F-adapted processes r and θ such that |f(t, y, z) −f(t, y′, z′)| ≤rt|y −y′| + θt|z −z′|, ∀(t, y, y′, z, z′) ∈[0, T] × R4. (ii) Let a2
t := rt + |θt|2 for any t ∈[0, T].
We suppose that a2
t > 0, dt ⊗dP-a.e.,
E [Aa
T ] < +∞and
f(t, 0, 0) at
∈H2p,β,a. (iii) ξ satisﬁes
E
h
epβAa
T |ξ|2pi
< +∞. 6 (iv) If p ∈( 1 2, 1), there exists a positive constant L such that Aa
T < L, P-a.s. Remark 3.1. Notice that the case a ≡0 is excluded according to (ii). However, this
case can be studied easily since a ≡0 implies that f is constant with respect to y and z.
Then, we can provide an explicit expression for the solution to this kind of BSDE. The main diﬃculty in this study is that the process a is not bounded and the stochastic
integral of a is not a BMO-martingale under Assumption (sLp,β). We recall the following
result which can be found in [48] and extends the results in [16]. Theorem 3.1 (Theorem 4.1 together with Proposition 3.6 in [48]). Let p >
1
2 and
β > max {2/(2p −1); 3} and assume that Assumption (sLp,β) holds. Then BSDE (3.1)
admits a unique solution (Y, Z) in (S2p,β ∩Ha
2p,β) × H2p,β. Moreover, (i) if p ≥1, there exists a positive constant Cp,β depending only on p and β such that ∥Y ∥2p
S2p,β,a + ∥Y ∥2p
Ha
2p,β,a + ∥Z∥2p
H2p,β,a ≤Cp,β E
h
epβAa
T |ξ|2pi
+




f(t, 0, 0) at 2p H2p,β,a ! , (3.2) (ii) if p ∈( 1 2, 1), there exists a positive constant Cp,β,L depending only on p, β, L such
that Estimate (3.2) holds with Cp,β,L. We now turn to the Malliavin diﬀerentiability of solutions to BSDE (3.1) under Assump-
tion (sLp,β ). Such a result requires additional assumptions that we now list. Assumption (DsLp,β).
There exist p > 1 2 and β > 0 such that for any h ∈H, (i) ξ ∈D1,2, lim
ε→0 E "" epβAa
T




ξ ◦τεh −ξ ε
−⟨∇ξ, h⟩H 2p# = 0, and
E
h
eβAa
T |⟨∇ξ, h⟩H|2i
< +∞. (ii) ω 7−
→f(t, ω, y, z) ∈D1,2 for any (t, y, z) ∈[0, T] × R × R, lim
ε→0 f(t, ω ◦τεh, Yt, Zt) −f(t, ω, Yt, Zt) ε
−⟨∇f(t, Yt, Zt), h⟩H at H2p,β,a = 0 and




⟨∇f(t, Yt, Zt), h⟩H at H2,β,a
< +∞. (iii) Let (εn)n∈N be a sequence in (0, 1] such that
lim
n→+∞εn = 0, and let (Y n, Zn)n be a sequence of random variables which converges in S2p,β,a × H2p,β,a, for any (p, β) ∈
( 1 2, 1) × (0, +∞), to some (Y, Z). Then there exists η > 0 such that for all h ∈H, the
following convergences hold in probability ∥fy(·, ω + εnh, Y n
· , Z·) −fy(·, ω, Y·, Z·)∥L2+η([0,T ])
−
→
n→+∞0, ess sup
t∈[0,T ] fz(·, ω + εnh, Y n
· , Zn
· ) −fz(·, ω, Y·, Z·) at −
→
n→+∞0
(3.3) or ∥fy(·, ω + εnh, Y n
· , Zn
· ) −fy(·, ω, Y·, Z·)∥L2+η([0,T ])
−
→
n→+∞0, ess sup
t∈[0,T ] fz(·, ω + εnh, Y·, Zn
· ) −fz(·, ω, Y·, Z·) at −
→
n→+∞0.
(3.4) (iv) For any q ≥1, E
hR T
0 rsds
qi
< +∞. 7 Remark 3.2. Concerning Property (ii) of Assumption (DsLp,β), notice that for ﬁxed
(y, z), the process (s, ω) 7−
→Df(s, ω, y, z) is deﬁned outside a P-negligible set which
depends generally on (y, z). Hence, it is not clear1 that this process is well-deﬁned at
the point (Ys(ω), Zs(ω)). However, under appropriate continuity conditions on the map
(y, z) 7−
→Df(s, ·, y, z), these negligible sets can actually be aggregated into a universal
one, outside of which Df(s, Ys, Zs) is indeed well-deﬁned. Nonetheless, let us point out an alternative approach for which no extra conditions on the
Malliavin derivative of f is required. The main problem is that the Malliavin derivative
of a random variable is in general only deﬁned P-a.s. (except for instance when it is a
cylindrical random variable), as a limit in probability of a sequence of random variables
(which are deﬁned for every ω, again since they are cylindrical functions). There ex-
ists however a notion of limit, called medial limits (lim med for short), which has the
particular property that under very general set theoretic axioms (see below), we have the
following result (see e.g. [32]): Let (Zn) be a sequence of random variables, then Z(ω) := lim med
n→+∞Zn(ω) is universally measurable and if Zn converges to some random variable ZP in probability, then Z = ZP,
P-a.s. In our case, let F be in D1,2, there exists a sequence of cylindrical elements Fn which
converges to F in D1,2. Hence, DF n converges in L2(H) to the Malliavin derivative of
F denoted by DF, deﬁned P-a.s. Let g
DF be the medial limit of DF n, deﬁned for every
ω. By the above result, DF = g
DF, P −a.s. This approach, which as far as we know has not been considered in the context of Malli-
avin calculus before (but see [37] for its use for stochastic integrals), allows to give a
complete pathwise deﬁnition of the Malliavin derivative of any random variable in D1,2.
We emphasize nonetheless that the existence of medial limits depends on set-theoretic
framework that one is using for instance Zermelo-Fraenkel set theory, plus the axiom of
choice (ZFC for short), and either the continuum hypothesis or Martin’s axiom (which
is compatible with the negation of the continuum hypothesis). See e.g. the footnote in
[42, Remark 4.1] for more explanations and the weakest known conditions ensuring the
existence of medial limits. Before going further, we compare these assumptions with those made in [30]. Assump-
tions (DsLp,β) (i) and (ii) seem quite reasonable in order to prove that the Malliavin
derivatives of Yt and Zt are well-deﬁned as the solution in S2,β,a×H2,β,a to the stochastic
linear BSDE (3.5) below, in view of Theorem 3.1. We now turn to Assumption (DsLp,β)
(iii) which is less natural and stronger than its equivalent (H2) in [30]. Indeed, if we
compare for instance (3.3) with its equivalent (H2) in [30], we ﬁrst notice that we assume
that there exists η > 0 such that ∥fy(·, ω + εnh, Y n
· , Z·) −fy(·, ω, Y·, Z·)∥L2+η([0,T])
−
→
n→+∞0, which provide a condition of order strictly more than 2, unlike Assumption (H2) in [30]
which deals with an L2 norm. This assumption is necessary for our study and comes
in fact directly from the a priori estimates in Theorem 3.1 (see [48, Proposition 3.6])
and the deﬁnition of H2p,β,a. We now turn to the second assumption in (3.3). This
assumption is quite strong, and is intrinsically linked to the fact Z ∈H2p,β,a. Indeed,
to obtain (3.10) in the proof of the Theorem 3.2 below, we are not able to conclude
without this assumption since an Hölder Inequality will provide a term with Z2+η
s
in
the integral and in view of the deﬁnition of the space H2p,β,a, we can not prove the
convergence. Concerning (iv), this assumption is quite similar to those obtained in the
following Section 3.2, and is satisﬁed as soon as the stochastic integral of r is for instance
a BMO-martingale. 1This gap was pointed by Laurent Denis, during a review of the PhD thesis of the author, concerning
Assumption (D) in [30] which corresponds to (DsLp,β). Remark 3.2 has to be also taken into account for the
latter paper. 8 We thus have the following theorem. Theorem 3.2. Let p be in ∈
",0
"This paper presents hydrodynamic-like model of business cycles aggregate fluctuations of economic and financial variables. We model macroeconomics as ensemble of economic agents on economic space and agent’s risk ratings play role of their coordinates. Sum of economic variables of agents with coordinate x define macroeconomic variables as functions of time and coordinates x. We describe evolution and interactions between macro variables on economic space by hydrodynamic-like equations. Integral of macro variables over economic space defines aggregate economic or financial variables as functions of time t only. Hydrodynamic-like equations define fluctuations of aggregate variables. Motion of agents from low risk to high risk area and back define the origin for repeated fluctuations of aggregate variables. Economic or financial variables on economic space may define statistical moments like mean risk, mean square risk and higher. Fluctuations of statistical moments describe phases of financial and economic cycles. As example we present a simple model relations between Assets and Revenue-on-Assets and derive hydrodynamic-like equations that describe evolution and interaction between these variables. Hydrodynamic-like equations permit derive systems of ordinary differential equations that describe fluctuations of aggregate Assets, Assets mean risks and Assets mean square risks. Our approach allows describe business cycle aggregate fluctuations induced by interactions between any number of economic or financial variables. Keywords: Business cycles, Aggregate fluctuations, Risk Ratings, Economic Space JEL: C00, E00, F00, G00 1 1 This research did not receive any specific grant or financial support from TVEL or funding 
agencies in the public, commercial, or not-for-profit sectors. 2 1. 
Introduction Fluctuations and waves are core properties of any complex system. Macroeconomic fluctuations are the most valuable processes that have impact on all characters of economic evolution and state. Modeling and forecasting business cycles aggregate fluctuations for decades remain focal point of economic studies [1-8]. Origin and drivers of aggregate fluctuations of economic and financial variables establish the key problems of business cycles. Let quote only three statements “Theories of business cycles should presumably help us to understand the salient characteristics of the observed pervasive and persistent non seasonal fluctuations of the economy”. [3]. “Why aggregate variables undergo repeated fluctuations about trend, all of essentially the same character? Prior to Keynes’ General Theory, the resolution of this question was regarded as one of the main outstanding challenges to economic research, and attempts to meet this challenge were called business cycle theory.” [4]. “One of the most controversial questions in macroeconomics is what explains business-cycle fluctuations?” [8]. Our paper presents a general model of aggregate fluctuations of economic and financial variables and proposes further features that describe state and evolution of business cycles phases. Due to [6] “The real business cycle theory is a business cycle application of the Arrow-Debreu model, which is the standard general equilibrium theory of market economies”. Our approach to business cycles is completely different from models based on assumptions of general equilibrium [9], decisions making [10] and behavioral economics [11]. We omit review of current state of business cycles and site [1-8] for great in-depth discussion and numerous references. We regard business cycles as essential property of economic evolution and describe aggregate fluctuations of economic and financial variables as necessary feature of economic processes. Our model of aggregate fluctuations does not require existence any external shocks and disturbances of economic variables. Aggregate fluctuations reflect hidden evolution of economic and financial variables of economic agents on economic space [12- 18]. Agent-based economic models are well known [19] but we suggest a different approach. Let assume that it is possible estimate risk rating for all agents of entire economics like huge banks and corporations and small firms and households. Let treat agent’s risk ratings as their coordinates alike to coordinates of physical particles. Each economic agent has a lot of economic and financial variables as Assets and Debts, Credits and Loans, Production Function and Consumption and so on. Huge number of economic agents of entire economics 3 can be treated alike to “economic gas”. Certain parallels to kinetic theory of gases allows establish transition from description of economic variables of separate agents with coordinate x to description of economic variables defined as cumulative variables of all agents with coordinate x. Such transition has parallels to transfer from description of kinetic multi particle system to hydrodynamic approximation that neglect particle granulation and describe system as a continuous media. Similar considerations allow develop transition from description of macroeconomics as system of numerous separate economic agents with economic and financial variables to macroeconomic model that neglect agent’s granularity and describe economic and financial variables as functions of time and coordinates on economic space alike to continuous media or hydrodynamic approximation in physics. Vital distinctions between economics and physical systems prohibit any “deep analogies”. Nevertheless certain parallels between multi-agent systems on economic space and multi- particle systems on physical space allow derive hydrodynamic-like equations that describe evolution of cumulative economic and financial variables. Such approach to macroeconomic modeling makes visible and describes a wide range of internal economic and financial waves that are induced by perturbations of variables on economic space [12-18]. In this paper we show that our model can describe business cycle aggregate fluctuations of economic and financial variables. Hydrodynamic-like equations describe dynamics of economic and financial variables as functions of time and coordinates on economic space. Integrals of economic variables over space coordinates define aggregate economic and financial variables as functions of time only. For example integral by Assets A(t,x) over economic space defines aggregate Assets A(t) as function of time only: ܣሺݐሻ= ∫݀𝒙 ܣሺݐ, 𝒙ሻ Usage of hydrodynamic-like equations allow derive ordinary differential equations on aggregate variables and model their growth and fluctuations in time. Non-linear character of hydrodynamic-like equations induces chains of factors that could govern growth and fluctuations of aggregate economic and financial variables. Our approach reveals that observed aggregate fluctuations may depend not on other economic variables but on various hidden economic factors that define evolution of economic system on economic space. For example we show that aggregate fluctuations of macroeconomic Assets A(t) may depend on factor that have meaning alike to energy EA(t) of Assets flow in hydrodynamics - EA(t) is proportional to Assets density A(t,x) and square of velocity of Assets flow on economic space. 4 Usage of economic space with coordinates that have meaning of agent’s risk ratings allows describe business cycles not only as aggregate fluctuations but by additional new parameters. Most economic and financial variables are positive defined functions on economic space. Each of positive distribution can be used as probability distribution and can define mean risks or mean coordinates of particular economic variable. For example distribution of Assets A(t,x) on economic space can define Assets mean risk X(t) as ࢄሺݐሻܣሺݐሻ= ∫݀𝒙 𝒙 ܣሺݐ, 𝒙ሻ As we show below, hydrodynamic-like equations on Assets distribution A(t,x) allow derive equations that describe dynamics and fluctuations of ࢄሺݐሻܣሺݐሻ. In this paper for simple model we show that time fluctuations of aggregate Assets A(t) and action of additional factors define fluctuations of Assets mean risks X(t) with frequencies that can be different from frequencies of aggregate Assets A(t) fluctuations. We propose that economic phases or business cycles should be characterized my many parameters and aggregate fluctuations are only the simplest ones. For example Assets mean square risk ܺଶ
̅
̅
̅
̅ሺݐሻ ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ= ∫݀𝒙 𝑥𝟐 ܣሺݐ, 𝒙ሻ defines second statistical moment of Assets distribution on economic space and determines its rate of risk uncertainty due to Assets dispersion 𝜎ଶሺ𝑥ሻ 𝜎ଶሺ𝑥ሻ= ܺଶ
̅
̅
̅
̅ሺݐሻ−ܺ𝟐ሺݐሻ Growth of assets dispersion indicates rise of macroeconomic Assets risk uncertainties and decrees of dispersion reflect clustering of macroeconomic Assets near mean Assets mean risk X(t). Description of mean risks, mean square risks and further statistical moments for different economic and financial variables can describe economic phases and business cycles in greater detail. Different economic and financial variables induce different values of mean risks, mean square risks and etc. Description of aggregate fluctuations of economic and financial variables requires taking into account fluctuations of other aggregate variables and factors that are not observed up now. For example, modeling of Assets A(t) aggregate fluctuations may require factor that have the appearance of Assets energy EA(t) 𝐸ܣሺݐሻ= ∫݀𝑥𝐸ܣሺݐ, 𝒙ሻ= ∫݀𝑥 ܣሺݐ, 𝒙ሻ ݒଶሺݐ, 𝒙ሻ Energy EA(t,x) of the Assets A(t,x) is proportional to Assets density A(t,x) and square of Assets velocity ݒଶሺݐ, 𝒙ሻ on economic space alike to energy of hydrodynamic flow but no 5 parallels to physical energy exist. Such unexpected factors can induce major impact on business cycle aggregate fluctuations and their possible influence should be studied further. Economic origin of aggregate fluctuations described by our model is very “simple”. There is no need in any perturbations or external shock to derive equations and describe aggregate fluctuations. Economic evolution induces motions of economic agents from low risk affairs to high risk business and back and that motion of agent’s risks cause corresponding flows and tides of economic and financial variables on economic space. Such tides of economic and financial variables from low to high risks areas and back govern growth and aggregate fluctuations of all economic and financial variables as GDP, Investment, Assets and so on. Enormous number of economic and financial variables and complexity of their mutual interactions makes that “simple” problem extremely difficult. In this paper we present simplest model that describe mutual interactions between two variables only - between Assets A(t,x) and Revenue-on-Assets B(t,x) on economic space. We derive equations that describe growth and fluctuations of aggregate Assets A(t) and aggregate Revenue-on-Assets B(t). We derive equations on Assets mean risks X(t) and mean square risks ܺଶ
̅
̅
̅
̅ሺݐሻ and describe their fluctuations also. The rest of the paper is organized as follows. In Section 2 we argue the model setup and remind major issues for economic space modeling [12-14]. We argue multi agent systems and present reasons for usage of hydrodynamic-like equations. In Section 3 we discuss business cycle aggregate fluctuations in terms of economic space model. We explain dependence of aggregate fluctuations on description of economic distributions by hydrodynamic-like equations on economic space. As example we present a simple model and hydrodynamic-like equations that describe mutual interactions between Assets A(t,x) and Revenue-on-Assets B(t,x) economic variables. We show that solutions of aggregate Assets A(t) and Revenue-on-Assets B(t) follow exponential growth and fluctuations in time. We derive equations that describe Assets mean risks X(t) and Revenue-on-Assets mean risks Y(t) and show that they depend on factors different from those determine aggregate Assets A(t) and their solutions follow time fluctuations also but with different frequency. The same considerations can be applied to any aggregate economic and financial variables as GDP, Investment, Demand and etc. Conclusions are in Section 4. In Appendix A we derive system of ordinary differential equations that describe time fluctuations of aggregate Assets A(t) and aggregate Revenue-on-Assets B(t). Appendix B gives derivation of ordinary differential equations that describe time fluctuations of Assets mean risks X(t) and Revenue-on-Assets 6 mean risks Y(t). Appendix C presents derivation of equations on Assets mean square risks ܺଶ
̅
̅
̅
̅ሺݐሻ. 2. 
 Model Setup Extreme complexity of macroeconomic and financial processes indicates that their description via time-series analysis of economic variables might be not sufficient for development of adequate models and forecasts. We propose that economic modeling should be based on description of economic and financial variables treated as functions of time and coordinates on certain space that reflect essential economic properties. Economics is so different from natural processes that no physical or geographical space can play such a role and our approach has nothing common with spacial economics [20]. We propose to use well- known economic issues but transform them in such a way to uncover and outline their properties of “economic space”. For decades macroeconomics and finance measure risks associated with huge banks and corporations by their risk ratings issued by international rating agencies [21-23]. Let make several suppositions. Let assume that current risks assessment methodology can be extended and generalized in order to estimate risk ratings as for major banks and corporations as for all agents of entire economics. Let propose that it is possible to estimate ratings for all risks that may affect macroeconomic and financial evolution. If so let use agent’s risk ratings as their coordinates. Let call such a space that imbed agent’s risk ratings as coordinates – economic space. Usage of such well known economic issues as risk ratings allow describe relations between economic and financial variables by partial differential equations and that uncovers a hidden complexity of economic evolution processes. This approach allows model concealed origin of business cycle aggregate fluctuations of economic and financial variables and describe other features that characterize states of economic evolution phases. Below we introduce economic space modeling relations according to [12-18]. 2.1. Economic Space Let regard extensive (additive) macroeconomic variables as Assets, Debts, Production Function, Consumption and Investment, Credits and Loans and etc. All macroeconomic and financial variables are determined as aggregates of corresponding variables of economic agents. Assets of entire economics are defined by composition of agent’s Assets. Aggregative Production Functions of economic agents define macroeconomic Production Function. Economic growth and fluctuations of macroeconomic and financial variables are determined 7 by economic growth and fluctuations of corresponding variables of economic agents. To describe evolution of economic and financial variables one should model corresponding evolution of economic agents. We propose that description of macroeconomic and financial time-series is not sufficient to model drivers and hidden interactions that govern economic evolution. To develop adequate models of evolution of economic agents and their influence on dynamics of macroeconomic and financial variables we introduce economic space notion [12-18]. Our approach is completely different from general equilibrium [9], economic decision-making [10], behavioral economics [11], agent-based economics [19] and spatial economics [20]. We propose that business cycles observed as time fluctuations of macroeconomic and financial variables are induced by a concealed dynamics and wave propagation of economic variables. Description of waves of economic and financial variables requires a certain space where economic waves can propagate. Introduction of essential economic space as ground for modeling agents evolution allows describe dynamics of agent’s economic and financial variables alike to description of multi particle systems. We outline vital differences between economic and physical systems but demonstrate that usage of similar concepts allows develop useful parallels between description of continuous media in physics and modeling business cycles, growth and fluctuations of macroeconomic and financial variables. Main issue of economic space notion is simple. To describe dynamics of numerous economic agents we propose use agent’s risk ratings as their coordinates on economic space. We don’t study ratings of specific risks like credit, liquidity, market risks and propose regard ratings of any risks that can distress agents and hence macroeconomics and finance as coordinates of agents on economic space. Such simple proposition hides many problems. Up now risk ratings are provided by international rating agencies [21-23] not for all economic agents but for major banks and corporations only. Thus to establish our model let assume that generalization of risk methodologies and development of available econometric data can make possible assessment of risk ratings for all economic agents of macroeconomic system - for huge corporations and for small firms and even for households, and for any risks that may affect evolution of macroeconomic and finance. Below we present brief reasons for economic space definition due to [12-14]. International rating agencies [21-23] estimate risk ratings of huge corporations and banks and these ratings are widespread in current economics and finance. Risk ratings take values of risk grades and noted as AAA, BB, C and so on. Let treat such risk grades like AAA, BB, C as points x1, x2,.. xm of discreet space. Let propose, that risk assessments methodologies 8 can be extended to estimate risk ratings for all agents of entire economics. That will distribute all agents over points of finite discreet space determined by set of risk grades. Many risks impact macroeconomics. Let regard grades of single risk as points of one-dimensional space and simultaneous assessments of n different risks as measurements of coordinates of agents on n-dimensional economic space. Let propose, that risk assessments methodologies can be generalized in such a way that risk grades can take continuous values and define space R. Thus risk grades of n different risks establish Rn. Modeling on economic space uncovers extreme complexity of macroeconomics and finance and can’t take into account all possible risks. Description of agents of entire macroeconomics on economic space Rn requires choice of n major risks that cause major effects on economic and financial processes. To determine economic space one should estimate current risks and select two, three, four major risks as main factors affecting economic system. That establishes economic space with two or three dimensions. To select most valuable risks one should compare impact of different risks on economic and financial processes and chose few most valuable. Selection of n major risks defines initial representation of economic space Rn and that is a separate and very tough problem. It is well known that risks can suddenly arise and then vanish. To describe evolution of economic system and its agents in a time term T one should forecast m main risks that can play major role in a particular time term. Such forecast define target state of economic space Rm in a time term T. To describe evolution of economics taking into account variations of major risks during time term T one should define transition from initial economic space Rn determined by n main risks to target economic space Rm determined by m main risks. Such transitions describe how initial set of n risks can decline its action on economic agents and how new m risks grow up. Transition from initial set of n main risk to target set of m risks describes evolution from initial economic space Rn of to the target one Rm. Such unpredictable changes of major risks and corresponding changes of economic space representation are origin of irremovable randomness of economic and financial evolution. Selection of main risks simplifies description and allows neglect “small risks”. Selections of major risks give opportunity to validate initial and target sets of risks and to prove or disprove initial model assumptions. It makes possible to compare predictions of economic model with real data and outlines disagreements between predictions and observations. Below we develop models on economic space Rn in the simple assumption that macroeconomics and economic agents are under permanent action n risks. We don’t study 9 transitions from one set of major risks to another and describe simple models on constant economic space Rn. Agent’s risk ratings play role of their coordinates on economic space Rn. Introduction of economic space allows describe evolution of economic agents alike to description of multi-particle systems. We repeat that distinctions between economics and physics are absolutely vital but certain parallels between them allow develop economic models similar to description of multi-particle systems and hydrodynamics. 2.2 Multi-agent system Introduction of economic space allows substitute widespread partition of agents by economic sectors and industries with partition of agents by their coordinates on economic space [12-15]. Decomposition of economics by sectors allows define Assets or Profits of Bank sector as cumulative Assets or Profits of all agents of this particular sector. Let replace decomposition of economics by sectors and let allocate agents by their risk ratings x as coordinates x on economic space. Such allocation allows define macroeconomic variables as functions of x on economic space. For example, cumulative Assets of all agents with coordinate x define macroeconomic Assets as function of coordinate x. Such approach allows describe economic and financial processes on economic space alike to description of multi- particle system in physics in the continuous media or hydrodynamic approximation. Indeed, agents risk ratings x or agents coordinates x change under the action of economic and financial processes. Agents move on economic space alike to economic particles or “economic gas”. Motion of agents on economic space causes change of agent’s economic and financial variables. Let describe agents and their variables by probability distributions. Averaging of agent’s economic and financial variables by probability distributions allow describe economics alike to continuous media or hydrodynamic-like approximation. In such approximation we neglect granularity of variables like Assets or Capital that belong to separate agents at point x and describe macroeconomic Assets or Capital as function of x on economic space alike to “Assets fluid” or “Capital fluid” in hydrodynamics. In some sense such transition has parallels to partition of Assets by sectors or industries. The “small” difference: in common approach agents and their variables belong to permanent industry or sector. In our model agent’s risk ratings define linear space and agents can move on economic space due to change of their risk ratings. These small distinctions allow model economics as a “continuous economic media”. Below for convenience we present definition of macroeconomic and financial variables according to [12-18]. For brevity let further call economic agents as economic 10 particles or e-particles and economic space as e-space. Let introduce macro variables at point x as sum of variables of e-particles with coordinates x on e-space. Each e-particle has many economic and financial variables like Assets and Debts, Investment and Savings, Credits and Loans, Production Function and Consumption and etc. Let call e-particles as “independent” if sum of extensive (additive) variables of any group of e-particles equals same variable of entire group. For example: sum of Assets of n e-particles equals Assets of entire group. Let assume that all e-particles are “independent” and sum of extensive variables of any group of agents equals same variable of entire group. For example, aggregation of Assets of e-particles with coordinates x on e-space define Assets as function of time t and x. Integral of Assets A(t,x) by dx over e-space equals aggregate Assets A(t) of entire macroeconomics. We mention Assets as example of macroeconomic variable only and our considerations valid for any extensive economic or financial variable. Below we show that description of dynamics of economic and financial variables as functions of time t and coordinate x on economic space allows model business cycles, growth and fluctuations of macro variables of entire economics as functions of time t. Coordinates of e-particles represent their risk ratings and hence they are under random motion on e-space. Thus sum of Assets of e-particles near point x is random also. To obtain regular values of macro variables like Assets at point x let average Assets at point x by probability distribution f. Let state that distribution f define probability to observe N(x) e- particles with value of Assets equal a1,…aN(x). That determine density of Assets at point x on e-space (Eq.(2.1) below). Macro Assets as function of time t and coordinate x behave alike to Assets fluid - similar to fluids in hydrodynamics. To describe motion of Assets fluid [12] let define velocity of such a fluid. Let mention that velocities of e-particles are not additive variables and their sum doesn’t define velocity of Assets motion. To define velocities of Assets fluid correctly one should define “Asset’s impulses” pj at point x as product of Assets aj of particular j-e-particle and its velocity 𝝊࢐ (Eq. (2.2) below). Such “Asset’s impulses” pj = ܽ௝ 𝝊࢐ - are additive variables and sum of “Asset’s impulses” can be averaged by similar probability distribution f. Densities of Assets and densities of Assets impulses permit define velocities of Assets fluid (Eq.(2.3) below). Different economic and financial fluids can flow with different velocities. For example flow of Capital on e-space can have velocity higher then flow of Assets, nevertheless they are determined by motion of same e-particles. Macroeconomics can be modeled as interaction between numerous economic fluids and that makes description extremely difficult. Let present these issues in a more formal way. 11 Let assume that each e-particle on e-space Rn at moment t is described by l extensive variables (u1,…ul). Extensive variables are additive and admit averaging by probability distributions. Intensive variables, like Prices or Interest Rates, cannot be averaged directly. Enormous number of extensive variables like Capital and Credits, Investment and Assets, Profits and Savings, etc., describe each e-particle and make economic modelling very complex. As usual, macro variables are defined as aggregates of corresponding values of all e-particles of entire economics. For example, macro Investment equal aggregate Investment and Assets can be calculated as cumulative Assets of all e-particles. Let define macro variables as functions of time t and coordinates x on e-space. Let assume that there are N(x) e-particles at point x. Let state that velocities of e- particles at point x equal υ=(υ1,… υN(x)). Each e-particle has l extensive variables (u1,…ul). Let assume that values of variables equal u=(u1i,…uli), i=1,..N(x). Each extensive variable uj at point x defines macro variable Uj as sum of variables uji of N(x) e-particles at point x ܷ
௝= ∑ݑ௝௜ ;    ݆= ͳ, . . 𝑙
௜
;    ݅= ͳ, … 𝑁ሺ𝒙ሻ To describe motion of variable Uj let establish additive variable alike to impulse in physics. For e-particle i let define impulses pji (1.1) as product of extensive variable uj that takes value uji and its velocity υi: ݌௝௜= ݑ௝௜𝝊࢏ 
 
 
 
 
 
 
 
 
 
(1.1) For example if Assets a of e-particle i take value ai and velocity of e-particle i equals υi then impulse pai of Assets of e-particle i equals pai = aiυi. Thus if e-particle has l extensive variables (u1,…ul) and velocity υ then it has l impulses (p1,p2,..pl)=(u1υ,…ulυ). Let define impulse Pj (1.2) of variable Uj as ࡼ௝= ∑ݑ௝௜∙𝝊࢏ ;    ݆= ͳ, . . 𝑙
௜
;    ݅= ͳ, … 𝑁ሺ𝒙ሻ 
 
 
 
 
(1.2) Let introduce economic distribution function f=f(t,x;U1,..Ul, P1,..Pl) that determine probability to observe variables Uj and impulses Pj at point x at time t. Uj and Pj are determined by corresponding values of e-particles that have coordinates x at time t. They take random values at point x due to random motion of e-particles on e-space. Averaging of Uj and Pj within economic distribution function f allows establish transition from approximation that takes into account variables of separate e-particles to continuous “economic media” or hydrodynamic-like approximation that neglect e-particles granularity and describe averaged macro variables as functions of time and coordinates on e-space. Let define economic or financial density functions ܷ
௝ሺݐ, 𝒙ሻ= ∫ܷ
௝ ݂ሺݐ, 𝒙, ܷଵ, … ܷ௟, ࡼଵ, . . ࡼ௟ሻܷ݀ଵ. . ܷ݀௟݀ࡼଵ. . ݀ࡼ௟  
 
(2.1) 12 and impulse density functions Pj(t,x) ࡼ௝ሺݐ, 𝒙ሻ= ∫ࡼ௝ ݂ሺݐ, 𝒙, ܷଵ, … ܷ௟, ܲ
ଵ, . . ܲ
௟ሻܷ݀ଵ. . ܷ݀௟݀ࡼଵ. . ݀ࡼ௟   
 
(2.2) That allows define e-space velocities υj(t,x) (2.3) of densities Uj(t,x) as ܷ
௝ሺݐ, 𝒙ሻ࢜࢐ሺݐ, 𝒙ሻ= ࡼ௝ሺݐ, 𝒙ሻ  
 
 
 
 
 
 
(2.3) Densities Uj(t,x) (2.1) and impulses Pj(t,x) (2.2) are determined as mean values of aggregates of corresponding variables of separate e-particles with coordinates x. Functions Uj(t,x) can describe macro densities of Investment and Loans, Assets and Debts and so on. For example, Assets density A(t,x), impulse P(t,x) and velocity υ(t,x) can be defined as ܣሺݐ, 𝒙ሻ= ∫ܽ ݂ሺݐ, 𝒙, ܽ, ࡼሻ݀ܽ݀ࡼ  
 
 
 
 
 
(2.4) ࡼሺݐ, 𝒙ሻ= ∫ࡼ ݂ሺݐ, 𝒙, ܽ, ࡼሻ݀ܽ݀ࡼ   
 
 
 
 
 
(2.5) ܣሺݐ, 𝒙ሻ࢜ሺݐ, 𝒙ሻ= ࡼሺݐ, 𝒙ሻ  
 
 
 
 
 
 
(2.6) Here a and P denote sum of Assets and impulses of all e-particles with coordinates x. To describe evolution of macro densities like Investment and Loans, Assets and Debts and etc., let derive hydrodynamic-like equations and use Assets density A(t,x), impulse P(t,x) and velocity υ(t,x) as example. 2.3. Hydrodynamic-like equations In this section we present hydrodynamic-like equations for economic and financial densities like Capital and Assets, Investment and Credits and etc. [12-18]. Let follow [24] and for any extensive economic or financial density like Assets A(t,x) (2.4), its impulse ࡼሺݐ, 𝒙ሻ (2.5) and its velocity ࢜ሺݐ, 𝒙ሻ (2.6) on e-space hydrodynamic-like equations take form: 𝜕஺ 𝜕௧+ 𝛻∙ሺ࢜ܣሻ= ܳଵ 
 
 
 
 
 
 
 
(3.1) Left hand side describes two factors that can change value of Assets A(t,x) in a unit volume on e-space. First factor 
𝜕஺ 𝜕௧ describes change of A(t,x) in time. Second factor 𝛻∙ሺ࢜ܣሻ describes change of A(t,x) due to flux υA through surface of unit volume according to the Gauss-Ostrogradsky theorem: the integral of the divergence 𝛻∙ሺ࢜ܣሻ over volume V equals the surface integral of flux υA over the boundary of volume V. These two factors describe possible change of Assets density A(t,x)  in a unit volume. Right hand side factor Q1 describes any action of other factors or densities that can change left side. Assets impulse P(t,x) follows similar equations: 𝜕ࡼ 𝜕௧+ 𝛻∙ሺ࢜ࡼሻ= ࡽଶ 
 
 
 
 
 
 
 
(3.2) 13 Right hand side factor Q2 describes any action of other densities that can change left side. Parallels to equations of hydrodynamics are formal and extrinsic. We repeat that intrinsic, essential relations of economics and finance have nothing common with physical laws. Hydrodynamic-like equations (3.1; 3.2) describe very simple relations: left side describes to possible factors that can change amount of any extensive (additive) density in a unit volume: due to change in time and due to flux through unit surface. Right side describes any external factors that can change left side. Thus economic meaning of hydrodynamic-like equations (3.1; 3.2) is defined by right hand side factors Q1 and Q2. Due to parallels with hydrodynamics let call equations on densities (3.1) as Continuity Equations and equations on impulses (3.2) as Equations of Motion. It seems that equations (3.1; 3.2) are simple. But one should remember that enormous number of macroeconomic and financial variables and diversity of possible interactions between them makes entire problem much more complex then any problem of physical hydrodynamics. To define factors Q1 and Q2 let outline the following. E-particles (economic agents) don’t collide in e-space alike to physical particles and agent’s variables don’t obey any conservation laws alike to conservation laws of mass, impulse, energy etc. Factors Q1 and Q2 don’t take into account any elements alike to viscosity, pressure and etc. Let state that factors Q1 and Q2 in the right hand of hydrodynamic-like equations (3.1; 3.2) on any density A(t,x) and impulses P(t,x) depend on densities Uj(t,x) and impulses Pj(t,x) different from A(t,x) and P(t,x). Let call such variables Uj(t,x), Pj(t,x) as conjugate to variables A(t,x) and P(t,x) if variables Uj(t,x), Pj(t,x) determine Q1 and Q2 factors in right hand side of hydrodynamic-like equations on A(t,x) and P(t,x). For example, Investment may have conjugate variables like Cost of Capital or Return on Investment and their velocities. Demand may be conjugate to Supply and vice versa. Let state, that conjugate variables define right hand side of Continuity Equation and Motion Equations (3.1, 3.2). As we mentioned above, economic densities like Assets A(t,x) are determined by corresponding variables of e-particles (economic agents) with coordinates x. Any agent’s variables change due to economic and financial transactions between agents. Thus Q1 and Q2 factors that model action of conjugate variables should reflect relations defined by transactions between e-particles. In this paper for simplicity we describe a local approximation of economic and financial transactions between e-particles that takes into account transactions between e- particles with same coordinates only. Such simplification [12-15, 17] allows describe factors Q1 and Q2 by simple linear differential operators on conjugate densities. We use this assumption in the next Section. More complex models with non-local approximations that 14 take into account “action-at-a-distance” transactions between e-particles (economic agents) on economic space are presented in [16, 18]. 3. Business Cycles Let try respond to question: “Why aggregate variables undergo repeated fluctuations about trend, all of essentially the same character?” [4]. We propose that [1-8] present sufficiently reasonable general considerations for business cycle motivations. We simply present a model that describes business cycle aggregate fluctuations of economic and financial variables about growth trend. We develop hydrodynamic-like models that describe evolution of economic and financial densities on economic space determined by dynamics of cumulative risk ratings. We don’t argue problem: why it happens? We describe - what happens. Distribution of economic agents by their risk ratings as coordinates on economic space allows describe “repeated fluctuations about trend”. Definition of economic and financial densities (2.1-2.6) as functions of time t and coordinate x allows define variables of the entire macroeconomics as integrals by dx over e-space. For example integral (4.1) ܣሺݐሻ= ∫݀𝒙 ܣሺݐ, 𝒙ሻ   
 
 
 
 
 
 
(4.1) of Assets density A(t,x) over e-space defines aggregate Assets A(t) of entire economics. Same relations define all aggregate economic and financial variables of entire economics as function of time t only. Aggregate GDP, Investment, Credits, Taxes etc., can be presented similar to (4.1). Thus evolution of macroeconomic variables in time is determined by hidden dynamics of corresponding economic and financial densities on e-space. For example, evolution of macroeconomic aggregate Assets A(t) in time is determined by dynamics of Assets density A(t,x) on e-space. Economic growth and time fluctuations of all macroeconomic and financial variables are determined by evolution of densities on e-space. Economic densities at point (t,x) are determined by variables of agents at the same point. Thus business cycles treated as “fluctuations about trend” of macroeconomic and financial variables are determined by dynamics of agent’s variables under their motion on economic space. Usage of e-space enhances methods for economic modeling and allows indicate new factors that describe state and evolution of entire economics. Indeed, we use agent’s risk ratings as their coordinates on e-space. For each economic or financial density like Assets density A(t,x) let define mean risk X(t) of particular density as ࢄሺݐሻܣሺݐሻ= ∫݀𝒙 𝒙 ܣሺݐ, 𝒙ሻ   
 
 
 
 
 
(4.2) 15 Mean risk X(t) (4.2) reflects distribution of Assets by risk ratings – by coordinates of e-space. If agents and their Assets move to more risky domain then Assets density A(t,x) shifts to more risky values and same happens with X(t). When agents run from risks and move to safe area then their Assets and Assets density A(t,x) as well shift to lower risks and that induce corresponding motion of mean Assets risk X(t). So, Assets mean risk X(t) follows time fluctuations near certain stationary state on e-space and such fluctuations describe phases of economic evolution. We propose that fluctuations of mean risks can describe phases of business cycles. Mean risk X(t) is a first statistical moment of probability distribution p(t,x) defined by economic or financial density. For example, Assets density A(t,x) can define Assets probability distribution pA(t,x): ݌஺ሺݐ, 𝒙ሻ= ܣ−ଵሺݐሻܣሺݐ, 𝒙ሻ   ; ∫݀𝒙 ݌஺ሺݐ, 𝒙ሻ= ͳ   
 
 
 
 
(4.3) and (4.3) define mean risk of Assets density: ࢄሺݐሻ= ∫݀𝒙 𝒙 ݌஺ሺݐ, 𝒙ሻ 
 
 
 
 
 
 
(4.4) Width of distribution pA(t,x) reflects width of Assets density on e-space and second moment of distribution pA(t,x) ܺଶ
̅
̅
̅
̅ሺݐሻ= ∫݀𝒙 𝑥𝟐 ݌஺ሺݐ, 𝒙ሻ  ;  𝜎ଶሺ𝑥ሻ= ܺଶ
̅
̅
̅
̅ሺݐሻ−ܺ𝟐ሺݐሻ 
 
 
 
(4.5) Mean square ܺଶ
̅
̅
̅
̅ሺݐሻ and dispersion σ2(x) also should follow time fluctuations and can be useful for description business cycle phases. Growth of dispersion σ2(x) describes rise of economic risk uncertainty and σ2(x) reduction reflects clustering of economic agents and economic densities near mean risk value. Let show how hydrodynamic-like equations (3.1; 3.2) allow describe evolution and aggregate fluctuations of macroeconomic variables and mean risks. To do that let define factors Q1 and Q2 for simple model of two self-conjugate economic densities. 3.1. Model equations As example let describe relations between two extensive (additive) variables as Assets density A(t,x) and Revenue-on-Assets B(t,x). Returns on Assets are more widespread variable but it is intensive (not additive) variable. Revenue-on-Assets is extensive and additive variable and relations between these variables are well known. Let simplify the problem and propose that Revenue-on-Assets B(t,x) and its impulse PB(t,x) are only conjugate variable that defines Q1 and Q2 factors for equations (3.1; 3.2) on Assets A(t,x) and its impulse PA(t,x) and vise versa: Assets A(t,x) and its impulse PA(t,x) are only conjugate variable that defines Q1 and Q2 factors for equations (3.1; 3.2) on Revenue-on-Assets B(t,x) and its impulse PB(t,x). Let take simplest relations between two self-conjugate economic variables and that 16 allow describe their mutual interaction in self-consistent manner. To simplify the model let propose that Revenue-on-Assets B(t,x) and its impulse in moment t depend on Assets and its impulse at same moment t and vice versa. Let assume that factor Q1A for Continuity Equation (3.1) on Assets density A(t,x) is proportional to scalar product between Revenue-on-Assets impulse PB(t,x) and vector x, so: ܳଵ஺= ܽ 𝒙∙ࡼ࡮ሺݐ, 𝒙ሻ  
 
 
 
 
 
 
(5.1) and same relations define factor Q1B for Continuity Equation (3.1) on Revenue-on-Assets density B(t,x) ܳଵ஻= ܾ 𝒙∙ࡼ࡭ሺݐ, 𝒙ሻ  
 
 
 
 
 
 
(5.2) Here a and b – const. Economic meaning of (5.1; 5.2) is follows. Assets density in unit volume a point (t,x) grow up if scalar product of Revenue-on-Assets flow ࡼ࡮ሺݐ, 𝒙ሻ= ࢛ሺݐ, 𝒙ሻܤሺݐ, 𝒙ሻ is positive – Revenue flow grows up in the direction of vector x. The same relations increase Revenue-on-Assets density B(t,x) in unit volume at point (t,x) if Assets flow ࡼ࡭ሺݐ, 𝒙ሻ= ࢜ሺݐ, 𝒙ሻܣሺݐ, 𝒙ሻin the direction of vector x is positive. Simply speaking let assume that additional flow of Assets at point x increase Revenue-on-Assets at this point and vise versa. These assumptions neglect time gaps between Investments into Assets at point x and Revenue-on-Assets received and other factors that may impact on Assets allocations at point x and Revenue-on-Assets to simplify relations between them. Let take equations (3.1) on Assets density B(t,x) Revenue-on-Assets density B(t,x) as 𝜕஺ 𝜕௧+ 𝛻∙ሺ࢜ܣሻ= ܽ 𝒙∙ࡼ࡮ሺݐ, 𝒙ሻ  ;  
𝜕஻ 𝜕௧+ 𝛻∙ሺ࢛ܤሻ= ܾ 𝒙∙ࡼ࡭ሺݐ, 𝒙ሻ   
 
(5.3) To identify Equations of Motion (3.2) on impulses PA(t,x) and PB(t,x) let assume that Q2  is proportional to conjugate impulse. In other words let assume that growth of Assets impulse PA(t,x) is proportional to Revenue-on-Assets impulse PB(t,x) and vise versa. Let take Equations of Motion (3.2) as: 𝜕ࡼ࡭ 𝜕௧+ 𝛻∙ሺ࢜ ࡼ࡭ሻ= ܿ ࡼ࡮ሺݐ, 𝒙ሻ  ;  
𝜕ࡼ࡮ 𝜕௧+ 𝛻∙ሺ࢛ ࡼ࡮ሻ= ݀ ࡼ࡭ሺݐ, 𝒙ሻ    
 
(5.4) Economic meaning of (5.4) is follows. E-particles (economic agents) of entire economics fill certain domain on n-dimensional e-space. Due to current practice of international rating agencies [21-23] agent’s risk ratings take minimum or most secure and maximum or most risky values. Thus let assume that coordinates x=(x1,…xn) of economic domain on n- dimensional e-space are reduced by relations Ͳ < 𝑥௜< ܺ௜ , ݅= ͳ, … ݊ 
 
 
 
 
 
 
 
(5.5) As we mentioned above, we assume that small xi<<1 correspond to secure area and ܺ௜− 𝑥௜≪ͳ , ݅= ͳ, … ݊ 17 to most risky area. Thus flow of Assets in such a bounded domain can’t grow up forever. Assets flow can move from low risks and secure area of economic domain to high risks area and that increase Assets risks. Then Assets tide goes back from high risks to low risk area. Thus tides of Assets that is described by Assets impulse PA(t,x) ࡼ࡭ሺݐ, 𝒙ሻ= ࢜ሺݐ, 𝒙ሻܣሺݐ, 𝒙ሻ follow certain fluctuations with frequency ߱ on economic domain of e-space. Let assume that ߱ଶ= −ܿ݀> Ͳ 
 
 
 
 
 
 
 
 
(5.6) As we show below equations (5.4) describe simple fluctuations of aggregate Assets impulse PA(t) and aggregate Revenue-on-Assets impulse PB(t). Let take model equations (5.3; 5.4) to study aggregate fluctuations of Assets A(t) and Revenue-on-Assets B(t). ܣሺݐሻ= ∫݀𝒙  ܣሺݐ, 𝒙ሻ  ;      ܤሺݐሻ= ∫݀𝒙  ܤሺݐ, 𝒙ሻ  
 
 
 
 
(5.7) 3.2. Aggregate fluctuations of Assets A(t) Equations (5.3; 5.4) on Assets A(t,x) and Revenue-on-Assets B(t,x) densities allow derive ordinary differential equations on aggregate Assets A(t) and Revenue-on-Assets B(t) (Appendix A). Here we briefly argue main results. Fluctuations of aggregate Assets A(t) depend on hidden aggregate variables XPA(t) and YPB(t) defined by (A.3); EA(t) (A.6) and EB(t) (A.7) (see Appendix A). Equations (A.5) on variables XPA(t) and YPB(t) (A.3) can be derived from Equations of Motion (5.4). XPA(t) is defined by (A.3) scalar product of vector x and impulse PA(t,x). Due to (5.6) variables XPA(t) follow oscillations with frequency ߱. Aggregate variable EA(t) (A.6) is proportional to Assets A(t,x) and squares of velocities ݒଶሺݐ, 𝒙ሻ and can be treated alike to Assets energy. Same concern Revenue-on-Assets energy EB(t) that is alike to energy of hydrodynamic flow. To derive equations on aggregate Assets we should complement Continuity Equations (5.3) and Equations of Motion (5.4) with additional equations (A.8) on Assets energy EA(t,x) and Revenue-on-Assets energy EB(t,x). Equations (A.8) assume that aggregate Assets energy EA(t) and Revenue-on-Assets energy EB(t) grow up in time as exponent. (A.10) Fluctuations of Assets impulses PA(t) and PB(t) (B.5) with frequency ߱ (5.6) lead to fluctuations of XPA(t) and YPB(t) (A.5 ) with same frequency. Exponential growth of Assets energy EA(t) and EB(t) (A.10) and fluctuations of XPA(t) and YPB(t) (A.5) cause fluctuations of aggregate assets A(t) with frequency ߱ near exponential growth trend ݁𝑥݌ሺ𝛾௘ݐሻ. Aggregate Assets A(t) equals: ܣሺݐሻ= ܣሺͳሻ+ ܣሺʹሻcos ߱ݐ+ ܣሺ͵ሻsin ߱ݐ+ ܣሺͶሻ݁𝑥݌ሺ𝛾௘ݐሻ+ ܣሺͷሻ݁𝑥݌ሺ−𝛾௘ݐሻ    (5.8) with constants A(j), j=1,..5 determined by initial values (A.11-A.13). 18 3.3. Fluctuations of Assets mean risk X(t) Equations (5.3; 5.4) allow describe dynamics and oscillations of mean risks X(t) and mean square risks ܺଶ
̅
̅
̅
̅ሺݐሻdefined by Assets and Revenue-on-Assets densities. As we mentioned above (4.3 - 4.5) extensive (additive) aggregate economic or financial densities like Assets and Investment, Credits and Loans, Demand and Supply and etc., may define corresponding probability distributions like (4.3). Statistical moments of these distributions define mean risks X(t), mean square risks ܺଶ
̅
̅
̅
̅ሺݐሻ and so on and can be used as properties of economic evolution phases. Business cycles as fluctuations of aggregate variables induce oscillations of corresponding mean risks, mean squares and etc. It is obvious that different aggregate variables define different mean risks functions, and their oscillations may have different frequencies. Equations (5.3; 5.4) on Assets and Revenue-on-Assets densities allow derive equations on mean risks X(t) and mean squares risks ܺଶ
̅
̅
̅
̅ሺݐሻ defined by Assets and Revenue-on-Assets densities (Appendices B and C) but that requires introduction of additional aggregate variables and derivation corresponding additional hydrodynamic-like equations. Appendix B presents derivation of ordinary differential equations that describe fluctuations of mean Assets risks X(t) and Appendix C presents derivation of equations that describe ܺଶ
̅
̅
̅
̅ሺݐሻ. Assets mean risk X(t) and Revenue-on-Assets mean risk Y(t) depend upon aggregate Assets A(t) and aggregate Revenue-on-Assets B(t) and on new variables defined in Appendix B as aggregate Assets impulses PA(t) and PB(t) (B.3); aggregate factors XP(t) and YP(t) (equations B.4); aggregate Assets energy risks XE(t) and aggregate Revenue-on-Assets energy risks YE(t) (equations B.8); on aggregate Assets energy impulses PEA(t) and aggregate Revenue-on-Assets energy impulses PEB(t) (equations B.14). Aggregate Assets A(t) and Assets mean risk X(t) (B.19) takes form: ࢄሺݐሻܣሺݐሻ= ࢄ࡭ሺͳሻ+ ࢄ࡭ሺʹሻݏ݅݊߱ݐ+ ࢄ࡭ሺ͵ሻܿ݋ݏ߱ݐ+ ࢄ࡭ሺͶሻܿ݋ݏ߱𝑝௘ݐ+ ࢄ࡭ሺͷሻ ݏ݅݊߱𝑝௘ݐ + ࢄ࡭ሺ͸ሻܿ݋ݏ߱௩ݐ+ ࢄ࡭ሺ͹ሻ ݏ݅݊߱௩ݐ+ ࢄ࡭ሺͺሻ݁𝑥݌ሺ𝛾௘ݐሻ+ ࢄ࡭ሺͻሻ݁𝑥݌ሺ−𝛾௘ݐሻ Constants ࢄ࡭ሺ݆ሻ, ݆= ͳ, . .ͻ are defined by initial values of (B.18.1-B.18.6). ࢄሺݐሻܣሺݐሻ has same exponential increment 𝛾௘ as aggregate assets A(t). Hence Assets mean risk X(t) don’t grow up but follow only time oscillations with frequencies ߱ (A.11) as aggregate Assets A(t) and has additional frequencies ߱𝑝௘ (B.15) induced by oscillations of “energy impulses” PEA(t) and PEB(t) (B.13) and frequencies ߱௩ induced by oscillations of factors VXPA and UYPB (B.17.3). Assets mean risk X(t) describes evolution of Assets distribution over e-space and its fluctuations reflect main drivers of Assets dynamics. 19 3.4. Fluctuations of Assets mean square risk ࢄ𝟐
̅
̅
̅
̅ሺ࢚ሻ Assets mean square risk ܺଶ
̅
̅
̅
̅ሺݐሻ and Revenue-on-Assets mean square risk ܻଶ
̅̅̅ሺݐሻ describe distribution of Assets and Revenue-on-Assets on e-space (Appendix C). Assets mean square risk ܺଶ
̅
̅
̅
̅ሺݐሻ defines Assets dispersion 𝜎
஺
ଶሺ𝑥ሻ 𝜎
஺
ଶሺ𝑥ሻ= ܺଶ
̅
̅
̅
̅ሺݐሻ−ܺ𝟐ሺݐሻ Growth of Assets dispersion 𝜎
஺
ଶሺ𝑥ሻ reflects growth on Assets risk uncertainty and decrease of 𝜎
஺
ଶሺ𝑥ሻ describes clustering of Assets near its mean risk. Assets mean squares risk ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ (Appendix C) depend on new factors different from those define aggregate Assets A(t) and mean risks ࢄሺݐሻ. ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ= ܺଶ
̅
̅
̅
̅ܣሺͲሻ+ ܺଶ
̅
̅
̅
̅ܣሺͳሻsin ߱ݐ+ ܺଶ
̅
̅
̅
̅ܣሺʹሻcos ߱ݐ+ ܺଶ
̅
̅
̅
̅ܣሺ͵ሻsin ߱𝑝௘ݐ + ܺଶ
̅
̅
̅
̅ܣሺͶሻexp 𝛾௘ݐ+ ܺଶ
̅
̅
̅
̅ܣሺͷሻ exp −𝛾௘ݐ+ ܺଶ
̅
̅
̅
̅ܣሺ͸ሻ exp 𝛾௩௨ݐ + ܺଶ
̅
̅
̅
̅ܣሺ͹ሻ exp −𝛾௩௨ݐ+ ܺଶ
̅
̅
̅
̅ܣሺͺሻ exp 𝛾𝑥௩ݐ+ ܺଶ
̅
̅
̅
̅ܣሺͻሻ exp −𝛾𝑥௩ݐ with constants ܺଶ
̅
̅
̅
̅ܣሺ݆ሻ, j=0,..9 defined by initial values of variables (A.3; A.6; A.7; C.6; C.7.2; C.10.1; C12.2) for t=0. Factor ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ fluctuates with frequencies ߱ and ߱𝑝௘ (5.4; B.13) and has exponential growth increments  𝛾௘ , 𝛾௩௨, 𝛾𝑥௩ (A.9; C.12.3; C.15). Aggregate Assets A(t) has exponential increment 𝛾௘ (A.9; A.14). As ܺଶ
̅
̅
̅
̅ሺݐሻ can’t grow up as exponent hence it is required that 𝛾௘> 𝛾௩௨ ; 𝛾௘> 𝛾𝑥௩. 4. Conclusions We present a general approach for modeling aggregate fluctuations for any system of economic and financial variables. Distributions of economic agents by their risk ratings as coordinates on economic space allow describe economic and financial states and processes by hydrodynamic-like equations. Interactions between four-five or more conjugate economic or financial variables can be modeled by equations (3.1; 3.2). Such representation uncovers hidden complexity of financial and economic processes and gives opportunity to apply a wide range of mathematical physics methods to financial and economic modeling. Vital distinctions between economic and physical systems prohibit any direct application of known results. Huge amount of variables that impact economic and financial evolution makes such description extremely difficult. Economic nature of business cycle fluctuations of aggregate variables is determined by repeated motion of economic agents and their variables from low risk to high-risk area on economic space and back. Fluctuations of agents between low and high-risk areas on 20 economic space induce oscillations of aggregate fluctuations with various frequencies. We model such tides of economic and financial variables from low risks to high risks on economic space and back by simple equations (5.4; B.13; B.17.3) that induce harmonique oscillations with frequencies ߱ଶ= −ܿ݀> Ͳ    ; ߱𝑝௘
ଶ= −ܿ௘𝑝 ݀௘𝑝> Ͳ  ;  ߱௩
ଶ= −ܿ௩݀௩> Ͳ Interactions between numerous economic and financial variables cause complex structure for fluctuating frequencies and can explain variety scales of aggregate fluctuations. Growth of hidden variables induces exponential growth of aggregated economic factors with increments (A.9; C.12.3; C.15): 𝛾௘
ଶ= ܿ௘݀௘> Ͳ  ; 𝛾ଶ
௩௨= ܿ௨݀௩> Ͳ   ; 𝛾𝑥௩
ଶ= ܿ𝑥௩݀𝑥௩> Ͳ We present simplest model of mutual dependence between two economic variables – Assets and Revenue-on-Assets to demonstrate advantages of our approach and to present simple derivation of equations that describe aggregate fluctuations of these variables. Even such simple model shows certain complexity and indicates that attempts to describe mutual relations between four-five and more variables will lead to modeling solutions for huge system of hydrodynamic-like equations and will require much more efforts and computer programs. The most interesting issue concerns structure of variables that define aggregate fluctuations of Assets. Nevertheless we model interaction between two variables - Assets and Revenue-on-Assets, system of hydrodynamic-like equations describe Assets density by Continuity equation (5.3), Assets impulses by Equation of Motion (5.4) and additional unexpected variable alike to Assets energy (A.6) and Revenue-on-Assets energy (A.7) that are described by equation (A.8). Naturally there are no substantive parallels between energy of hydrodynamic flow and Assets energy (A.8). Anyway, usage of this hidden variable extends parallels between economics and hydrodynamics. Let underline – we model interaction between Assets A(t,x) and Revenue-on-Assets B(t,x) on economic space and that allow derive equations on aggregate Assets A(t) and aggregate Revenue-on-Assets B(t). Aggregate Assets fluctuations don’t depend on aggregate Revenue-on-Assets, but on variables alike to aggregate Revenue-on-Assets energy and other (A.7; A.3). Thus it seems unbelievable to describe business cycles by models that take into account only direct dependence between time-series of aggregate variables as in equilibrium- based models of business cycles. Economic evolution is a very complex processes and description of evolution phases requires much more then description of aggregate fluctuations. Hydrodynamic-like equations 21 allow describe additional parameters of economic evolution phases that can be noted as mean risks, mean square risks, dispersions and etc. Indeed, most economic densities on e-space are positive and thus can be treated as certain probability distributions on e-space. For example Assets distribution of e-space (4.2-4.5) can define Assets mean risks X(t) (6) and Assets mean square ܺଶ
̅
̅
̅
̅ሺݐሻ (7). Each economic and financial variable can define its own mean risks and mean squares and these functions describe risk state and risk fluctuations of particular variable on economic space. Business cycles as aggregate fluctuations are accompanied by corresponding fluctuations of risk statistical moments of corresponding variables like mean risk or square risk. These parameters give additional information about uncertainty of economic variables and might be useful for economic and financial managing and policy- making. Complexities of relations (Appendix B and C) that determine dynamics of risk statistical moments leave few chances to derive similar relations by “mainstream” models. Current econometric data don’t have sufficient info that is required for economic space modeling, definition of Assets A(t,x) and Revenue-on-Assets B(t,x) distributions and their modeling by hydrodynamic-like equations. Thus our theoretical description of business cycle aggregate fluctuations can’t be verified by existing econometric data. Nevertheless we propose that there are no inscrutable and unsolvable problems that might prevent enhancement of risk ratings methodologies and development of econometric data to support economic modeling on risk ratings economic space. Proposed development will help better understand and respond economic and financial processes and improve policymaking. 22 Appendix A Aggregate Assets A(t) and Revenue-on-Assets B(t) Equations To derive equations on aggregate Assets A(t) let start with (5.3; 5.6): ௗ ௗ௧ܣሺݐሻ= ∫݀𝑥 
𝜕 𝜕௧ܣሺݐ, 𝒙ሻ= −∫݀𝑥 𝛻∙ሺ࢜ܣሻ+ ܽ ∫݀𝑥𝒙 ∙ࡼ࡮  
 
(A.1) Due to (5.5) economic domain on e-space [17,18] is reduced by most secure and by most risky e-particles (economic agents). Hence integral of divergence over e-space equals integral of flux through surface and that equals zero as no economic or financial fluxes exist far from boundaries (5.6). Thus first integral in the right hand of (A.1) equals zero. Equations on A(t) and B(t) take form: ௗ ௗ௧ܣሺݐሻ=  ܽ ܻܲܤሺݐሻ ;  
ௗ ௗ௧ܤሺݐሻ=  ܾ ܺܲܣሺݐሻ  
 
 
 
(A.2) ܺܲܣሺݐሻ= ∫݀𝑥𝒙 ∙ࡼ࡭ሺ࢚, 𝒙ሻ ;    ܻܲܤሺݐሻ= ∫݀𝑥𝒙 ∙ࡼ࡮ሺ࢚, 𝒙ሻ 
 
(A.3) To derive equations on XPA(t) и YPB(t) let use equation (5.4): ௗ ௗ௧ܺܲܣሺݐሻ= ∫݀𝑥𝒙 ∙
𝜕 𝜕௧ࡼ࡭ሺݐ, 𝒙ሻ= −∫݀𝑥𝒙 ∙𝛻∙ሺ࢜ ࡼ࡭ሻ+ ܿ ܻܲܤሺݐሻ  
 
(A.4) Let take first integral in the right hand by parts and take into account that integral by divergence equals zero: −∫݀𝑥𝒙 ∙𝛻∙ሺ࢜ ࡼ࡭ሻ= ∫݀𝑥  ܣሺݐ, 𝒙ሻݒଶሺݐ, 𝒙ሻ ௗ ௗ௧ܺܲܣሺݐሻ= 𝐸ܣሺݐሻ+ ܿ ܻܲܤሺݐሻ    ;      
ௗ ௗ௧ܻܲܤሺݐሻ= 𝐸ܤሺݐሻ+ ݀ ܺܲܣሺݐሻ    
(A.5) 𝐸ܣሺݐሻ= ݒଶሺݐሻܣሺݐሻ= ∫݀𝑥 ܣሺݐ, 𝒙ሻݒଶሺݐ, 𝒙ሻ= ∫݀𝑥 ܣሺݐ, 𝒙ሻ∑ݒ௜
ଶ
௜
  
 
(A.6) 𝐸ܤሺݐሻ= ݑଶሺݐሻܤሺݐሻ= ∫݀𝑥 ܤሺݐ, 𝒙ሻݑଶሺݐ, 𝒙ሻ= ∫݀𝑥 ܤሺݐ, 𝒙ሻ∑ݑ௜
ଶ
௜
 
 
 
(A.7) It is amazing that equations on aggregate Assets and Revenue-on-Assets require factors (A.6; A.7) that are alike to Assets energy EA(t) and Revenue-on-Assets energy EB(t). These factors enhance parallels between economic modeling and hydrodynamics. It is obvious that Assets energy EA(t) has only formal resemblance with physical energy used in hydrodynamics but as well it underlines very important issue: adequate modeling of aggregate macroeconomic variables can’t be based on usual economic variables only but require wide range of derivative factors like (A.3; A.6; A.7). To describe aggregate Assets A(t) in a closed form let take hydrodynamic-like equations on Assets energy density EA(t,x) and Revenue-on-Assets energy density EB(t,x) as: 𝜕𝐸஺ 𝜕௧+ 𝛻∙ሺ࢜𝐸ܣሻ= ܿ௘ 𝐸ܤሺݐ, 𝒙ሻ ;   
𝜕𝐸஻ 𝜕௧+ 𝛻∙ሺ࢛𝐸ܤሻ= ݀௘ 𝐸ܣሺݐ, 𝒙ሻ  
 
(A.8) 23 Economic meaning of (A.8) is follows. Flows of Assets A(t,x) from low risk area to high risk area and back are accompanied with growth of “Assets energy” proportional to growth of aggregate Assets A(t) and square of velocity ݒଶሺݐሻ. The same happens to flows of Revenue- on-Assets energy EB(t,x). Assumptions for right side of equations (A.8) for 𝛾௘
ଶ= ܿ௘݀௘> Ͳ 
 
 
 
 
 
 
 
(A.9) mean that aggregate Assets energy EA(t) and aggregate Revenue-on-Assets energy EB(t) grow up as exponent and (A.8; A.9) give equations on aggregate energies EA(t) and EA(t): ௗ ௗ௧𝐸ܣሺݐሻ= ܿ௘ 𝐸ܤሺݐሻ     ;  
ௗ ௗ௧𝐸ܤሺݐሻ= ݀௘ 𝐸ܣሺݐሻ  
 
 
 
 
(A.10) That allows close sequence of equations on aggregate Assets. Equations (A.2; A.5; A10) form a system of equations that describe aggregate Assets A(t) and Revenue-on-Assets B(t). Initial values of variables are defined as: ܣሺͲሻ= ∫݀𝑥ܣሺͲ, 𝒙ሻ ;    ܤሺͲሻ= ∫݀𝑥ܤሺͲ, 𝒙ሻ ; 
 
 
 
 
(A.11) ܺܲܣሺͲሻ= ∫݀𝑥𝒙 ∙࢜ሺͲ, 𝒙ሻܣሺͲ, 𝒙ሻ ;    ܻܲܤሺͲሻ= ∫݀𝑥𝒙 ∙࢛ሺͲ, 𝒙ሻܤሺͲ, 𝒙ሻ  
(A.12) 𝐸ܣሺͲሻ= ∫݀𝑥ݒଶሺͲ, 𝒙ሻܣሺͲ, 𝒙ሻ ;    𝐸ܤሺͲሻ= ∫݀𝑥ݑଶሺͲ, 𝒙ሻܤሺͲ, 𝒙ሻ  
 
(A.13) It is easy to show that initial values (A.11-A.13) define solution for aggregate Assets A(t) as: ܣሺݐሻ= ܣሺͳሻ+ ܣሺʹሻcos ߱ݐ+ ܣሺ͵ሻsin ߱ݐ+ ܣሺͶሻ݁𝑥݌ሺ𝛾௘ݐሻ+ ܣሺͷሻ݁𝑥݌ሺ−𝛾௘ݐሻ (A.14) A(j) for j=1,..5 are constants that are defined by (A.11-A.13) and simple solutions of Equations (A.2; A.5; A10) and we omit here exact derivation for brevity. Aggregate Assets A(t) and  Revenue-on-Assets B(t) takes form similar to (A.14) with its own constants B(j), j=1,..5. ܤሺݐሻ= ܤሺͳሻ+ ܤሺʹሻcos ߱ݐ+ ܤሺ͵ሻsin ߱ݐ+ ܤሺͶሻ݁𝑥݌ሺ𝛾௘ݐሻ+ ܤሺͷሻ݁𝑥݌ሺ−𝛾௘ݐሻ (A.15) Main conclusion: simple assumptions on exponential growth of aggregate Assets and Revenue-on-Assets energy with increment 𝛾௘ and oscillations of XAP(t) and YAP(t) with frequency ߱ induce fluctuations of aggregate Assets with frequency ߱ around exponential growth trend. Aggregate assets growth and fluctuations don’t depend directly on conjugate variable Revenue-on-Assets, but on hidden variables XAP(t), YAP(t), EA(t), EB(t)  that are alike to hydrodynamic impulses and energies. 24 Appendix B Assets and Revenue-on-Assets mean risks X(t) and Y(t) Derivations of Assets mean risks follow same scheme as Appendix A. Due to equation (5.3): ௗ ௗ௧ࢄሺݐሻܣሺݐሻ= ∫݀𝑥𝒙 
𝜕஺ 𝜕௧= − ∫݀𝑥𝒙 𝛻∙ሺ࢜ܣሻ+ ܽ∫݀𝑥𝒙 ሺ 𝒙 ∙ࡼ࡮ሻ 
 
(B.1) Let take first integral in the right side by parts and take into account that integral by divergence over e-space equals zero: − ∫݀𝑥𝒙 𝛻∙ሺ࢜ܣሻ= ∫݀𝑥࢜ܣሺݐ, 𝑥ሻ= ࡼ࡭ሺݐሻ ௗ ௗ௧ࢄሺݐሻܣሺݐሻ= ࡼ࡭ሺݐሻ+ ܽ ࢅࡼሺݐሻ ;   
ௗ ௗ௧ࢅሺݐሻܤሺݐሻ= ࡼ࡮ሺݐሻ+ ܾ ࢄࡼሺݐሻ  
 
(B.2) ࡼ࡭ሺݐሻ= ∫݀𝑥 ࡼ࡭ሺݐ, 𝑥ሻ= ∫݀𝑥࢜ܣሺݐ, 𝑥ሻ  ;  ࡼ࡮ሺݐሻ= ∫݀𝑥 ࡼ࡮ሺݐ, 𝑥ሻ= ∫݀𝑥࢛ܤሺݐ, 𝑥ሻ   (B.3) ࢄࡼሺݐሻ= ∫݀𝑥𝒙 ( 𝒙 ∙ࡼ࡭ ሺݐ, 𝑥ሻ)   ;    ࢅࡼሺݐሻ= ∫݀𝑥𝒙 ( 𝒙 ∙ࡼ࡮ ሺݐ, 𝑥ሻ) 
 
(B.4) Equations on aggregate Assets impulse PA(t) appears from (3.4) ݀
݀ݐࡼ࡭ሺݐሻ= ∫݀𝑥 ߲ࡼ࡭
߲ݐ= − ∫݀𝑥𝛻∙ሺ࢜ ࡼ࡭ሻ+ ܿ∫݀𝑥  ࡼ࡮ First integral in the right side equals zero and equations take form: ௗ ௗ௧ࡼ࡭ሺݐሻ= ܿ  ࡼ࡮ሺݐሻ   ;  
ௗ ௗ௧ࡼ࡮ሺݐሻ= ݀  ࡼ࡭ሺݐሻ  
 
 
 
 
(B.5) To derive equations on XP(t) and YP(t) let use equations (3.4): ௗ ௗ௧ࢄࡼሺݐሻ= ∫݀𝑥𝒙 ቀ𝒙∙
𝜕ࡼ࡭ 𝜕௧ቁ= −∫݀𝑥𝒙 (𝒙∙𝛻∙ሺ࢜ ࡼ࡭ሻ) +  ܿࢅࡼሺݐሻ 
 
(B.6.1) ௗ ௗ௧ࢅࡼሺݐሻ= −∫݀𝑥𝒙 (𝒙∙𝛻∙ሺ࢛ ࡼ࡮ሻ) +  ݀ ࢄࡼሺݐሻ  
 
 
 
(B.6.2) We omit long but simple calculations of integral in (B.6.2) and present the result as: ∫݀𝑥 𝒙 ሺ𝒙 ∙ሺ∇∙(࢜ ࡼ࡭ሺݐ, 𝒙ሻ)ሻ= −∫݀𝑥 𝒙 𝐸ܣሺݐሻ−∫݀𝑥࢜ ሺ𝒙∙ࡼ࡭ሺݐ, 𝒙ሻሻ  
(B.7) (B.7) define new factors: ࢄ𝑬ሺݐሻ= ∫݀𝑥𝒙 𝐸ܣሺݐ, 𝑥ሻ  ;    ࢅ𝑬ሺݐሻ= ∫݀𝑥𝒙 𝐸ܤሺݐ, 𝑥ሻ 
 
 
 
(B.8.1) ࢂࢄࡼ࡭ሺݐሻ= ∫݀𝑥࢜ሺݐ, 𝒙ሻ (𝒙∙ࡼ࡭ሺݐ, 𝒙ሻ) = ∫݀𝑥 ࢂࢄࡼ࡭ሺݐ, 𝒙ሻ 
 
 
(B.8.2) ࢁࢅࡼ࡮ሺݐሻ= ∫݀𝑥࢛ሺݐ, 𝒙ሻ (𝒙∙ࡼ࡮ሺݐ, 𝒙ሻ) = ∫݀𝑥 ࢁࢅࡼ࡮ሺݐ, 𝒙ሻ 
 
 
(B.8.3) Equations (B.6.1; B.6.2) take form: ௗ ௗ௧ࢄࡼሺݐሻ= ࢄ𝑬ሺݐሻ+ ࢂࢄࡼ࡭ሺݐሻ+  ܿ ࢅࡼሺݐሻ  
 
 
 
 
(B.9.1) ௗ ௗ௧ࢅࡼሺݐሻ= ࢅ𝑬ሺݐሻ+ ࢁࢅࡼ࡮ሺݐሻ+  ݀ ࢄࡼሺݐሻ  
 
 
 
 
(B.9.2) Equations (A.8) on Assets energy allow derive equations on ࢄ𝑬ሺݐሻ и ࢅ𝑬ሺݐሻ: 25 ௗ ௗ௧ࢄ𝑬ሺݐሻ= ∫݀𝑥𝒙 
𝜕𝐸஺ 𝜕௧= − ∫݀𝑥𝒙 𝛻∙ሺ࢜𝐸ܣሻ+ ܿ௘ࢅ𝑬ሺݐሻ  
 
(B.10) Alike to equations (A.5) or (B.6.1) first integral in the right side can be transformed to −∫݀𝑥𝒙 𝛻∙ሺ࢜𝐸ܣሻ= ∫݀𝑥 ࢜ 𝐸ܣሺݐ, 𝒙ሻ= ∫݀𝑥 ࡼ𝑬࡭ሺݐ, 𝒙ሻ factor in the right side is similar to energy impulse ࡼ𝑬࡭ሺݐ, 𝒙ሻ=  ࢜ሺݐ, 𝒙ሻ 𝐸ܣሺݐ, 𝒙ሻ   ;   ࡼ𝑬࡮ሺݐ, 𝒙ሻ=  ࢛ሺݐ, 𝒙ሻ 𝐸ܤሺݐ, 𝒙ሻ    
 
(B.11) Let complement initial equations (3.3; 3.4; A.8) with hydrodynamic-like equations on energy impulse as: 𝜕ࡼ𝑬࡭ 𝜕௧+ 𝛻∙ሺ࢜ࡼ𝑬࡭ሻ= ܿ𝑝௘ ࡼ𝑬࡮ሺݐ, 𝒙ሻ ; 
𝜕ࡼ𝑬࡮ 𝜕௧
+ 𝛻∙ሺ࢛ ࡼ𝑬࡮ሻ= ݀𝑝௘ ࡼ𝑬࡭ሺݐ, 𝒙ሻ 
(B.12) Here we assume that energy impulses PEA(t,x) and PEB(t,x) fluctuate with frequency ߱𝑝௘ due to same reasons as fluctuations of Assets impulse PA(t,x) with frequency ߱ (5.6): ߱𝑝௘
ଶ= −ܿ௘𝑝 ݀௘𝑝> Ͳ 
 
 
 
 
 
 
 
 
(B.13) Equations (B.12) allow derive equations on aggregate energy impulse PEA(t) and PEB(t) as: ࡼ𝑬࡭ሺݐሻ= ∫݀𝑥ࡼ𝑬࡭ሺݐ, 𝒙ሻ   ;  ࡼ𝑬࡮ሺݐሻ= ∫݀𝑥ࡼ𝑬࡮ሺݐ, 𝒙ሻ  
 
 
(B.14) ௗ ௗ௧ࡼ𝑬࡭ሺݐሻ= ܿ𝑝௘ࡼ𝑬࡮ሺݐሻ  ;   
ௗ ௗ௧ࡼ𝑬࡮ሺݐሻ= ݀𝑝௘ࡼ𝑬࡭ሺݐሻ  
 
 
 
(B.15) Equations (B.15) describe oscillations of PEA(t) and PEB(t) with frequency ߱𝑝௘. Thus equations (B.10) take form: ௗ ௗ௧ࢄ𝑬ሺݐሻ= ࡼ𝑬࡭ሺݐሻ+ ܿ௘ࢅ𝑬ሺݐ, 𝑥ሻ  ;   
ௗ ௗ௧ࢅ𝑬ሺݐሻ= ࡼ𝑬࡮ሺݐሻ+ ݀௘ࢄ𝑬ሺݐ, 𝑥ሻ  
(B.16) To define equations (B.9.2; B.9.3) on ࢄࡼሺݐሻ, ࢅࡼሺݐሻ let describe factors VXPA and UYPB (B.8.2; B.8.3). We propose that these factors follow similar fluctuations as impulses PA(t,x) and PB(t,x) that are described by (5.4; B.5) or factors PEA and PEB (B.12-B.15). Such fluctuations are induced my motion of e-particles from low risk to high-risk area and back on e-space. Let take equations on ࢂࢄࡼ࡭ሺݐ, 𝒙ሻ and ࢁࢅࡼ࡮ሺݐ, 𝒙ሻ as: 𝜕ࢂࢄࡼ࡭
𝜕௧
+ 𝛻∙ሺ࢜ ࢂࢄࡼ࡭ሻ= ܿ௩ࢁࢅࡼ࡮ሺݐ, 𝒙ሻ ; 
𝜕ࢁࢅࡼ࡮
𝜕௧
+  𝛻∙ሺ࢛ ࢁࢅࡼ࡮ሻ= ݀௩ࢂࢄࡼ࡭ሺݐ, 𝒙ሻ 
(B.17.1) ௗ ௗ௧ࢂࢄࡼ࡭ሺݐሻ= ܿ௩  ࢁࢅࡼ࡮ሺݐሻ   ;  
ௗ ௗ௧ࢁࢅࡼ࡮ሺݐሻ= ݀௩  ࢂࢄࡼ࡭ሺݐሻ 
 
 
(B.17.2) ߱௩
ଶ= −ܿ௩݀௩> Ͳ 
 
 
 
 
 
 
 
(B.17.3) Then equations on XP(t) and YP(t) (B.9.2; B.9.3) are determined. Hence we obtain system of equations (B.2; B.5; B.9.1; B.9.2; B.15; B.16; B.17.2) on Assets mean risk X(t) and on Revenue-on-Assets mean risk Y(t). Let define initial values as: ࢄሺͲሻܣሺͲሻ= ∫݀𝑥𝒙 ܣሺͲ, 𝒙ሻ   ;    ࢅሺͲሻܤሺͲሻ= ∫݀𝑥𝒙 ܤሺͲ, 𝒙ሻ 
 
 
(B.18.1) ࡼ࡭ሺͲሻ= ∫݀𝑥࢜ሺͲ, 𝒙ሻ ܣሺͲ, 𝒙ሻ   ;    ࡼ࡮ሺͲሻ= ∫݀𝑥࢛ሺͲ, 𝒙ሻ ܤሺͲ, 𝒙ሻ  
 
(B.18.2) 26 ࢄࡼሺͲሻ= ∫݀𝑥𝒙  𝒙 ∙࢜ሺͲ, 𝒙ሻܣ ሺͲ, 𝒙ሻ   ;    ࢅࡼሺͲሻ= ∫݀𝑥𝒙  𝒙 ∙࢛ሺͲ, 𝒙ሻܤ ሺͲ, 𝒙ሻ 
(B.18.3) ࡼ𝑬࡭ሺͲሻ= ∫݀𝑥࢜ሺͲ, 𝒙ሻ𝐸ܣሺͲ, 𝒙ሻ  ;  ࡼ𝑬࡮ሺͲሻ= ∫݀𝑥࢛ሺͲ, 𝒙ሻ𝐸ܤሺͲ, 𝒙ሻ  
 
(B.18.4) ࢄ𝑬ሺͲሻ= ∫݀𝑥𝒙 ݒଶሺͲ, 𝑥ሻ ܣሺͲ, 𝑥ሻ;    ࢅ𝑬ሺݐሻ= ∫݀𝑥𝒙 ݑଶሺͲ, 𝑥ሻ ܤሺͲ, 𝑥ሻ  
 
(B.18.5) ࢂࢄࡼ࡭ሺͲሻ= ∫݀𝑥 ࢜ሺͲ, 𝒙ሻ(𝒙∙ࡼ࡭ሺͲ, 𝒙ሻ) ; ࢁࢅࡼ࡮ሺͲሻ= ∫݀𝑥 ࢛ሺͲ, 𝒙ሻ(𝒙∙ࡼ࡮ሺͲ, 𝒙ሻ)  (B.18.6) We omit long but simple calculations that allow derive solution as ࢄሺݐሻܣሺݐሻ= ࢄ࡭ሺͳሻ+ ࢄ࡭ሺʹሻݏ݅݊߱ݐ+ ࢄ࡭ሺ͵ሻܿ݋ݏ߱ݐ+ ࢄ࡭ሺͶሻܿ݋ݏ߱𝑝௘ݐ+ ࢄ࡭ሺͷሻ ݏ݅݊߱𝑝௘ݐ+ ࢄ࡭ሺ͸ሻܿ݋ݏ߱௩ݐ+ ࢄ࡭ሺ͹ሻ ݏ݅݊߱௩ݐ+ ࢄ࡭ሺͺሻ݁𝑥݌ሺ𝛾௘ݐሻ+ ࢄ࡭ሺͻሻ݁𝑥݌ሺ−𝛾௘ݐሻ 
 
        (B.19) with constants XA(j), j=1,..9 determined by initial values (B.18.1-6). Aggregate Assets A(t) (A.14) has same exponential trend with increment 𝛾௘ thus mean risks X(t) don’t grow up but follow fluctuations with frequencies ߱ and ߱𝑝௘ and ߱௩ . Revenue-on-Assets mean risk Y(t) is determined by relations similar to (B.19) and (A.15). 27 Appendix C Assets Mean Squares Risks 𝐗𝟐
̅
̅
̅
̅ሺ𝐭ሻ and Dispersions Let take Assets and Revenue-on-assets mean square risks ܺଶ
̅
̅
̅
̅ሺݐሻ and ܻଶ
̅̅̅ሺݐሻ as ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ= ∫݀𝑥 𝑥ଶ ܣሺݐ, 𝑥ሻ   ;    ܻଶ
̅̅̅ሺݐሻܤሺݐሻ= ∫݀𝑥 𝑥ଶ ܤሺݐ, 𝑥ሻ     
 
(C.1) Then Assets dispersion 𝜎
஺
ଶሺݐሻ= ܺଶ
̅
̅
̅
̅ሺݐሻ−ܺଶሺݐሻ 
 
 
 
 
 
 
 
(C2) Assets dispersion describes width of Assets distribution on e-space. Growth of (C.2) indicates increase of spreading Assets over risks on e-space. Small Assets dispersion means that most Assets of macroeconomics are clustered near mean Assets risks X(t). To describe Assets dispersion let derive equations on Assets mean squares risks. Let use (C.1) and (5.3): ௗ ௗ௧ ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ= −∫݀𝑥 ∑𝑥௜
ଶ
௜
 𝛻∙ሺݒܣሻ+  ܽ∫݀𝑥 ∑𝑥௜
ଶ
௜
  𝒙 ∙ࡼ࡮  
 
(C3) For each i first integral in the right side equals ∫݀𝑥 𝑥௜
ଶ 𝛻
௝∙ሺݒܣሻ= ∫݀𝑥௜𝑥௜
ଶ ∫݀𝑥௞≠௜𝛻
௝∙ሺݒܣሻ+ ∫݀𝑥𝑥௜
ଶ𝛻௜∙ሺݒܣሻ As we mentioned above integral over divergence equals zero and ∫݀𝑥𝑥௜
ଶ𝛻௜∙ሺݒܣሻ= ∫݀𝑥௞≠௜ ∫݀𝑥௜𝑥௜
ଶ𝛻௜∙ሺݒ௜ܣሻ= −ʹ ∫݀𝑥𝑥௜∙ݒ௜ܣ Thus obtain ∫݀𝑥𝑥ଶ𝛻∙ሺݒܣሻ= −ʹ ∫݀𝑥 𝒙 ∙࢜ሺݐ, 𝒙ሻ ܣሺݐ, 𝒙ሻ= −ʹ ܺܲܣሺݐሻ 
 
 
(C4) (A.3; A.5) define XPA(t) and YPA(t). Equations (C.3) take form: ௗ ௗ௧ ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ= ʹ ܺܲܣ+  ܻܽܲܤܻଶሺݐሻ   ;  
ௗ ௗ௧ ܻଶ
̅̅̅ሺݐሻܤሺݐሻ= ʹ ܻܲܤ+  ܾܺܲܣܺଶሺݐሻ (C.5) ܺܲܣܺଶሺݐሻ= ∫݀𝑥 𝑥ଶ  𝒙 ∙ࡼ࡭ሺݐ, 𝒙ሻ   ;  ܻܲܤܻଶሺݐሻ= ∫݀𝑥 𝑥ଶ  𝒙 ∙ࡼ࡮ሺݐ, 𝒙ሻ 
(C.6) To derive equations on ܺܲܣܺଶሺݐሻ and ܻܲܤܻଶሺݐሻ let use (5.4): ௗ ௗ௧ܺܲܣܺଶሺݐሻ= −∫݀𝑥𝑥ଶ  𝒙 ∙ሺ𝛻∙ሺ࢜ ࡼ࡭ሻሻ+ ܿ ܻܲܤܻଶሺݐሻ  
 
 
(C.7.1) We omit long but simple evaluation of integral in (C.7.1) and present the result as: −∫݀𝑥𝑥ଶ  𝒙 ∙ሺ𝛻∙ሺ࢜ ࡼ࡭ሻሻ= ∫݀𝑥 𝑥ଶ 𝐸ܣሺݐ, 𝒙ሻ+ ʹ ∫݀𝑥 ሺ𝒙∙࢜ሻ𝟐ܣ ܺଶ𝐸ܣሺݐሻ= ∫݀𝑥 𝑥ଶ𝐸ܣሺݐ, 𝒙ሻ    ;   ܻଶ𝐸ܤሺݐሻ= ∫݀𝑥 𝑥ଶ𝐸ܤሺݐ, 𝒙ሻ   
 
 
(C.7.2) ܸܺܣሺݐሻ= ∫݀𝑥 ሺ𝒙∙࢜ሺݐ, 𝒙ሻሻ𝟐ܣሺݐ, 𝒙ሻ= ∫݀𝑥 ܸܺܣሺݐ, 𝒙ሻ 
 
 
 
(C.7.3) ܻܷܤሺݐሻ= ∫݀𝑥 ሺ𝒙∙࢛ሺݐ, 𝒙ሻሻ𝟐ܤሺݐ, 𝒙ሻ= ∫݀𝑥  ܻܷܤሺݐ, 𝒙ሻ  
 
 
(C.7.4) Thus equation (C.7.1) takes form: 28 ௗ ௗ௧ܺܲܣܺଶሺݐሻ= ܺଶ𝐸ܣሺݐሻ+ ʹܸܺܣሺݐሻ+ ܿ ܻܲܤܻଶሺݐሻ 
 
 
 
(C.8.1) ௗ ௗ௧ܻܲܤܻଶሺݐሻ= ܻଶ𝐸ܤሺݐሻ+ ʹܻܷܣሺݐሻ+ ݀ ܺܲܣܺଶሺݐሻ 
 
 
 
(C.8.2) To derive equations on X2EA(t) and Y2EB(t) let use equations (A.8): ݀
݀ݐܺଶ𝐸ܣሺݐሻ= −∫݀𝑥 𝑥ଶ𝛻∙ሺ࢜𝐸ܣሻ+ ܿ௘ ܻଶ𝐸ܤሺݐሻ ∫݀𝑥∑𝑥௜
ଶ  𝛻∙ሺ࢜𝐸ܣሻ= ∫݀𝑥 ∑𝑥௜
ଶ 𝛻
௝≠௜∙(ݒ௝ 𝐸ܣ) + ∫݀𝑥∑𝑥௜
ଶ  𝛻௜∙ሺݒ௜ 𝐸ܣሻ First integral by divergence equals zero and second integral by parts gives: ∫݀𝑥∑𝑥௜
ଶ  𝛻௜∙ሺݒ௜ 𝐸ܣሻ= −ʹ ∫݀𝑥 𝒙∙࢜ 𝐸ܣሺݐ, 𝑥ሻ= −ʹ ∫݀𝑥 𝒙∙ࡼ𝑬࡭ሺݐ, 𝑥ሻ Thus obtain equations on X2EA(t) and Y2EB(t): ௗ ௗ௧ܺଶ𝐸ܣሺݐሻ= ʹܺܲ𝐸ܣሺݐሻ+ ܿ௘ܻଶ𝐸ܤሺݐሻ ; 
ௗ ௗ௧ܻଶ𝐸ܤሺݐሻ= ʹܻܲ𝐸ܤሺݐሻ+ ݀௘ܺଶ𝐸ܣሺݐሻ 
    (C.9) ܺܲ𝐸ܣሺݐሻ= ∫݀𝑥 𝒙∙ࡼ𝑬࡭ሺݐ, 𝑥ሻ  ;      ܻܲ𝐸ܤሺݐሻ= ∫݀𝑥 𝒙∙ࡼ𝑬࡮ሺݐ, 𝑥ሻ  
 
(C.10.1) To derive equations on XPEA(t) and YPEB(t) let use equations (B.12) on energy impulse PEA(t,x) and PEB(t,x). Equations on XPEA(t) and YPEB(t) take form: ௗ ௗ௧ܺܲ𝐸ܣሺݐሻ= −∫݀𝑥 𝒙∙ 𝛻∙ሺ࢜ࡼ𝑬࡭ሻ+ ܿ𝑝௘ ܻܲ𝐸ܤሺݐሻ  
 
 
 
(C.10.2) ∫݀𝑥 𝒙∙ 𝛻∙ሺ࢜ࡼ𝑬࡭ሻ= ∫݀𝑥 𝑥௝ 𝛻௜≠௝ ݒ௜ݒ௝𝐸ܣሺݐ, 𝒙ሻ+ ∫݀𝑥 𝑥௝ 𝛻
௝ݒ௝
ଶ 𝐸ܣሺݐ, 𝒙ሻ First integral by divergence equals zero and second integral by parts gives ∫݀𝑥 𝑥௝ 𝛻
௝ݒ௝
ଶ 𝐸ܣሺݐ, 𝒙ሻ= −∫݀𝑥ݒ௝
ଶ 𝐸ܣሺݐ, 𝒙ሻ= −∫݀𝑥ሺݒଶሺݐ, 𝒙ሻሻଶܣሺݐ, 𝒙ሻ Let denote: ݒ4ܣሺݐሻ= ∫݀𝑥ሺݒଶሺݐ, 𝒙ሻሻଶܣሺݐ, 𝒙ሻ=  ∫݀𝑥ݒ4ܣሺݐ, 𝒙ሻ  
 
 
 
(C.10.3) ݑ4ܤሺݐሻ= ∫݀𝑥ሺݑଶሺݐ, 𝒙ሻሻଶܤሺݐ, 𝒙ሻ= ∫݀𝑥ݑ4ܤሺݐ, 𝒙ሻ  
 
 
 
(C.10.4) New variables ݒ4ܣሺݐ, 𝒙ሻ and ݑ4ܤሺݐ, 𝒙ሻ are proportional to forth order of velocity ݒ4 and Assets density A(t,x). Equations on XPEA(t) and YPEB(t) take form: ௗ ௗ௧ܺܲ𝐸ܣሺݐሻ= ݒ4ܣሺݐሻ+ ܿ𝑝௘ ܻܲ𝐸ܤሺݐሻ  ;  
ௗ ௗ௧ܻܲ𝐸ܤሺݐሻ= ݑ4ܤሺݐሻ+ ܿ𝑝௘ ܺܲ𝐸ܣሺݐሻ    (C.11) Let introduce equations on variables ݒ4ܣሺݐ, 𝒙ሻ and ݑ4ܤሺݐ, 𝒙ሻ similar to equations (A.8) on Assets energy and assume that ݒ4ܣሺݐሻ and ݑ4ܤሺݐሻ should grow up as exponent: 𝜕௩4஺ 𝜕௧+ 𝛻∙ሺ࢜ݒ4ܣሻ= ܿ௨ ݑ4ܤሺݐ, 𝑥ሻ ;  
𝜕௨4஻ 𝜕௧+ 𝛻∙ሺ࢛ ݑ4ܤሻ= ݀௩ ݒ4ܣሺݐ, 𝑥ሻ  
(C.12.1) ௗ ௗ௧ݒ4ܣሺݐሻ= ܿ௨௘ ݑ4ܤሺݐሻ   ;    
ௗ ௗ௧ݑ4ܤሺݐሻ= ܿ௩௘ ݒ4ܣሺݐሻ 
 
 
 
(C.12.2) 𝛾ଶ
௩௨= ܿ௨݀௩> Ͳ 
 
 
 
 
 
 
 
 
(C.12.3) 29 Equations (C.12.1-3) describe exponential growth of ݒ4ܣሺݐሻ and ݑ4ܤሺݐሻ alike to equations (A.10). To close system of equations let define XVA and YUB (C.7.3; C.7.4). For simplicity let assume that these factors are described by equations alike to (A.8; A.10): 𝜕 𝜕௧ܸܺܣሺݐ, 𝒙ሻ+ 𝛻∙ሺ࢜ሺݐ, 𝒙ሻܸܺܣሻ= ܿ𝑥௩ ܻܷܤሺݐ, 𝒙ሻ  
 
 
 
(C.13.1) 𝜕 𝜕௧ܻܷܤሺݐ, 𝒙ሻ+ 𝛻∙ሺ࢛ሺݐ, 𝒙ሻܻܷܤሻ= ݀𝑥௩ ܸܺܣሺݐ, 𝒙ሻ  
 
 
 
(C.13.2) Then equations on ܸܺܣሺݐሻ and YUB(t) take form: ௗ ௗ௧ܸܺܣሺݐሻ= ܿ𝑥௩ ܻܷܤሺݐሻ     ;    
ௗ ௗ௧ܻܷܤሺݐሻ= ݀𝑥௩ ܸܺܣሺݐሻ   
 
 
(C.14) 𝛾𝑥௩
ଶ= ܿ𝑥௩݀𝑥௩> Ͳ 
 
 
 
 
 
 
 
 
(C.15) Equations (A.5; A.10; C.5; C.8.1; C.8.2; C.9; C.11; C.12.2; C.14) describe mean squares ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ and ܻଶ
̅̅̅ሺݐሻܤሺݐሻ and all variables that determine these factors: XPA(t), YPB(t), EA(t), EB(t), XPAX2(t), YPBY2(t), X2EA(t), Y2EB(t), XPEA(t), YPEB(t), ݒଶ𝐸ܣሺݐሻ, ݑଶ𝐸ܤሺݐሻ XVA(t) and YUB(t).We omit long but simple calculations that allow derive solution of system and present general result as ܺଶ
̅
̅
̅
̅ሺݐሻܣሺݐሻ= ܺଶ
̅
̅
̅
̅ܣሺͲሻ+ ܺଶ
̅
̅
̅
̅ܣሺͳሻsin߱ݐ+ ܺଶ
̅
̅
̅
̅ܣሺʹሻcos ߱ݐ+ ܺଶ
̅
̅
̅
̅ܣሺ͵ሻsin ߱𝑝௘ݐ + ܺଶ
̅
̅
̅
̅ܣሺͶሻexp 𝛾௘ݐ+ ܺଶ
̅
̅
̅
̅ܣሺͷሻ exp −𝛾௘ݐ+ ܺଶ
̅
̅
̅
̅ܣሺ͸ሻ exp 𝛾௩௨ݐ+ ܺଶ
̅
̅
̅
̅ܣሺ͹ሻ exp −𝛾௩௨ݐ+ ܺଶ
̅
̅
̅
̅ܣሺͺሻ exp 𝛾𝑥௩ݐ+ ܺଶ
̅
̅
̅
̅ܣሺͻሻ exp −𝛾𝑥௩ݐ 
 
 
 
 
 
 
 
 
(C.16) with constants ܺଶ
̅
̅
̅
̅ܣሺ݆ሻ, j=0,..9 defined by initial value of variables (A.12; A.13; C.6; C.7.2; C.7.3; C.7.4; C.10.1; C.10.3; C.10.4) for t=0. A(t) has form (A.14) with exponential increment 𝛾௘. As ܺଶ
̅
̅
̅
̅ሺݐሻ can’t grow up as exponent hence it is required that (A.12; A.13; C.6; C.7.2; C.7.3; C.7.4; C.10.1; C.10.3; C.10.4)𝛾௘> 𝛾௩௨  ;   𝛾௘> 𝛾𝑥௩ Revenue-on-Assets mean square risk Yଶ
̅̅̅ሺtሻBሺtሻ is defined by relations similar to (C.16) with constants defined by initial value of variables (A.12; A.13; C.6; C.7.2; C.7.3; C.7.4; C.10.1; C.10.3; C.10.4). 30",0
"We review a numerical technique, referred to as the Transport-based Mesh-free Method (TMM),
and we discuss its applications. We recently introduced this method from a numerical standpoint and
investigated the accuracy of integration formulas based on the Monte-Carlo methodology: quantitative
error bounds were discussed and, in this short note, we outline the main ideas of our approach.
The techniques of transportation and reproducing kernels lead us to a very eﬃcient methodology for
numerical simulations in many practical applications, and provide some light on the methods used by
the artiﬁcial intelligence community. For applications in the ﬁnance industry, our method allows us
to compute many types of risk measures with accurate and fast algorithms. We propose theoretical
arguments as well as extensive numerical tests in order to justify sharp convergence rates, leading to
rather optimal computational times. Cases arising in ﬁnance applications support our claims and,
ﬁnally, the problem of the curse of dimensionality in ﬁnance is brieﬂy discussed. 1
Introduction Relying on our recent papers [11]–[13], we present
and discuss here a numerical technique, that we
refer to as the Transport-based Mesh-free Method
(TMM), which is of direct interest in numerical sim-
ulations. Our method is mesh-free (cf. for instance
[7, 16]) and somewhat similar to a Lagrangian mesh-
free method. Importantly, our method can handle
transport as well as diﬀusive terms and was intro-
duced ﬁrst in [11].
Our motivation was to reduce as much as pos-
sible the algorithmic burden of solving partial dif-
ferential equations (PDEs) especially for problems
in large dimensions, met for instance in mathemat-
ical ﬁnance and machine learning. Computational
times reﬂect, in a concrete manner, the complexity
of an algorithm. For PDEs solvers, the algorithmic
complexity can be measured by establishing suitable
error estimates. For our TMM approach, in [12] we
were able to establish some Monte-Carlo type error
estimates, at least via heuristic arguments, as we
outline below in Section 2.
This allowed to perform a precise error analysis
of this method in [13], which is outlined in Section 5. Finally, in Section 6 we discuss the limitations com-
ing from the curse of dimensionality for applications
to ﬁnance.
The TMM methodology has wide applications in
mathematical ﬁnance, since it allows one to compute
almost any risk measures, quite accurately and with
a fast algorithm. A risk measure is here understood
as a price, future prices, future sensitivities, Value
at Risk (VaR), or Counterparty Value Adjustment
(CVA), and may concern a simple asset, a complex
derivative or an investment strategy, as well as a big
portfolio of such instruments; they can be written
on any number of underlyings, themselves depend-
ing on any Markov-type stochastic processes.
The proposed method was extensively tested in
mathematical ﬁnance ones; see [11]-[13] as well as
[14] for a business case in asset and liability man-
agement using the so-called Libor market model [2].
Another business case, for front-oﬃce equity deriva-
tives, was treated using this method: it consists in
computing metrics for speciﬁc customer needs for
big portfolios of autocalls, that are useful for pre-
sales purposes.
The modeling of shares uses the
Buelher dividend models [3], and the algorithm de-
scribed in [11] for local volatility calibration. For an ∗Laboratoire Jacques-Louis Lions, Centre National de la Recherche Scientiﬁque, Sorbonne Universit´
e, 4 Place Jussieu,
75252 Paris, France. Email: contact@philippelefloch.org.
†MPG-Partners, 136 Boulevard Haussmann, 75008 Paris, France. Email: jean-marc.mercier@mpg-partners.com. 1 arXiv:1911.00992v2  [math.AP]  17 Nov 2019 application to nonlinear propagation, see [10]-[13]. 2
Monte-Carlo-type strategy We postpone the discussion of earlier references at
end of this section and outline now our strategy for
deriving a priori error estimates on multidimensional
integrals. One of our task is to investigate the valid-
ity of Monte-Carlo-type error estimates of the form Z RD ϕ(x)dµ−1 N X 1≤n≤N
ϕ(yn)


 ≤EK(Y, N, D)∥ϕ∥HK. (2.1)
Here, µ ∈P(RD) is a probability measure (whose
support
supp(µ)
must
be
convex)
and
Y
=
(y1, . . . , yN) is a set of N distinct points in RN. We
have denoted here by HK a kernel-based Hilbert
space depending upon the choice of an admissi-
ble kernel K, that is, a continuous and symmet-
ric function K : (x, y) ∈RD × RD 7→R with
K(x, y) = K(y, x).
Admissibility means that the
matrix K(Y, Y ) :=
",0
"Accuracy of economic theories and efficiency of economic policy strictly depend on the choice of the economic variables and processes mostly liable for description of economic reality. That states the general problem of assessment of any possible economic variables and processes chargeable for economic evolution. We show that economic variables and processes described by current economic theories constitute only a negligible fraction of factors responsible for economic dynamics. We consider numerous unnoted economic variables and overlooked economic processes those determine the states and predictions of the real economics. We regard collective economic variables, collective transactions and expectations, mean risks of economic variables and transactions, collective velocities and flows of economic variables, transactions and expectations as overlooked factors of economic evolution. We introduce market-based probability of the asset price and consider unnoticed influence of market stochasticity on randomness of macroeconomic variables. We introduce economic domain composed by continuous numeric risk grades and outline that the bounds of the economic domain result in unnoticed inherent cyclical motion of collective variables, transactions and expectations those are responsible for observed business cycles. Our treatment of unnoticed and overlooked factors of theoretical economics and policy decisions preserves a wide field of studies for many decades for academic researchers, economic authorities and high-level politicians. Keywords: theoretical economics, macroeconomic variables, market trade, expectations, economic policy JEL: This research received no support, specific grant or financial assistance from funding 
agencies in the public, commercial or nonprofit sectors. We welcome funding our studies. 2 1. Introduction Aspirations to find “correct” economic theory that should describe and predict markets growth, price change, economic development, employment demand and etc., etc., result endless economic research. The similar aspirations call for “genuine” economic policy that could solve all problems, improve all economic failures and give birth for economic prosperity. Both tied – the economic policy is verified by economic theory and the economic theory is permanently adjusted by economic policy. What is the possible origin of both endless failures? Early economic studies (Cantillon, 1755; Cournot, 1838; Clark, 1915) look absolutely contemporary and discuss the same issues and problems that are considered by modern researchers. “The Price and Intrinsic Value of a Thing; The Circulation and Exchange of Goods and Merchandise; Production carried on by Entrepreneurs at a risk; Money and Interest; Market Prices; The Circulation of Money; International Trade and Business Cycles; Banks and their Credit” – these are the Chapters of Cantillon (1755) written 265 years ago. “On the Law of Demand; On the Influence of Taxation on Commodities; On the Competition of Producers; On the Communication of Markets” – are the Chapters of Cournot (1838). “Wealth and its Origin; Wages; The Law of Interest; Economic Dynamics” – are the Chapters of “Essentials Of Economic Theory” by Clark (1915). Since then, studies of economic and financial theory move far ahead (Keynes, 1936; Burns, 1954; Blaug, 1985; Dimson and Mussavian, 1999; Vines and Wills, 2018), but the catalog of economic subjects and the list of economic and financial variables under consideration remain almost the same. For sure contribution to the development of the economic and financial theory was made by numerous contemporary researches (Muth, 1961; Sharpe, 1964; Lucas, 1972; Leontief, 1973; Diebold, 1998; Campbell, 2000; Cochrane, 2001; Wickens, 2008; Shubik, 2011; Hansen, 2013). The list of references for sure doesn’t include many researchers but serves as illustration of great progress in economic and financial theories during last decades. The main subject of almost all economic theories concern investigation and modelling factors those impact evolution of few economic and financial variables: asset prices, economic growth, investment, business cycles, inflation and employment, economic risks and uncertainty, demand and supply relations and so on. Most economic policies pretend solve the same economic problems as well as all other, but quickly, chop-chop, before the next election time term (Blinder, 2019). Economic problems are really tough and allow various treatments and numerous solutions. However, above important economic problems don’t 3 deplete the questions, cases and issues that should be studied to make economic theory and policy more consistent with economic reality. We propose that one of the reasons of the long-term failure of economic theories and policies may be explained by the gap between the manifold of properties and processes that conduct real economics and the “short” list of economic variables and relations described by current economic models. Indeed, there are almost no chances to reach the ambition to model, forecast and mange real economics if one considers and uses only 2-3% of existing economic parameters and processes. We believe that it is reasonable consider general relations and issues that arise in real economics without hasty attempts select “key” economic variables and then derive their impact on economic notions of particular interest. Even politicians know that “key” issue today could be useless tomorrow. Thus excess attention of researchers to “key” economic properties and relations only has not much sense. We propose assess the list of economic variables and properties those describe economic evolution and study their general mutual relations. The number of economic and financial variables under consideration is enormous and many types and classes of economic variables and parameters elude attention of researchers, business and politicians. Such omissions cause wrong economic predictions, false investor’s expectations and mistaken economic policy followed by great financial losses of entire economy. In our paper we consider economic and financial variables and processes on equal basis without selection “key” or “significant” variables. We regard unnoticed economic parameters that are responsible for description of economic dynamics and uncertainty. Both are of great importance and we don’t waste time explaining why. Macroeconomic dynamics is determined by change of macroeconomic variables. It is of great importance to study the economic origin of factors those form and change macroeconomic variables. Econometrics provides perfect methods for assessment of the macroeconomic variables (Fox, et al. 2017). However, considerations of the processes those govern composition of the macroeconomic variables may deliver many surprises. Currently one considers economic variables of separate agents, particular industry and variables that describe macroeconomics as a whole, like investment and credits, supply and demand of the entire economy. However we believe that definitions of economic variables should reflect certain economic approximation. In this paper we propose definitions of economic variables those depend on the choice of numeric scales. It is impossible “exactly” describe real economy. Each economic theory and model should provide certain approximation, 4 simplification of the real economy. We argue why and how economic variables and processes determined by different numeric scales describe approximations of the economic evolution. Economics is a system with strong internal ties. Dynamics of the economic variables depend on economic uncertainty. Roots of economic uncertainty and ties with uncertainty of financial markets and market trade stochasticity establish complex puzzle that worth investigation. We consider the roots of economic uncertainty and uncover overlooked relations between uncertainty of financial markets and macroeconomic uncertainty. To explain these relations we complement the existing economic variables by additional sets of unnoticed variables and processes those describe uncertainty of financial markets and economic uncertainty as a whole. In the next section we consider the nature of economic variables. Economic agents perform all economic transactions under action of different risks. We propose replace current letter notations of risk grades by numeric continuous risk grades. We take that risk grades of n risks fill unit cube [0,1]n in Rn and call it economic domain. We introduce economic continuous media approximation in economic domain and describe collective economic variables as functions of risk coordinates. Transition from letter risk grades to numeric continuous risk grades uncovers unnoticed motions of economic agents in economic domain that generate flows of economic variables. In Sec. 3 we consider description of collective market trades and collective agents expectations and their flows in economic domain. We demonstrate that unnoticed flows of collective economic variables and market trades in bounded economic domain [0,1]n in Rn give origin of oscillations those usually notated as business cycles. In Sec.4 we consider market roots of economic uncertainty and introduce new market-based probability of the market price. Conclusion - in Sec.5. We believe that readers are familiar with modern economic theories, preliminaries of probability theory, partial differential equations and etc. We use roman A, B to denote scalars and bold P, υ, x for vector variables. 2. Economic agents and variables in economic domain Any economic theory describes processes and relations those obey approximations determined by definite scales. In particular, any economic theory approximates evolution of variables and relations between them that are averaged or aggregated during selected time interval Δ. For example, economic model can approximately describe variables aggregated during interval Δ that equals week, month, quarter, etc. Theory with averaging interval Δ that equals 1 month doesn’t model variations during hour, day or week. However, choice of time averaging interval Δ is not sufficient to develop adequate model of real economic processes. 5 One should consider at least one more scale – “space” scale that determines aggregation of economic agents and their variables by “space” parameter. Actually, common treatment of the economic variables those describe industry sectors is based on aggregation of variables, like energy consumption, investment, labor etc., of economic agents those belong to particular industry (Fox, et.al., 2017). Variables those describe entire economy are composed by aggregation of all economic agents. Economic parameters also can be formed by aggregation of agent’s variables those belong to similar age range, wealth level, gender, geographical position and etc. All these cases consider economic agents as initial bricks for definition of economic variables. Economic agents are distributed by parameters like industries, age, wealth, gender and etc. We would like attract attention to other parameters that are in use for decades. Indeed, during at least four decades major risk rating companies (S&P, 2014; Fitch, 2018; Moody’s, 2018) assess credit risk rating of large international banks and distribute banks by their risk grades. Each rating agency utilizes its own risk rating methodology and uses own risk grade system. Risk grades are noted by letters like: AAA, AA, BB, and etc. and list of risk grades may have 30-40 different letter notations. Moreover, rating agencies assess transition probabilities. Rating agencies assess probabilities that during selected time term (6 months, 1 year, etc.) credit rating of the particular bank can change from the current value to a different one and become more secure or more risky (Metz Cantor, 2007; Moody’s, 2009; Fitch, 2017; S&P, 2018). Simply speaking, risk transition matrices describe probable motion of economic agents – large international banks – from one risk grade to another during the selected time term. Above examples confirm that attributing economic agents by parameters like: particular sector of economy, age or wealth range, geographical position or risk rating – is a well- known conventional tool of business and economic research for decades. Attributing agents by parameters allow aggregate economic agents by different ranges of these parameters and develop different approximations of economic evolution. However, agent’s parameters mentioned above have certain common flaw. Indeed, their usage doesn’t allow enjoy benefits those can be achieved by description of economic agents and their variables by parameters or coordinates of some metric space that reflects properties of macroeconomics. We believe that usage of geographical positions of agents as “metric space” has almost no economic meaning. That is the key point of our treatment of theoretical economics. It is habitual that economic agents can be considered as primary bricks of the economic relations. Variety of economic agents and diversity of their economic and financial variables encourage aggregation of 6 agents by some parameters to assess their collective variables. That inspires distribution of agents by industries or economic sectors to assess collective variables of sectors. Such approximation helps model mutual impact of economic variables of different industry sectors. Distribution of agents by industries allows Wassily Leontief develop his Input- Output Analysis as a model of the world economy (Leontief, 1955; 1973). Distribution of agents by parameters as wealth, age, risk ratings and etc., allows study collective variables of groups of agents. Introduction of parameters those help aggregate agents within certain range of parameters establishes ground for definition of collective economic variables of group of agents. For example, collective energy consumption of agents those belong to same industry determines energy consumption of the particular industry. We believe that usage of risk ratings in a way similar to the current common practice (Fitch, 2017; 2018; Moody’s, 2009; 2018; S&P, 2014; 2018) can help describe economic evolution in economic metric spaces. Assessment of risk ratings of large banks – economic agents, can be treated alike to assessment of coordinates of economic agents in metric space. To do that the current methodologies for assessments of risk ratings should be upgraded. Indeed, the key issue of risk assessment is the notation of risk grades. Current notations of risk grades serve the only goal – they protect the business model of the particular rating agency. Risk grade systems of different agencies have different letter notations and agencies use different number of grades. Each agency has its own risk rating methodology. Business of Fitch, Moody’s and S&P to a great extent is determined and protected by their specific methodology, risk grade notations and reputation. However, letter notation of risk grades is not the only way denote the value of risk grade and for sure is not the best one. Almost 80 years ago Durand (1941) and later Myers and Forgy (1963) proposed numerical credit grade systems. Actually, there are no any obstacles in transition from letter grade to numerical grade system. There is absolutely no difference how one note risk grades: as AAA, A, BB or as 1, 3, 5. The only impact of transition from letter to numerical grades concerns unification of methodologies of different risk rating agencies. Only prevention of major rating agencies in their ambition to protect their business hinders the transition from letter to numerical risk grade systems. In our opinion the effect of transition will be the opposite. Transition from letter grades to numerical grades system will boost the world risk rating business by much more than 100 times. We assume that the first enterprise that will understand that issue can take over the multi-hundred billions market. Actually, one can consider letter credit risk grades AAA, AA, BB, etc., as points x1, x2,… on numerical axis. Risk grades have conditional meaning and one can always take grades as 7 points x1, x2,… of the unit interval [0,1]. One can take most secure grade AAA to be equal zero and most risky grade to be equal 1. Unification of different risk grade methodologies of major rating agencies allows project their letter risk grades as unified numerical grades of the unit interval [0,1]. Such transition is not the problem of economics or risk management, but only the problem of common agreement between major rating agencies. We propose introduce numeric continuous risk grades those fill interval [0,1] instead of discrete numeric grades. Usage of continuous risk grades is also the problem of the methodology of risk assessment only. There are absolutely no economic obstacles that prevent usage of numeric continuous risk grades. We avoid here discuss unified risk methodology. We are sure that high professional teams of researchers of rating agencies or team as Fox, et al. (2017) can solve that problem and develop reasonable rating methodologies for assessment of risk rating of different economic agents under action of different risks. We propose that unified methodology can deliver assessment of ratings for all economic agents of the entire economy for different economic and financial risks those affect economic evolution. We show below, that the proposed transition from letter risk grades to numeric continuous risk grades gives great advantages for the development of the theoretical economics, description of business cycles, economic fluctuations, financial markets, economic forecasting and etc. Usage of continuous risk grades establishes new approach to the description of the economic evolution, financial markets and uncovers unnoticed tough hurdles and overlooked economic notions on the way to “correct” economic theory. Economic domain - continuous risk grades Economic agents always act under action of economic, financial, market, technological, political etc. risks. Moreover, agents generate risks themselves. Agents economic, market, investment, credit, production, political, technological decisions generate almost all risks. We consider risks as essential part, as integral condition of any economic and financial processes. We believe that economic growth is possible under the action of risks only. The disappearance of risk will initiate halt of all economic processes and destruction of the macroeconomics. Numerous risks impact agents and economic processes. Some of risks have more effect on economic processes than others. It seems almost impossible take into account and estimate risk ratings of all possible risks. The choice of one, two, three most influential risks and assessments of agents’ ratings under action of these risks give definite approximation of the economic processes. Continuous risk ratings of the single risk fill the unit interval [0,1] and ratings of two or three most influential risks fill the unit cubes in the R2 or R3 space. Thus, economic agents under the action of 2-3 risks move inside the unit cubes 8 in the R2 or R3 space. Indeed, agents’ economic activity, market, technology and numerous other factors impact entire economy and force change of agents’ risk ratings. Such “motion” of risk rating of large banks is already described for decades by risk transition matrices provided by rating agencies (Metz Cantor, 2007; Moody’s, 2009; Fitch, 2017; S&P, 2018). Usage of continuous risk grades helps clarify agents’ risk motion. Let’s take that element of risk transition matrix aij describes probability of transition of particular agent from risk rating xi to rating xj during time term T. Then one can assess probable velocity υij of agent’s risk motion from xi to xj with probability aij in economic domain during time T and mean risk velocity υi of agent at point xi as: 𝒗𝑖𝑗=
𝒙𝑗−𝒙𝑖
𝑇
    ;    𝒗𝑖= ∑𝑎𝑖𝑗
𝑗
 𝒗𝑖𝑗   ;    ∑𝑎𝑖𝑗
𝑗
= 1 
 
 
(2.1) We remind, that current notations of letter risk grades don’t allow interpret the difference between two grades xj - xi as a length (2.1). Hence one can’t introduce such a notion as a mean velocity υi (2.1) of agent with risk rate coordinate xi. Change of notations as proposed by Durand (1941) and Myers and Forgy (1963) opens a new look on description of evolution of the economic agents and the entire economy under action of n risks in the economic domain determined as unit cube [0,1]n. Dimension n of the unit cube defines the number of risks under consideration of particular economic approximation. And now it is time to remind about the scales of the economic approximation. The long road above now led us to important point: for the given number n of major risks, economic approximations in the economic domain [0,1]n is determined by the choice of two scales: by the time interval Δ and by the space scale l:  0<l≤1. The time interval Δ determines scale of time aggregation and averaging of agents’ economic variables and parameters during Δ and establishes the divisions of the time axis multiply of Δ. The space scale l determines aggregation and averaging of economic variables and parameters of agents with risk coordinates near point x in a small volume dV~ln of the economic domain. Such aggregation makes description of agents by their risk coordinates more roughen by the scale dV~ln. Space scale l and aggregation by dV~ln allows develop economic approximations starting with assumption of imaginable precise assessment of agents’ risk ratings and approximate their evolution for l: 0<l<1 and up to description of the entire economy for l=1. Thus space scales 0<l≤1 delivers the spectrum, sequence of economic approximations determined by aggregation of agents by different volumes dV~ln in the economic domain [0,1]n. In a sense, Leontief’s input-output analysis is based on division of the entire economy by industries. In a certain manner that division is alike to the division of the economy by volumes dV~ln in the 9 economic domain. Both approaches distribute economic agents by different “boxes”. Leontief collects agents by “boxes” that define different industries and studies input-output transactions between industries. We collect agents by risk coordinates inside “boxes” dV~ln near risk point x, study evolution of their collective economic variables and describe market transactions between agents with coordinates at risk points x and y. That “small” difference opens wide opportunities for development of the economic theory in the economic domain. The main distinction: we replace aggregation of agents by non-metric “industry boxes” with aggregation of agents inside small “boxes” dV~ln near risk points of metric space Rn. That uncovers complex and important unnoticed and overlooked economic variables, relations and processes those define the state and conduct the dynamics of macroeconomics. Disregarding of these hidden variables and processes cause lack of the adequate description of the current state of the economy and incapacity of trustworthy durable economic forecasting. We consider here main definitions of collective variables and their flows and refer (Olkhov, 2016a-2020) for further details. As we mentioned above, economic decisions of agents, market trade, economic fluctuations and other factors cause the change of agents’ risk coordinates. That results in motion of agents with risk velocity υi (2.1) in economic domain [0,1]n. Motion of particular agent with velocity υi causes that agent carries his economic variables in the economic domain. Aggregation of agents and their variables near risk point x of economic domain inside small “box” dV~ln during time Δ determines collective variables inside that “box” - volume dV~ln. Let Ai(tj,xi) denote additive variable A of agent i with coordinates xi at moment tj. As additive variable one can consider agent’s asset, credit, investment, demand, profits and etc. Sum of additive variables of group of agents equals collective variable of the group. Let take that time series tj obey (2.2) ∆= [𝑡−
∆ 2 ; 𝑡+
∆ 2]     ;   𝑡−
∆ 2  ≤𝑡𝑗≤𝑡+
∆ 2  ;  𝑗= 1, . . 𝑁 
 
(2.2) Then collective variable A of agents inside “box” dV~ln near point x averaged during the interval Δ (2.2) equals 𝐴(𝑡, 𝒙) =
1 𝑁∑
∑
𝐴𝑖(𝑡𝑗, 𝒙𝒊)
𝑖∈𝑑𝑉(𝑥)
𝑁
𝑗=1
  
 
(2.3) First sum in (2.3) collects variable A of all agents i with coordinates xi inside dV and the second sum average that value during the time interval Δ (2.2). Relation (2.3) introduces mean additive variable A(t,x) of agents inside dV near point x. Function A(t,x) defines distribution of variable A over the economic domain. In some sense (2.3) is similar to distribution of variable A(t,x) by different industries, different age, wealth and etc. However, usage of continuous risk grades in economic domain uncovers unnoticed motion of collective 10 economic variables that are overlooked by current economic models. Indeed, as we outlined above, each agent i at point (tj,xi) with variable Ai(tj,xi) carries that variable with risk velocity υi (2.1). Collective result of such risk motion for all agents inside “box” dV averaged during Δ (2.2) defines the collective flow PA(t,x) that carries collective variable A(t,x) with collective velocity υA(t,x) (2.4) in the economic domain [0,1]n 𝑷𝐴(𝑡, 𝒙) =
1 𝑁∑
∑
𝐴𝑖(𝑡𝑗, 𝒙𝒊)
𝑖∈𝑑𝑉(𝑥)
𝑁
𝑗=1
𝒗𝑖(𝑡𝑗, 𝒙𝒊) = 𝐴(𝑡, 𝒙)𝒗𝐴(𝑡, 𝒙)  
(2.4) It is important to underline that notions of collective flow and collective velocity have sense even if one considers aggregation of all agents of the entire economy and “box” dV coincides with entire economic domain [0,1]n, so that dV = [0,1]n. In that case macroeconomic variable A(t) takes form 𝐴(𝑡) =
1 𝑁∑
∑𝐴𝑖(𝑡𝑗, 𝒙𝒊)
𝑖
𝑁
𝑗=1
  
 
 
 
(2.5) 𝑷𝐴(𝑡) =
1 𝑁∑
∑𝐴𝑖(𝑡𝑗, 𝒙𝒊)
𝑖
𝑁
𝑗=1
𝒗𝑖(𝑡𝑗, 𝒙𝒊) = 𝐴(𝑡)𝒗𝐴(𝑡) 
 
(2.6) We call the transition from description of economic variables assigned with particular economic agent Ai(tj,xi) to description of economic variables A(t,x) (2.3) as functions of coordinates x in economic domain as economic continuous media approximation. Flows PA(t,x) and velocities υA(t,x) (2.4) of each particular additive economic variable A describe its motion in the economic domain. We consider continuous media approximations for different time and space scales (Δ,l) as set of intermediate approximations between description of economics as separate agents and macroeconomic approximation derived by collecting variables of all agents of the economy. As we show (Olkhov, 2017c; 2017d; 2018; 2019a- 2020), to derive correct description of macroeconomic approximation one should take into account collective flows and velocities (2.6). However, these macroeconomic flows and velocities (2.6) related with numerous different macroeconomic variables A(t) (2.5) are unnoted and overlooked by modern economic theories and that gap definitely causes failures of macroeconomic forecasting. Relations (2.5) introduce one more important and still unnoticed economic factor – macroeconomic mean risk XA(t) (2.7) related with particular collective economic variable A: 𝐴(𝑡)𝑿𝐴(𝑡) =
1 𝑁∑
∑𝐴𝑖(𝑡𝑗, 𝒙𝒊)
𝑖
𝑁
𝑗=1
𝒙𝒊  
 
 
(2.7) It is obvious that motion of macroeconomic variable A(t) with velocity υA(t) (2.6) as well as motion of mean risk XA(t) (2.7) in the unit cube [0,1]n – economic domain can’t go beyond its borders. Hence such a motion should follow complex oscillations inside the economic domain. These oscillations of mean risk XA(t) (2.7) of economic variable A(t), fluctuations of 11 velocity υA(t) (2.6) as well as fluctuations of collective velocity υA(t,x) (2.4) inside economic domain reflect processes that are currently noted as business cycles (Olkhov, 2017c; 2017d; 2019a; 2019c; 2020). Different economic variables A,B,C define different collective velocities υA(t,x), υB(t,x), υC(t,x) and different mean risks XA(t), XB(t), XC(t). Their mutual interactions and their oscillations inside the unit cube [0,1]n of the economic domain establish complex picture of collective fluctuations of macroeconomic variables those observed and treated as business cycles. We consider these overlooked hidden collective oscillations of economic variables, their mean risks and velocities as the origin of observed economic business cycles. Economic continuous media approximation in the economic domain [0,1]n uncovers important and unnoticed wave generation and propagation of small disturbances of collective economic variables and market transactions. In economics the term “wave” is in use at least since Kondratieff’s waves (Kondratieff, 1935). However, Kondratieff’s waves as well as economic cycles describe oscillations of economic variables in time only. To observe and describe economic waves one should consider economic processes in certain space. Introduction of the economic domain as a unit cube [0,1]n in Rn uncovers possible wave propagation of small disturbances of collective economic variables and market transactions as functions of risk coordinates in the economic domain. We described (Olkhov, 2016a-2017b; 2019c) possible propagation of small waves through the economic domain and a different type of “surface-like” waves those propagate along the borders of the economic domain. We show that possible exponential amplification of the wave amplitudes during the propagation can cause rise of perturbations’ amplitudes and result in development of crises processes. Currently, the influence of the economic wave propagation on macroeconomic evolution is completely unnoted. However, the hidden complexity of the collective economic variables and their flows in economic domain delivers only small fraction of the difficulties on the way for development of the comprehensive economic theory. In the next section we consider unnoticed variables and overlooked economic properties related with the main drivers of the economic development – market trade and expectations. 3. Market trade and expectations Market trade is the origin and generator of the economic development. Markets redistribute the volumes of the existing assets and commodities as well as assets and commodities generated by industrial production over agents in the economic domain. Markets establish the 12 prices of assets and commodities that are adopted by the economy. Each agent involved into particular market transaction takes own decision on the amount of the trade value and volume under personal expectations. Relations between expectations of agents and performance of market transactions establish a complex puzzle for the theoretical economics. Expectations determine preferences and decisions of agents those result in market trade performance. Impacts of expectations on world markets and economic evolution are under research for decades (Muth, 1961; Lucas, 1972; Hansen and Sargent 1979; Blume and Easley 1984; Brock and Hommes, 1998; Manski 2004; Brunnermeier and Parker 2005; Manski, 2017; Farmer, 2019). Expectations generate market trade stochasticity and their measuring and modelling remain the major puzzle of financial economics. Ties between expectations and economic policy (Sargent and Wallace, 1976) result complex impact on world markets and economic development. Methods and description of collective economic variables as functions of risk coordinates in economic domain delivers the unified approach to description of the world markets, economic expectations and economic policy. A first approximation of the relations between market trades and economic expectations described as functions of risk coordinates in the economic domain was developed by Olkhov (2019b; 2019c; 2020). For simplicity in this paper we consider only preliminary, general notions and definitions required for description of the market trade and expectations in the economic domain. The idea for description of market trade in economic domain is simple and in some sense is alike to Leontief’s model (Leontief, 1955; 1973). Leontief collects agents by industries, but we collect agents inside small volume dV~ln near points of economic domain [0,1]n. Leontief considers input-output trade relations between industries. We describe collective market trade between agents those belong to small volumes dV~ln near points x and y of the economic domain [0,1]n. That “small” difference from Leontief’s model opens wide opportunities for description collective market trade in the economic domain [0,1]n of metric space Rn. Actually, we reproduce economic continuous media approximation introduced for description of collective economic variables in Sec.2 to model collective market trade and their flows in the economic domain of double dimension. Motion of agents in the economic domain generates flows of collective market trades. Assumptions on relations between market trade and agents expectations allow introduce notions of collective expectations and their collective flows in the economic domain. Notions of collective market trade and expectations as well as notions of their collective flows in the economic domain are unnoticed and overlooked by current economic theories. 13 Let us explain briefly above argumentation in more details. Let us consider seller and buyer at points xi and yj of the economic domain [0,1]n. These two agents at time tm perform market transaction with a particular asset or commodity at volume U(tm,xi,yj) and value C(tm,xi,yj). Let denote C(t,x,y) and U(t,x,y) as collective trade value and volume between sellers and buyers inside small volumes dV~ln near point x and y averaged during time interval Δ. 𝑈(𝑡, 𝒛) =
1
𝑁∑
∑
𝑈(𝑡𝑚, 𝒛𝑖𝑗)
𝑖∈𝑑𝑉(𝒙);𝑗∈𝑑𝑉(𝒚)
𝑁
𝑚=1
   ;     𝒛= (𝒙, 𝒚)  ;  𝒛𝑖𝑗= (𝒙𝑖, 𝒚𝑗) 
 
(3.1) 𝐶(𝑡, 𝒛) =
1
𝑁∑
∑
𝐶(𝑡𝑚, 𝒛𝑖𝑗)
𝑖∈𝑑𝑉(𝒙);𝑗∈𝑑𝑉(𝒚)
𝑁
𝑚=1
 
 
 
 
(3.2) Transactions between agents at points x and y can be treaded as functions at point z=(x,y) (3.1) and that easily transfers description of the market transactions in the economic domain of double dimension 2n. Motion of economic agents in economic domain induces corresponding motion of market transactions (3.1; 3.4) in the economic domain of double dimension 2n. For example, collective flow PU(t,z) of trade volume depends on collective flows PxU(t,z) of sellers at point x and collective flows PyU(t,z) of buyers at point y: 𝑷𝑈(𝑡, 𝒛) = (𝑷𝑥𝑈(𝑡, 𝒛);𝑷𝑦𝑈(𝑡, 𝒛)) 
 
 
 
(3.3) 𝑷𝑥𝑈(𝑡, 𝒛) = 𝑈(𝑡, 𝒛)𝝊𝑥𝑈(𝑡, 𝒛) =
1
𝑁 ∑
∑
𝑈𝑖𝑗(𝑡𝑚, 𝒛𝑖𝑗)𝝊𝑖𝑥(𝑡𝑚, 𝒙)
𝑖∈𝑑𝑉(𝒙);𝑗∈𝑑𝑉(𝒚)
𝑁
𝑚=1
  
(3.4) 𝑷𝑦𝑈(𝑡, 𝒛) = 𝑈(𝑡, 𝒛)𝝊𝑦𝑈(𝑡, 𝒛) =
1
𝑁 ∑
∑
𝑈𝑖𝑗(𝑡𝑚, 𝒛𝑖𝑗)𝝊𝑖𝑦(𝑡𝑚, 𝒚)
𝑖∈𝑑𝑉(𝒙);𝑗∈𝑑𝑉(𝒚)
𝑁
𝑚=1
   
(3.5) Velocities (3.6) υxU and υyU determined in (3.4; 3.5) as average velocities of sellers and buyers in small volume dV~ln during time interval Δ with respect to the average trade volume U(t,z) (3.1). 𝒗𝑈(𝑡, 𝒛) = (𝒗𝑥𝑈(𝑡, 𝒛) ; 𝒗𝑦𝑈(𝑡, 𝒛)) 
 
 
 
(3.6) Equations that describe evolution of the collective trade volume U(t,z) (3.1) and value C(t,z) (3.2) depend on flows of the collective trade volume PU(t,z) and value PC(t,z). Derivation of these equations and other math methods required for description of the market trade evolution in the economic domain of double dimension are presented in (Olkhov, 2017b; 2017c; 2017d; 2018; 2019b; 2019c; 2020). Expectations Expectations of agents remain the most mysteries factors of the economic theory. Variety, variability and uncertainty of expectations of agents make them the headache for econometric observations and theoretical economics. Expectations of agents, as “dimension” component of economic theory, are generated by at least other three “dimensions” of economic theory – economic variables, market transactions and economic policy (Olkhov, 2022d). The role of 14 expectations can be treated as a  “glue” that ties up impact of the variables, transactions and economic policy and produces market trade decisions of agents. Even simplified model of mutual dependence of expectations and market trades uncovers unnoticed properties of collective expectations determined by collective trade value and volume (Olkhov, 2019b; 2019c; 2021d). Indeed, according to economic continuous media approximation one should define notions of collective expectations as functions of risk coordinate in economic domain. To do that one should collect expectations of agents in a small volume dV and average them during the interval Δ (2.2). Sum of different agents’ expectations is a tough problem that requires methodological introduction of unified measure of different expectations. That is one of the overlooked problems of current economic studies of agents’ expectations and the problem is far from solution. However, if one assumes that the unified measure of expectations is selected and expectations of different agents can be observed and assessed by the unified measure, definition of collective agents’ expectations still remains a tough problem. Indeed, different agents’ expectations are “responsible” for trade decisions of different value. It seems not fair sum on equal basis expectation 1 that is responsible for trade worth $1 and expectation 2 responsible for trade worth $10MM. We propose that to collect expectations of different agents one should multiply the measure of the expectation responsible for the value of particular trade by the value of that trade. Thereafter, one should multiply by the trade volume the measure of expectation that is responsible for the particular trade volume. These simple rules result definitions of collective expectations as weighted by the market trade value and volume they approved. For example, if U(tm,xi,yj;k,l) at time tm denotes trade volume between seller at xi and buyer at yj (3.1) under sellers expectations exsU(tm,xi;k) of type k and buyers expectation exbU(tm,yj;l) of type l then collective sellers expectations ExsU(t,z) of the trade volume take form: 𝑈(𝑡, 𝒛)𝐸𝑥𝑠𝑈(𝑡, 𝒛) =
1
𝑁∑
∑
∑
𝑈(𝑡𝑚, 𝒛𝑖𝑗; 𝑘, 𝑙)
𝑖∈𝑑𝑉(𝒙);𝑗∈𝑑𝑉(𝒚)
𝑁
𝑚=1
𝑘,𝑙
 𝑒𝑥𝑠𝑈(𝑡𝑚, 𝒙𝑖; 𝑘)  
(3.7) To avoid excess complexity we omit definitions and equations those describe evolution of collective market trade, collective expectations, collective flows of market trade, collective flows of expectations and their mutual interactions and refer (Olkhov, 2017 - 2020; 2021d). Withal one should remember that each trade is performed by at least two agents – by seller and by buyer. Expectations of seller and buyer can be different and thus definition of collective expectations should identify collective expectations of sellers and buyers. Thus description of collective expectations those impact market trade with selected asset or commodity requires introduction of four different collective expectations: two expectations 15 those impact the trade value and volume of sellers and two similar expectations of buyers. As we mentioned above, motion of agents in economic domain [0,1]n cause motion of trades in economic domain of double dimension and that result motion of collective expectations. Flows of collective expectation in bounded economic domain result slow fluctuations that are alike to business cycles of expectations. Econometric observations of such cycles of expectations are absent. However, overlooked impact of expectation’s cycles on cycles of market trade and conventional business cycles of economic growth, investment and etc. hide important tools and relations of their management. We believe that the nature of economic processes, flows of economic variables, market transactions and expectations in the bounded economic domain make absolutely impossible maintaining permanent economic growth without crises and recessions. Nevertheless, econometric observations and theoretical predictions of mutual dependence between collective expectations, market trade and variables could deliver financial authorities, Central Banks and politicians more tools, reasons and argumentations for achieving economic prosperity with less losses. At the end of this subsection we outline that expectations of agents to a great extend are determined by existing economic, financial, market and etc., laws and regulations. Regularly adjusted political decisions those disturb current economic laws are usually noted as economic and financial policy. Economic laws that conduct market trade, taxes, production and mining, environment protection and etc., – all that amount of economic laws and regulations are permanently adjusted by economic policy. That result unpredictable disturbances of agents expectations those projected into market fluctuations and macroeconomic uncertainty. Modelling of that jurisdictive “black box” as fourth dimension of the economic theory and description of its impact on economic evolution, risk variations, business cycles and etc., - the though problem for future. However, it could be important keep in mind existing of the four dimensions of the economic theory – collective variables, market transactions, expectations and economic policy&laws (Olkhov, 2022d). 4. Market roots of economic uncertainty Randomness of the market trade is studied for decades. In that section we demonstrate that market stochasticity should be treated as important source of economic uncertainty that impact on much more macroeconomic variables and properties than it is recognized now. Economic and financial transactions between agents determine the major tool that supports economic development and growth. Market trade mysteries broadly define the enigma of the 16 economic theory and forecasting. Agents variables collected by entire economy or by different “boxes” as industries, wealth, age range, risk coordinates and etc., determine the economic state, the starting conditions for the economic development and growth. However, agents variables itself even collected in any manner over the economy unable generate economic evolution. Only market trade establishes and causes the economic and financial development. Only market trade moves the current state of the economy to the future prosperity. One should remember that all theories and forecasts those utilize relations between economic variables only are eluding and fudging description of the real economic ties and laws those conduct economic dynamics. Usual common statements like “supply depends on demand” or “investment depend on bank rate” describe relations between variables, but omit several intermediate and complex chains of market transactions and neglect economic relations those conduct market trade. That is understandable desire to derive predictions under simplified assumptions. Below we show that such shortening economic relations result in disregarding numerous significant economic properties and variables those conduct market trade and economic forecasts. To start with let us consider as example the market trade of particular asset or commodity. Let denote as C(ti) and U(ti) the value and volume of the market trade at time ti at a price p(ti), so trivial relations between the trade value, volume and price take form: 𝐶(𝑡𝑖) = 𝑝(𝑡𝑖)𝑈(𝑡𝑖) 
 
 
 
 
(4.1) Simple trade relations define price p(ti) of particular trade at ti. Modern market trade is a high-frequency process with high variations of the trade value, volume and price. Most economic and financial models consider averaging and smoothing procedures of market trade time-series during selected time averaging interval Δ. Financial market models can take the averaging interval Δ to be equal minutes, hours or days and macroeconomic models deal with the averaging interval Δ to be equal weeks, months and years. Time averaging during the interval Δ establishes time axis division multiply of Δ of the theoretical description of economics and financial markets. Different choice of the averaging interval Δ determines different approximations of the market dynamics and economic evolution. Let us select the averaging interval Δ and study statistical properties of the market trade value, volume and price treated as random variables during Δ. Below we explain why and how statistical properties of the trade value and volume determine statistical properties of the market price and how all that define statistical properties of the economic variables. For simplicity let us take initial market trade time-series at time ti as multiple of ε and assume that there are N members of time-series in each interval Δ: 17 𝑡𝑖= 𝜀 ∙𝑖  ;   𝜀≪∆= 2𝑛∙𝜀    ;   𝑁= 2𝑛+ 1  ;    𝑖= 0, 1, 2, … 
 
(4.2) Averaging of time-series ti during Δ replaces initial time axis division multiply of ε by division multiply of Δ and generates time-series tk (4.3) 𝑡𝑘= ∆∙𝑘      ;    ∆𝑘= [𝑡𝑘−
∆ 2 ; 𝑡𝑘+
∆ 2]     ;     𝑘= 0, 1, 2, …    
(4.3) Time-series of the market trade value C(ti), volume U(ti) and price p(ti) during each averaging interval Δk are very irregular. Aggregation or averaging of irregular variables during interval Δk helps derive smooth variables and develop reliable predictions. For simplicity we consider initial time-series of the market trade value C(ti), volume U(ti) and price p(ti) as random variables during each averaging interval Δk. Asset pricing theory is the key problem of financial economics and the literature is endless. We refer only few studies (Bachelier, 1900; Kendall and Hill, 1953; Muth, 1961; Sharpe, 1964; Fama, 1965; Karpoff, 1987; Cochrane and Hansen, 1992; Cochrane, 2001; Hansen, 2013). Any reader can add to that list hundreds of his preferred references. Above references confirm only one issue – any new treatment of the asset pricing is very important for economic and financial modelling. We present derivation of the statistical properties of the market random price time-series during averaging interval Δ due to Olkhov (2021a-2022d) and refer there for details. Our consideration of the market price probability follows simple thesis: relations (4.1) state that it is impossible independently define probabilities of the trade value C(ti), volume U(ti) and price p(ti) during Δ. Given random properties of the market trade value C(ti) and volume U(ti) determine random properties of the price time-series p(ti) during Δ. In some extend definition of the mean price p(tk;1) during the interval Δk (4.3) as function of the mean trade value C(tk;1) and volume U(tk;1) was given more then 30 years ago as volume weighted average price (VWAP) (Berkowitz et.al, 1988; Buryak and Guo, 2014; Busseti and Boyd, 2015; CME Group, 2020). Definition of VWAP p(tk;1) (4.4) averaged during interval Δk is based on total value CΣ(tk;1) and volume UΣ(tk;1) aggregated during Δk (4.3) and follows (4.1): 𝑝(𝑡𝑘; 1) = 𝐸[𝑝(𝑡𝑖)] =
1
∑
𝑈(𝑡𝑖)
𝑁
𝑖=1
 ∑
𝑝(𝑡𝑖)
𝑁
𝑖=1
𝑈(𝑡𝑖) =
𝐶Σ(𝑡𝑘;1) 𝑈Σ(𝑡𝑘;1)  
 
(4.4) 𝐶Σ(𝑡𝑘; 1) = ∑
𝐶(𝑡𝑖)
𝑁
𝑖=1
= ∑
𝑝(𝑡𝑖)
𝑁
𝑖=1
𝑈(𝑡𝑖)    ;   𝑈Σ(𝑡𝑘; 1) = ∑
𝑈(𝑡𝑖)
𝑁
𝑖=1
 
 
(4.5) Relations (4.5) define total trade value CΣ(tk;1) and volume UΣ(tk;1) during Δk (4.3). Let us note mathematical expectation during Δk as E[…]. As we assume there are N trades during Δk. Hence, mean trade value C(tk;1) and mean volume U(tk;1) during Δk equal: 𝐶(𝑡𝑘; 1) = 𝐸[𝐶(𝑡𝑖)] =
1 𝑁𝐶Σ(𝑡𝑘; 1) =
1 𝑁∑
𝐶(𝑡𝑖)
𝑁
𝑖=1
  
 
(4.6) 𝑈(𝑡𝑘; 1) = 𝐸[𝑈(𝑡𝑖)] =
1 𝑁𝑈Σ(𝑡𝑘; 1) =
1 𝑁∑
𝑈(𝑡𝑖)
𝑁
𝑖=1
  
 
(4.7) 18 Thus VWAP p(tk;1) (4.1; 4.4) averaged during interval Δk (4.3) equals: 𝑝(𝑡𝑘; 1) =
1
1
𝑁∑
𝑈(𝑡𝑖)
𝑁
𝑖=1 ∙ 
1 𝑁∑
𝑝(𝑡𝑖)
𝑁
𝑖=1
𝑈(𝑡𝑖) =
1
1
𝑁∑
𝑈(𝑡𝑖)
𝑁
𝑖=1 ∙ 
1 𝑁∑
𝐶(𝑡𝑖)
𝑁
𝑖=1
=
𝐶(𝑡𝑘;1) 𝑈(𝑡𝑘;1) 
(4.8) These trivial relations  (4.4-4.8) uncover important consequence that is usually overlooked. Indeed, relations (4.1; 4.4-4.8) cause zero correlations corr{p(ti)U(ti)}=0 between time-series of the market trade volume U(ti) and price p(ti) during averaging interval Δk: 𝐸[𝐶(𝑡𝑖)] = 𝐸[𝑝(𝑡𝑖)𝑈(𝑡𝑖)] =
1 𝑁∑
𝑝(𝑡𝑖)
𝑁
𝑖=1
𝑈(𝑡𝑖) ≡
1
∑
𝑈(𝑡𝑖)
𝑁
𝑖=1
 ∑
𝑝(𝑡𝑖)
𝑁
𝑖=1
𝑈(𝑡𝑖) 
1 𝑁∑
𝑈(𝑡𝑖)
𝑁
𝑖=1
= 𝐸[𝑝(𝑡𝑖)]𝐸[𝑈(𝑡𝑖)]     (4.9) Hence, from (4.9) obtain (4.10) that corr{p(ti)U(ti)} equals zero: 𝑐𝑜𝑟𝑟{𝑝(𝑡𝑖)𝑈(𝑡𝑖)} =  𝐸[𝑝(𝑡𝑖)𝑈(𝑡𝑖)] −𝐸[𝑝(𝑡𝑖)]𝐸[𝑈(𝑡𝑖)] = 0 
(4.10) We underline that unnoticed zero correlations between price and trade volume (4.10) is result of the definition of the VWAP p(tk;1) (4.4 - 4.8). That is an important issue, as studies of the price-volume correlations have long history (Tauchen and Pitts, 1983; Gallant et.al., 1992; Campbell et.al., 1993; Odean, 1998). The choice of the VWAP p(tk;1) (4.4-4.8) states zero correlations between price and trade volume. Hence studies of the market trade price-volume time-series should be reconsidered with respect to definite price-volume averaging procedures. Economic meaning of price-volume correlations should be considered with respect to economic meaning of the trade volume and price probabilities. We underline that definition of the VWAP p(tk;1) (4.4-4.8) is not sufficient to define all random properties of the price p(ti) during Δk (4.3). To do that one should define all price n-th statistical moments p(tk;n)=E[pn(ti)]. We introduce all price n-th statistical moments as extension of (4.1; 4.4-4.8). For all n=1,2,3,… we take usual n-th statistical moments of the trade value C(tk;n) and total sums of n-th power of trade value CΣ(tk;n) (4.11) and n-th statistical moments of the trade volume U(tk;n) and total sums of n-th power of volume UΣ(tk;n) (4.12) during Δk (4.3) as: 𝐶(𝑡𝑘; 𝑛) = 𝐸[𝐶𝑛(𝑡𝑖)] = 
1 𝑁 𝐶Σ(𝑡𝑘; 𝑛)     ;    𝐶Σ(𝑡𝑘; 𝑛) = ∑
𝐶𝑛(𝑡𝑖)
𝑁
𝑖=1
 
 
(4.11) 𝑈(𝑡𝑘; 𝑛) = 𝐸[𝑈𝑛(𝑡𝑖)] = 
1 𝑁 𝑈Σ(𝑡𝑘; 𝑛)     ;    𝑈Σ(𝑡𝑘; 𝑛) = ∑
𝑈𝑛(𝑡𝑖)
𝑁
𝑖=1
 
 
(4.12) Taking n-th power of (4.1) we obtain: 𝐶𝑛(𝑡𝑖) = 𝑝𝑛(𝑡𝑖)𝑈𝑛(𝑡𝑖) 
 
 
 
(4.13) We extend definition of VWAP (4.4-4.8) and for all n=1,2,3,… introduce price n-th statistical moments p(tk;n)=E[pn(ti)] (4.14) as: 𝑝(𝑡𝑘; 𝑛) = 𝐸[𝑝𝑛(𝑡𝑖)] =
1
∑
𝑈𝑛(𝑡𝑖)
𝑁
𝑖=1
 ∑
𝑝𝑛(𝑡𝑖)
𝑁
𝑖=1
𝑈𝑛(𝑡𝑖) =
𝐶Σ(𝑡𝑘;𝑛) 𝑈Σ(𝑡𝑘;𝑛) =
𝐶(𝑡𝑘;𝑛) 𝑈(𝑡𝑘;𝑛)  
(4.14) It is obvious, that (4.14) results: 𝐶(𝑡𝑘; 𝑛) = 𝑝(𝑡𝑘; 𝑛)𝑈(𝑡𝑘; 𝑛)         ;      𝐶Σ(𝑡𝑘; 𝑛) = 𝑝(𝑡𝑘; 𝑛)𝑈Σ(𝑡𝑘; 𝑛) 
(4.15) 19 To explain and justify our definition (4.14; 4.15) of the market price n-th statistical moments p(tk;n)=E[pn(ti)] we mention that we extend VWAP logic for (4.13) as n-th power of (4.1). Average n-th power of price E[pn(ti)] is determined as n-th power volume Un(ti) weighted average of n-th power of price pn(ti) (4.14). Both relations (4.14; 4.15) retain the meaning of market n-th power of price pn(ti) as coefficient between n-th power of trade value Cn(ti) and trade volume Un(ti) (4.13). Definitions (4.14; 4.15) maintain meaning of price n-th statistical moment p(t;n)= E[pn(ti)] as ratio of total sum of n-th power of value Cn(ti) to sum of n-th power volume Un(ti) during averaging interval Δk (4.3). One can easy obtain that (4.11- 4.15) cause zero correlations between time-series of n-th power of price pn(ti) and trade volume Un(ti): 𝐸[𝑝𝑛(𝑡𝑖)𝑈𝑛(𝑡𝑖)] =
1 𝑁∑
𝑝𝑛(𝑡𝑖)
𝑁
𝑖=1
𝑈𝑛(𝑡𝑖) =
∑
𝑝𝑛(𝑡𝑖)
𝑁
𝑖=1
𝑈𝑛(𝑡𝑖) ∑
𝑈𝑛(𝑡𝑖)
𝑁
𝑖=1
 
∑
𝑈𝑛(𝑡𝑖)
𝑁
𝑖=1
𝑁
= 𝐸[𝑝𝑛(𝑡𝑖)]𝐸[𝑈𝑛(𝑡𝑖)] Hence correlations corr{pn(ti)Un(ti)} between time series of n-th power of price pn(ti) and trade volume Un(ti) equal zero: 𝑐𝑜𝑟𝑟{𝑝𝑛(𝑡𝑖)𝑈𝑛(𝑡𝑖)} =  𝐸[𝑝𝑛(𝑡𝑖)𝑈𝑛(𝑡𝑖)] −𝐸[𝑝𝑛(𝑡𝑖)]𝐸[𝑈𝑛(𝑡𝑖)] = 0 
(4.16) Zero correlations (4.16) for all n=1,2,3,… don’t imply statistical independence between price and volume time-series. For example, from (4.14; 4.15) one can easy obtain correlations corr(pU2) between price p(ti) and squares of trade volume Un(ti) (Olkhov, 2022c). For brevity we omit ti and obtain: 𝐸[𝑝𝑈2] = 𝐸[𝑝]𝐸[𝑈2] + 𝑐𝑜𝑟𝑟{𝑝𝑈2}     ;    𝐸[𝑝𝑈2] = 𝐸[𝐶𝑈] = 𝐸[𝐶]𝐸[𝑈] + 𝑐𝑜𝑟𝑟{𝐶𝑈} 𝑐𝑜𝑟𝑟{𝑝𝑈2} = 𝑐𝑜𝑟𝑟{𝐶𝑈} −𝑝(𝑡𝑘; 1)𝜎2(𝑈) 
 
 
(4.17) Here we denote market trade volume volatility during Δk (4.3) as σ2(U): 𝜎2(𝑈) = 𝑈(𝑡𝑘; 2) −𝑈2(𝑡𝑘; 1) 
 
 
(4.18) Correlation corr{pU2} of price and squares of trade volume depends on correlation corr{CU} of the trade value C and trade volume U, on VWAP p(tk;1) and on trade volume volatility σ2(U). We repeat, that unnoticed zero correlations between price and trade volume as a consequence of VWAP introduced 30 years ago underlines importance of market-based approach to description of price random properties. Price should be studied as result of market trade (4.1) and that causes (4.4-4.10). Our definition (4.14-4.15) of price statistical moments p(tk;n)=E[pn(ti)] for all n=1,2,3,… causes zero correlations between n-th power of price pn(ti) and trade volume Un(ti). Relations (4.14; 4.15) define all price statistical moments p(tk;n) and hence completely determine price as random variable during Δk (4.3) as functions of statistical moments of the market trade value C(tk;n) (4.11) and volume U(tk;n) (4.12). 20 However, any econometric records of market trade during Δk (4.3) allow assess only finite number of market statistical moments C(tk;n) and U(tk;n). Thus, researchers should consider approximations of the market trade value and volume probabilities, determined by finite number of statistical moments. That causes assessment of finite number of the market price statistical moments those determine approximations of the price probability measure. Predictions of the price probability should follow forecasts of the market trade statistical moments. These ties between stochasticity of the market trade and randomness of the market price underline their economic nature and mutual dependence. Predictions of the market trade n-th statistical moments for n=2,3,.. uncover additional unnoticed variables and overlooked economic problems. Indeed, current economic theories consider variables composed by sums of the 1-st degree variables of economic agents. Macroeconomic investment, credits, assets are determined as sums of corresponding variables of agents of the entire economy. In turn, if one ignores impact of consumption and production, then agent’s additive economic variables during interval Δk (4.3) are composed by sums of 1-st degree market trade values CΣ(tk;1) (4.11) and volumes UΣ(tk;1) (4.12). Actually, sums of n-th power of market trade values CΣ(tk;n) (4.11) and volumes UΣ(tk;n) (4.12) during Δk (4.3) determine change of the macroeconomic variables of the n-th degree. These n-th degree macroeconomic variables establish direct ties between market stochasticity determined by market statistical moments C(tk;n) (4.11) and U(tk;n) (4.12) and unnoticed macroeconomic randomness. One should study evolution of n-th degree investment, credits, assets, because they record market stochasticity and project it into macroeconomic randomness. Evolution of the n-th degree macroeconomic variables depend on and impact on sums of n-th power of market trade values CΣ(tk;n) (4.11) and volumes UΣ(tk;n) (4.12). Their mutual interactions on one hand affect macroeconomic evolution and on other hand determine stochastic properties of the market trade statistical moments C(tk;n) and U(tk;n) (4.11; 4.12) and through them the market price statistical moments p(tk;n) (4.14; 4.15). Stochasticity of the market trade and price is intertwined into evolution of n-th degree economic variables. That is a really though puzzle for theoretical description. Meanwhile, that “can of worms” formed by interrelations between macroeconomic variables, market trade and market price is only prelude, only preliminary introduction to the complexity of the comprehensive theory of economic reality. To imagine it to some extent one should remember that market trades to a large degree are conducted by collective agents expectations. Expectations, in their turn, are largely shaped and established under economic policy decisions and economic legislative environment, regulations and laws. In their turn, 21 economic policy decisions are established by academic economists and politicians on base of their observations of macroeconomic variables their treatment of economic theory and their desires and goals to “improve” economic performance. Each novel policy decision disturbs and changes existing economic legislation and laws and cause perturbations of agents’ expectations. In their turn disturbed expectations perturb market trade, market price and cause macroeconomic shocks. Actually, politicians supported by elite economists play one of the leading roles in regular perturbations of agents’ expectations, market trade, price and, eventually, of macroeconomic performance. 5. Conclusion It is clear that one paper cannot address and discuss all unnoticed issues and overlooked factors that may cause failures of the modern economic theories and policies. Many tough problems of economic modelling those arise within the proposed unified approach to description of economic variables, market transactions and expectations as functions of risk coordinates in the economic domain are left unnoticed. However, we assume that the above considerations of collective economic variables, collective transactions and expectations, mean risks of economic variables and transactions, collective velocities and flows of economic variables, transactions and expectations and other specific features of modelling their evolution and mutual interactions in the economic domain uncovers a great amount of unnoticed economic variables and overlooked economic processes those configure economic performance. The bounds of the n-dimensional economic domain result inherent cyclical motion of collective variables, transactions and expectations those are responsible for observed business cycles. Slow motion of collective economic variables and market transactions in the economic domain is complemented by propagation of numerous waves generated by small perturbations of different economic variables and market transactions. Ensemble of economic waves transfer perturbations over the economic domain and possible amplification of wave amplitudes can cause growth of economic and financial instabilities. Collective expectations those generally determine market trade and economic, financial, tax, trade and etc., laws and regulations those repeatedly disturbed by the next economic policy decisions complement that general view on vast variety of the theoretical economics puzzle. The simplest way to solve the economic problems we mentioned above is to declare that all these complexities don’t exist. Probably, that is the best choice for majority. However, few readers may wonder in details of the results obtained and in the constructive proposal for the further development of the above problems and theoretical economics. 22 We believe that the general look on theoretical economics and numerous unnoticed and overlooked economic processes helps develop successive approximations of economic models piece by piece those complement each other. The comprehensive economic theory that can respond above queries is the goal of the far future. However, we propose that possible gains of econometric observations and theoretical description of the unnoted and overlooked economic factors and processes will reward any efforts. We hope that our treatment of unnoticed and overlooked factors of theoretical economics and policy decisions preserves a wide field of studies for many decades for academic researchers, economic authorities and high-level politicians. 23",0
