{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_data(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# dataset class\n",
    "class LiteratureDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "data_file = \"raw_combined_2.csv\"\n",
    "texts, labels = load_medical_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of texts:  1998\n",
      "Total number of labels:  1998\n",
      "The unique labels are:  [0, 1]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"raw_combined_2.csv\") \n",
    "\n",
    "# get the length of section\n",
    "print(\"Total number of texts: \", len(data['text']))\n",
    "print(\"Total number of labels: \", len(data['label']))\n",
    "\n",
    "#get unique labels\n",
    "unique_labels = list(set(data['label']))\n",
    "print(\"The unique labels are: \", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1198\n",
      "Validation size: 400\n",
      "Test size: 400\n"
     ]
    }
   ],
   "source": [
    "# Stratified Split: 60% Train, 20% Validation, 20% Test\n",
    "# First split: 80% (train+val) and 20% (test)\n",
    "split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_val_idx, test_idx in split1.split(data, data['label']):\n",
    "    train_val_data = data.iloc[train_val_idx]\n",
    "    test_data = data.iloc[test_idx]\n",
    "\n",
    "# Second split: 60% train and 20% validation from 80% train+val\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)  # 0.25 of 80% = 20%\n",
    "for train_idx, val_idx in split2.split(train_val_data, train_val_data['label']):\n",
    "    train_data = train_val_data.iloc[train_idx]\n",
    "    val_data = train_val_data.iloc[val_idx]\n",
    "\n",
    "# Verify the splits\n",
    "print(\"Train size:\n",
    "print(\"Validation size:\", len(val_data))\n",
    "print(\"Test size:\", len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
