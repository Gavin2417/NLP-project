{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visulize(df1,df2):\n",
    "    # Remove the \".pdf\" extension from filenames in df2\n",
    "    df2['filename'] = df2['filename'].str.replace('.pdf', '', regex=False)\n",
    "\n",
    "    # Merge on filename to compare predicted labels\n",
    "    comparison = df1.merge(df2, on=\"filename\", suffixes=(\"_actual\", \"_predicted\"))\n",
    "\n",
    "    # Map labels to binary values: YES -> 1, NO -> 0\n",
    "    y_true = comparison[\"predicted_label_actual\"].map({\"YES\": 1, \"NO\": 0})\n",
    "    y_pred = comparison[\"predicted_label_predicted\"].map({\"YES\": 1, \"NO\": 0})\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # # Visualize the confusion matrix\n",
    "    # fig, ax = plt.subplots()\n",
    "    # im = ax.imshow(conf_matrix, cmap='Blues')\n",
    "    # ax.set_title(\"Confusion Matrix\")\n",
    "    # ax.set_xlabel(\"Predicted Label\")\n",
    "    # ax.set_ylabel(\"Actual Label\")\n",
    "\n",
    "    # # Set tick marks and labels (assuming binary classification: NO=0, YES=1)\n",
    "    # ax.set_xticks(np.arange(2))\n",
    "    # ax.set_yticks(np.arange(2))\n",
    "    # ax.set_xticklabels([\"NO\", \"YES\"])\n",
    "    # ax.set_yticklabels([\"NO\", \"YES\"])\n",
    "\n",
    "    # # Annotate the confusion matrix with counts\n",
    "    # for i in range(conf_matrix.shape[0]):\n",
    "    #     for j in range(conf_matrix.shape[1]):\n",
    "    #         ax.text(j, i, conf_matrix[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    # fig.colorbar(im, ax=ax)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "Precision: 0.43\n",
      "Recall: 0.38\n",
      "F1 Score: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "file1_path = r\"../NLP/csv_data/test_actual.csv\"\n",
    "file2_path = r\"../NLP/csv_data/test_predictions_10.csv\"  # Update with the actual filename if needed\n",
    "\n",
    "# Read CSV files\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "visulize(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60\n",
      "Precision: 0.50\n",
      "Recall: 0.75\n",
      "F1 Score: 0.60\n"
     ]
    }
   ],
   "source": [
    "file2_path = r\"../NLP/csv_data/test_predictions_20.csv\"  # Update with the actual filename if needed\n",
    "df2 = pd.read_csv(file2_path)\n",
    "visulize(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "Precision: 0.43\n",
      "Recall: 0.38\n",
      "F1 Score: 0.40\n"
     ]
    }
   ],
   "source": [
    "file3_path = r\"../NLP/csv_data/test_predictions_30.csv\"  # Update with the actual filename if needed\n",
    "df3 = pd.read_csv(file3_path)\n",
    "visulize(df1, df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.64\n",
      "Recall: 0.88\n",
      "F1 Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "file2_path = r\"../NLP/csv_data/test_predictions_40.csv\"  # Update with the actual filename if needed\n",
    "df2 = pd.read_csv(file2_path)\n",
    "visulize(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved at: ../NLP/csv_data/merged_predictions_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files\n",
    "actual_df = pd.read_csv(\"../NLP/csv_data/test_actual.csv\")\n",
    "pred_10_df = pd.read_csv(\"../NLP/csv_data/test_predictions_10.csv\")\n",
    "pred_20_df = pd.read_csv(\"../NLP/csv_data/test_predictions_20.csv\")\n",
    "pred_40_df = pd.read_csv(\"../NLP/csv_data/test_predictions_40.csv\")\n",
    "\n",
    "# Merge all files properly, using suffixes to differentiate predictions\n",
    "merged_df = actual_df.copy()\n",
    "merged_df = merged_df.merge(pred_10_df, how='outer', on=actual_df.columns[0], suffixes=('', '_10'))\n",
    "merged_df = merged_df.merge(pred_20_df, how='outer', on=actual_df.columns[0], suffixes=('', '_20'))\n",
    "merged_df = merged_df.merge(pred_40_df, how='outer', on=actual_df.columns[0], suffixes=('', '_40'))\n",
    "\n",
    "columns = [col for col in merged_df.columns if col != 'actual_label'] + ['actual_label']\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Save to an Excel file\n",
    "excel_path = \"../NLP/csv_data/merged_predictions_1.xlsx\"\n",
    "merged_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved at: {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
